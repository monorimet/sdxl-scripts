#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {target_arch = "gfx942"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 1, sets = [<0, bindings = [<0, storage_buffer>, <1, storage_buffer>, <2, storage_buffer, ReadOnly>, <3, storage_buffer, ReadOnly>]>]>
module @compiled_scheduled_unet {
  util.global private @_params.unet.conv_in.weight {noinline} = #stream.parameter.named<"model"::"unet.conv_in.weight"> : tensor<320x4x3x3xf16>
  util.global private @_params.unet.conv_in.bias {noinline} = #stream.parameter.named<"model"::"unet.conv_in.bias"> : tensor<320xf16>
  util.global private @_params.unet.time_embedding.linear_1.weight {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_1.weight"> : tensor<1280x320xf16>
  util.global private @_params.unet.time_embedding.linear_1.bias {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.time_embedding.linear_2.weight {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_2.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.time_embedding.linear_2.bias {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.add_embedding.linear_1.weight {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_1.weight"> : tensor<1280x2816xf16>
  util.global private @_params.unet.add_embedding.linear_1.bias {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.add_embedding.linear_2.weight {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_2.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.add_embedding.linear_2.bias {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm1.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv1.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm1.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv1.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.downsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.downsamplers.0.conv.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.downsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.downsamplers.0.conv.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm1.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv1.weight"> : tensor<640x320x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv_shortcut.weight"> : tensor<640x320x1x1xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv1.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.downsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.downsamplers.0.conv.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.downsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.downsamplers.0.conv.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv1.weight"> : tensor<1280x640x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv_shortcut.weight"> : tensor<1280x640x1x1xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm1.weight"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm1.bias"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv1.weight"> : tensor<1280x2560x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv_shortcut.weight"> : tensor<1280x2560x1x1xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm1.weight"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm1.bias"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv1.weight"> : tensor<1280x2560x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv_shortcut.weight"> : tensor<1280x2560x1x1xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm1.weight"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm1.bias"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv1.weight"> : tensor<1280x1920x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv_shortcut.weight"> : tensor<1280x1920x1x1xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.upsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.upsamplers.0.conv.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.upsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.upsamplers.0.conv.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm1.weight"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm1.bias"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv1.weight"> : tensor<640x1920x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv_shortcut.weight"> : tensor<640x1920x1x1xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv1.weight"> : tensor<640x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv_shortcut.weight"> : tensor<640x1280x1x1xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm1.weight"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm1.bias"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv1.weight"> : tensor<640x960x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv_shortcut.weight"> : tensor<640x960x1x1xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.upsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.upsamplers.0.conv.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.upsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.upsamplers.0.conv.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm1.weight"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm1.bias"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv1.weight"> : tensor<320x960x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv_shortcut.weight"> : tensor<320x960x1x1xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv_shortcut.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv1.weight"> : tensor<320x640x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv_shortcut.weight"> : tensor<320x640x1x1xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv_shortcut.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv1.weight"> : tensor<320x640x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv_shortcut.weight"> : tensor<320x640x1x1xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv_shortcut.bias"> : tensor<320xf16>
  util.global private @_params.unet.mid_block.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.conv_norm_out.weight {noinline} = #stream.parameter.named<"model"::"unet.conv_norm_out.weight"> : tensor<320xf16>
  util.global private @_params.unet.conv_norm_out.bias {noinline} = #stream.parameter.named<"model"::"unet.conv_norm_out.bias"> : tensor<320xf16>
  util.global private @_params.unet.conv_out.weight {noinline} = #stream.parameter.named<"model"::"unet.conv_out.weight"> : tensor<4x320x3x3xf16>
  util.global private @_params.unet.conv_out.bias {noinline} = #stream.parameter.named<"model"::"unet.conv_out.bias"> : tensor<4xf16>
  func.func @run_initialize(%arg0: tensor<1x4x128x128xf16>) -> (tensor<1x4x128x128xf16>, tensor<2x6xf16>, tensor<i64>) attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.return_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]}]"} {
    %0 = torch.vtensor.literal(dense_resource<torch_tensor_1_6_torch.int64> : tensor<1x6xsi64>) : !torch.vtensor<[1,6],si64>
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int5 = torch.constant.int 5
    %1 = torch.vtensor.literal(dense<30> : tensor<si64>) : !torch.vtensor<[],si64>
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %2 = torch_c.from_builtin_tensor %arg0 : tensor<1x4x128x128xf16> -> !torch.vtensor<[1,4,128,128],f16>
    %3 = torch.prim.ListConstruct %0, %0 : (!torch.vtensor<[1,6],si64>, !torch.vtensor<[1,6],si64>) -> !torch.list<vtensor>
    %4 = torch.aten.cat %3, %int0 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,6],si64>
    %5 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6 = torch.aten.repeat %4, %5 : !torch.vtensor<[2,6],si64>, !torch.list<int> -> !torch.vtensor<[2,6],si64>
    %7 = torch.prims.convert_element_type %6, %int5 : !torch.vtensor<[2,6],si64>, !torch.int -> !torch.vtensor<[2,6],f16>
    %8 = torch.aten.mul.Scalar %2, %float1.000000e00 : !torch.vtensor<[1,4,128,128],f16>, !torch.float -> !torch.vtensor<[1,4,128,128],f16>
    %9 = torch_c.to_builtin_tensor %8 : !torch.vtensor<[1,4,128,128],f16> -> tensor<1x4x128x128xf16>
    %10 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[2,6],f16> -> tensor<2x6xf16>
    %11 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[],si64> -> tensor<i64>
    return %9, %10, %11 : tensor<1x4x128x128xf16>, tensor<2x6xf16>, tensor<i64>
  }
  func.func @run_forward(%arg0: tensor<1x4x128x128xf16>, %arg1: tensor<2x64x2048xf16>, %arg2: tensor<2x1280xf16>, %arg3: tensor<2x6xf16>, %arg4: tensor<1xf16>, %arg5: tensor<1xi64>) -> tensor<1x4x128x128xf16> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = torch.vtensor.literal(dense_resource<torch_tensor_30_torch.int64> : tensor<30xsi64>) : !torch.vtensor<[30],si64>
    %int0 = torch.constant.int 0
    %int2 = torch.constant.int 2
    %false = torch.constant.bool false
    %int160 = torch.constant.int 160
    %int6 = torch.constant.int 6
    %none = torch.constant.none
    %float-9.210340e00 = torch.constant.float -9.2103403719761836
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1 = torch.constant.int 1
    %int-1 = torch.constant.int -1
    %int5 = torch.constant.int 5
    %int12 = torch.constant.int 12
    %int128 = torch.constant.int 128
    %int32 = torch.constant.int 32
    %int10 = torch.constant.int 10
    %int16384 = torch.constant.int 16384
    %int3 = torch.constant.int 3
    %true = torch.constant.bool true
    %float1.000000e-05 = torch.constant.float 1.000000e-05
    %int320 = torch.constant.int 320
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %int4096 = torch.constant.int 4096
    %int64 = torch.constant.int 64
    %int20 = torch.constant.int 20
    %int640 = torch.constant.int 640
    %float9.999990e-07 = torch.constant.float 9.9999999999999995E-7
    %int8192 = torch.constant.int 8192
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %int2048 = torch.constant.int 2048
    %int5120 = torch.constant.int 5120
    %int2560 = torch.constant.int 2560
    %str = torch.constant.str "none"
    %int1024 = torch.constant.int 1024
    %int40 = torch.constant.int 40
    %int1280 = torch.constant.int 1280
    %int10240 = torch.constant.int 10240
    %int80 = torch.constant.int 80
    %int60 = torch.constant.int 60
    %int1920 = torch.constant.int 1920
    %float5.000000e-01 = torch.constant.float 5.000000e-01
    %int4 = torch.constant.int 4
    %int30 = torch.constant.int 30
    %int960 = torch.constant.int 960
    %int34 = torch.constant.int 34
    %1 = torch.vtensor.literal(dense_resource<torch_tensor_1000_torch.float32> : tensor<1000xf32>) : !torch.vtensor<[1000],f32>
    %int1000 = torch.constant.int 1000
    %2 = torch.vtensor.literal(dense<0.999149978> : tensor<f32>) : !torch.vtensor<[],f32>
    %c1 = arith.constant 1 : index
    %c16 = arith.constant 16 : index
    %c4096 = arith.constant 4096 : index
    %c0_i32 = arith.constant 0 : i32
    %c1065353216_i32 = arith.constant 1065353216 : i32
    %c1638400_i32 = arith.constant 1638400 : i32
    %c2621440_i32 = arith.constant 2621440 : i32
    %c1280_i32 = arith.constant 1280 : i32
    %c2048_i32 = arith.constant 2048 : i32
    %c17301761_i32 = arith.constant 17301761 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %3 = torch_c.from_builtin_tensor %arg0 : tensor<1x4x128x128xf16> -> !torch.vtensor<[1,4,128,128],f16>
    %4 = torch_c.from_builtin_tensor %arg1 : tensor<2x64x2048xf16> -> !torch.vtensor<[2,64,2048],f16>
    %5 = torch_c.from_builtin_tensor %arg2 : tensor<2x1280xf16> -> !torch.vtensor<[2,1280],f16>
    %6 = torch_c.from_builtin_tensor %arg3 : tensor<2x6xf16> -> !torch.vtensor<[2,6],f16>
    %7 = torch_c.from_builtin_tensor %arg4 : tensor<1xf16> -> !torch.vtensor<[1],f16>
    %8 = torch_c.from_builtin_tensor %arg5 : tensor<1xi64> -> !torch.vtensor<[1],si64>
    %9 = torch.prim.ListConstruct %8 : (!torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
    %10 = torch.aten.index.Tensor %0, %9 : !torch.vtensor<[30],si64>, !torch.list<optional<vtensor>> -> !torch.vtensor<[1],si64>
    %11 = torch.prim.ListConstruct %3, %3 : (!torch.vtensor<[1,4,128,128],f16>, !torch.vtensor<[1,4,128,128],f16>) -> !torch.list<vtensor>
    %12 = torch.aten.cat %11, %int0 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,4,128,128],f16>
    %13 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14 = torch.aten.expand %10, %13, %false : !torch.vtensor<[1],si64>, !torch.list<int>, !torch.bool -> !torch.vtensor<[2],si64>
    %cpu = torch.constant.device "cpu"
    %15 = torch.aten.arange.start %int0, %int160, %int6, %none, %cpu, %false : !torch.int, !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[160],f32>
    %16 = torch.aten.mul.Scalar %15, %float-9.210340e00 : !torch.vtensor<[160],f32>, !torch.float -> !torch.vtensor<[160],f32>
    %17 = torch.aten.div.Scalar %16, %int160 : !torch.vtensor<[160],f32>, !torch.int -> !torch.vtensor<[160],f32>
    %18 = torch.aten.exp %17 : !torch.vtensor<[160],f32> -> !torch.vtensor<[160],f32>
    %19 = torch.aten.unsqueeze %14, %int1 : !torch.vtensor<[2],si64>, !torch.int -> !torch.vtensor<[2,1],si64>
    %20 = torch.prims.convert_element_type %19, %int6 : !torch.vtensor<[2,1],si64>, !torch.int -> !torch.vtensor<[2,1],f32>
    %21 = torch.aten.unsqueeze %18, %int0 : !torch.vtensor<[160],f32>, !torch.int -> !torch.vtensor<[1,160],f32>
    %22 = torch.aten.mul.Tensor %20, %21 : !torch.vtensor<[2,1],f32>, !torch.vtensor<[1,160],f32> -> !torch.vtensor<[2,160],f32>
    %23 = torch.aten.mul.Scalar %22, %int1 : !torch.vtensor<[2,160],f32>, !torch.int -> !torch.vtensor<[2,160],f32>
    %24 = torch.aten.sin %23 : !torch.vtensor<[2,160],f32> -> !torch.vtensor<[2,160],f32>
    %25 = torch.aten.cos %23 : !torch.vtensor<[2,160],f32> -> !torch.vtensor<[2,160],f32>
    %26 = torch.prim.ListConstruct %24, %25 : (!torch.vtensor<[2,160],f32>, !torch.vtensor<[2,160],f32>) -> !torch.list<vtensor>
    %27 = torch.aten.cat %26, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,320],f32>
    %28 = torch.aten.slice.Tensor %27, %int1, %int160, %int9223372036854775807, %int1 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,160],f32>
    %29 = torch.aten.slice.Tensor %27, %int1, %int0, %int160, %int1 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,160],f32>
    %30 = torch.prim.ListConstruct %28, %29 : (!torch.vtensor<[2,160],f32>, !torch.vtensor<[2,160],f32>) -> !torch.list<vtensor>
    %31 = torch.aten.cat %30, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,320],f32>
    %32 = torch.prims.convert_element_type %31, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %_params.unet.time_embedding.linear_1.weight = util.global.load @_params.unet.time_embedding.linear_1.weight : tensor<1280x320xf16>
    %33 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_1.weight : tensor<1280x320xf16> -> !torch.vtensor<[1280,320],f16>
    %34 = torch.aten.transpose.int %33, %int0, %int1 : !torch.vtensor<[1280,320],f16>, !torch.int, !torch.int -> !torch.vtensor<[320,1280],f16>
    %_params.unet.time_embedding.linear_1.bias = util.global.load @_params.unet.time_embedding.linear_1.bias : tensor<1280xf16>
    %35 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %36 = torch.prims.convert_element_type %35, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %37 = torch.prims.convert_element_type %32, %int6 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320],f32>
    %38 = torch.prims.convert_element_type %34, %int6 : !torch.vtensor<[320,1280],f16>, !torch.int -> !torch.vtensor<[320,1280],f32>
    %39 = torch.aten.mm %37, %38 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %40 = torch.aten.mul.Scalar %39, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %41 = torch.aten.mul.Scalar %36, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %42 = torch.aten.add.Tensor %40, %41, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %43 = torch.prims.convert_element_type %42, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %44 = torch.aten.silu %43 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.time_embedding.linear_2.weight = util.global.load @_params.unet.time_embedding.linear_2.weight : tensor<1280x1280xf16>
    %45 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_2.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %46 = torch.aten.transpose.int %45, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.time_embedding.linear_2.bias = util.global.load @_params.unet.time_embedding.linear_2.bias : tensor<1280xf16>
    %47 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %48 = torch.prims.convert_element_type %47, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %49 = torch.prims.convert_element_type %44, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %50 = torch.prims.convert_element_type %46, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %51 = torch.aten.mm %49, %50 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %52 = torch.aten.mul.Scalar %51, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %53 = torch.aten.mul.Scalar %48, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %54 = torch.aten.add.Tensor %52, %53, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %55 = torch.prims.convert_element_type %54, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %56 = torch.prim.ListConstruct %int12 : (!torch.int) -> !torch.list<int>
    %57 = torch.aten.view %6, %56 : !torch.vtensor<[2,6],f16>, !torch.list<int> -> !torch.vtensor<[12],f16>
    %cpu_0 = torch.constant.device "cpu"
    %58 = torch.aten.arange.start %int0, %int128, %int6, %none, %cpu_0, %false : !torch.int, !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],f32>
    %59 = torch.aten.mul.Scalar %58, %float-9.210340e00 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %60 = torch.aten.div.Scalar %59, %int128 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %61 = torch.aten.exp %60 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %62 = torch.aten.unsqueeze %57, %int1 : !torch.vtensor<[12],f16>, !torch.int -> !torch.vtensor<[12,1],f16>
    %63 = torch.prims.convert_element_type %62, %int6 : !torch.vtensor<[12,1],f16>, !torch.int -> !torch.vtensor<[12,1],f32>
    %64 = torch.aten.unsqueeze %61, %int0 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %65 = torch.aten.mul.Tensor %63, %64 : !torch.vtensor<[12,1],f32>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[12,128],f32>
    %66 = torch.aten.mul.Scalar %65, %int1 : !torch.vtensor<[12,128],f32>, !torch.int -> !torch.vtensor<[12,128],f32>
    %67 = torch.aten.sin %66 : !torch.vtensor<[12,128],f32> -> !torch.vtensor<[12,128],f32>
    %68 = torch.aten.cos %66 : !torch.vtensor<[12,128],f32> -> !torch.vtensor<[12,128],f32>
    %69 = torch.prim.ListConstruct %67, %68 : (!torch.vtensor<[12,128],f32>, !torch.vtensor<[12,128],f32>) -> !torch.list<vtensor>
    %70 = torch.aten.cat %69, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[12,256],f32>
    %71 = torch.aten.slice.Tensor %70, %int1, %int128, %int9223372036854775807, %int1 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,128],f32>
    %72 = torch.aten.slice.Tensor %70, %int1, %int0, %int128, %int1 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,128],f32>
    %73 = torch.prim.ListConstruct %71, %72 : (!torch.vtensor<[12,128],f32>, !torch.vtensor<[12,128],f32>) -> !torch.list<vtensor>
    %74 = torch.aten.cat %73, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[12,256],f32>
    %75 = torch.prim.ListConstruct %int2, %int-1 : (!torch.int, !torch.int) -> !torch.list<int>
    %76 = torch.aten.view %74, %75 : !torch.vtensor<[12,256],f32>, !torch.list<int> -> !torch.vtensor<[2,1536],f32>
    %77 = torch.prim.ListConstruct %5, %76 : (!torch.vtensor<[2,1280],f16>, !torch.vtensor<[2,1536],f32>) -> !torch.list<vtensor>
    %78 = torch.aten.cat %77, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,2816],f32>
    %79 = torch.prims.convert_element_type %78, %int5 : !torch.vtensor<[2,2816],f32>, !torch.int -> !torch.vtensor<[2,2816],f16>
    %_params.unet.add_embedding.linear_1.weight = util.global.load @_params.unet.add_embedding.linear_1.weight : tensor<1280x2816xf16>
    %80 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_1.weight : tensor<1280x2816xf16> -> !torch.vtensor<[1280,2816],f16>
    %81 = torch.aten.transpose.int %80, %int0, %int1 : !torch.vtensor<[1280,2816],f16>, !torch.int, !torch.int -> !torch.vtensor<[2816,1280],f16>
    %_params.unet.add_embedding.linear_1.bias = util.global.load @_params.unet.add_embedding.linear_1.bias : tensor<1280xf16>
    %82 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %83 = torch.prims.convert_element_type %82, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %84 = torch.prims.convert_element_type %79, %int6 : !torch.vtensor<[2,2816],f16>, !torch.int -> !torch.vtensor<[2,2816],f32>
    %85 = torch.prims.convert_element_type %81, %int6 : !torch.vtensor<[2816,1280],f16>, !torch.int -> !torch.vtensor<[2816,1280],f32>
    %86 = torch.aten.mm %84, %85 : !torch.vtensor<[2,2816],f32>, !torch.vtensor<[2816,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %87 = torch.aten.mul.Scalar %86, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %88 = torch.aten.mul.Scalar %83, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %89 = torch.aten.add.Tensor %87, %88, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %90 = torch.prims.convert_element_type %89, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %91 = torch.aten.silu %90 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.add_embedding.linear_2.weight = util.global.load @_params.unet.add_embedding.linear_2.weight : tensor<1280x1280xf16>
    %92 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_2.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %93 = torch.aten.transpose.int %92, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.add_embedding.linear_2.bias = util.global.load @_params.unet.add_embedding.linear_2.bias : tensor<1280xf16>
    %94 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %95 = torch.prims.convert_element_type %94, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %96 = torch.prims.convert_element_type %91, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %97 = torch.prims.convert_element_type %93, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %98 = torch.aten.mm %96, %97 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %99 = torch.aten.mul.Scalar %98, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %100 = torch.aten.mul.Scalar %95, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %101 = torch.aten.add.Tensor %99, %100, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %102 = torch.prims.convert_element_type %101, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %103 = torch.aten.add.Tensor %55, %102, %int1 : !torch.vtensor<[2,1280],f16>, !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %_params.unet.conv_in.weight = util.global.load @_params.unet.conv_in.weight : tensor<320x4x3x3xf16>
    %104 = torch_c.from_builtin_tensor %_params.unet.conv_in.weight : tensor<320x4x3x3xf16> -> !torch.vtensor<[320,4,3,3],f16>
    %_params.unet.conv_in.bias = util.global.load @_params.unet.conv_in.bias : tensor<320xf16>
    %105 = torch_c.from_builtin_tensor %_params.unet.conv_in.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %106 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %107 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %108 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %109 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %110 = torch.aten.convolution %12, %104, %105, %106, %107, %108, %false, %109, %int1 : !torch.vtensor<[2,4,128,128],f16>, !torch.vtensor<[320,4,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %111 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %112 = torch.aten.view %110, %111 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %113 = torch.prims.convert_element_type %112, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %114 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0, %result1 = torch.aten.var_mean.correction %113, %114, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %115 = torch.aten.add.Scalar %result0, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %116 = torch.aten.rsqrt %115 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %117 = torch.aten.sub.Tensor %112, %result1, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %118 = torch.aten.mul.Tensor %117, %116 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %119 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %120 = torch.aten.view %118, %119 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.0.norm1.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.norm1.bias : tensor<320xf16>
    %121 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %122 = torch.aten.unsqueeze %121, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %123 = torch.aten.unsqueeze %122, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %124 = torch.aten.unsqueeze %123, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.0.norm1.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.norm1.weight : tensor<320xf16>
    %125 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %126 = torch.aten.unsqueeze %125, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %127 = torch.aten.unsqueeze %126, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %128 = torch.aten.unsqueeze %127, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %129 = torch.aten.mul.Tensor %120, %128 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %130 = torch.aten.add.Tensor %129, %124, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %131 = torch.prims.convert_element_type %130, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %132 = torch.prims.convert_element_type %result1, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %133 = torch.prims.convert_element_type %116, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %134 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %135 = torch.prims.squeeze %132, %134 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %136 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %137 = torch.prims.squeeze %135, %136 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %138 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %139 = torch.prims.squeeze %133, %138 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %140 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %141 = torch.prims.squeeze %139, %140 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %142 = torch.aten.silu %131 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.0.conv1.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.conv1.weight : tensor<320x320x3x3xf16>
    %143 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv1.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.0.conv1.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.conv1.bias : tensor<320xf16>
    %144 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %145 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %146 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %147 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %148 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %149 = torch.aten.convolution %142, %143, %144, %145, %146, %147, %false, %148, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %150 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight : tensor<320x1280xf16>
    %151 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %152 = torch.aten.transpose.int %151, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias : tensor<320xf16>
    %153 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %154 = torch.prims.convert_element_type %153, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %155 = torch.prims.convert_element_type %150, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %156 = torch.prims.convert_element_type %152, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %157 = torch.aten.mm %155, %156 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %158 = torch.aten.mul.Scalar %157, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %159 = torch.aten.mul.Scalar %154, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %160 = torch.aten.add.Tensor %158, %159, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %161 = torch.prims.convert_element_type %160, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %162 = torch.aten.unsqueeze %161, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %163 = torch.aten.unsqueeze %162, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %164 = torch.aten.add.Tensor %149, %163, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %165 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %166 = torch.aten.view %164, %165 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %167 = torch.prims.convert_element_type %166, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %168 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_1, %result1_2 = torch.aten.var_mean.correction %167, %168, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %169 = torch.aten.add.Scalar %result0_1, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %170 = torch.aten.rsqrt %169 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %171 = torch.aten.sub.Tensor %166, %result1_2, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %172 = torch.aten.mul.Tensor %171, %170 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %173 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %174 = torch.aten.view %172, %173 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.0.norm2.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.norm2.bias : tensor<320xf16>
    %175 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %176 = torch.aten.unsqueeze %175, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %177 = torch.aten.unsqueeze %176, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %178 = torch.aten.unsqueeze %177, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.0.norm2.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.norm2.weight : tensor<320xf16>
    %179 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %180 = torch.aten.unsqueeze %179, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %181 = torch.aten.unsqueeze %180, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %182 = torch.aten.unsqueeze %181, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %183 = torch.aten.mul.Tensor %174, %182 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %184 = torch.aten.add.Tensor %183, %178, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %185 = torch.prims.convert_element_type %184, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %186 = torch.prims.convert_element_type %result1_2, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %187 = torch.prims.convert_element_type %170, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %188 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %189 = torch.prims.squeeze %186, %188 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %190 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %191 = torch.prims.squeeze %189, %190 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %192 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %193 = torch.prims.squeeze %187, %192 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %194 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %195 = torch.prims.squeeze %193, %194 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %196 = torch.aten.silu %185 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.0.conv2.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.conv2.weight : tensor<320x320x3x3xf16>
    %197 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.0.conv2.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.conv2.bias : tensor<320xf16>
    %198 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %199 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %200 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %201 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %202 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %203 = torch.aten.convolution %196, %197, %198, %199, %200, %201, %false, %202, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %204 = torch.aten.add.Tensor %110, %203, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %205 = torch.aten.div.Scalar %204, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %206 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %207 = torch.aten.view %205, %206 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %208 = torch.prims.convert_element_type %207, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %209 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_3, %result1_4 = torch.aten.var_mean.correction %208, %209, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %210 = torch.aten.add.Scalar %result0_3, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %211 = torch.aten.rsqrt %210 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %212 = torch.aten.sub.Tensor %207, %result1_4, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %213 = torch.aten.mul.Tensor %212, %211 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %214 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %215 = torch.aten.view %213, %214 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.1.norm1.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.norm1.bias : tensor<320xf16>
    %216 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %217 = torch.aten.unsqueeze %216, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %218 = torch.aten.unsqueeze %217, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %219 = torch.aten.unsqueeze %218, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.1.norm1.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.norm1.weight : tensor<320xf16>
    %220 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %221 = torch.aten.unsqueeze %220, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %222 = torch.aten.unsqueeze %221, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %223 = torch.aten.unsqueeze %222, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %224 = torch.aten.mul.Tensor %215, %223 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %225 = torch.aten.add.Tensor %224, %219, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %226 = torch.prims.convert_element_type %225, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %227 = torch.prims.convert_element_type %result1_4, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %228 = torch.prims.convert_element_type %211, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %229 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %230 = torch.prims.squeeze %227, %229 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %231 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %232 = torch.prims.squeeze %230, %231 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %233 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %234 = torch.prims.squeeze %228, %233 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %235 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %236 = torch.prims.squeeze %234, %235 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %237 = torch.aten.silu %226 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.1.conv1.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.conv1.weight : tensor<320x320x3x3xf16>
    %238 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv1.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.1.conv1.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.conv1.bias : tensor<320xf16>
    %239 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %240 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %241 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %242 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %243 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %244 = torch.aten.convolution %237, %238, %239, %240, %241, %242, %false, %243, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %245 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight : tensor<320x1280xf16>
    %246 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %247 = torch.aten.transpose.int %246, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias : tensor<320xf16>
    %248 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %249 = torch.prims.convert_element_type %248, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %250 = torch.prims.convert_element_type %245, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %251 = torch.prims.convert_element_type %247, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %252 = torch.aten.mm %250, %251 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %253 = torch.aten.mul.Scalar %252, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %254 = torch.aten.mul.Scalar %249, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %255 = torch.aten.add.Tensor %253, %254, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %256 = torch.prims.convert_element_type %255, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %257 = torch.aten.unsqueeze %256, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %258 = torch.aten.unsqueeze %257, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %259 = torch.aten.add.Tensor %244, %258, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %260 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %261 = torch.aten.view %259, %260 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %262 = torch.prims.convert_element_type %261, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %263 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_5, %result1_6 = torch.aten.var_mean.correction %262, %263, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %264 = torch.aten.add.Scalar %result0_5, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %265 = torch.aten.rsqrt %264 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %266 = torch.aten.sub.Tensor %261, %result1_6, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %267 = torch.aten.mul.Tensor %266, %265 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %268 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %269 = torch.aten.view %267, %268 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.1.norm2.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.norm2.bias : tensor<320xf16>
    %270 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %271 = torch.aten.unsqueeze %270, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %272 = torch.aten.unsqueeze %271, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %273 = torch.aten.unsqueeze %272, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.1.norm2.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.norm2.weight : tensor<320xf16>
    %274 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %275 = torch.aten.unsqueeze %274, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %276 = torch.aten.unsqueeze %275, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %277 = torch.aten.unsqueeze %276, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %278 = torch.aten.mul.Tensor %269, %277 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %279 = torch.aten.add.Tensor %278, %273, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %280 = torch.prims.convert_element_type %279, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %281 = torch.prims.convert_element_type %result1_6, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %282 = torch.prims.convert_element_type %265, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %283 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %284 = torch.prims.squeeze %281, %283 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %285 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %286 = torch.prims.squeeze %284, %285 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %287 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %288 = torch.prims.squeeze %282, %287 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %289 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %290 = torch.prims.squeeze %288, %289 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %291 = torch.aten.silu %280 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.1.conv2.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.conv2.weight : tensor<320x320x3x3xf16>
    %292 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.1.conv2.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.conv2.bias : tensor<320xf16>
    %293 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %294 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %295 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %296 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %297 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %298 = torch.aten.convolution %291, %292, %293, %294, %295, %296, %false, %297, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %299 = torch.aten.add.Tensor %205, %298, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %300 = torch.aten.div.Scalar %299, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.downsamplers.0.conv.weight = util.global.load @_params.unet.down_blocks.0.downsamplers.0.conv.weight : tensor<320x320x3x3xf16>
    %301 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.downsamplers.0.conv.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.downsamplers.0.conv.bias = util.global.load @_params.unet.down_blocks.0.downsamplers.0.conv.bias : tensor<320xf16>
    %302 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.downsamplers.0.conv.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %303 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %304 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %305 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %306 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %307 = torch.aten.convolution %300, %301, %302, %303, %304, %305, %false, %306, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,64,64],f16>
    %308 = torch.prim.ListConstruct %int2, %int32, %int10, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %309 = torch.aten.view %307, %308 : !torch.vtensor<[2,320,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,4096],f16>
    %310 = torch.prims.convert_element_type %309, %int6 : !torch.vtensor<[2,32,10,4096],f16>, !torch.int -> !torch.vtensor<[2,32,10,4096],f32>
    %311 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_7, %result1_8 = torch.aten.var_mean.correction %310, %311, %int0, %true : !torch.vtensor<[2,32,10,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %312 = torch.aten.add.Scalar %result0_7, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %313 = torch.aten.rsqrt %312 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %314 = torch.aten.sub.Tensor %309, %result1_8, %int1 : !torch.vtensor<[2,32,10,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,4096],f32>
    %315 = torch.aten.mul.Tensor %314, %313 : !torch.vtensor<[2,32,10,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,4096],f32>
    %316 = torch.prim.ListConstruct %int2, %int320, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %317 = torch.aten.view %315, %316 : !torch.vtensor<[2,32,10,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,320,64,64],f32>
    %_params.unet.down_blocks.1.resnets.0.norm1.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.norm1.bias : tensor<320xf16>
    %318 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %319 = torch.aten.unsqueeze %318, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %320 = torch.aten.unsqueeze %319, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %321 = torch.aten.unsqueeze %320, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.1.resnets.0.norm1.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.norm1.weight : tensor<320xf16>
    %322 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %323 = torch.aten.unsqueeze %322, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %324 = torch.aten.unsqueeze %323, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %325 = torch.aten.unsqueeze %324, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %326 = torch.aten.mul.Tensor %317, %325 : !torch.vtensor<[2,320,64,64],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,64,64],f32>
    %327 = torch.aten.add.Tensor %326, %321, %int1 : !torch.vtensor<[2,320,64,64],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,64,64],f32>
    %328 = torch.prims.convert_element_type %327, %int5 : !torch.vtensor<[2,320,64,64],f32>, !torch.int -> !torch.vtensor<[2,320,64,64],f16>
    %329 = torch.prims.convert_element_type %result1_8, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %330 = torch.prims.convert_element_type %313, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %331 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %332 = torch.prims.squeeze %329, %331 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %333 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %334 = torch.prims.squeeze %332, %333 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %335 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %336 = torch.prims.squeeze %330, %335 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %337 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %338 = torch.prims.squeeze %336, %337 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %339 = torch.aten.silu %328 : !torch.vtensor<[2,320,64,64],f16> -> !torch.vtensor<[2,320,64,64],f16>
    %_params.unet.down_blocks.1.resnets.0.conv1.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.conv1.weight : tensor<640x320x3x3xf16>
    %340 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv1.weight : tensor<640x320x3x3xf16> -> !torch.vtensor<[640,320,3,3],f16>
    %_params.unet.down_blocks.1.resnets.0.conv1.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.conv1.bias : tensor<640xf16>
    %341 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %342 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %343 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %344 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %345 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %346 = torch.aten.convolution %339, %340, %341, %342, %343, %344, %false, %345, %int1 : !torch.vtensor<[2,320,64,64],f16>, !torch.vtensor<[640,320,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %347 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16>
    %348 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %349 = torch.aten.transpose.int %348, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16>
    %350 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %351 = torch.prims.convert_element_type %350, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %352 = torch.prims.convert_element_type %347, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %353 = torch.prims.convert_element_type %349, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %354 = torch.aten.mm %352, %353 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %355 = torch.aten.mul.Scalar %354, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %356 = torch.aten.mul.Scalar %351, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %357 = torch.aten.add.Tensor %355, %356, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %358 = torch.prims.convert_element_type %357, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %359 = torch.aten.unsqueeze %358, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %360 = torch.aten.unsqueeze %359, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %361 = torch.aten.add.Tensor %346, %360, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %362 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %363 = torch.aten.view %361, %362 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %364 = torch.prims.convert_element_type %363, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %365 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_9, %result1_10 = torch.aten.var_mean.correction %364, %365, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %366 = torch.aten.add.Scalar %result0_9, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %367 = torch.aten.rsqrt %366 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %368 = torch.aten.sub.Tensor %363, %result1_10, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %369 = torch.aten.mul.Tensor %368, %367 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %370 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %371 = torch.aten.view %369, %370 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.resnets.0.norm2.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.norm2.bias : tensor<640xf16>
    %372 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %373 = torch.aten.unsqueeze %372, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %374 = torch.aten.unsqueeze %373, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %375 = torch.aten.unsqueeze %374, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.resnets.0.norm2.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.norm2.weight : tensor<640xf16>
    %376 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %377 = torch.aten.unsqueeze %376, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %378 = torch.aten.unsqueeze %377, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %379 = torch.aten.unsqueeze %378, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %380 = torch.aten.mul.Tensor %371, %379 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %381 = torch.aten.add.Tensor %380, %375, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %382 = torch.prims.convert_element_type %381, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %383 = torch.prims.convert_element_type %result1_10, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %384 = torch.prims.convert_element_type %367, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %385 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %386 = torch.prims.squeeze %383, %385 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %387 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %388 = torch.prims.squeeze %386, %387 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %389 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %390 = torch.prims.squeeze %384, %389 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %391 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %392 = torch.prims.squeeze %390, %391 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %393 = torch.aten.silu %382 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.0.conv2.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16>
    %394 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.resnets.0.conv2.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.conv2.bias : tensor<640xf16>
    %395 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %396 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %397 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %398 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %399 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %400 = torch.aten.convolution %393, %394, %395, %396, %397, %398, %false, %399, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x320x1x1xf16>
    %401 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x320x1x1xf16> -> !torch.vtensor<[640,320,1,1],f16>
    %_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16>
    %402 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %403 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %404 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %405 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %406 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %407 = torch.aten.convolution %307, %401, %402, %403, %404, %405, %false, %406, %int1 : !torch.vtensor<[2,320,64,64],f16>, !torch.vtensor<[640,320,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %408 = torch.aten.add.Tensor %407, %400, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %409 = torch.aten.div.Scalar %408, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %410 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %411 = torch.aten.view %409, %410 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %412 = torch.prims.convert_element_type %411, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %413 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_11, %result1_12 = torch.aten.var_mean.correction %412, %413, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %414 = torch.aten.add.Scalar %result0_11, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %415 = torch.aten.rsqrt %414 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %416 = torch.aten.sub.Tensor %411, %result1_12, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %417 = torch.aten.mul.Tensor %416, %415 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %418 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %419 = torch.aten.view %417, %418 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.attentions.0.norm.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.norm.bias : tensor<640xf16>
    %420 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %421 = torch.aten.unsqueeze %420, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %422 = torch.aten.unsqueeze %421, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %423 = torch.aten.unsqueeze %422, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.attentions.0.norm.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.norm.weight : tensor<640xf16>
    %424 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %425 = torch.aten.unsqueeze %424, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %426 = torch.aten.unsqueeze %425, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %427 = torch.aten.unsqueeze %426, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %428 = torch.aten.mul.Tensor %419, %427 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %429 = torch.aten.add.Tensor %428, %423, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %430 = torch.prims.convert_element_type %429, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %431 = torch.prims.convert_element_type %result1_12, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %432 = torch.prims.convert_element_type %415, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %433 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %434 = torch.prims.squeeze %431, %433 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %435 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %436 = torch.prims.squeeze %434, %435 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %437 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %438 = torch.prims.squeeze %432, %437 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %439 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %440 = torch.prims.squeeze %438, %439 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %441 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %442 = torch.aten.permute %430, %441 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %443 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %444 = torch.aten.view %442, %443 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_in.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16>
    %445 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %446 = torch.aten.transpose.int %445, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %447 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %448 = torch.aten._unsafe_view %444, %447 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %449 = torch.aten.mm %448, %446 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %450 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %451 = torch.aten.view %449, %450 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_in.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_in.bias : tensor<640xf16>
    %452 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %453 = torch.aten.add.Tensor %451, %452, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %454 = torch.prims.convert_element_type %453, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %455 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_13, %result1_14 = torch.aten.var_mean.correction %454, %455, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %456 = torch.aten.add.Scalar %result0_13, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %457 = torch.aten.rsqrt %456 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %458 = torch.aten.sub.Tensor %453, %result1_14, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %459 = torch.aten.mul.Tensor %458, %457 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %460 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %461 = torch.aten.mul.Tensor %459, %460 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %462 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %463 = torch.aten.add.Tensor %461, %462, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %464 = torch.prims.convert_element_type %463, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %465 = torch.prims.convert_element_type %result1_14, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %466 = torch.prims.convert_element_type %457, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %467 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %468 = torch.aten.transpose.int %467, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %469 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %470 = torch.aten.view %464, %469 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %471 = torch.aten.mm %470, %468 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %472 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %473 = torch.aten.view %471, %472 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %474 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %475 = torch.aten.transpose.int %474, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %476 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %477 = torch.aten.view %464, %476 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %478 = torch.aten.mm %477, %475 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %479 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %480 = torch.aten.view %478, %479 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %481 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %482 = torch.aten.transpose.int %481, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %483 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %484 = torch.aten.view %464, %483 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %485 = torch.aten.mm %484, %482 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %486 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %487 = torch.aten.view %485, %486 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %488 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %489 = torch.aten.view %473, %488 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %490 = torch.aten.transpose.int %489, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %491 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %492 = torch.aten.view %480, %491 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %493 = torch.aten.transpose.int %492, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %494 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %495 = torch.aten.view %487, %494 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %496 = torch.aten.transpose.int %495, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %497:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%490, %493, %496, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %498 = torch.aten.transpose.int %497#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %499 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %500 = torch.aten.view %498, %499 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %501 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %502 = torch.aten.view %500, %501 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %503 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %504 = torch.aten.transpose.int %503, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %505 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %506 = torch.prims.convert_element_type %505, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %507 = torch.prims.convert_element_type %502, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %508 = torch.prims.convert_element_type %504, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %509 = torch.aten.mm %507, %508 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %510 = torch.aten.mul.Scalar %509, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %511 = torch.aten.mul.Scalar %506, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %512 = torch.aten.add.Tensor %510, %511, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %513 = torch.prims.convert_element_type %512, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %514 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %515 = torch.aten.view %513, %514 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %516 = torch.aten.div.Scalar %515, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %517 = torch.aten.add.Tensor %516, %453, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %518 = torch.prims.convert_element_type %517, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %519 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_15, %result1_16 = torch.aten.var_mean.correction %518, %519, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %520 = torch.aten.add.Scalar %result0_15, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %521 = torch.aten.rsqrt %520 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %522 = torch.aten.sub.Tensor %517, %result1_16, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %523 = torch.aten.mul.Tensor %522, %521 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %524 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %525 = torch.aten.mul.Tensor %523, %524 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %526 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %527 = torch.aten.add.Tensor %525, %526, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %528 = torch.prims.convert_element_type %527, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %529 = torch.prims.convert_element_type %result1_16, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %530 = torch.prims.convert_element_type %521, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %531 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %532 = torch.aten.transpose.int %531, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %533 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %534 = torch.aten.view %528, %533 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %535 = torch.aten.mm %534, %532 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %536 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %537 = torch.aten.view %535, %536 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %538 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %539 = torch.aten.transpose.int %538, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %540 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %541 = torch.aten.view %4, %540 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %542 = torch.aten.mm %541, %539 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %543 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %544 = torch.aten.view %542, %543 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %545 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %546 = torch.aten.transpose.int %545, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %547 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %548 = torch.aten.view %4, %547 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %549 = torch.aten.mm %548, %546 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %550 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %551 = torch.aten.view %549, %550 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %552 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %553 = torch.aten.view %537, %552 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %554 = torch.aten.transpose.int %553, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %555 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %556 = torch.aten.view %544, %555 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %557 = torch.aten.transpose.int %556, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %558 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %559 = torch.aten.view %551, %558 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %560 = torch.aten.transpose.int %559, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %561:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%554, %557, %560, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %562 = torch.aten.transpose.int %561#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %563 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %564 = torch.aten.view %562, %563 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %565 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %566 = torch.aten.view %564, %565 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %567 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %568 = torch.aten.transpose.int %567, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %569 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %570 = torch.prims.convert_element_type %569, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %571 = torch.prims.convert_element_type %566, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %572 = torch.prims.convert_element_type %568, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %573 = torch.aten.mm %571, %572 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %574 = torch.aten.mul.Scalar %573, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %575 = torch.aten.mul.Scalar %570, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %576 = torch.aten.add.Tensor %574, %575, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %577 = torch.prims.convert_element_type %576, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %578 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %579 = torch.aten.view %577, %578 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %580 = torch.aten.div.Scalar %579, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %581 = torch.aten.add.Tensor %580, %517, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %582 = torch.prims.convert_element_type %581, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %583 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_17, %result1_18 = torch.aten.var_mean.correction %582, %583, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %584 = torch.aten.add.Scalar %result0_17, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %585 = torch.aten.rsqrt %584 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %586 = torch.aten.sub.Tensor %581, %result1_18, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %587 = torch.aten.mul.Tensor %586, %585 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %588 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %589 = torch.aten.mul.Tensor %587, %588 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %590 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %591 = torch.aten.add.Tensor %589, %590, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %592 = torch.prims.convert_element_type %591, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %593 = torch.prims.convert_element_type %result1_18, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %594 = torch.prims.convert_element_type %585, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %595 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %596 = torch.aten.view %592, %595 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %597 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %598 = torch.aten.transpose.int %597, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %599 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %600 = torch.prims.convert_element_type %599, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %601 = torch.prims.convert_element_type %596, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %602 = torch.prims.convert_element_type %598, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %603 = torch.aten.mm %601, %602 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %604 = torch.aten.mul.Scalar %603, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %605 = torch.aten.mul.Scalar %600, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %606 = torch.aten.add.Tensor %604, %605, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %607 = torch.prims.convert_element_type %606, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %608 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %609 = torch.aten.view %607, %608 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %610 = torch.aten.slice.Tensor %609, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %611 = torch.aten.slice.Tensor %609, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %612 = torch.aten.gelu %611, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %613 = torch.aten.mul.Tensor %610, %612 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %614 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %615 = torch.aten.view %613, %614 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %616 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %617 = torch.aten.transpose.int %616, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %618 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %619 = torch.prims.convert_element_type %618, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %620 = torch.prims.convert_element_type %615, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %621 = torch.prims.convert_element_type %617, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %622 = torch.aten.mm %620, %621 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %623 = torch.aten.mul.Scalar %622, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %624 = torch.aten.mul.Scalar %619, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %625 = torch.aten.add.Tensor %623, %624, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %626 = torch.prims.convert_element_type %625, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %627 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %628 = torch.aten.view %626, %627 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %629 = torch.aten.add.Tensor %628, %581, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %630 = torch.prims.convert_element_type %629, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %631 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_19, %result1_20 = torch.aten.var_mean.correction %630, %631, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %632 = torch.aten.add.Scalar %result0_19, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %633 = torch.aten.rsqrt %632 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %634 = torch.aten.sub.Tensor %629, %result1_20, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %635 = torch.aten.mul.Tensor %634, %633 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %636 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %637 = torch.aten.mul.Tensor %635, %636 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %638 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %639 = torch.aten.add.Tensor %637, %638, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %640 = torch.prims.convert_element_type %639, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %641 = torch.prims.convert_element_type %result1_20, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %642 = torch.prims.convert_element_type %633, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %643 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %644 = torch.aten.transpose.int %643, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %645 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %646 = torch.aten.view %640, %645 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %647 = torch.aten.mm %646, %644 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %648 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %649 = torch.aten.view %647, %648 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %650 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %651 = torch.aten.transpose.int %650, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %652 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %653 = torch.aten.view %640, %652 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %654 = torch.aten.mm %653, %651 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %655 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %656 = torch.aten.view %654, %655 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %657 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %658 = torch.aten.transpose.int %657, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %659 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %660 = torch.aten.view %640, %659 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %661 = torch.aten.mm %660, %658 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %662 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %663 = torch.aten.view %661, %662 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %664 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %665 = torch.aten.view %649, %664 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %666 = torch.aten.transpose.int %665, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %667 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %668 = torch.aten.view %656, %667 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %669 = torch.aten.transpose.int %668, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %670 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %671 = torch.aten.view %663, %670 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %672 = torch.aten.transpose.int %671, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %673:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%666, %669, %672, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %674 = torch.aten.transpose.int %673#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %675 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %676 = torch.aten.view %674, %675 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %677 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %678 = torch.aten.view %676, %677 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %679 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %680 = torch.aten.transpose.int %679, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %681 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %682 = torch.prims.convert_element_type %681, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %683 = torch.prims.convert_element_type %678, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %684 = torch.prims.convert_element_type %680, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %685 = torch.aten.mm %683, %684 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %686 = torch.aten.mul.Scalar %685, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %687 = torch.aten.mul.Scalar %682, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %688 = torch.aten.add.Tensor %686, %687, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %689 = torch.prims.convert_element_type %688, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %690 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %691 = torch.aten.view %689, %690 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %692 = torch.aten.div.Scalar %691, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %693 = torch.aten.add.Tensor %692, %629, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %694 = torch.prims.convert_element_type %693, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %695 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_21, %result1_22 = torch.aten.var_mean.correction %694, %695, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %696 = torch.aten.add.Scalar %result0_21, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %697 = torch.aten.rsqrt %696 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %698 = torch.aten.sub.Tensor %693, %result1_22, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %699 = torch.aten.mul.Tensor %698, %697 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %700 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %701 = torch.aten.mul.Tensor %699, %700 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %702 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %703 = torch.aten.add.Tensor %701, %702, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %704 = torch.prims.convert_element_type %703, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %705 = torch.prims.convert_element_type %result1_22, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %706 = torch.prims.convert_element_type %697, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %707 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %708 = torch.aten.transpose.int %707, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %709 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %710 = torch.aten.view %704, %709 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %711 = torch.aten.mm %710, %708 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %712 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %713 = torch.aten.view %711, %712 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %714 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %715 = torch.aten.transpose.int %714, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %716 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %717 = torch.aten.view %4, %716 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %718 = torch.aten.mm %717, %715 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %719 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %720 = torch.aten.view %718, %719 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %721 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %722 = torch.aten.transpose.int %721, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %723 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %724 = torch.aten.view %4, %723 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %725 = torch.aten.mm %724, %722 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %726 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %727 = torch.aten.view %725, %726 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %728 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %729 = torch.aten.view %713, %728 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %730 = torch.aten.transpose.int %729, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %731 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %732 = torch.aten.view %720, %731 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %733 = torch.aten.transpose.int %732, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %734 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %735 = torch.aten.view %727, %734 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %736 = torch.aten.transpose.int %735, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %737:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%730, %733, %736, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %738 = torch.aten.transpose.int %737#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %739 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %740 = torch.aten.view %738, %739 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %741 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %742 = torch.aten.view %740, %741 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %743 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %744 = torch.aten.transpose.int %743, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %745 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %746 = torch.prims.convert_element_type %745, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %747 = torch.prims.convert_element_type %742, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %748 = torch.prims.convert_element_type %744, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %749 = torch.aten.mm %747, %748 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %750 = torch.aten.mul.Scalar %749, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %751 = torch.aten.mul.Scalar %746, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %752 = torch.aten.add.Tensor %750, %751, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %753 = torch.prims.convert_element_type %752, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %754 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %755 = torch.aten.view %753, %754 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %756 = torch.aten.div.Scalar %755, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %757 = torch.aten.add.Tensor %756, %693, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %758 = torch.prims.convert_element_type %757, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %759 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_23, %result1_24 = torch.aten.var_mean.correction %758, %759, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %760 = torch.aten.add.Scalar %result0_23, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %761 = torch.aten.rsqrt %760 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %762 = torch.aten.sub.Tensor %757, %result1_24, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %763 = torch.aten.mul.Tensor %762, %761 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %764 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %765 = torch.aten.mul.Tensor %763, %764 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %766 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %767 = torch.aten.add.Tensor %765, %766, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %768 = torch.prims.convert_element_type %767, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %769 = torch.prims.convert_element_type %result1_24, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %770 = torch.prims.convert_element_type %761, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %771 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %772 = torch.aten.view %768, %771 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %773 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %774 = torch.aten.transpose.int %773, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %775 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %776 = torch.prims.convert_element_type %775, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %777 = torch.prims.convert_element_type %772, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %778 = torch.prims.convert_element_type %774, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %779 = torch.aten.mm %777, %778 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %780 = torch.aten.mul.Scalar %779, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %781 = torch.aten.mul.Scalar %776, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %782 = torch.aten.add.Tensor %780, %781, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %783 = torch.prims.convert_element_type %782, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %784 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %785 = torch.aten.view %783, %784 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %786 = torch.aten.slice.Tensor %785, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %787 = torch.aten.slice.Tensor %785, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %788 = torch.aten.gelu %787, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %789 = torch.aten.mul.Tensor %786, %788 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %790 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %791 = torch.aten.view %789, %790 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %792 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %793 = torch.aten.transpose.int %792, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %794 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %795 = torch.prims.convert_element_type %794, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %796 = torch.prims.convert_element_type %791, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %797 = torch.prims.convert_element_type %793, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %798 = torch.aten.mm %796, %797 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %799 = torch.aten.mul.Scalar %798, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %800 = torch.aten.mul.Scalar %795, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %801 = torch.aten.add.Tensor %799, %800, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %802 = torch.prims.convert_element_type %801, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %803 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %804 = torch.aten.view %802, %803 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %805 = torch.aten.add.Tensor %804, %757, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %806 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %807 = torch.aten.view %805, %806 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_out.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16>
    %808 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %809 = torch.aten.transpose.int %808, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_out.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_out.bias : tensor<640xf16>
    %810 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %811 = torch.prims.convert_element_type %810, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %812 = torch.prims.convert_element_type %807, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %813 = torch.prims.convert_element_type %809, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %814 = torch.aten.mm %812, %813 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %815 = torch.aten.mul.Scalar %814, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %816 = torch.aten.mul.Scalar %811, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %817 = torch.aten.add.Tensor %815, %816, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %818 = torch.prims.convert_element_type %817, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %819 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %820 = torch.aten.view %818, %819 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %821 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %822 = torch.aten.view %820, %821 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %823 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %824 = torch.aten.permute %822, %823 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %825 = torch.aten.add.Tensor %824, %409, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %826 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %827 = torch.aten.view %825, %826 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %828 = torch.prims.convert_element_type %827, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %829 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_25, %result1_26 = torch.aten.var_mean.correction %828, %829, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %830 = torch.aten.add.Scalar %result0_25, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %831 = torch.aten.rsqrt %830 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %832 = torch.aten.sub.Tensor %827, %result1_26, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %833 = torch.aten.mul.Tensor %832, %831 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %834 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %835 = torch.aten.view %833, %834 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.resnets.1.norm1.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.norm1.bias : tensor<640xf16>
    %836 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %837 = torch.aten.unsqueeze %836, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %838 = torch.aten.unsqueeze %837, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %839 = torch.aten.unsqueeze %838, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.resnets.1.norm1.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.norm1.weight : tensor<640xf16>
    %840 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %841 = torch.aten.unsqueeze %840, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %842 = torch.aten.unsqueeze %841, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %843 = torch.aten.unsqueeze %842, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %844 = torch.aten.mul.Tensor %835, %843 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %845 = torch.aten.add.Tensor %844, %839, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %846 = torch.prims.convert_element_type %845, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %847 = torch.prims.convert_element_type %result1_26, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %848 = torch.prims.convert_element_type %831, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %849 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %850 = torch.prims.squeeze %847, %849 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %851 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %852 = torch.prims.squeeze %850, %851 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %853 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %854 = torch.prims.squeeze %848, %853 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %855 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %856 = torch.prims.squeeze %854, %855 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %857 = torch.aten.silu %846 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.1.conv1.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.conv1.weight : tensor<640x640x3x3xf16>
    %858 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv1.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.resnets.1.conv1.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.conv1.bias : tensor<640xf16>
    %859 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %860 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %861 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %862 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %863 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %864 = torch.aten.convolution %857, %858, %859, %860, %861, %862, %false, %863, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %865 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16>
    %866 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %867 = torch.aten.transpose.int %866, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16>
    %868 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %869 = torch.prims.convert_element_type %868, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %870 = torch.prims.convert_element_type %865, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %871 = torch.prims.convert_element_type %867, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %872 = torch.aten.mm %870, %871 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %873 = torch.aten.mul.Scalar %872, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %874 = torch.aten.mul.Scalar %869, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %875 = torch.aten.add.Tensor %873, %874, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %876 = torch.prims.convert_element_type %875, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %877 = torch.aten.unsqueeze %876, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %878 = torch.aten.unsqueeze %877, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %879 = torch.aten.add.Tensor %864, %878, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %880 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %881 = torch.aten.view %879, %880 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %882 = torch.prims.convert_element_type %881, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %883 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_27, %result1_28 = torch.aten.var_mean.correction %882, %883, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %884 = torch.aten.add.Scalar %result0_27, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %885 = torch.aten.rsqrt %884 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %886 = torch.aten.sub.Tensor %881, %result1_28, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %887 = torch.aten.mul.Tensor %886, %885 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %888 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %889 = torch.aten.view %887, %888 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.resnets.1.norm2.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.norm2.bias : tensor<640xf16>
    %890 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %891 = torch.aten.unsqueeze %890, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %892 = torch.aten.unsqueeze %891, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %893 = torch.aten.unsqueeze %892, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.resnets.1.norm2.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.norm2.weight : tensor<640xf16>
    %894 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %895 = torch.aten.unsqueeze %894, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %896 = torch.aten.unsqueeze %895, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %897 = torch.aten.unsqueeze %896, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %898 = torch.aten.mul.Tensor %889, %897 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %899 = torch.aten.add.Tensor %898, %893, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %900 = torch.prims.convert_element_type %899, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %901 = torch.prims.convert_element_type %result1_28, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %902 = torch.prims.convert_element_type %885, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %903 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %904 = torch.prims.squeeze %901, %903 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %905 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %906 = torch.prims.squeeze %904, %905 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %907 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %908 = torch.prims.squeeze %902, %907 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %909 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %910 = torch.prims.squeeze %908, %909 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %911 = torch.aten.silu %900 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.1.conv2.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16>
    %912 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.resnets.1.conv2.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.conv2.bias : tensor<640xf16>
    %913 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %914 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %915 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %916 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %917 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %918 = torch.aten.convolution %911, %912, %913, %914, %915, %916, %false, %917, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %919 = torch.aten.add.Tensor %825, %918, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %920 = torch.aten.div.Scalar %919, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %921 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %922 = torch.aten.view %920, %921 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %923 = torch.prims.convert_element_type %922, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %924 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_29, %result1_30 = torch.aten.var_mean.correction %923, %924, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %925 = torch.aten.add.Scalar %result0_29, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %926 = torch.aten.rsqrt %925 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %927 = torch.aten.sub.Tensor %922, %result1_30, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %928 = torch.aten.mul.Tensor %927, %926 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %929 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %930 = torch.aten.view %928, %929 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.attentions.1.norm.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.norm.bias : tensor<640xf16>
    %931 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %932 = torch.aten.unsqueeze %931, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %933 = torch.aten.unsqueeze %932, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %934 = torch.aten.unsqueeze %933, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.attentions.1.norm.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.norm.weight : tensor<640xf16>
    %935 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %936 = torch.aten.unsqueeze %935, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %937 = torch.aten.unsqueeze %936, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %938 = torch.aten.unsqueeze %937, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %939 = torch.aten.mul.Tensor %930, %938 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %940 = torch.aten.add.Tensor %939, %934, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %941 = torch.prims.convert_element_type %940, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %942 = torch.prims.convert_element_type %result1_30, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %943 = torch.prims.convert_element_type %926, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %944 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %945 = torch.prims.squeeze %942, %944 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %946 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %947 = torch.prims.squeeze %945, %946 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %948 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %949 = torch.prims.squeeze %943, %948 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %950 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %951 = torch.prims.squeeze %949, %950 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %952 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %953 = torch.aten.permute %941, %952 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %954 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %955 = torch.aten.view %953, %954 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_in.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16>
    %956 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %957 = torch.aten.transpose.int %956, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %958 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %959 = torch.aten._unsafe_view %955, %958 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %960 = torch.aten.mm %959, %957 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %961 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %962 = torch.aten.view %960, %961 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_in.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_in.bias : tensor<640xf16>
    %963 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %964 = torch.aten.add.Tensor %962, %963, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %965 = torch.prims.convert_element_type %964, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %966 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_31, %result1_32 = torch.aten.var_mean.correction %965, %966, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %967 = torch.aten.add.Scalar %result0_31, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %968 = torch.aten.rsqrt %967 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %969 = torch.aten.sub.Tensor %964, %result1_32, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %970 = torch.aten.mul.Tensor %969, %968 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %971 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %972 = torch.aten.mul.Tensor %970, %971 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %973 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %974 = torch.aten.add.Tensor %972, %973, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %975 = torch.prims.convert_element_type %974, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %976 = torch.prims.convert_element_type %result1_32, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %977 = torch.prims.convert_element_type %968, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %978 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %979 = torch.aten.transpose.int %978, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %980 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %981 = torch.aten.view %975, %980 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %982 = torch.aten.mm %981, %979 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %983 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %984 = torch.aten.view %982, %983 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %985 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %986 = torch.aten.transpose.int %985, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %987 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %988 = torch.aten.view %975, %987 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %989 = torch.aten.mm %988, %986 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %990 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %991 = torch.aten.view %989, %990 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %992 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %993 = torch.aten.transpose.int %992, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %994 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %995 = torch.aten.view %975, %994 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %996 = torch.aten.mm %995, %993 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %997 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %998 = torch.aten.view %996, %997 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %999 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1000 = torch.aten.view %984, %999 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1001 = torch.aten.transpose.int %1000, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1002 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1003 = torch.aten.view %991, %1002 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1004 = torch.aten.transpose.int %1003, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1005 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1006 = torch.aten.view %998, %1005 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1007 = torch.aten.transpose.int %1006, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1008:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1001, %1004, %1007, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1009 = torch.aten.transpose.int %1008#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1010 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1011 = torch.aten.view %1009, %1010 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1012 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1013 = torch.aten.view %1011, %1012 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %1014 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1015 = torch.aten.transpose.int %1014, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %1016 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1017 = torch.prims.convert_element_type %1016, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1018 = torch.prims.convert_element_type %1013, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1019 = torch.prims.convert_element_type %1015, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1020 = torch.aten.mm %1018, %1019 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1021 = torch.aten.mul.Scalar %1020, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1022 = torch.aten.mul.Scalar %1017, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1023 = torch.aten.add.Tensor %1021, %1022, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1024 = torch.prims.convert_element_type %1023, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1025 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1026 = torch.aten.view %1024, %1025 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1027 = torch.aten.div.Scalar %1026, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1028 = torch.aten.add.Tensor %1027, %964, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1029 = torch.prims.convert_element_type %1028, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1030 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_33, %result1_34 = torch.aten.var_mean.correction %1029, %1030, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1031 = torch.aten.add.Scalar %result0_33, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1032 = torch.aten.rsqrt %1031 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1033 = torch.aten.sub.Tensor %1028, %result1_34, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1034 = torch.aten.mul.Tensor %1033, %1032 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %1035 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1036 = torch.aten.mul.Tensor %1034, %1035 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %1037 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1038 = torch.aten.add.Tensor %1036, %1037, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1039 = torch.prims.convert_element_type %1038, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1040 = torch.prims.convert_element_type %result1_34, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1041 = torch.prims.convert_element_type %1032, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %1042 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1043 = torch.aten.transpose.int %1042, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1044 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1045 = torch.aten.view %1039, %1044 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1046 = torch.aten.mm %1045, %1043 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1047 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1048 = torch.aten.view %1046, %1047 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %1049 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1050 = torch.aten.transpose.int %1049, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1051 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1052 = torch.aten.view %4, %1051 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1053 = torch.aten.mm %1052, %1050 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1054 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1055 = torch.aten.view %1053, %1054 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %1056 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1057 = torch.aten.transpose.int %1056, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1058 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1059 = torch.aten.view %4, %1058 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1060 = torch.aten.mm %1059, %1057 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1061 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1062 = torch.aten.view %1060, %1061 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %1063 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1064 = torch.aten.view %1048, %1063 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1065 = torch.aten.transpose.int %1064, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1066 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1067 = torch.aten.view %1055, %1066 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1068 = torch.aten.transpose.int %1067, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1069 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1070 = torch.aten.view %1062, %1069 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1071 = torch.aten.transpose.int %1070, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1072:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1065, %1068, %1071, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1073 = torch.aten.transpose.int %1072#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1074 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1075 = torch.aten.view %1073, %1074 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1076 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1077 = torch.aten.view %1075, %1076 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %1078 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1079 = torch.aten.transpose.int %1078, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %1080 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1081 = torch.prims.convert_element_type %1080, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1082 = torch.prims.convert_element_type %1077, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1083 = torch.prims.convert_element_type %1079, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1084 = torch.aten.mm %1082, %1083 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1085 = torch.aten.mul.Scalar %1084, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1086 = torch.aten.mul.Scalar %1081, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1087 = torch.aten.add.Tensor %1085, %1086, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1088 = torch.prims.convert_element_type %1087, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1089 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1090 = torch.aten.view %1088, %1089 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1091 = torch.aten.div.Scalar %1090, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1092 = torch.aten.add.Tensor %1091, %1028, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1093 = torch.prims.convert_element_type %1092, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1094 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_35, %result1_36 = torch.aten.var_mean.correction %1093, %1094, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1095 = torch.aten.add.Scalar %result0_35, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1096 = torch.aten.rsqrt %1095 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1097 = torch.aten.sub.Tensor %1092, %result1_36, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1098 = torch.aten.mul.Tensor %1097, %1096 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %1099 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1100 = torch.aten.mul.Tensor %1098, %1099 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %1101 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1102 = torch.aten.add.Tensor %1100, %1101, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1103 = torch.prims.convert_element_type %1102, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1104 = torch.prims.convert_element_type %result1_36, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1105 = torch.prims.convert_element_type %1096, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1106 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1107 = torch.aten.view %1103, %1106 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %1108 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %1109 = torch.aten.transpose.int %1108, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %1110 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %1111 = torch.prims.convert_element_type %1110, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %1112 = torch.prims.convert_element_type %1107, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1113 = torch.prims.convert_element_type %1109, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %1114 = torch.aten.mm %1112, %1113 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %1115 = torch.aten.mul.Scalar %1114, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1116 = torch.aten.mul.Scalar %1111, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %1117 = torch.aten.add.Tensor %1115, %1116, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1118 = torch.prims.convert_element_type %1117, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %1119 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1120 = torch.aten.view %1118, %1119 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %1121 = torch.aten.slice.Tensor %1120, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1122 = torch.aten.slice.Tensor %1120, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1123 = torch.aten.gelu %1122, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %1124 = torch.aten.mul.Tensor %1121, %1123 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %1125 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %1126 = torch.aten.view %1124, %1125 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %1127 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %1128 = torch.aten.transpose.int %1127, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %1129 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1130 = torch.prims.convert_element_type %1129, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1131 = torch.prims.convert_element_type %1126, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %1132 = torch.prims.convert_element_type %1128, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %1133 = torch.aten.mm %1131, %1132 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1134 = torch.aten.mul.Scalar %1133, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1135 = torch.aten.mul.Scalar %1130, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1136 = torch.aten.add.Tensor %1134, %1135, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1137 = torch.prims.convert_element_type %1136, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1138 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1139 = torch.aten.view %1137, %1138 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1140 = torch.aten.add.Tensor %1139, %1092, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1141 = torch.prims.convert_element_type %1140, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1142 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_37, %result1_38 = torch.aten.var_mean.correction %1141, %1142, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1143 = torch.aten.add.Scalar %result0_37, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1144 = torch.aten.rsqrt %1143 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1145 = torch.aten.sub.Tensor %1140, %result1_38, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1146 = torch.aten.mul.Tensor %1145, %1144 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %1147 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1148 = torch.aten.mul.Tensor %1146, %1147 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %1149 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1150 = torch.aten.add.Tensor %1148, %1149, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1151 = torch.prims.convert_element_type %1150, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1152 = torch.prims.convert_element_type %result1_38, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1153 = torch.prims.convert_element_type %1144, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %1154 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1155 = torch.aten.transpose.int %1154, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1156 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1157 = torch.aten.view %1151, %1156 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1158 = torch.aten.mm %1157, %1155 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1159 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1160 = torch.aten.view %1158, %1159 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %1161 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1162 = torch.aten.transpose.int %1161, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1163 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1164 = torch.aten.view %1151, %1163 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1165 = torch.aten.mm %1164, %1162 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1166 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1167 = torch.aten.view %1165, %1166 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %1168 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1169 = torch.aten.transpose.int %1168, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1170 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1171 = torch.aten.view %1151, %1170 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1172 = torch.aten.mm %1171, %1169 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1173 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1174 = torch.aten.view %1172, %1173 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1175 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1176 = torch.aten.view %1160, %1175 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1177 = torch.aten.transpose.int %1176, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1178 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1179 = torch.aten.view %1167, %1178 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1180 = torch.aten.transpose.int %1179, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1181 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1182 = torch.aten.view %1174, %1181 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1183 = torch.aten.transpose.int %1182, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1184:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1177, %1180, %1183, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1185 = torch.aten.transpose.int %1184#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1186 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1187 = torch.aten.view %1185, %1186 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1188 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1189 = torch.aten.view %1187, %1188 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %1190 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1191 = torch.aten.transpose.int %1190, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %1192 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1193 = torch.prims.convert_element_type %1192, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1194 = torch.prims.convert_element_type %1189, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1195 = torch.prims.convert_element_type %1191, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1196 = torch.aten.mm %1194, %1195 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1197 = torch.aten.mul.Scalar %1196, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1198 = torch.aten.mul.Scalar %1193, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1199 = torch.aten.add.Tensor %1197, %1198, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1200 = torch.prims.convert_element_type %1199, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1201 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1202 = torch.aten.view %1200, %1201 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1203 = torch.aten.div.Scalar %1202, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1204 = torch.aten.add.Tensor %1203, %1140, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1205 = torch.prims.convert_element_type %1204, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1206 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_39, %result1_40 = torch.aten.var_mean.correction %1205, %1206, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1207 = torch.aten.add.Scalar %result0_39, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1208 = torch.aten.rsqrt %1207 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1209 = torch.aten.sub.Tensor %1204, %result1_40, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1210 = torch.aten.mul.Tensor %1209, %1208 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %1211 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1212 = torch.aten.mul.Tensor %1210, %1211 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %1213 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1214 = torch.aten.add.Tensor %1212, %1213, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1215 = torch.prims.convert_element_type %1214, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1216 = torch.prims.convert_element_type %result1_40, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1217 = torch.prims.convert_element_type %1208, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %1218 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1219 = torch.aten.transpose.int %1218, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1220 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1221 = torch.aten.view %1215, %1220 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1222 = torch.aten.mm %1221, %1219 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1223 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1224 = torch.aten.view %1222, %1223 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %1225 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1226 = torch.aten.transpose.int %1225, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1227 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1228 = torch.aten.view %4, %1227 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1229 = torch.aten.mm %1228, %1226 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1230 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1231 = torch.aten.view %1229, %1230 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %1232 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1233 = torch.aten.transpose.int %1232, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1234 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1235 = torch.aten.view %4, %1234 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1236 = torch.aten.mm %1235, %1233 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1237 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1238 = torch.aten.view %1236, %1237 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %1239 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1240 = torch.aten.view %1224, %1239 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1241 = torch.aten.transpose.int %1240, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1242 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1243 = torch.aten.view %1231, %1242 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1244 = torch.aten.transpose.int %1243, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1245 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1246 = torch.aten.view %1238, %1245 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1247 = torch.aten.transpose.int %1246, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1248:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1241, %1244, %1247, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1249 = torch.aten.transpose.int %1248#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1250 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1251 = torch.aten.view %1249, %1250 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1252 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1253 = torch.aten.view %1251, %1252 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %1254 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1255 = torch.aten.transpose.int %1254, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %1256 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1257 = torch.prims.convert_element_type %1256, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1258 = torch.prims.convert_element_type %1253, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1259 = torch.prims.convert_element_type %1255, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1260 = torch.aten.mm %1258, %1259 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1261 = torch.aten.mul.Scalar %1260, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1262 = torch.aten.mul.Scalar %1257, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1263 = torch.aten.add.Tensor %1261, %1262, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1264 = torch.prims.convert_element_type %1263, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1265 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1266 = torch.aten.view %1264, %1265 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1267 = torch.aten.div.Scalar %1266, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1268 = torch.aten.add.Tensor %1267, %1204, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1269 = torch.prims.convert_element_type %1268, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1270 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_41, %result1_42 = torch.aten.var_mean.correction %1269, %1270, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1271 = torch.aten.add.Scalar %result0_41, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1272 = torch.aten.rsqrt %1271 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1273 = torch.aten.sub.Tensor %1268, %result1_42, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1274 = torch.aten.mul.Tensor %1273, %1272 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %1275 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1276 = torch.aten.mul.Tensor %1274, %1275 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %1277 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1278 = torch.aten.add.Tensor %1276, %1277, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1279 = torch.prims.convert_element_type %1278, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1280 = torch.prims.convert_element_type %result1_42, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1281 = torch.prims.convert_element_type %1272, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1282 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1283 = torch.aten.view %1279, %1282 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %1284 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %1285 = torch.aten.transpose.int %1284, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %1286 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %1287 = torch.prims.convert_element_type %1286, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %1288 = torch.prims.convert_element_type %1283, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1289 = torch.prims.convert_element_type %1285, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %1290 = torch.aten.mm %1288, %1289 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %1291 = torch.aten.mul.Scalar %1290, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1292 = torch.aten.mul.Scalar %1287, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %1293 = torch.aten.add.Tensor %1291, %1292, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1294 = torch.prims.convert_element_type %1293, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %1295 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1296 = torch.aten.view %1294, %1295 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %1297 = torch.aten.slice.Tensor %1296, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1298 = torch.aten.slice.Tensor %1296, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1299 = torch.aten.gelu %1298, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %1300 = torch.aten.mul.Tensor %1297, %1299 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %1301 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %1302 = torch.aten.view %1300, %1301 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %1303 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %1304 = torch.aten.transpose.int %1303, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %1305 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1306 = torch.prims.convert_element_type %1305, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1307 = torch.prims.convert_element_type %1302, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %1308 = torch.prims.convert_element_type %1304, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %1309 = torch.aten.mm %1307, %1308 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1310 = torch.aten.mul.Scalar %1309, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1311 = torch.aten.mul.Scalar %1306, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1312 = torch.aten.add.Tensor %1310, %1311, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1313 = torch.prims.convert_element_type %1312, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1314 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1315 = torch.aten.view %1313, %1314 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1316 = torch.aten.add.Tensor %1315, %1268, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1317 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1318 = torch.aten.view %1316, %1317 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_out.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16>
    %1319 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1320 = torch.aten.transpose.int %1319, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_out.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_out.bias : tensor<640xf16>
    %1321 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1322 = torch.prims.convert_element_type %1321, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1323 = torch.prims.convert_element_type %1318, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1324 = torch.prims.convert_element_type %1320, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1325 = torch.aten.mm %1323, %1324 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1326 = torch.aten.mul.Scalar %1325, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1327 = torch.aten.mul.Scalar %1322, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1328 = torch.aten.add.Tensor %1326, %1327, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1329 = torch.prims.convert_element_type %1328, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1330 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1331 = torch.aten.view %1329, %1330 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1332 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1333 = torch.aten.view %1331, %1332 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %1334 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1335 = torch.aten.permute %1333, %1334 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %1336 = torch.aten.add.Tensor %1335, %920, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.downsamplers.0.conv.weight = util.global.load @_params.unet.down_blocks.1.downsamplers.0.conv.weight : tensor<640x640x3x3xf16>
    %1337 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.downsamplers.0.conv.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.downsamplers.0.conv.bias = util.global.load @_params.unet.down_blocks.1.downsamplers.0.conv.bias : tensor<640xf16>
    %1338 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.downsamplers.0.conv.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1339 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %1340 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1341 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1342 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1343 = torch.aten.convolution %1336, %1337, %1338, %1339, %1340, %1341, %false, %1342, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,32,32],f16>
    %1344 = torch.prim.ListConstruct %int2, %int32, %int20, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1345 = torch.aten.view %1343, %1344 : !torch.vtensor<[2,640,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,1024],f16>
    %1346 = torch.prims.convert_element_type %1345, %int6 : !torch.vtensor<[2,32,20,1024],f16>, !torch.int -> !torch.vtensor<[2,32,20,1024],f32>
    %1347 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_43, %result1_44 = torch.aten.var_mean.correction %1346, %1347, %int0, %true : !torch.vtensor<[2,32,20,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %1348 = torch.aten.add.Scalar %result0_43, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1349 = torch.aten.rsqrt %1348 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %1350 = torch.aten.sub.Tensor %1345, %result1_44, %int1 : !torch.vtensor<[2,32,20,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,1024],f32>
    %1351 = torch.aten.mul.Tensor %1350, %1349 : !torch.vtensor<[2,32,20,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,1024],f32>
    %1352 = torch.prim.ListConstruct %int2, %int640, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1353 = torch.aten.view %1351, %1352 : !torch.vtensor<[2,32,20,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,640,32,32],f32>
    %_params.unet.down_blocks.2.resnets.0.norm1.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.norm1.bias : tensor<640xf16>
    %1354 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1355 = torch.aten.unsqueeze %1354, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %1356 = torch.aten.unsqueeze %1355, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %1357 = torch.aten.unsqueeze %1356, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.2.resnets.0.norm1.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.norm1.weight : tensor<640xf16>
    %1358 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1359 = torch.aten.unsqueeze %1358, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %1360 = torch.aten.unsqueeze %1359, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %1361 = torch.aten.unsqueeze %1360, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %1362 = torch.aten.mul.Tensor %1353, %1361 : !torch.vtensor<[2,640,32,32],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,32,32],f32>
    %1363 = torch.aten.add.Tensor %1362, %1357, %int1 : !torch.vtensor<[2,640,32,32],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,32,32],f32>
    %1364 = torch.prims.convert_element_type %1363, %int5 : !torch.vtensor<[2,640,32,32],f32>, !torch.int -> !torch.vtensor<[2,640,32,32],f16>
    %1365 = torch.prims.convert_element_type %result1_44, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1366 = torch.prims.convert_element_type %1349, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1367 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1368 = torch.prims.squeeze %1365, %1367 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1369 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1370 = torch.prims.squeeze %1368, %1369 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1371 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1372 = torch.prims.squeeze %1366, %1371 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1373 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1374 = torch.prims.squeeze %1372, %1373 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1375 = torch.aten.silu %1364 : !torch.vtensor<[2,640,32,32],f16> -> !torch.vtensor<[2,640,32,32],f16>
    %_params.unet.down_blocks.2.resnets.0.conv1.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.conv1.weight : tensor<1280x640x3x3xf16>
    %1376 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv1.weight : tensor<1280x640x3x3xf16> -> !torch.vtensor<[1280,640,3,3],f16>
    %_params.unet.down_blocks.2.resnets.0.conv1.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.conv1.bias : tensor<1280xf16>
    %1377 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1378 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1379 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1380 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1381 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1382 = torch.aten.convolution %1375, %1376, %1377, %1378, %1379, %1380, %false, %1381, %int1 : !torch.vtensor<[2,640,32,32],f16>, !torch.vtensor<[1280,640,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1383 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %1384 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1385 = torch.aten.transpose.int %1384, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %1386 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1387 = torch.prims.convert_element_type %1386, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1388 = torch.prims.convert_element_type %1383, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %1389 = torch.prims.convert_element_type %1385, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1390 = torch.aten.mm %1388, %1389 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %1391 = torch.aten.mul.Scalar %1390, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %1392 = torch.aten.mul.Scalar %1387, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1393 = torch.aten.add.Tensor %1391, %1392, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %1394 = torch.prims.convert_element_type %1393, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %1395 = torch.aten.unsqueeze %1394, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %1396 = torch.aten.unsqueeze %1395, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %1397 = torch.aten.add.Tensor %1382, %1396, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1398 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1399 = torch.aten.view %1397, %1398 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %1400 = torch.prims.convert_element_type %1399, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1401 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_45, %result1_46 = torch.aten.var_mean.correction %1400, %1401, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %1402 = torch.aten.add.Scalar %result0_45, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1403 = torch.aten.rsqrt %1402 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %1404 = torch.aten.sub.Tensor %1399, %result1_46, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1405 = torch.aten.mul.Tensor %1404, %1403 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %1406 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1407 = torch.aten.view %1405, %1406 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.resnets.0.norm2.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.norm2.bias : tensor<1280xf16>
    %1408 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1409 = torch.aten.unsqueeze %1408, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1410 = torch.aten.unsqueeze %1409, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1411 = torch.aten.unsqueeze %1410, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.resnets.0.norm2.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.norm2.weight : tensor<1280xf16>
    %1412 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1413 = torch.aten.unsqueeze %1412, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1414 = torch.aten.unsqueeze %1413, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1415 = torch.aten.unsqueeze %1414, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %1416 = torch.aten.mul.Tensor %1407, %1415 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %1417 = torch.aten.add.Tensor %1416, %1411, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %1418 = torch.prims.convert_element_type %1417, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1419 = torch.prims.convert_element_type %result1_46, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1420 = torch.prims.convert_element_type %1403, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1421 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1422 = torch.prims.squeeze %1419, %1421 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1423 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1424 = torch.prims.squeeze %1422, %1423 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1425 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1426 = torch.prims.squeeze %1420, %1425 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1427 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1428 = torch.prims.squeeze %1426, %1427 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1429 = torch.aten.silu %1418 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.0.conv2.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %1430 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.down_blocks.2.resnets.0.conv2.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.conv2.bias : tensor<1280xf16>
    %1431 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1432 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1433 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1434 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1435 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1436 = torch.aten.convolution %1429, %1430, %1431, %1432, %1433, %1434, %false, %1435, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight : tensor<1280x640x1x1xf16>
    %1437 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight : tensor<1280x640x1x1xf16> -> !torch.vtensor<[1280,640,1,1],f16>
    %_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias : tensor<1280xf16>
    %1438 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1439 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1440 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1441 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1442 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1443 = torch.aten.convolution %1343, %1437, %1438, %1439, %1440, %1441, %false, %1442, %int1 : !torch.vtensor<[2,640,32,32],f16>, !torch.vtensor<[1280,640,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1444 = torch.aten.add.Tensor %1443, %1436, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1445 = torch.aten.div.Scalar %1444, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %1446 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1447 = torch.aten.view %1445, %1446 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %1448 = torch.prims.convert_element_type %1447, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1449 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_47, %result1_48 = torch.aten.var_mean.correction %1448, %1449, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %1450 = torch.aten.add.Scalar %result0_47, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1451 = torch.aten.rsqrt %1450 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %1452 = torch.aten.sub.Tensor %1447, %result1_48, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1453 = torch.aten.mul.Tensor %1452, %1451 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %1454 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1455 = torch.aten.view %1453, %1454 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.attentions.0.norm.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.norm.bias : tensor<1280xf16>
    %1456 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1457 = torch.aten.unsqueeze %1456, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1458 = torch.aten.unsqueeze %1457, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1459 = torch.aten.unsqueeze %1458, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.attentions.0.norm.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.norm.weight : tensor<1280xf16>
    %1460 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1461 = torch.aten.unsqueeze %1460, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1462 = torch.aten.unsqueeze %1461, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1463 = torch.aten.unsqueeze %1462, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %1464 = torch.aten.mul.Tensor %1455, %1463 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %1465 = torch.aten.add.Tensor %1464, %1459, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %1466 = torch.prims.convert_element_type %1465, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1467 = torch.prims.convert_element_type %result1_48, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1468 = torch.prims.convert_element_type %1451, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1469 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1470 = torch.prims.squeeze %1467, %1469 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1471 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1472 = torch.prims.squeeze %1470, %1471 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1473 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1474 = torch.prims.squeeze %1468, %1473 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1475 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1476 = torch.prims.squeeze %1474, %1475 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1477 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1478 = torch.aten.permute %1466, %1477 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %1479 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1480 = torch.aten.view %1478, %1479 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_in.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %1481 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1482 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1483 = torch.aten._unsafe_view %1480, %1482 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1484 = torch_c.to_builtin_tensor %1483 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1485 = torch_c.to_builtin_tensor %1481 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1486 = tensor.empty() : tensor<2048x1280xf32>
    %1487 = linalg.fill ins(%cst : f32) outs(%1486 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1488 = tensor.empty() : tensor<2048x1280xf32>
    %1489 = linalg.fill ins(%cst : f32) outs(%1488 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1490:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1487, %1489, %1484, %1485, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1487, %1489)
    %1491 = arith.truncf %1490#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1492 = torch_c.from_builtin_tensor %1491 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1493 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1494 = torch.aten.view %1492, %1493 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_in.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_in.bias : tensor<1280xf16>
    %1495 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1496 = torch.aten.add.Tensor %1494, %1495, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1497 = torch.prims.convert_element_type %1496, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1498 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_49, %result1_50 = torch.aten.var_mean.correction %1497, %1498, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1499 = torch.aten.add.Scalar %result0_49, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1500 = torch.aten.rsqrt %1499 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1501 = torch.aten.sub.Tensor %1496, %result1_50, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1502 = torch.aten.mul.Tensor %1501, %1500 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %1503 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1504 = torch.aten.mul.Tensor %1502, %1503 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %1505 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1506 = torch.aten.add.Tensor %1504, %1505, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1507 = torch.prims.convert_element_type %1506, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1508 = torch.prims.convert_element_type %result1_50, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1509 = torch.prims.convert_element_type %1500, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %1510 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1511 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1512 = torch.aten.view %1507, %1511 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1513 = torch_c.to_builtin_tensor %1512 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1514 = torch_c.to_builtin_tensor %1510 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1515 = tensor.empty() : tensor<2048x1280xf32>
    %1516 = linalg.fill ins(%cst : f32) outs(%1515 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1517 = tensor.empty() : tensor<2048x1280xf32>
    %1518 = linalg.fill ins(%cst : f32) outs(%1517 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1519:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1516, %1518, %1513, %1514, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1516, %1518)
    %1520 = arith.truncf %1519#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1521 = torch_c.from_builtin_tensor %1520 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1522 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1523 = torch.aten.view %1521, %1522 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %1524 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1525 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1526 = torch.aten.view %1507, %1525 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1527 = torch_c.to_builtin_tensor %1526 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1528 = torch_c.to_builtin_tensor %1524 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1529 = tensor.empty() : tensor<2048x1280xf32>
    %1530 = linalg.fill ins(%cst : f32) outs(%1529 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1531 = tensor.empty() : tensor<2048x1280xf32>
    %1532 = linalg.fill ins(%cst : f32) outs(%1531 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1533:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1530, %1532, %1527, %1528, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1530, %1532)
    %1534 = arith.truncf %1533#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1535 = torch_c.from_builtin_tensor %1534 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1536 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1537 = torch.aten.view %1535, %1536 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %1538 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1539 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1540 = torch.aten.view %1507, %1539 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1541 = torch_c.to_builtin_tensor %1540 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1542 = torch_c.to_builtin_tensor %1538 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1543 = tensor.empty() : tensor<2048x1280xf32>
    %1544 = linalg.fill ins(%cst : f32) outs(%1543 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1545 = tensor.empty() : tensor<2048x1280xf32>
    %1546 = linalg.fill ins(%cst : f32) outs(%1545 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1547:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1544, %1546, %1541, %1542, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1544, %1546)
    %1548 = arith.truncf %1547#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1549 = torch_c.from_builtin_tensor %1548 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1550 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1551 = torch.aten.view %1549, %1550 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1552 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1553 = torch.aten.view %1523, %1552 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1554 = torch.aten.transpose.int %1553, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1555 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1556 = torch.aten.view %1537, %1555 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1557 = torch.aten.transpose.int %1556, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1558 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1559 = torch.aten.view %1551, %1558 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1560 = torch.aten.transpose.int %1559, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1561:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1554, %1557, %1560, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1562 = torch.aten.transpose.int %1561#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1563 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1564 = torch.aten.view %1562, %1563 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1565 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1566 = torch.aten.view %1564, %1565 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1567 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1568 = torch.aten.transpose.int %1567, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %1569 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1570 = torch.prims.convert_element_type %1569, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1571 = torch.prims.convert_element_type %1566, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1572 = torch.prims.convert_element_type %1568, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1573 = torch.aten.mm %1571, %1572 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1574 = torch.aten.mul.Scalar %1573, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1575 = torch.aten.mul.Scalar %1570, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1576 = torch.aten.add.Tensor %1574, %1575, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1577 = torch.prims.convert_element_type %1576, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1578 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1579 = torch.aten.view %1577, %1578 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1580 = torch.aten.div.Scalar %1579, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1581 = torch.aten.add.Tensor %1580, %1496, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1582 = torch.prims.convert_element_type %1581, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1583 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_51, %result1_52 = torch.aten.var_mean.correction %1582, %1583, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1584 = torch.aten.add.Scalar %result0_51, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1585 = torch.aten.rsqrt %1584 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1586 = torch.aten.sub.Tensor %1581, %result1_52, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1587 = torch.aten.mul.Tensor %1586, %1585 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %1588 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1589 = torch.aten.mul.Tensor %1587, %1588 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %1590 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1591 = torch.aten.add.Tensor %1589, %1590, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1592 = torch.prims.convert_element_type %1591, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1593 = torch.prims.convert_element_type %result1_52, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1594 = torch.prims.convert_element_type %1585, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %1595 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1596 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1597 = torch.aten.view %1592, %1596 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1598 = torch_c.to_builtin_tensor %1597 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1599 = torch_c.to_builtin_tensor %1595 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1600 = tensor.empty() : tensor<2048x1280xf32>
    %1601 = linalg.fill ins(%cst : f32) outs(%1600 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1602 = tensor.empty() : tensor<2048x1280xf32>
    %1603 = linalg.fill ins(%cst : f32) outs(%1602 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1604:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1601, %1603, %1598, %1599, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1601, %1603)
    %1605 = arith.truncf %1604#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1606 = torch_c.from_builtin_tensor %1605 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1607 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1608 = torch.aten.view %1606, %1607 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %1609 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1610 = torch.aten.transpose.int %1609, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1611 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1612 = torch.aten.view %4, %1611 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1613 = torch.aten.mm %1612, %1610 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %1614 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1615 = torch.aten.view %1613, %1614 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %1616 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1617 = torch.aten.transpose.int %1616, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1618 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1619 = torch.aten.view %4, %1618 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1620 = torch.aten.mm %1619, %1617 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %1621 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1622 = torch.aten.view %1620, %1621 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %1623 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1624 = torch.aten.view %1608, %1623 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1625 = torch.aten.transpose.int %1624, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1626 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1627 = torch.aten.view %1615, %1626 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1628 = torch.aten.transpose.int %1627, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1629 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1630 = torch.aten.view %1622, %1629 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1631 = torch.aten.transpose.int %1630, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1632:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1625, %1628, %1631, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1633 = torch.aten.transpose.int %1632#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1634 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1635 = torch.aten.view %1633, %1634 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1636 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1637 = torch.aten.view %1635, %1636 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %1638 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1639 = torch.aten.transpose.int %1638, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %1640 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1641 = torch.prims.convert_element_type %1640, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1642 = torch.prims.convert_element_type %1637, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1643 = torch.prims.convert_element_type %1639, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1644 = torch.aten.mm %1642, %1643 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1645 = torch.aten.mul.Scalar %1644, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1646 = torch.aten.mul.Scalar %1641, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1647 = torch.aten.add.Tensor %1645, %1646, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1648 = torch.prims.convert_element_type %1647, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1649 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1650 = torch.aten.view %1648, %1649 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1651 = torch.aten.div.Scalar %1650, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1652 = torch.aten.add.Tensor %1651, %1581, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1653 = torch.prims.convert_element_type %1652, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1654 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_53, %result1_54 = torch.aten.var_mean.correction %1653, %1654, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1655 = torch.aten.add.Scalar %result0_53, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1656 = torch.aten.rsqrt %1655 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1657 = torch.aten.sub.Tensor %1652, %result1_54, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1658 = torch.aten.mul.Tensor %1657, %1656 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %1659 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1660 = torch.aten.mul.Tensor %1658, %1659 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %1661 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1662 = torch.aten.add.Tensor %1660, %1661, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1663 = torch.prims.convert_element_type %1662, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1664 = torch.prims.convert_element_type %result1_54, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1665 = torch.prims.convert_element_type %1656, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1666 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1667 = torch.aten.view %1663, %1666 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %1668 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %1669 = torch.aten.transpose.int %1668, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %1670 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %1671 = torch.prims.convert_element_type %1670, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %1672 = torch.prims.convert_element_type %1667, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1673 = torch.prims.convert_element_type %1669, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %1674 = torch.aten.mm %1672, %1673 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %1675 = torch.aten.mul.Scalar %1674, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1676 = torch.aten.mul.Scalar %1671, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %1677 = torch.aten.add.Tensor %1675, %1676, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1678 = torch.prims.convert_element_type %1677, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %1679 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1680 = torch.aten.view %1678, %1679 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %1681 = torch.aten.slice.Tensor %1680, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1682 = torch.aten.slice.Tensor %1680, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1683 = torch.aten.gelu %1682, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %1684 = torch.aten.mul.Tensor %1681, %1683 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %1685 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %1686 = torch.aten.view %1684, %1685 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %1687 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %1688 = torch.aten.transpose.int %1687, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %1689 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1690 = torch.prims.convert_element_type %1689, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1691 = torch.prims.convert_element_type %1686, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %1692 = torch.prims.convert_element_type %1688, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1693 = torch.aten.mm %1691, %1692 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1694 = torch.aten.mul.Scalar %1693, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1695 = torch.aten.mul.Scalar %1690, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1696 = torch.aten.add.Tensor %1694, %1695, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1697 = torch.prims.convert_element_type %1696, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1698 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1699 = torch.aten.view %1697, %1698 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1700 = torch.aten.add.Tensor %1699, %1652, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1701 = torch.prims.convert_element_type %1700, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1702 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_55, %result1_56 = torch.aten.var_mean.correction %1701, %1702, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1703 = torch.aten.add.Scalar %result0_55, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1704 = torch.aten.rsqrt %1703 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1705 = torch.aten.sub.Tensor %1700, %result1_56, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1706 = torch.aten.mul.Tensor %1705, %1704 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %1707 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1708 = torch.aten.mul.Tensor %1706, %1707 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %1709 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1710 = torch.aten.add.Tensor %1708, %1709, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1711 = torch.prims.convert_element_type %1710, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1712 = torch.prims.convert_element_type %result1_56, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1713 = torch.prims.convert_element_type %1704, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %1714 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1715 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1716 = torch.aten.view %1711, %1715 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1717 = torch_c.to_builtin_tensor %1716 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1718 = torch_c.to_builtin_tensor %1714 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1719 = tensor.empty() : tensor<2048x1280xf32>
    %1720 = linalg.fill ins(%cst : f32) outs(%1719 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1721 = tensor.empty() : tensor<2048x1280xf32>
    %1722 = linalg.fill ins(%cst : f32) outs(%1721 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1723:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1720, %1722, %1717, %1718, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1720, %1722)
    %1724 = arith.truncf %1723#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1725 = torch_c.from_builtin_tensor %1724 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1726 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1727 = torch.aten.view %1725, %1726 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %1728 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1729 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1730 = torch.aten.view %1711, %1729 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1731 = torch_c.to_builtin_tensor %1730 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1732 = torch_c.to_builtin_tensor %1728 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1733 = tensor.empty() : tensor<2048x1280xf32>
    %1734 = linalg.fill ins(%cst : f32) outs(%1733 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1735 = tensor.empty() : tensor<2048x1280xf32>
    %1736 = linalg.fill ins(%cst : f32) outs(%1735 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1737:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1734, %1736, %1731, %1732, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1734, %1736)
    %1738 = arith.truncf %1737#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1739 = torch_c.from_builtin_tensor %1738 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1740 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1741 = torch.aten.view %1739, %1740 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %1742 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1743 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1744 = torch.aten.view %1711, %1743 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1745 = torch_c.to_builtin_tensor %1744 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1746 = torch_c.to_builtin_tensor %1742 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1747 = tensor.empty() : tensor<2048x1280xf32>
    %1748 = linalg.fill ins(%cst : f32) outs(%1747 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1749 = tensor.empty() : tensor<2048x1280xf32>
    %1750 = linalg.fill ins(%cst : f32) outs(%1749 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1751:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1748, %1750, %1745, %1746, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1748, %1750)
    %1752 = arith.truncf %1751#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1753 = torch_c.from_builtin_tensor %1752 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1754 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1755 = torch.aten.view %1753, %1754 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1756 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1757 = torch.aten.view %1727, %1756 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1758 = torch.aten.transpose.int %1757, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1759 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1760 = torch.aten.view %1741, %1759 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1761 = torch.aten.transpose.int %1760, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1762 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1763 = torch.aten.view %1755, %1762 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1764 = torch.aten.transpose.int %1763, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1765:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1758, %1761, %1764, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1766 = torch.aten.transpose.int %1765#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1767 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1768 = torch.aten.view %1766, %1767 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1769 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1770 = torch.aten.view %1768, %1769 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1771 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1772 = torch.aten.transpose.int %1771, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %1773 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1774 = torch.prims.convert_element_type %1773, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1775 = torch.prims.convert_element_type %1770, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1776 = torch.prims.convert_element_type %1772, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1777 = torch.aten.mm %1775, %1776 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1778 = torch.aten.mul.Scalar %1777, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1779 = torch.aten.mul.Scalar %1774, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1780 = torch.aten.add.Tensor %1778, %1779, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1781 = torch.prims.convert_element_type %1780, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1782 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1783 = torch.aten.view %1781, %1782 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1784 = torch.aten.div.Scalar %1783, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1785 = torch.aten.add.Tensor %1784, %1700, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1786 = torch.prims.convert_element_type %1785, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1787 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_57, %result1_58 = torch.aten.var_mean.correction %1786, %1787, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1788 = torch.aten.add.Scalar %result0_57, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1789 = torch.aten.rsqrt %1788 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1790 = torch.aten.sub.Tensor %1785, %result1_58, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1791 = torch.aten.mul.Tensor %1790, %1789 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %1792 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1793 = torch.aten.mul.Tensor %1791, %1792 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %1794 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1795 = torch.aten.add.Tensor %1793, %1794, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1796 = torch.prims.convert_element_type %1795, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1797 = torch.prims.convert_element_type %result1_58, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1798 = torch.prims.convert_element_type %1789, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %1799 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1800 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1801 = torch.aten.view %1796, %1800 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1802 = torch_c.to_builtin_tensor %1801 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1803 = torch_c.to_builtin_tensor %1799 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1804 = tensor.empty() : tensor<2048x1280xf32>
    %1805 = linalg.fill ins(%cst : f32) outs(%1804 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1806 = tensor.empty() : tensor<2048x1280xf32>
    %1807 = linalg.fill ins(%cst : f32) outs(%1806 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1808:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1805, %1807, %1802, %1803, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1805, %1807)
    %1809 = arith.truncf %1808#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1810 = torch_c.from_builtin_tensor %1809 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1811 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1812 = torch.aten.view %1810, %1811 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %1813 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1814 = torch.aten.transpose.int %1813, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1815 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1816 = torch.aten.view %4, %1815 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1817 = torch.aten.mm %1816, %1814 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %1818 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1819 = torch.aten.view %1817, %1818 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %1820 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1821 = torch.aten.transpose.int %1820, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1822 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1823 = torch.aten.view %4, %1822 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1824 = torch.aten.mm %1823, %1821 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %1825 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1826 = torch.aten.view %1824, %1825 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %1827 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1828 = torch.aten.view %1812, %1827 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1829 = torch.aten.transpose.int %1828, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1830 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1831 = torch.aten.view %1819, %1830 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1832 = torch.aten.transpose.int %1831, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1833 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1834 = torch.aten.view %1826, %1833 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1835 = torch.aten.transpose.int %1834, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1836:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1829, %1832, %1835, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1837 = torch.aten.transpose.int %1836#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1838 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1839 = torch.aten.view %1837, %1838 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1840 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1841 = torch.aten.view %1839, %1840 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %1842 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1843 = torch.aten.transpose.int %1842, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %1844 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1845 = torch.prims.convert_element_type %1844, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1846 = torch.prims.convert_element_type %1841, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1847 = torch.prims.convert_element_type %1843, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1848 = torch.aten.mm %1846, %1847 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1849 = torch.aten.mul.Scalar %1848, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1850 = torch.aten.mul.Scalar %1845, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1851 = torch.aten.add.Tensor %1849, %1850, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1852 = torch.prims.convert_element_type %1851, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1853 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1854 = torch.aten.view %1852, %1853 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1855 = torch.aten.div.Scalar %1854, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1856 = torch.aten.add.Tensor %1855, %1785, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1857 = torch.prims.convert_element_type %1856, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1858 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_59, %result1_60 = torch.aten.var_mean.correction %1857, %1858, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1859 = torch.aten.add.Scalar %result0_59, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1860 = torch.aten.rsqrt %1859 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1861 = torch.aten.sub.Tensor %1856, %result1_60, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1862 = torch.aten.mul.Tensor %1861, %1860 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %1863 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1864 = torch.aten.mul.Tensor %1862, %1863 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %1865 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1866 = torch.aten.add.Tensor %1864, %1865, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1867 = torch.prims.convert_element_type %1866, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1868 = torch.prims.convert_element_type %result1_60, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1869 = torch.prims.convert_element_type %1860, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1870 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1871 = torch.aten.view %1867, %1870 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %1872 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %1873 = torch.aten.transpose.int %1872, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %1874 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %1875 = torch.prims.convert_element_type %1874, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %1876 = torch.prims.convert_element_type %1871, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1877 = torch.prims.convert_element_type %1873, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %1878 = torch.aten.mm %1876, %1877 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %1879 = torch.aten.mul.Scalar %1878, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1880 = torch.aten.mul.Scalar %1875, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %1881 = torch.aten.add.Tensor %1879, %1880, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1882 = torch.prims.convert_element_type %1881, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %1883 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1884 = torch.aten.view %1882, %1883 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %1885 = torch.aten.slice.Tensor %1884, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1886 = torch.aten.slice.Tensor %1884, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1887 = torch.aten.gelu %1886, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %1888 = torch.aten.mul.Tensor %1885, %1887 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %1889 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %1890 = torch.aten.view %1888, %1889 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %1891 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %1892 = torch.aten.transpose.int %1891, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %1893 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1894 = torch.prims.convert_element_type %1893, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1895 = torch.prims.convert_element_type %1890, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %1896 = torch.prims.convert_element_type %1892, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1897 = torch.aten.mm %1895, %1896 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1898 = torch.aten.mul.Scalar %1897, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1899 = torch.aten.mul.Scalar %1894, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1900 = torch.aten.add.Tensor %1898, %1899, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1901 = torch.prims.convert_element_type %1900, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1902 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1903 = torch.aten.view %1901, %1902 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1904 = torch.aten.add.Tensor %1903, %1856, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1905 = torch.prims.convert_element_type %1904, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1906 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_61, %result1_62 = torch.aten.var_mean.correction %1905, %1906, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1907 = torch.aten.add.Scalar %result0_61, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1908 = torch.aten.rsqrt %1907 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1909 = torch.aten.sub.Tensor %1904, %result1_62, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1910 = torch.aten.mul.Tensor %1909, %1908 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %1911 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1912 = torch.aten.mul.Tensor %1910, %1911 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %1913 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1914 = torch.aten.add.Tensor %1912, %1913, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1915 = torch.prims.convert_element_type %1914, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1916 = torch.prims.convert_element_type %result1_62, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1917 = torch.prims.convert_element_type %1908, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %1918 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1919 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1920 = torch.aten.view %1915, %1919 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1921 = torch_c.to_builtin_tensor %1920 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1922 = torch_c.to_builtin_tensor %1918 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1923 = tensor.empty() : tensor<2048x1280xf32>
    %1924 = linalg.fill ins(%cst : f32) outs(%1923 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1925 = tensor.empty() : tensor<2048x1280xf32>
    %1926 = linalg.fill ins(%cst : f32) outs(%1925 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1927:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1924, %1926, %1921, %1922, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1924, %1926)
    %1928 = arith.truncf %1927#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1929 = torch_c.from_builtin_tensor %1928 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1930 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1931 = torch.aten.view %1929, %1930 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %1932 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1933 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1934 = torch.aten.view %1915, %1933 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1935 = torch_c.to_builtin_tensor %1934 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1936 = torch_c.to_builtin_tensor %1932 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1937 = tensor.empty() : tensor<2048x1280xf32>
    %1938 = linalg.fill ins(%cst : f32) outs(%1937 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1939 = tensor.empty() : tensor<2048x1280xf32>
    %1940 = linalg.fill ins(%cst : f32) outs(%1939 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1941:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1938, %1940, %1935, %1936, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1938, %1940)
    %1942 = arith.truncf %1941#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1943 = torch_c.from_builtin_tensor %1942 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1944 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1945 = torch.aten.view %1943, %1944 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %1946 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1947 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1948 = torch.aten.view %1915, %1947 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1949 = torch_c.to_builtin_tensor %1948 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1950 = torch_c.to_builtin_tensor %1946 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1951 = tensor.empty() : tensor<2048x1280xf32>
    %1952 = linalg.fill ins(%cst : f32) outs(%1951 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1953 = tensor.empty() : tensor<2048x1280xf32>
    %1954 = linalg.fill ins(%cst : f32) outs(%1953 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1955:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1952, %1954, %1949, %1950, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1952, %1954)
    %1956 = arith.truncf %1955#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1957 = torch_c.from_builtin_tensor %1956 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1958 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1959 = torch.aten.view %1957, %1958 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1960 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1961 = torch.aten.view %1931, %1960 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1962 = torch.aten.transpose.int %1961, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1963 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1964 = torch.aten.view %1945, %1963 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1965 = torch.aten.transpose.int %1964, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1966 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1967 = torch.aten.view %1959, %1966 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1968 = torch.aten.transpose.int %1967, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1969:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1962, %1965, %1968, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1970 = torch.aten.transpose.int %1969#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1971 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1972 = torch.aten.view %1970, %1971 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1973 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1974 = torch.aten.view %1972, %1973 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1975 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1976 = torch.aten.transpose.int %1975, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %1977 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1978 = torch.prims.convert_element_type %1977, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1979 = torch.prims.convert_element_type %1974, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1980 = torch.prims.convert_element_type %1976, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1981 = torch.aten.mm %1979, %1980 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1982 = torch.aten.mul.Scalar %1981, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1983 = torch.aten.mul.Scalar %1978, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1984 = torch.aten.add.Tensor %1982, %1983, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1985 = torch.prims.convert_element_type %1984, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1986 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1987 = torch.aten.view %1985, %1986 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1988 = torch.aten.div.Scalar %1987, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1989 = torch.aten.add.Tensor %1988, %1904, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1990 = torch.prims.convert_element_type %1989, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1991 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_63, %result1_64 = torch.aten.var_mean.correction %1990, %1991, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1992 = torch.aten.add.Scalar %result0_63, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1993 = torch.aten.rsqrt %1992 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1994 = torch.aten.sub.Tensor %1989, %result1_64, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1995 = torch.aten.mul.Tensor %1994, %1993 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %1996 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1997 = torch.aten.mul.Tensor %1995, %1996 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %1998 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1999 = torch.aten.add.Tensor %1997, %1998, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2000 = torch.prims.convert_element_type %1999, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2001 = torch.prims.convert_element_type %result1_64, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2002 = torch.prims.convert_element_type %1993, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %2003 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2004 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2005 = torch.aten.view %2000, %2004 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2006 = torch_c.to_builtin_tensor %2005 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2007 = torch_c.to_builtin_tensor %2003 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2008 = tensor.empty() : tensor<2048x1280xf32>
    %2009 = linalg.fill ins(%cst : f32) outs(%2008 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2010 = tensor.empty() : tensor<2048x1280xf32>
    %2011 = linalg.fill ins(%cst : f32) outs(%2010 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2012:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2009, %2011, %2006, %2007, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2009, %2011)
    %2013 = arith.truncf %2012#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2014 = torch_c.from_builtin_tensor %2013 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2015 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2016 = torch.aten.view %2014, %2015 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %2017 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2018 = torch.aten.transpose.int %2017, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2019 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2020 = torch.aten.view %4, %2019 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2021 = torch.aten.mm %2020, %2018 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2022 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2023 = torch.aten.view %2021, %2022 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %2024 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2025 = torch.aten.transpose.int %2024, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2026 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2027 = torch.aten.view %4, %2026 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2028 = torch.aten.mm %2027, %2025 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2029 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2030 = torch.aten.view %2028, %2029 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2031 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2032 = torch.aten.view %2016, %2031 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2033 = torch.aten.transpose.int %2032, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2034 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2035 = torch.aten.view %2023, %2034 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2036 = torch.aten.transpose.int %2035, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2037 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2038 = torch.aten.view %2030, %2037 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2039 = torch.aten.transpose.int %2038, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2040:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2033, %2036, %2039, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2041 = torch.aten.transpose.int %2040#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2042 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2043 = torch.aten.view %2041, %2042 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2044 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2045 = torch.aten.view %2043, %2044 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2046 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2047 = torch.aten.transpose.int %2046, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %2048 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2049 = torch.prims.convert_element_type %2048, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2050 = torch.prims.convert_element_type %2045, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2051 = torch.prims.convert_element_type %2047, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2052 = torch.aten.mm %2050, %2051 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2053 = torch.aten.mul.Scalar %2052, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2054 = torch.aten.mul.Scalar %2049, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2055 = torch.aten.add.Tensor %2053, %2054, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2056 = torch.prims.convert_element_type %2055, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2057 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2058 = torch.aten.view %2056, %2057 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2059 = torch.aten.div.Scalar %2058, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2060 = torch.aten.add.Tensor %2059, %1989, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2061 = torch.prims.convert_element_type %2060, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2062 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_65, %result1_66 = torch.aten.var_mean.correction %2061, %2062, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2063 = torch.aten.add.Scalar %result0_65, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2064 = torch.aten.rsqrt %2063 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2065 = torch.aten.sub.Tensor %2060, %result1_66, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2066 = torch.aten.mul.Tensor %2065, %2064 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %2067 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2068 = torch.aten.mul.Tensor %2066, %2067 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %2069 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2070 = torch.aten.add.Tensor %2068, %2069, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2071 = torch.prims.convert_element_type %2070, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2072 = torch.prims.convert_element_type %result1_66, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2073 = torch.prims.convert_element_type %2064, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2074 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2075 = torch.aten.view %2071, %2074 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2076 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2077 = torch.aten.transpose.int %2076, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %2078 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2079 = torch.prims.convert_element_type %2078, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2080 = torch.prims.convert_element_type %2075, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2081 = torch.prims.convert_element_type %2077, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2082 = torch.aten.mm %2080, %2081 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2083 = torch.aten.mul.Scalar %2082, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2084 = torch.aten.mul.Scalar %2079, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2085 = torch.aten.add.Tensor %2083, %2084, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2086 = torch.prims.convert_element_type %2085, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2087 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2088 = torch.aten.view %2086, %2087 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2089 = torch.aten.slice.Tensor %2088, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2090 = torch.aten.slice.Tensor %2088, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2091 = torch.aten.gelu %2090, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2092 = torch.aten.mul.Tensor %2089, %2091 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2093 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2094 = torch.aten.view %2092, %2093 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %2095 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2096 = torch.aten.transpose.int %2095, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %2097 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2098 = torch.prims.convert_element_type %2097, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2099 = torch.prims.convert_element_type %2094, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2100 = torch.prims.convert_element_type %2096, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2101 = torch.aten.mm %2099, %2100 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2102 = torch.aten.mul.Scalar %2101, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2103 = torch.aten.mul.Scalar %2098, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2104 = torch.aten.add.Tensor %2102, %2103, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2105 = torch.prims.convert_element_type %2104, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2106 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2107 = torch.aten.view %2105, %2106 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2108 = torch.aten.add.Tensor %2107, %2060, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2109 = torch.prims.convert_element_type %2108, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2110 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_67, %result1_68 = torch.aten.var_mean.correction %2109, %2110, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2111 = torch.aten.add.Scalar %result0_67, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2112 = torch.aten.rsqrt %2111 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2113 = torch.aten.sub.Tensor %2108, %result1_68, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2114 = torch.aten.mul.Tensor %2113, %2112 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %2115 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2116 = torch.aten.mul.Tensor %2114, %2115 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %2117 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2118 = torch.aten.add.Tensor %2116, %2117, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2119 = torch.prims.convert_element_type %2118, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2120 = torch.prims.convert_element_type %result1_68, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2121 = torch.prims.convert_element_type %2112, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %2122 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2123 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2124 = torch.aten.view %2119, %2123 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2125 = torch_c.to_builtin_tensor %2124 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2126 = torch_c.to_builtin_tensor %2122 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2127 = tensor.empty() : tensor<2048x1280xf32>
    %2128 = linalg.fill ins(%cst : f32) outs(%2127 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2129 = tensor.empty() : tensor<2048x1280xf32>
    %2130 = linalg.fill ins(%cst : f32) outs(%2129 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2131:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2128, %2130, %2125, %2126, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2128, %2130)
    %2132 = arith.truncf %2131#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2133 = torch_c.from_builtin_tensor %2132 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2134 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2135 = torch.aten.view %2133, %2134 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %2136 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2137 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2138 = torch.aten.view %2119, %2137 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2139 = torch_c.to_builtin_tensor %2138 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2140 = torch_c.to_builtin_tensor %2136 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2141 = tensor.empty() : tensor<2048x1280xf32>
    %2142 = linalg.fill ins(%cst : f32) outs(%2141 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2143 = tensor.empty() : tensor<2048x1280xf32>
    %2144 = linalg.fill ins(%cst : f32) outs(%2143 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2145:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2142, %2144, %2139, %2140, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2142, %2144)
    %2146 = arith.truncf %2145#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2147 = torch_c.from_builtin_tensor %2146 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2148 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2149 = torch.aten.view %2147, %2148 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %2150 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2151 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2152 = torch.aten.view %2119, %2151 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2153 = torch_c.to_builtin_tensor %2152 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2154 = torch_c.to_builtin_tensor %2150 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2155 = tensor.empty() : tensor<2048x1280xf32>
    %2156 = linalg.fill ins(%cst : f32) outs(%2155 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2157 = tensor.empty() : tensor<2048x1280xf32>
    %2158 = linalg.fill ins(%cst : f32) outs(%2157 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2159:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2156, %2158, %2153, %2154, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2156, %2158)
    %2160 = arith.truncf %2159#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2161 = torch_c.from_builtin_tensor %2160 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2162 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2163 = torch.aten.view %2161, %2162 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2164 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2165 = torch.aten.view %2135, %2164 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2166 = torch.aten.transpose.int %2165, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2167 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2168 = torch.aten.view %2149, %2167 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2169 = torch.aten.transpose.int %2168, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2170 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2171 = torch.aten.view %2163, %2170 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2172 = torch.aten.transpose.int %2171, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2173:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2166, %2169, %2172, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2174 = torch.aten.transpose.int %2173#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2175 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2176 = torch.aten.view %2174, %2175 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2177 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2178 = torch.aten.view %2176, %2177 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2179 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2180 = torch.aten.transpose.int %2179, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %2181 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2182 = torch.prims.convert_element_type %2181, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2183 = torch.prims.convert_element_type %2178, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2184 = torch.prims.convert_element_type %2180, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2185 = torch.aten.mm %2183, %2184 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2186 = torch.aten.mul.Scalar %2185, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2187 = torch.aten.mul.Scalar %2182, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2188 = torch.aten.add.Tensor %2186, %2187, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2189 = torch.prims.convert_element_type %2188, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2190 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2191 = torch.aten.view %2189, %2190 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2192 = torch.aten.div.Scalar %2191, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2193 = torch.aten.add.Tensor %2192, %2108, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2194 = torch.prims.convert_element_type %2193, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2195 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_69, %result1_70 = torch.aten.var_mean.correction %2194, %2195, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2196 = torch.aten.add.Scalar %result0_69, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2197 = torch.aten.rsqrt %2196 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2198 = torch.aten.sub.Tensor %2193, %result1_70, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2199 = torch.aten.mul.Tensor %2198, %2197 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %2200 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2201 = torch.aten.mul.Tensor %2199, %2200 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %2202 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2203 = torch.aten.add.Tensor %2201, %2202, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2204 = torch.prims.convert_element_type %2203, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2205 = torch.prims.convert_element_type %result1_70, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2206 = torch.prims.convert_element_type %2197, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %2207 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2208 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2209 = torch.aten.view %2204, %2208 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2210 = torch_c.to_builtin_tensor %2209 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2211 = torch_c.to_builtin_tensor %2207 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2212 = tensor.empty() : tensor<2048x1280xf32>
    %2213 = linalg.fill ins(%cst : f32) outs(%2212 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2214 = tensor.empty() : tensor<2048x1280xf32>
    %2215 = linalg.fill ins(%cst : f32) outs(%2214 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2216:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2213, %2215, %2210, %2211, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2213, %2215)
    %2217 = arith.truncf %2216#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2218 = torch_c.from_builtin_tensor %2217 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2219 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2220 = torch.aten.view %2218, %2219 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %2221 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2222 = torch.aten.transpose.int %2221, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2223 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2224 = torch.aten.view %4, %2223 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2225 = torch.aten.mm %2224, %2222 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2226 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2227 = torch.aten.view %2225, %2226 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %2228 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2229 = torch.aten.transpose.int %2228, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2230 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2231 = torch.aten.view %4, %2230 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2232 = torch.aten.mm %2231, %2229 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2233 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2234 = torch.aten.view %2232, %2233 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2235 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2236 = torch.aten.view %2220, %2235 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2237 = torch.aten.transpose.int %2236, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2238 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2239 = torch.aten.view %2227, %2238 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2240 = torch.aten.transpose.int %2239, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2241 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2242 = torch.aten.view %2234, %2241 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2243 = torch.aten.transpose.int %2242, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2244:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2237, %2240, %2243, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2245 = torch.aten.transpose.int %2244#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2246 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2247 = torch.aten.view %2245, %2246 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2248 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2249 = torch.aten.view %2247, %2248 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2250 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2251 = torch.aten.transpose.int %2250, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %2252 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2253 = torch.prims.convert_element_type %2252, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2254 = torch.prims.convert_element_type %2249, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2255 = torch.prims.convert_element_type %2251, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2256 = torch.aten.mm %2254, %2255 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2257 = torch.aten.mul.Scalar %2256, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2258 = torch.aten.mul.Scalar %2253, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2259 = torch.aten.add.Tensor %2257, %2258, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2260 = torch.prims.convert_element_type %2259, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2261 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2262 = torch.aten.view %2260, %2261 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2263 = torch.aten.div.Scalar %2262, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2264 = torch.aten.add.Tensor %2263, %2193, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2265 = torch.prims.convert_element_type %2264, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2266 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_71, %result1_72 = torch.aten.var_mean.correction %2265, %2266, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2267 = torch.aten.add.Scalar %result0_71, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2268 = torch.aten.rsqrt %2267 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2269 = torch.aten.sub.Tensor %2264, %result1_72, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2270 = torch.aten.mul.Tensor %2269, %2268 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %2271 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2272 = torch.aten.mul.Tensor %2270, %2271 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %2273 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2274 = torch.aten.add.Tensor %2272, %2273, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2275 = torch.prims.convert_element_type %2274, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2276 = torch.prims.convert_element_type %result1_72, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2277 = torch.prims.convert_element_type %2268, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2278 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2279 = torch.aten.view %2275, %2278 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2280 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2281 = torch.aten.transpose.int %2280, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %2282 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2283 = torch.prims.convert_element_type %2282, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2284 = torch.prims.convert_element_type %2279, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2285 = torch.prims.convert_element_type %2281, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2286 = torch.aten.mm %2284, %2285 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2287 = torch.aten.mul.Scalar %2286, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2288 = torch.aten.mul.Scalar %2283, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2289 = torch.aten.add.Tensor %2287, %2288, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2290 = torch.prims.convert_element_type %2289, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2291 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2292 = torch.aten.view %2290, %2291 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2293 = torch.aten.slice.Tensor %2292, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2294 = torch.aten.slice.Tensor %2292, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2295 = torch.aten.gelu %2294, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2296 = torch.aten.mul.Tensor %2293, %2295 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2297 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2298 = torch.aten.view %2296, %2297 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %2299 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2300 = torch.aten.transpose.int %2299, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %2301 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2302 = torch.prims.convert_element_type %2301, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2303 = torch.prims.convert_element_type %2298, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2304 = torch.prims.convert_element_type %2300, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2305 = torch.aten.mm %2303, %2304 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2306 = torch.aten.mul.Scalar %2305, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2307 = torch.aten.mul.Scalar %2302, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2308 = torch.aten.add.Tensor %2306, %2307, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2309 = torch.prims.convert_element_type %2308, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2310 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2311 = torch.aten.view %2309, %2310 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2312 = torch.aten.add.Tensor %2311, %2264, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2313 = torch.prims.convert_element_type %2312, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2314 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_73, %result1_74 = torch.aten.var_mean.correction %2313, %2314, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2315 = torch.aten.add.Scalar %result0_73, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2316 = torch.aten.rsqrt %2315 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2317 = torch.aten.sub.Tensor %2312, %result1_74, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2318 = torch.aten.mul.Tensor %2317, %2316 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %2319 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2320 = torch.aten.mul.Tensor %2318, %2319 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %2321 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2322 = torch.aten.add.Tensor %2320, %2321, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2323 = torch.prims.convert_element_type %2322, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2324 = torch.prims.convert_element_type %result1_74, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2325 = torch.prims.convert_element_type %2316, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %2326 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2327 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2328 = torch.aten.view %2323, %2327 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2329 = torch_c.to_builtin_tensor %2328 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2330 = torch_c.to_builtin_tensor %2326 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2331 = tensor.empty() : tensor<2048x1280xf32>
    %2332 = linalg.fill ins(%cst : f32) outs(%2331 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2333 = tensor.empty() : tensor<2048x1280xf32>
    %2334 = linalg.fill ins(%cst : f32) outs(%2333 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2335:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2332, %2334, %2329, %2330, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2332, %2334)
    %2336 = arith.truncf %2335#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2337 = torch_c.from_builtin_tensor %2336 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2338 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2339 = torch.aten.view %2337, %2338 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %2340 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2341 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2342 = torch.aten.view %2323, %2341 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2343 = torch_c.to_builtin_tensor %2342 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2344 = torch_c.to_builtin_tensor %2340 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2345 = tensor.empty() : tensor<2048x1280xf32>
    %2346 = linalg.fill ins(%cst : f32) outs(%2345 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2347 = tensor.empty() : tensor<2048x1280xf32>
    %2348 = linalg.fill ins(%cst : f32) outs(%2347 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2349:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2346, %2348, %2343, %2344, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2346, %2348)
    %2350 = arith.truncf %2349#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2351 = torch_c.from_builtin_tensor %2350 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2352 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2353 = torch.aten.view %2351, %2352 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %2354 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2355 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2356 = torch.aten.view %2323, %2355 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2357 = torch_c.to_builtin_tensor %2356 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2358 = torch_c.to_builtin_tensor %2354 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2359 = tensor.empty() : tensor<2048x1280xf32>
    %2360 = linalg.fill ins(%cst : f32) outs(%2359 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2361 = tensor.empty() : tensor<2048x1280xf32>
    %2362 = linalg.fill ins(%cst : f32) outs(%2361 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2363:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2360, %2362, %2357, %2358, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2360, %2362)
    %2364 = arith.truncf %2363#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2365 = torch_c.from_builtin_tensor %2364 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2366 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2367 = torch.aten.view %2365, %2366 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2368 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2369 = torch.aten.view %2339, %2368 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2370 = torch.aten.transpose.int %2369, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2371 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2372 = torch.aten.view %2353, %2371 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2373 = torch.aten.transpose.int %2372, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2374 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2375 = torch.aten.view %2367, %2374 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2376 = torch.aten.transpose.int %2375, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2377:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2370, %2373, %2376, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2378 = torch.aten.transpose.int %2377#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2379 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2380 = torch.aten.view %2378, %2379 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2381 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2382 = torch.aten.view %2380, %2381 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2383 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2384 = torch.aten.transpose.int %2383, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %2385 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2386 = torch.prims.convert_element_type %2385, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2387 = torch.prims.convert_element_type %2382, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2388 = torch.prims.convert_element_type %2384, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2389 = torch.aten.mm %2387, %2388 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2390 = torch.aten.mul.Scalar %2389, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2391 = torch.aten.mul.Scalar %2386, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2392 = torch.aten.add.Tensor %2390, %2391, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2393 = torch.prims.convert_element_type %2392, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2394 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2395 = torch.aten.view %2393, %2394 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2396 = torch.aten.div.Scalar %2395, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2397 = torch.aten.add.Tensor %2396, %2312, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2398 = torch.prims.convert_element_type %2397, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2399 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_75, %result1_76 = torch.aten.var_mean.correction %2398, %2399, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2400 = torch.aten.add.Scalar %result0_75, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2401 = torch.aten.rsqrt %2400 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2402 = torch.aten.sub.Tensor %2397, %result1_76, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2403 = torch.aten.mul.Tensor %2402, %2401 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %2404 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2405 = torch.aten.mul.Tensor %2403, %2404 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %2406 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2407 = torch.aten.add.Tensor %2405, %2406, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2408 = torch.prims.convert_element_type %2407, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2409 = torch.prims.convert_element_type %result1_76, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2410 = torch.prims.convert_element_type %2401, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %2411 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2412 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2413 = torch.aten.view %2408, %2412 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2414 = torch_c.to_builtin_tensor %2413 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2415 = torch_c.to_builtin_tensor %2411 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2416 = tensor.empty() : tensor<2048x1280xf32>
    %2417 = linalg.fill ins(%cst : f32) outs(%2416 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2418 = tensor.empty() : tensor<2048x1280xf32>
    %2419 = linalg.fill ins(%cst : f32) outs(%2418 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2420:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2417, %2419, %2414, %2415, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2417, %2419)
    %2421 = arith.truncf %2420#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2422 = torch_c.from_builtin_tensor %2421 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2423 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2424 = torch.aten.view %2422, %2423 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %2425 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2426 = torch.aten.transpose.int %2425, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2427 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2428 = torch.aten.view %4, %2427 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2429 = torch.aten.mm %2428, %2426 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2430 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2431 = torch.aten.view %2429, %2430 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %2432 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2433 = torch.aten.transpose.int %2432, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2434 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2435 = torch.aten.view %4, %2434 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2436 = torch.aten.mm %2435, %2433 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2437 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2438 = torch.aten.view %2436, %2437 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2439 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2440 = torch.aten.view %2424, %2439 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2441 = torch.aten.transpose.int %2440, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2442 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2443 = torch.aten.view %2431, %2442 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2444 = torch.aten.transpose.int %2443, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2445 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2446 = torch.aten.view %2438, %2445 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2447 = torch.aten.transpose.int %2446, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2448:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2441, %2444, %2447, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2449 = torch.aten.transpose.int %2448#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2450 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2451 = torch.aten.view %2449, %2450 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2452 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2453 = torch.aten.view %2451, %2452 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2454 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2455 = torch.aten.transpose.int %2454, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %2456 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2457 = torch.prims.convert_element_type %2456, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2458 = torch.prims.convert_element_type %2453, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2459 = torch.prims.convert_element_type %2455, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2460 = torch.aten.mm %2458, %2459 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2461 = torch.aten.mul.Scalar %2460, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2462 = torch.aten.mul.Scalar %2457, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2463 = torch.aten.add.Tensor %2461, %2462, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2464 = torch.prims.convert_element_type %2463, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2465 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2466 = torch.aten.view %2464, %2465 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2467 = torch.aten.div.Scalar %2466, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2468 = torch.aten.add.Tensor %2467, %2397, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2469 = torch.prims.convert_element_type %2468, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2470 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_77, %result1_78 = torch.aten.var_mean.correction %2469, %2470, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2471 = torch.aten.add.Scalar %result0_77, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2472 = torch.aten.rsqrt %2471 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2473 = torch.aten.sub.Tensor %2468, %result1_78, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2474 = torch.aten.mul.Tensor %2473, %2472 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %2475 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2476 = torch.aten.mul.Tensor %2474, %2475 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %2477 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2478 = torch.aten.add.Tensor %2476, %2477, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2479 = torch.prims.convert_element_type %2478, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2480 = torch.prims.convert_element_type %result1_78, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2481 = torch.prims.convert_element_type %2472, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2482 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2483 = torch.aten.view %2479, %2482 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2484 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2485 = torch.aten.transpose.int %2484, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %2486 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2487 = torch.prims.convert_element_type %2486, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2488 = torch.prims.convert_element_type %2483, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2489 = torch.prims.convert_element_type %2485, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2490 = torch.aten.mm %2488, %2489 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2491 = torch.aten.mul.Scalar %2490, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2492 = torch.aten.mul.Scalar %2487, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2493 = torch.aten.add.Tensor %2491, %2492, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2494 = torch.prims.convert_element_type %2493, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2495 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2496 = torch.aten.view %2494, %2495 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2497 = torch.aten.slice.Tensor %2496, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2498 = torch.aten.slice.Tensor %2496, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2499 = torch.aten.gelu %2498, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2500 = torch.aten.mul.Tensor %2497, %2499 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2501 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2502 = torch.aten.view %2500, %2501 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %2503 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2504 = torch.aten.transpose.int %2503, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %2505 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2506 = torch.prims.convert_element_type %2505, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2507 = torch.prims.convert_element_type %2502, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2508 = torch.prims.convert_element_type %2504, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2509 = torch.aten.mm %2507, %2508 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2510 = torch.aten.mul.Scalar %2509, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2511 = torch.aten.mul.Scalar %2506, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2512 = torch.aten.add.Tensor %2510, %2511, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2513 = torch.prims.convert_element_type %2512, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2514 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2515 = torch.aten.view %2513, %2514 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2516 = torch.aten.add.Tensor %2515, %2468, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2517 = torch.prims.convert_element_type %2516, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2518 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_79, %result1_80 = torch.aten.var_mean.correction %2517, %2518, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2519 = torch.aten.add.Scalar %result0_79, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2520 = torch.aten.rsqrt %2519 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2521 = torch.aten.sub.Tensor %2516, %result1_80, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2522 = torch.aten.mul.Tensor %2521, %2520 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %2523 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2524 = torch.aten.mul.Tensor %2522, %2523 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %2525 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2526 = torch.aten.add.Tensor %2524, %2525, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2527 = torch.prims.convert_element_type %2526, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2528 = torch.prims.convert_element_type %result1_80, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2529 = torch.prims.convert_element_type %2520, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %2530 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2531 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2532 = torch.aten.view %2527, %2531 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2533 = torch_c.to_builtin_tensor %2532 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2534 = torch_c.to_builtin_tensor %2530 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2535 = tensor.empty() : tensor<2048x1280xf32>
    %2536 = linalg.fill ins(%cst : f32) outs(%2535 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2537 = tensor.empty() : tensor<2048x1280xf32>
    %2538 = linalg.fill ins(%cst : f32) outs(%2537 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2539:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2536, %2538, %2533, %2534, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2536, %2538)
    %2540 = arith.truncf %2539#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2541 = torch_c.from_builtin_tensor %2540 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2542 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2543 = torch.aten.view %2541, %2542 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %2544 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2545 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2546 = torch.aten.view %2527, %2545 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2547 = torch_c.to_builtin_tensor %2546 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2548 = torch_c.to_builtin_tensor %2544 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2549 = tensor.empty() : tensor<2048x1280xf32>
    %2550 = linalg.fill ins(%cst : f32) outs(%2549 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2551 = tensor.empty() : tensor<2048x1280xf32>
    %2552 = linalg.fill ins(%cst : f32) outs(%2551 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2553:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2550, %2552, %2547, %2548, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2550, %2552)
    %2554 = arith.truncf %2553#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2555 = torch_c.from_builtin_tensor %2554 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2556 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2557 = torch.aten.view %2555, %2556 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %2558 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2559 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2560 = torch.aten.view %2527, %2559 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2561 = torch_c.to_builtin_tensor %2560 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2562 = torch_c.to_builtin_tensor %2558 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2563 = tensor.empty() : tensor<2048x1280xf32>
    %2564 = linalg.fill ins(%cst : f32) outs(%2563 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2565 = tensor.empty() : tensor<2048x1280xf32>
    %2566 = linalg.fill ins(%cst : f32) outs(%2565 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2567:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2564, %2566, %2561, %2562, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2564, %2566)
    %2568 = arith.truncf %2567#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2569 = torch_c.from_builtin_tensor %2568 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2570 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2571 = torch.aten.view %2569, %2570 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2572 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2573 = torch.aten.view %2543, %2572 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2574 = torch.aten.transpose.int %2573, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2575 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2576 = torch.aten.view %2557, %2575 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2577 = torch.aten.transpose.int %2576, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2578 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2579 = torch.aten.view %2571, %2578 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2580 = torch.aten.transpose.int %2579, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2581:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2574, %2577, %2580, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2582 = torch.aten.transpose.int %2581#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2583 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2584 = torch.aten.view %2582, %2583 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2585 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2586 = torch.aten.view %2584, %2585 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2587 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2588 = torch.aten.transpose.int %2587, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %2589 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2590 = torch.prims.convert_element_type %2589, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2591 = torch.prims.convert_element_type %2586, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2592 = torch.prims.convert_element_type %2588, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2593 = torch.aten.mm %2591, %2592 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2594 = torch.aten.mul.Scalar %2593, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2595 = torch.aten.mul.Scalar %2590, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2596 = torch.aten.add.Tensor %2594, %2595, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2597 = torch.prims.convert_element_type %2596, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2598 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2599 = torch.aten.view %2597, %2598 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2600 = torch.aten.div.Scalar %2599, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2601 = torch.aten.add.Tensor %2600, %2516, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2602 = torch.prims.convert_element_type %2601, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2603 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_81, %result1_82 = torch.aten.var_mean.correction %2602, %2603, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2604 = torch.aten.add.Scalar %result0_81, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2605 = torch.aten.rsqrt %2604 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2606 = torch.aten.sub.Tensor %2601, %result1_82, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2607 = torch.aten.mul.Tensor %2606, %2605 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %2608 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2609 = torch.aten.mul.Tensor %2607, %2608 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %2610 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2611 = torch.aten.add.Tensor %2609, %2610, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2612 = torch.prims.convert_element_type %2611, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2613 = torch.prims.convert_element_type %result1_82, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2614 = torch.prims.convert_element_type %2605, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %2615 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2616 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2617 = torch.aten.view %2612, %2616 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2618 = torch_c.to_builtin_tensor %2617 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2619 = torch_c.to_builtin_tensor %2615 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2620 = tensor.empty() : tensor<2048x1280xf32>
    %2621 = linalg.fill ins(%cst : f32) outs(%2620 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2622 = tensor.empty() : tensor<2048x1280xf32>
    %2623 = linalg.fill ins(%cst : f32) outs(%2622 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2624:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2621, %2623, %2618, %2619, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2621, %2623)
    %2625 = arith.truncf %2624#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2626 = torch_c.from_builtin_tensor %2625 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2627 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2628 = torch.aten.view %2626, %2627 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %2629 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2630 = torch.aten.transpose.int %2629, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2631 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2632 = torch.aten.view %4, %2631 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2633 = torch.aten.mm %2632, %2630 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2634 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2635 = torch.aten.view %2633, %2634 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %2636 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2637 = torch.aten.transpose.int %2636, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2638 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2639 = torch.aten.view %4, %2638 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2640 = torch.aten.mm %2639, %2637 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2641 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2642 = torch.aten.view %2640, %2641 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2643 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2644 = torch.aten.view %2628, %2643 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2645 = torch.aten.transpose.int %2644, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2646 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2647 = torch.aten.view %2635, %2646 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2648 = torch.aten.transpose.int %2647, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2649 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2650 = torch.aten.view %2642, %2649 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2651 = torch.aten.transpose.int %2650, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2652:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2645, %2648, %2651, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2653 = torch.aten.transpose.int %2652#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2654 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2655 = torch.aten.view %2653, %2654 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2656 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2657 = torch.aten.view %2655, %2656 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2658 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2659 = torch.aten.transpose.int %2658, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %2660 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2661 = torch.prims.convert_element_type %2660, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2662 = torch.prims.convert_element_type %2657, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2663 = torch.prims.convert_element_type %2659, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2664 = torch.aten.mm %2662, %2663 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2665 = torch.aten.mul.Scalar %2664, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2666 = torch.aten.mul.Scalar %2661, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2667 = torch.aten.add.Tensor %2665, %2666, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2668 = torch.prims.convert_element_type %2667, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2669 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2670 = torch.aten.view %2668, %2669 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2671 = torch.aten.div.Scalar %2670, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2672 = torch.aten.add.Tensor %2671, %2601, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2673 = torch.prims.convert_element_type %2672, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2674 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_83, %result1_84 = torch.aten.var_mean.correction %2673, %2674, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2675 = torch.aten.add.Scalar %result0_83, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2676 = torch.aten.rsqrt %2675 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2677 = torch.aten.sub.Tensor %2672, %result1_84, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2678 = torch.aten.mul.Tensor %2677, %2676 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %2679 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2680 = torch.aten.mul.Tensor %2678, %2679 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %2681 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2682 = torch.aten.add.Tensor %2680, %2681, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2683 = torch.prims.convert_element_type %2682, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2684 = torch.prims.convert_element_type %result1_84, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2685 = torch.prims.convert_element_type %2676, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2686 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2687 = torch.aten.view %2683, %2686 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2688 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2689 = torch.aten.transpose.int %2688, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %2690 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2691 = torch.prims.convert_element_type %2690, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2692 = torch.prims.convert_element_type %2687, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2693 = torch.prims.convert_element_type %2689, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2694 = torch.aten.mm %2692, %2693 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2695 = torch.aten.mul.Scalar %2694, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2696 = torch.aten.mul.Scalar %2691, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2697 = torch.aten.add.Tensor %2695, %2696, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2698 = torch.prims.convert_element_type %2697, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2699 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2700 = torch.aten.view %2698, %2699 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2701 = torch.aten.slice.Tensor %2700, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2702 = torch.aten.slice.Tensor %2700, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2703 = torch.aten.gelu %2702, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2704 = torch.aten.mul.Tensor %2701, %2703 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2705 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2706 = torch.aten.view %2704, %2705 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %2707 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2708 = torch.aten.transpose.int %2707, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %2709 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2710 = torch.prims.convert_element_type %2709, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2711 = torch.prims.convert_element_type %2706, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2712 = torch.prims.convert_element_type %2708, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2713 = torch.aten.mm %2711, %2712 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2714 = torch.aten.mul.Scalar %2713, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2715 = torch.aten.mul.Scalar %2710, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2716 = torch.aten.add.Tensor %2714, %2715, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2717 = torch.prims.convert_element_type %2716, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2718 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2719 = torch.aten.view %2717, %2718 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2720 = torch.aten.add.Tensor %2719, %2672, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2721 = torch.prims.convert_element_type %2720, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2722 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_85, %result1_86 = torch.aten.var_mean.correction %2721, %2722, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2723 = torch.aten.add.Scalar %result0_85, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2724 = torch.aten.rsqrt %2723 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2725 = torch.aten.sub.Tensor %2720, %result1_86, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2726 = torch.aten.mul.Tensor %2725, %2724 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %2727 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2728 = torch.aten.mul.Tensor %2726, %2727 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %2729 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2730 = torch.aten.add.Tensor %2728, %2729, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2731 = torch.prims.convert_element_type %2730, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2732 = torch.prims.convert_element_type %result1_86, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2733 = torch.prims.convert_element_type %2724, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %2734 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2735 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2736 = torch.aten.view %2731, %2735 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2737 = torch_c.to_builtin_tensor %2736 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2738 = torch_c.to_builtin_tensor %2734 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2739 = tensor.empty() : tensor<2048x1280xf32>
    %2740 = linalg.fill ins(%cst : f32) outs(%2739 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2741 = tensor.empty() : tensor<2048x1280xf32>
    %2742 = linalg.fill ins(%cst : f32) outs(%2741 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2743:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2740, %2742, %2737, %2738, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2740, %2742)
    %2744 = arith.truncf %2743#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2745 = torch_c.from_builtin_tensor %2744 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2746 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2747 = torch.aten.view %2745, %2746 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %2748 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2749 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2750 = torch.aten.view %2731, %2749 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2751 = torch_c.to_builtin_tensor %2750 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2752 = torch_c.to_builtin_tensor %2748 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2753 = tensor.empty() : tensor<2048x1280xf32>
    %2754 = linalg.fill ins(%cst : f32) outs(%2753 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2755 = tensor.empty() : tensor<2048x1280xf32>
    %2756 = linalg.fill ins(%cst : f32) outs(%2755 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2757:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2754, %2756, %2751, %2752, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2754, %2756)
    %2758 = arith.truncf %2757#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2759 = torch_c.from_builtin_tensor %2758 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2760 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2761 = torch.aten.view %2759, %2760 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %2762 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2763 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2764 = torch.aten.view %2731, %2763 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2765 = torch_c.to_builtin_tensor %2764 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2766 = torch_c.to_builtin_tensor %2762 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2767 = tensor.empty() : tensor<2048x1280xf32>
    %2768 = linalg.fill ins(%cst : f32) outs(%2767 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2769 = tensor.empty() : tensor<2048x1280xf32>
    %2770 = linalg.fill ins(%cst : f32) outs(%2769 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2771:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2768, %2770, %2765, %2766, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2768, %2770)
    %2772 = arith.truncf %2771#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2773 = torch_c.from_builtin_tensor %2772 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2774 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2775 = torch.aten.view %2773, %2774 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2776 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2777 = torch.aten.view %2747, %2776 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2778 = torch.aten.transpose.int %2777, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2779 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2780 = torch.aten.view %2761, %2779 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2781 = torch.aten.transpose.int %2780, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2782 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2783 = torch.aten.view %2775, %2782 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2784 = torch.aten.transpose.int %2783, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2785:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2778, %2781, %2784, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2786 = torch.aten.transpose.int %2785#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2787 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2788 = torch.aten.view %2786, %2787 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2789 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2790 = torch.aten.view %2788, %2789 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2791 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2792 = torch.aten.transpose.int %2791, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %2793 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2794 = torch.prims.convert_element_type %2793, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2795 = torch.prims.convert_element_type %2790, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2796 = torch.prims.convert_element_type %2792, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2797 = torch.aten.mm %2795, %2796 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2798 = torch.aten.mul.Scalar %2797, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2799 = torch.aten.mul.Scalar %2794, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2800 = torch.aten.add.Tensor %2798, %2799, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2801 = torch.prims.convert_element_type %2800, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2802 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2803 = torch.aten.view %2801, %2802 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2804 = torch.aten.div.Scalar %2803, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2805 = torch.aten.add.Tensor %2804, %2720, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2806 = torch.prims.convert_element_type %2805, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2807 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_87, %result1_88 = torch.aten.var_mean.correction %2806, %2807, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2808 = torch.aten.add.Scalar %result0_87, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2809 = torch.aten.rsqrt %2808 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2810 = torch.aten.sub.Tensor %2805, %result1_88, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2811 = torch.aten.mul.Tensor %2810, %2809 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %2812 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2813 = torch.aten.mul.Tensor %2811, %2812 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %2814 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2815 = torch.aten.add.Tensor %2813, %2814, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2816 = torch.prims.convert_element_type %2815, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2817 = torch.prims.convert_element_type %result1_88, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2818 = torch.prims.convert_element_type %2809, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %2819 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2820 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2821 = torch.aten.view %2816, %2820 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2822 = torch_c.to_builtin_tensor %2821 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2823 = torch_c.to_builtin_tensor %2819 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2824 = tensor.empty() : tensor<2048x1280xf32>
    %2825 = linalg.fill ins(%cst : f32) outs(%2824 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2826 = tensor.empty() : tensor<2048x1280xf32>
    %2827 = linalg.fill ins(%cst : f32) outs(%2826 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2828:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2825, %2827, %2822, %2823, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2825, %2827)
    %2829 = arith.truncf %2828#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2830 = torch_c.from_builtin_tensor %2829 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2831 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2832 = torch.aten.view %2830, %2831 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %2833 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2834 = torch.aten.transpose.int %2833, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2835 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2836 = torch.aten.view %4, %2835 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2837 = torch.aten.mm %2836, %2834 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2838 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2839 = torch.aten.view %2837, %2838 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %2840 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2841 = torch.aten.transpose.int %2840, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2842 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2843 = torch.aten.view %4, %2842 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2844 = torch.aten.mm %2843, %2841 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %2845 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2846 = torch.aten.view %2844, %2845 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2847 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2848 = torch.aten.view %2832, %2847 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2849 = torch.aten.transpose.int %2848, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2850 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2851 = torch.aten.view %2839, %2850 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2852 = torch.aten.transpose.int %2851, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2853 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2854 = torch.aten.view %2846, %2853 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2855 = torch.aten.transpose.int %2854, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2856:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2849, %2852, %2855, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2857 = torch.aten.transpose.int %2856#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2858 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2859 = torch.aten.view %2857, %2858 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2860 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2861 = torch.aten.view %2859, %2860 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2862 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2863 = torch.aten.transpose.int %2862, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %2864 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2865 = torch.prims.convert_element_type %2864, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2866 = torch.prims.convert_element_type %2861, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2867 = torch.prims.convert_element_type %2863, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2868 = torch.aten.mm %2866, %2867 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2869 = torch.aten.mul.Scalar %2868, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2870 = torch.aten.mul.Scalar %2865, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2871 = torch.aten.add.Tensor %2869, %2870, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2872 = torch.prims.convert_element_type %2871, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2873 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2874 = torch.aten.view %2872, %2873 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2875 = torch.aten.div.Scalar %2874, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2876 = torch.aten.add.Tensor %2875, %2805, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2877 = torch.prims.convert_element_type %2876, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2878 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_89, %result1_90 = torch.aten.var_mean.correction %2877, %2878, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2879 = torch.aten.add.Scalar %result0_89, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2880 = torch.aten.rsqrt %2879 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2881 = torch.aten.sub.Tensor %2876, %result1_90, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2882 = torch.aten.mul.Tensor %2881, %2880 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %2883 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2884 = torch.aten.mul.Tensor %2882, %2883 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %2885 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2886 = torch.aten.add.Tensor %2884, %2885, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2887 = torch.prims.convert_element_type %2886, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2888 = torch.prims.convert_element_type %result1_90, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2889 = torch.prims.convert_element_type %2880, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2890 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2891 = torch.aten.view %2887, %2890 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2892 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2893 = torch.aten.transpose.int %2892, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %2894 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2895 = torch.prims.convert_element_type %2894, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2896 = torch.prims.convert_element_type %2891, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2897 = torch.prims.convert_element_type %2893, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2898 = torch.aten.mm %2896, %2897 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2899 = torch.aten.mul.Scalar %2898, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2900 = torch.aten.mul.Scalar %2895, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2901 = torch.aten.add.Tensor %2899, %2900, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2902 = torch.prims.convert_element_type %2901, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2903 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2904 = torch.aten.view %2902, %2903 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2905 = torch.aten.slice.Tensor %2904, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2906 = torch.aten.slice.Tensor %2904, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2907 = torch.aten.gelu %2906, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2908 = torch.aten.mul.Tensor %2905, %2907 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2909 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2910 = torch.aten.view %2908, %2909 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %2911 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2912 = torch.aten.transpose.int %2911, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %2913 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2914 = torch.prims.convert_element_type %2913, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2915 = torch.prims.convert_element_type %2910, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2916 = torch.prims.convert_element_type %2912, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2917 = torch.aten.mm %2915, %2916 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2918 = torch.aten.mul.Scalar %2917, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2919 = torch.aten.mul.Scalar %2914, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2920 = torch.aten.add.Tensor %2918, %2919, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2921 = torch.prims.convert_element_type %2920, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2922 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2923 = torch.aten.view %2921, %2922 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2924 = torch.aten.add.Tensor %2923, %2876, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2925 = torch.prims.convert_element_type %2924, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2926 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_91, %result1_92 = torch.aten.var_mean.correction %2925, %2926, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2927 = torch.aten.add.Scalar %result0_91, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2928 = torch.aten.rsqrt %2927 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2929 = torch.aten.sub.Tensor %2924, %result1_92, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2930 = torch.aten.mul.Tensor %2929, %2928 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %2931 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2932 = torch.aten.mul.Tensor %2930, %2931 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %2933 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2934 = torch.aten.add.Tensor %2932, %2933, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2935 = torch.prims.convert_element_type %2934, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2936 = torch.prims.convert_element_type %result1_92, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2937 = torch.prims.convert_element_type %2928, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %2938 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2939 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2940 = torch.aten.view %2935, %2939 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2941 = torch_c.to_builtin_tensor %2940 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2942 = torch_c.to_builtin_tensor %2938 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2943 = tensor.empty() : tensor<2048x1280xf32>
    %2944 = linalg.fill ins(%cst : f32) outs(%2943 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2945 = tensor.empty() : tensor<2048x1280xf32>
    %2946 = linalg.fill ins(%cst : f32) outs(%2945 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2947:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2944, %2946, %2941, %2942, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2944, %2946)
    %2948 = arith.truncf %2947#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2949 = torch_c.from_builtin_tensor %2948 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2950 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2951 = torch.aten.view %2949, %2950 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %2952 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2953 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2954 = torch.aten.view %2935, %2953 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2955 = torch_c.to_builtin_tensor %2954 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2956 = torch_c.to_builtin_tensor %2952 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2957 = tensor.empty() : tensor<2048x1280xf32>
    %2958 = linalg.fill ins(%cst : f32) outs(%2957 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2959 = tensor.empty() : tensor<2048x1280xf32>
    %2960 = linalg.fill ins(%cst : f32) outs(%2959 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2961:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2958, %2960, %2955, %2956, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2958, %2960)
    %2962 = arith.truncf %2961#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2963 = torch_c.from_builtin_tensor %2962 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2964 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2965 = torch.aten.view %2963, %2964 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %2966 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2967 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2968 = torch.aten.view %2935, %2967 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2969 = torch_c.to_builtin_tensor %2968 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2970 = torch_c.to_builtin_tensor %2966 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2971 = tensor.empty() : tensor<2048x1280xf32>
    %2972 = linalg.fill ins(%cst : f32) outs(%2971 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2973 = tensor.empty() : tensor<2048x1280xf32>
    %2974 = linalg.fill ins(%cst : f32) outs(%2973 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2975:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2972, %2974, %2969, %2970, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2972, %2974)
    %2976 = arith.truncf %2975#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2977 = torch_c.from_builtin_tensor %2976 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2978 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2979 = torch.aten.view %2977, %2978 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2980 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2981 = torch.aten.view %2951, %2980 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2982 = torch.aten.transpose.int %2981, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2983 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2984 = torch.aten.view %2965, %2983 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2985 = torch.aten.transpose.int %2984, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2986 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2987 = torch.aten.view %2979, %2986 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2988 = torch.aten.transpose.int %2987, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2989:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2982, %2985, %2988, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2990 = torch.aten.transpose.int %2989#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2991 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2992 = torch.aten.view %2990, %2991 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2993 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2994 = torch.aten.view %2992, %2993 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2995 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2996 = torch.aten.transpose.int %2995, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %2997 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2998 = torch.prims.convert_element_type %2997, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2999 = torch.prims.convert_element_type %2994, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3000 = torch.prims.convert_element_type %2996, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3001 = torch.aten.mm %2999, %3000 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3002 = torch.aten.mul.Scalar %3001, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3003 = torch.aten.mul.Scalar %2998, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3004 = torch.aten.add.Tensor %3002, %3003, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3005 = torch.prims.convert_element_type %3004, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3006 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3007 = torch.aten.view %3005, %3006 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3008 = torch.aten.div.Scalar %3007, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3009 = torch.aten.add.Tensor %3008, %2924, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3010 = torch.prims.convert_element_type %3009, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3011 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_93, %result1_94 = torch.aten.var_mean.correction %3010, %3011, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3012 = torch.aten.add.Scalar %result0_93, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3013 = torch.aten.rsqrt %3012 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3014 = torch.aten.sub.Tensor %3009, %result1_94, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3015 = torch.aten.mul.Tensor %3014, %3013 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %3016 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3017 = torch.aten.mul.Tensor %3015, %3016 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %3018 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3019 = torch.aten.add.Tensor %3017, %3018, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3020 = torch.prims.convert_element_type %3019, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3021 = torch.prims.convert_element_type %result1_94, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3022 = torch.prims.convert_element_type %3013, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %3023 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3024 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3025 = torch.aten.view %3020, %3024 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3026 = torch_c.to_builtin_tensor %3025 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3027 = torch_c.to_builtin_tensor %3023 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3028 = tensor.empty() : tensor<2048x1280xf32>
    %3029 = linalg.fill ins(%cst : f32) outs(%3028 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3030 = tensor.empty() : tensor<2048x1280xf32>
    %3031 = linalg.fill ins(%cst : f32) outs(%3030 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3032:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3029, %3031, %3026, %3027, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3029, %3031)
    %3033 = arith.truncf %3032#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3034 = torch_c.from_builtin_tensor %3033 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3035 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3036 = torch.aten.view %3034, %3035 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %3037 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3038 = torch.aten.transpose.int %3037, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3039 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3040 = torch.aten.view %4, %3039 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3041 = torch.aten.mm %3040, %3038 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3042 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3043 = torch.aten.view %3041, %3042 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %3044 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3045 = torch.aten.transpose.int %3044, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3046 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3047 = torch.aten.view %4, %3046 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3048 = torch.aten.mm %3047, %3045 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3049 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3050 = torch.aten.view %3048, %3049 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3051 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3052 = torch.aten.view %3036, %3051 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3053 = torch.aten.transpose.int %3052, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3054 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3055 = torch.aten.view %3043, %3054 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3056 = torch.aten.transpose.int %3055, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3057 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3058 = torch.aten.view %3050, %3057 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3059 = torch.aten.transpose.int %3058, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3060:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3053, %3056, %3059, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3061 = torch.aten.transpose.int %3060#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3062 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3063 = torch.aten.view %3061, %3062 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3064 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3065 = torch.aten.view %3063, %3064 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3066 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3067 = torch.aten.transpose.int %3066, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %3068 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3069 = torch.prims.convert_element_type %3068, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3070 = torch.prims.convert_element_type %3065, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3071 = torch.prims.convert_element_type %3067, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3072 = torch.aten.mm %3070, %3071 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3073 = torch.aten.mul.Scalar %3072, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3074 = torch.aten.mul.Scalar %3069, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3075 = torch.aten.add.Tensor %3073, %3074, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3076 = torch.prims.convert_element_type %3075, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3077 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3078 = torch.aten.view %3076, %3077 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3079 = torch.aten.div.Scalar %3078, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3080 = torch.aten.add.Tensor %3079, %3009, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3081 = torch.prims.convert_element_type %3080, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3082 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_95, %result1_96 = torch.aten.var_mean.correction %3081, %3082, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3083 = torch.aten.add.Scalar %result0_95, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3084 = torch.aten.rsqrt %3083 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3085 = torch.aten.sub.Tensor %3080, %result1_96, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3086 = torch.aten.mul.Tensor %3085, %3084 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %3087 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3088 = torch.aten.mul.Tensor %3086, %3087 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %3089 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3090 = torch.aten.add.Tensor %3088, %3089, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3091 = torch.prims.convert_element_type %3090, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3092 = torch.prims.convert_element_type %result1_96, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3093 = torch.prims.convert_element_type %3084, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3094 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3095 = torch.aten.view %3091, %3094 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3096 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %3097 = torch.aten.transpose.int %3096, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %3098 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %3099 = torch.prims.convert_element_type %3098, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %3100 = torch.prims.convert_element_type %3095, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3101 = torch.prims.convert_element_type %3097, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3102 = torch.aten.mm %3100, %3101 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %3103 = torch.aten.mul.Scalar %3102, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3104 = torch.aten.mul.Scalar %3099, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %3105 = torch.aten.add.Tensor %3103, %3104, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3106 = torch.prims.convert_element_type %3105, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3107 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3108 = torch.aten.view %3106, %3107 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3109 = torch.aten.slice.Tensor %3108, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3110 = torch.aten.slice.Tensor %3108, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3111 = torch.aten.gelu %3110, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3112 = torch.aten.mul.Tensor %3109, %3111 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3113 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3114 = torch.aten.view %3112, %3113 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %3115 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3116 = torch.aten.transpose.int %3115, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %3117 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3118 = torch.prims.convert_element_type %3117, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3119 = torch.prims.convert_element_type %3114, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3120 = torch.prims.convert_element_type %3116, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3121 = torch.aten.mm %3119, %3120 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3122 = torch.aten.mul.Scalar %3121, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3123 = torch.aten.mul.Scalar %3118, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3124 = torch.aten.add.Tensor %3122, %3123, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3125 = torch.prims.convert_element_type %3124, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3126 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3127 = torch.aten.view %3125, %3126 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3128 = torch.aten.add.Tensor %3127, %3080, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3129 = torch.prims.convert_element_type %3128, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3130 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_97, %result1_98 = torch.aten.var_mean.correction %3129, %3130, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3131 = torch.aten.add.Scalar %result0_97, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3132 = torch.aten.rsqrt %3131 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3133 = torch.aten.sub.Tensor %3128, %result1_98, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3134 = torch.aten.mul.Tensor %3133, %3132 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %3135 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3136 = torch.aten.mul.Tensor %3134, %3135 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %3137 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3138 = torch.aten.add.Tensor %3136, %3137, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3139 = torch.prims.convert_element_type %3138, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3140 = torch.prims.convert_element_type %result1_98, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3141 = torch.prims.convert_element_type %3132, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %3142 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3143 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3144 = torch.aten.view %3139, %3143 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3145 = torch_c.to_builtin_tensor %3144 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3146 = torch_c.to_builtin_tensor %3142 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3147 = tensor.empty() : tensor<2048x1280xf32>
    %3148 = linalg.fill ins(%cst : f32) outs(%3147 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3149 = tensor.empty() : tensor<2048x1280xf32>
    %3150 = linalg.fill ins(%cst : f32) outs(%3149 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3151:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3148, %3150, %3145, %3146, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3148, %3150)
    %3152 = arith.truncf %3151#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3153 = torch_c.from_builtin_tensor %3152 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3154 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3155 = torch.aten.view %3153, %3154 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %3156 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3157 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3158 = torch.aten.view %3139, %3157 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3159 = torch_c.to_builtin_tensor %3158 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3160 = torch_c.to_builtin_tensor %3156 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3161 = tensor.empty() : tensor<2048x1280xf32>
    %3162 = linalg.fill ins(%cst : f32) outs(%3161 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3163 = tensor.empty() : tensor<2048x1280xf32>
    %3164 = linalg.fill ins(%cst : f32) outs(%3163 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3165:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3162, %3164, %3159, %3160, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3162, %3164)
    %3166 = arith.truncf %3165#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3167 = torch_c.from_builtin_tensor %3166 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3168 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3169 = torch.aten.view %3167, %3168 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %3170 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3171 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3172 = torch.aten.view %3139, %3171 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3173 = torch_c.to_builtin_tensor %3172 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3174 = torch_c.to_builtin_tensor %3170 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3175 = tensor.empty() : tensor<2048x1280xf32>
    %3176 = linalg.fill ins(%cst : f32) outs(%3175 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3177 = tensor.empty() : tensor<2048x1280xf32>
    %3178 = linalg.fill ins(%cst : f32) outs(%3177 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3179:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3176, %3178, %3173, %3174, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3176, %3178)
    %3180 = arith.truncf %3179#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3181 = torch_c.from_builtin_tensor %3180 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3182 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3183 = torch.aten.view %3181, %3182 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3184 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3185 = torch.aten.view %3155, %3184 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3186 = torch.aten.transpose.int %3185, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3187 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3188 = torch.aten.view %3169, %3187 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3189 = torch.aten.transpose.int %3188, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3190 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3191 = torch.aten.view %3183, %3190 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3192 = torch.aten.transpose.int %3191, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3193:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3186, %3189, %3192, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3194 = torch.aten.transpose.int %3193#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3195 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3196 = torch.aten.view %3194, %3195 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3197 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3198 = torch.aten.view %3196, %3197 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3199 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3200 = torch.aten.transpose.int %3199, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %3201 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3202 = torch.prims.convert_element_type %3201, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3203 = torch.prims.convert_element_type %3198, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3204 = torch.prims.convert_element_type %3200, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3205 = torch.aten.mm %3203, %3204 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3206 = torch.aten.mul.Scalar %3205, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3207 = torch.aten.mul.Scalar %3202, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3208 = torch.aten.add.Tensor %3206, %3207, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3209 = torch.prims.convert_element_type %3208, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3210 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3211 = torch.aten.view %3209, %3210 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3212 = torch.aten.div.Scalar %3211, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3213 = torch.aten.add.Tensor %3212, %3128, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3214 = torch.prims.convert_element_type %3213, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3215 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_99, %result1_100 = torch.aten.var_mean.correction %3214, %3215, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3216 = torch.aten.add.Scalar %result0_99, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3217 = torch.aten.rsqrt %3216 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3218 = torch.aten.sub.Tensor %3213, %result1_100, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3219 = torch.aten.mul.Tensor %3218, %3217 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %3220 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3221 = torch.aten.mul.Tensor %3219, %3220 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %3222 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3223 = torch.aten.add.Tensor %3221, %3222, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3224 = torch.prims.convert_element_type %3223, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3225 = torch.prims.convert_element_type %result1_100, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3226 = torch.prims.convert_element_type %3217, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %3227 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3228 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3229 = torch.aten.view %3224, %3228 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3230 = torch_c.to_builtin_tensor %3229 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3231 = torch_c.to_builtin_tensor %3227 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3232 = tensor.empty() : tensor<2048x1280xf32>
    %3233 = linalg.fill ins(%cst : f32) outs(%3232 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3234 = tensor.empty() : tensor<2048x1280xf32>
    %3235 = linalg.fill ins(%cst : f32) outs(%3234 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3236:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3233, %3235, %3230, %3231, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3233, %3235)
    %3237 = arith.truncf %3236#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3238 = torch_c.from_builtin_tensor %3237 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3239 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3240 = torch.aten.view %3238, %3239 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %3241 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3242 = torch.aten.transpose.int %3241, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3243 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3244 = torch.aten.view %4, %3243 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3245 = torch.aten.mm %3244, %3242 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3246 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3247 = torch.aten.view %3245, %3246 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %3248 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3249 = torch.aten.transpose.int %3248, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3250 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3251 = torch.aten.view %4, %3250 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3252 = torch.aten.mm %3251, %3249 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3253 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3254 = torch.aten.view %3252, %3253 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3255 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3256 = torch.aten.view %3240, %3255 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3257 = torch.aten.transpose.int %3256, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3258 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3259 = torch.aten.view %3247, %3258 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3260 = torch.aten.transpose.int %3259, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3261 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3262 = torch.aten.view %3254, %3261 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3263 = torch.aten.transpose.int %3262, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3264:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3257, %3260, %3263, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3265 = torch.aten.transpose.int %3264#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3266 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3267 = torch.aten.view %3265, %3266 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3268 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3269 = torch.aten.view %3267, %3268 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3270 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3271 = torch.aten.transpose.int %3270, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %3272 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3273 = torch.prims.convert_element_type %3272, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3274 = torch.prims.convert_element_type %3269, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3275 = torch.prims.convert_element_type %3271, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3276 = torch.aten.mm %3274, %3275 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3277 = torch.aten.mul.Scalar %3276, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3278 = torch.aten.mul.Scalar %3273, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3279 = torch.aten.add.Tensor %3277, %3278, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3280 = torch.prims.convert_element_type %3279, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3281 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3282 = torch.aten.view %3280, %3281 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3283 = torch.aten.div.Scalar %3282, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3284 = torch.aten.add.Tensor %3283, %3213, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3285 = torch.prims.convert_element_type %3284, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3286 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_101, %result1_102 = torch.aten.var_mean.correction %3285, %3286, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3287 = torch.aten.add.Scalar %result0_101, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3288 = torch.aten.rsqrt %3287 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3289 = torch.aten.sub.Tensor %3284, %result1_102, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3290 = torch.aten.mul.Tensor %3289, %3288 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %3291 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3292 = torch.aten.mul.Tensor %3290, %3291 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %3293 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3294 = torch.aten.add.Tensor %3292, %3293, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3295 = torch.prims.convert_element_type %3294, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3296 = torch.prims.convert_element_type %result1_102, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3297 = torch.prims.convert_element_type %3288, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3298 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3299 = torch.aten.view %3295, %3298 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3300 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %3301 = torch.aten.transpose.int %3300, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %3302 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %3303 = torch.prims.convert_element_type %3302, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %3304 = torch.prims.convert_element_type %3299, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3305 = torch.prims.convert_element_type %3301, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3306 = torch.aten.mm %3304, %3305 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %3307 = torch.aten.mul.Scalar %3306, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3308 = torch.aten.mul.Scalar %3303, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %3309 = torch.aten.add.Tensor %3307, %3308, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3310 = torch.prims.convert_element_type %3309, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3311 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3312 = torch.aten.view %3310, %3311 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3313 = torch.aten.slice.Tensor %3312, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3314 = torch.aten.slice.Tensor %3312, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3315 = torch.aten.gelu %3314, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3316 = torch.aten.mul.Tensor %3313, %3315 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3317 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3318 = torch.aten.view %3316, %3317 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %3319 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3320 = torch.aten.transpose.int %3319, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %3321 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3322 = torch.prims.convert_element_type %3321, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3323 = torch.prims.convert_element_type %3318, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3324 = torch.prims.convert_element_type %3320, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3325 = torch.aten.mm %3323, %3324 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3326 = torch.aten.mul.Scalar %3325, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3327 = torch.aten.mul.Scalar %3322, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3328 = torch.aten.add.Tensor %3326, %3327, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3329 = torch.prims.convert_element_type %3328, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3330 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3331 = torch.aten.view %3329, %3330 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3332 = torch.aten.add.Tensor %3331, %3284, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3333 = torch.prims.convert_element_type %3332, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3334 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_103, %result1_104 = torch.aten.var_mean.correction %3333, %3334, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3335 = torch.aten.add.Scalar %result0_103, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3336 = torch.aten.rsqrt %3335 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3337 = torch.aten.sub.Tensor %3332, %result1_104, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3338 = torch.aten.mul.Tensor %3337, %3336 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %3339 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3340 = torch.aten.mul.Tensor %3338, %3339 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %3341 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3342 = torch.aten.add.Tensor %3340, %3341, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3343 = torch.prims.convert_element_type %3342, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3344 = torch.prims.convert_element_type %result1_104, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3345 = torch.prims.convert_element_type %3336, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %3346 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3347 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3348 = torch.aten.view %3343, %3347 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3349 = torch_c.to_builtin_tensor %3348 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3350 = torch_c.to_builtin_tensor %3346 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3351 = tensor.empty() : tensor<2048x1280xf32>
    %3352 = linalg.fill ins(%cst : f32) outs(%3351 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3353 = tensor.empty() : tensor<2048x1280xf32>
    %3354 = linalg.fill ins(%cst : f32) outs(%3353 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3355:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3352, %3354, %3349, %3350, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3352, %3354)
    %3356 = arith.truncf %3355#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3357 = torch_c.from_builtin_tensor %3356 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3358 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3359 = torch.aten.view %3357, %3358 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %3360 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3361 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3362 = torch.aten.view %3343, %3361 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3363 = torch_c.to_builtin_tensor %3362 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3364 = torch_c.to_builtin_tensor %3360 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3365 = tensor.empty() : tensor<2048x1280xf32>
    %3366 = linalg.fill ins(%cst : f32) outs(%3365 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3367 = tensor.empty() : tensor<2048x1280xf32>
    %3368 = linalg.fill ins(%cst : f32) outs(%3367 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3369:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3366, %3368, %3363, %3364, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3366, %3368)
    %3370 = arith.truncf %3369#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3371 = torch_c.from_builtin_tensor %3370 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3372 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3373 = torch.aten.view %3371, %3372 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %3374 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3375 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3376 = torch.aten.view %3343, %3375 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3377 = torch_c.to_builtin_tensor %3376 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3378 = torch_c.to_builtin_tensor %3374 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3379 = tensor.empty() : tensor<2048x1280xf32>
    %3380 = linalg.fill ins(%cst : f32) outs(%3379 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3381 = tensor.empty() : tensor<2048x1280xf32>
    %3382 = linalg.fill ins(%cst : f32) outs(%3381 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3383:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3380, %3382, %3377, %3378, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3380, %3382)
    %3384 = arith.truncf %3383#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3385 = torch_c.from_builtin_tensor %3384 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3386 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3387 = torch.aten.view %3385, %3386 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3388 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3389 = torch.aten.view %3359, %3388 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3390 = torch.aten.transpose.int %3389, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3391 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3392 = torch.aten.view %3373, %3391 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3393 = torch.aten.transpose.int %3392, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3394 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3395 = torch.aten.view %3387, %3394 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3396 = torch.aten.transpose.int %3395, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3397:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3390, %3393, %3396, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3398 = torch.aten.transpose.int %3397#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3399 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3400 = torch.aten.view %3398, %3399 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3401 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3402 = torch.aten.view %3400, %3401 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3403 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3404 = torch.aten.transpose.int %3403, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %3405 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3406 = torch.prims.convert_element_type %3405, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3407 = torch.prims.convert_element_type %3402, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3408 = torch.prims.convert_element_type %3404, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3409 = torch.aten.mm %3407, %3408 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3410 = torch.aten.mul.Scalar %3409, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3411 = torch.aten.mul.Scalar %3406, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3412 = torch.aten.add.Tensor %3410, %3411, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3413 = torch.prims.convert_element_type %3412, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3414 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3415 = torch.aten.view %3413, %3414 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3416 = torch.aten.div.Scalar %3415, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3417 = torch.aten.add.Tensor %3416, %3332, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3418 = torch.prims.convert_element_type %3417, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3419 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_105, %result1_106 = torch.aten.var_mean.correction %3418, %3419, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3420 = torch.aten.add.Scalar %result0_105, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3421 = torch.aten.rsqrt %3420 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3422 = torch.aten.sub.Tensor %3417, %result1_106, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3423 = torch.aten.mul.Tensor %3422, %3421 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %3424 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3425 = torch.aten.mul.Tensor %3423, %3424 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %3426 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3427 = torch.aten.add.Tensor %3425, %3426, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3428 = torch.prims.convert_element_type %3427, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3429 = torch.prims.convert_element_type %result1_106, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3430 = torch.prims.convert_element_type %3421, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %3431 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3432 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3433 = torch.aten.view %3428, %3432 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3434 = torch_c.to_builtin_tensor %3433 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3435 = torch_c.to_builtin_tensor %3431 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3436 = tensor.empty() : tensor<2048x1280xf32>
    %3437 = linalg.fill ins(%cst : f32) outs(%3436 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3438 = tensor.empty() : tensor<2048x1280xf32>
    %3439 = linalg.fill ins(%cst : f32) outs(%3438 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3440:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3437, %3439, %3434, %3435, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3437, %3439)
    %3441 = arith.truncf %3440#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3442 = torch_c.from_builtin_tensor %3441 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3443 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3444 = torch.aten.view %3442, %3443 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %3445 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3446 = torch.aten.transpose.int %3445, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3447 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3448 = torch.aten.view %4, %3447 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3449 = torch.aten.mm %3448, %3446 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3450 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3451 = torch.aten.view %3449, %3450 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %3452 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3453 = torch.aten.transpose.int %3452, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3454 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3455 = torch.aten.view %4, %3454 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3456 = torch.aten.mm %3455, %3453 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3457 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3458 = torch.aten.view %3456, %3457 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3459 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3460 = torch.aten.view %3444, %3459 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3461 = torch.aten.transpose.int %3460, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3462 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3463 = torch.aten.view %3451, %3462 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3464 = torch.aten.transpose.int %3463, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3465 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3466 = torch.aten.view %3458, %3465 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3467 = torch.aten.transpose.int %3466, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3468:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3461, %3464, %3467, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3469 = torch.aten.transpose.int %3468#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3470 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3471 = torch.aten.view %3469, %3470 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3472 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3473 = torch.aten.view %3471, %3472 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3474 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3475 = torch.aten.transpose.int %3474, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %3476 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3477 = torch.prims.convert_element_type %3476, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3478 = torch.prims.convert_element_type %3473, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3479 = torch.prims.convert_element_type %3475, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3480 = torch.aten.mm %3478, %3479 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3481 = torch.aten.mul.Scalar %3480, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3482 = torch.aten.mul.Scalar %3477, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3483 = torch.aten.add.Tensor %3481, %3482, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3484 = torch.prims.convert_element_type %3483, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3485 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3486 = torch.aten.view %3484, %3485 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3487 = torch.aten.div.Scalar %3486, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3488 = torch.aten.add.Tensor %3487, %3417, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3489 = torch.prims.convert_element_type %3488, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3490 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_107, %result1_108 = torch.aten.var_mean.correction %3489, %3490, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3491 = torch.aten.add.Scalar %result0_107, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3492 = torch.aten.rsqrt %3491 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3493 = torch.aten.sub.Tensor %3488, %result1_108, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3494 = torch.aten.mul.Tensor %3493, %3492 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %3495 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3496 = torch.aten.mul.Tensor %3494, %3495 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %3497 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3498 = torch.aten.add.Tensor %3496, %3497, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3499 = torch.prims.convert_element_type %3498, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3500 = torch.prims.convert_element_type %result1_108, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3501 = torch.prims.convert_element_type %3492, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3502 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3503 = torch.aten.view %3499, %3502 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3504 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %3505 = torch.aten.transpose.int %3504, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %3506 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %3507 = torch.prims.convert_element_type %3506, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %3508 = torch.prims.convert_element_type %3503, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3509 = torch.prims.convert_element_type %3505, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3510 = torch.aten.mm %3508, %3509 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %3511 = torch.aten.mul.Scalar %3510, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3512 = torch.aten.mul.Scalar %3507, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %3513 = torch.aten.add.Tensor %3511, %3512, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3514 = torch.prims.convert_element_type %3513, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3515 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3516 = torch.aten.view %3514, %3515 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3517 = torch.aten.slice.Tensor %3516, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3518 = torch.aten.slice.Tensor %3516, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3519 = torch.aten.gelu %3518, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3520 = torch.aten.mul.Tensor %3517, %3519 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3521 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3522 = torch.aten.view %3520, %3521 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %3523 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3524 = torch.aten.transpose.int %3523, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %3525 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3526 = torch.prims.convert_element_type %3525, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3527 = torch.prims.convert_element_type %3522, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3528 = torch.prims.convert_element_type %3524, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3529 = torch.aten.mm %3527, %3528 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3530 = torch.aten.mul.Scalar %3529, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3531 = torch.aten.mul.Scalar %3526, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3532 = torch.aten.add.Tensor %3530, %3531, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3533 = torch.prims.convert_element_type %3532, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3534 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3535 = torch.aten.view %3533, %3534 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3536 = torch.aten.add.Tensor %3535, %3488, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3537 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3538 = torch.aten.view %3536, %3537 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_out.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %3539 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3540 = torch.aten.transpose.int %3539, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_out.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_out.bias : tensor<1280xf16>
    %3541 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3542 = torch.prims.convert_element_type %3541, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3543 = torch.prims.convert_element_type %3538, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3544 = torch.prims.convert_element_type %3540, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3545 = torch.aten.mm %3543, %3544 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3546 = torch.aten.mul.Scalar %3545, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3547 = torch.aten.mul.Scalar %3542, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3548 = torch.aten.add.Tensor %3546, %3547, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3549 = torch.prims.convert_element_type %3548, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3550 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3551 = torch.aten.view %3549, %3550 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3552 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3553 = torch.aten.view %3551, %3552 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %3554 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3555 = torch.aten.permute %3553, %3554 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %3556 = torch.aten.add.Tensor %3555, %1445, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3557 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3558 = torch.aten.view %3556, %3557 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %3559 = torch.prims.convert_element_type %3558, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3560 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_109, %result1_110 = torch.aten.var_mean.correction %3559, %3560, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %3561 = torch.aten.add.Scalar %result0_109, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3562 = torch.aten.rsqrt %3561 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %3563 = torch.aten.sub.Tensor %3558, %result1_110, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3564 = torch.aten.mul.Tensor %3563, %3562 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %3565 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3566 = torch.aten.view %3564, %3565 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.resnets.1.norm1.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.norm1.bias : tensor<1280xf16>
    %3567 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3568 = torch.aten.unsqueeze %3567, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3569 = torch.aten.unsqueeze %3568, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3570 = torch.aten.unsqueeze %3569, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.resnets.1.norm1.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.norm1.weight : tensor<1280xf16>
    %3571 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3572 = torch.aten.unsqueeze %3571, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3573 = torch.aten.unsqueeze %3572, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3574 = torch.aten.unsqueeze %3573, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3575 = torch.aten.mul.Tensor %3566, %3574 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %3576 = torch.aten.add.Tensor %3575, %3570, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %3577 = torch.prims.convert_element_type %3576, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3578 = torch.prims.convert_element_type %result1_110, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3579 = torch.prims.convert_element_type %3562, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3580 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3581 = torch.prims.squeeze %3578, %3580 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3582 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3583 = torch.prims.squeeze %3581, %3582 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3584 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3585 = torch.prims.squeeze %3579, %3584 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3586 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3587 = torch.prims.squeeze %3585, %3586 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3588 = torch.aten.silu %3577 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.1.conv1.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16>
    %3589 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.down_blocks.2.resnets.1.conv1.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.conv1.bias : tensor<1280xf16>
    %3590 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3591 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3592 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3593 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3594 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %3595 = torch.aten.convolution %3588, %3589, %3590, %3591, %3592, %3593, %false, %3594, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3596 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %3597 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3598 = torch.aten.transpose.int %3597, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %3599 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3600 = torch.prims.convert_element_type %3599, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3601 = torch.prims.convert_element_type %3596, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %3602 = torch.prims.convert_element_type %3598, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3603 = torch.aten.mm %3601, %3602 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %3604 = torch.aten.mul.Scalar %3603, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %3605 = torch.aten.mul.Scalar %3600, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3606 = torch.aten.add.Tensor %3604, %3605, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %3607 = torch.prims.convert_element_type %3606, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %3608 = torch.aten.unsqueeze %3607, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %3609 = torch.aten.unsqueeze %3608, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %3610 = torch.aten.add.Tensor %3595, %3609, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3611 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3612 = torch.aten.view %3610, %3611 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %3613 = torch.prims.convert_element_type %3612, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3614 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_111, %result1_112 = torch.aten.var_mean.correction %3613, %3614, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %3615 = torch.aten.add.Scalar %result0_111, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3616 = torch.aten.rsqrt %3615 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %3617 = torch.aten.sub.Tensor %3612, %result1_112, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3618 = torch.aten.mul.Tensor %3617, %3616 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %3619 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3620 = torch.aten.view %3618, %3619 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.resnets.1.norm2.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.norm2.bias : tensor<1280xf16>
    %3621 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3622 = torch.aten.unsqueeze %3621, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3623 = torch.aten.unsqueeze %3622, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3624 = torch.aten.unsqueeze %3623, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.resnets.1.norm2.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.norm2.weight : tensor<1280xf16>
    %3625 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3626 = torch.aten.unsqueeze %3625, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3627 = torch.aten.unsqueeze %3626, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3628 = torch.aten.unsqueeze %3627, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3629 = torch.aten.mul.Tensor %3620, %3628 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %3630 = torch.aten.add.Tensor %3629, %3624, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %3631 = torch.prims.convert_element_type %3630, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3632 = torch.prims.convert_element_type %result1_112, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3633 = torch.prims.convert_element_type %3616, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3634 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3635 = torch.prims.squeeze %3632, %3634 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3636 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3637 = torch.prims.squeeze %3635, %3636 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3638 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3639 = torch.prims.squeeze %3633, %3638 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3640 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3641 = torch.prims.squeeze %3639, %3640 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3642 = torch.aten.silu %3631 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.1.conv2.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %3643 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.down_blocks.2.resnets.1.conv2.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.conv2.bias : tensor<1280xf16>
    %3644 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3645 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3646 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3647 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3648 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %3649 = torch.aten.convolution %3642, %3643, %3644, %3645, %3646, %3647, %false, %3648, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3650 = torch.aten.add.Tensor %3556, %3649, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3651 = torch.aten.div.Scalar %3650, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %3652 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3653 = torch.aten.view %3651, %3652 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %3654 = torch.prims.convert_element_type %3653, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3655 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_113, %result1_114 = torch.aten.var_mean.correction %3654, %3655, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %3656 = torch.aten.add.Scalar %result0_113, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3657 = torch.aten.rsqrt %3656 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %3658 = torch.aten.sub.Tensor %3653, %result1_114, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3659 = torch.aten.mul.Tensor %3658, %3657 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %3660 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3661 = torch.aten.view %3659, %3660 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.attentions.1.norm.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.norm.bias : tensor<1280xf16>
    %3662 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3663 = torch.aten.unsqueeze %3662, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3664 = torch.aten.unsqueeze %3663, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3665 = torch.aten.unsqueeze %3664, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.attentions.1.norm.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.norm.weight : tensor<1280xf16>
    %3666 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3667 = torch.aten.unsqueeze %3666, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3668 = torch.aten.unsqueeze %3667, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3669 = torch.aten.unsqueeze %3668, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3670 = torch.aten.mul.Tensor %3661, %3669 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %3671 = torch.aten.add.Tensor %3670, %3665, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %3672 = torch.prims.convert_element_type %3671, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3673 = torch.prims.convert_element_type %result1_114, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3674 = torch.prims.convert_element_type %3657, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3675 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3676 = torch.prims.squeeze %3673, %3675 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3677 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3678 = torch.prims.squeeze %3676, %3677 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3679 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3680 = torch.prims.squeeze %3674, %3679 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3681 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3682 = torch.prims.squeeze %3680, %3681 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3683 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3684 = torch.aten.permute %3672, %3683 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %3685 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3686 = torch.aten.view %3684, %3685 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_in.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_in.weight : tensor<1280x1280xf16>
    %3687 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3688 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3689 = torch.aten._unsafe_view %3686, %3688 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3690 = torch_c.to_builtin_tensor %3689 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3691 = torch_c.to_builtin_tensor %3687 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3692 = tensor.empty() : tensor<2048x1280xf32>
    %3693 = linalg.fill ins(%cst : f32) outs(%3692 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3694 = tensor.empty() : tensor<2048x1280xf32>
    %3695 = linalg.fill ins(%cst : f32) outs(%3694 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3696:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3693, %3695, %3690, %3691, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3693, %3695)
    %3697 = arith.truncf %3696#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3698 = torch_c.from_builtin_tensor %3697 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3699 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3700 = torch.aten.view %3698, %3699 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_in.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_in.bias : tensor<1280xf16>
    %3701 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3702 = torch.aten.add.Tensor %3700, %3701, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3703 = torch.prims.convert_element_type %3702, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3704 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_115, %result1_116 = torch.aten.var_mean.correction %3703, %3704, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3705 = torch.aten.add.Scalar %result0_115, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3706 = torch.aten.rsqrt %3705 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3707 = torch.aten.sub.Tensor %3702, %result1_116, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3708 = torch.aten.mul.Tensor %3707, %3706 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %3709 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3710 = torch.aten.mul.Tensor %3708, %3709 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %3711 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3712 = torch.aten.add.Tensor %3710, %3711, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3713 = torch.prims.convert_element_type %3712, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3714 = torch.prims.convert_element_type %result1_116, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3715 = torch.prims.convert_element_type %3706, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %3716 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3717 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3718 = torch.aten.view %3713, %3717 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3719 = torch_c.to_builtin_tensor %3718 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3720 = torch_c.to_builtin_tensor %3716 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3721 = tensor.empty() : tensor<2048x1280xf32>
    %3722 = linalg.fill ins(%cst : f32) outs(%3721 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3723 = tensor.empty() : tensor<2048x1280xf32>
    %3724 = linalg.fill ins(%cst : f32) outs(%3723 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3725:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3722, %3724, %3719, %3720, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3722, %3724)
    %3726 = arith.truncf %3725#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3727 = torch_c.from_builtin_tensor %3726 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3728 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3729 = torch.aten.view %3727, %3728 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %3730 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3731 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3732 = torch.aten.view %3713, %3731 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3733 = torch_c.to_builtin_tensor %3732 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3734 = torch_c.to_builtin_tensor %3730 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3735 = tensor.empty() : tensor<2048x1280xf32>
    %3736 = linalg.fill ins(%cst : f32) outs(%3735 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3737 = tensor.empty() : tensor<2048x1280xf32>
    %3738 = linalg.fill ins(%cst : f32) outs(%3737 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3739:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3736, %3738, %3733, %3734, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3736, %3738)
    %3740 = arith.truncf %3739#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3741 = torch_c.from_builtin_tensor %3740 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3742 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3743 = torch.aten.view %3741, %3742 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %3744 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3745 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3746 = torch.aten.view %3713, %3745 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3747 = torch_c.to_builtin_tensor %3746 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3748 = torch_c.to_builtin_tensor %3744 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3749 = tensor.empty() : tensor<2048x1280xf32>
    %3750 = linalg.fill ins(%cst : f32) outs(%3749 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3751 = tensor.empty() : tensor<2048x1280xf32>
    %3752 = linalg.fill ins(%cst : f32) outs(%3751 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3753:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3750, %3752, %3747, %3748, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3750, %3752)
    %3754 = arith.truncf %3753#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3755 = torch_c.from_builtin_tensor %3754 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3756 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3757 = torch.aten.view %3755, %3756 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3758 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3759 = torch.aten.view %3729, %3758 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3760 = torch.aten.transpose.int %3759, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3761 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3762 = torch.aten.view %3743, %3761 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3763 = torch.aten.transpose.int %3762, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3764 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3765 = torch.aten.view %3757, %3764 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3766 = torch.aten.transpose.int %3765, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3767:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3760, %3763, %3766, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3768 = torch.aten.transpose.int %3767#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3769 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3770 = torch.aten.view %3768, %3769 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3771 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3772 = torch.aten.view %3770, %3771 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3773 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3774 = torch.aten.transpose.int %3773, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %3775 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3776 = torch.prims.convert_element_type %3775, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3777 = torch.prims.convert_element_type %3772, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3778 = torch.prims.convert_element_type %3774, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3779 = torch.aten.mm %3777, %3778 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3780 = torch.aten.mul.Scalar %3779, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3781 = torch.aten.mul.Scalar %3776, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3782 = torch.aten.add.Tensor %3780, %3781, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3783 = torch.prims.convert_element_type %3782, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3784 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3785 = torch.aten.view %3783, %3784 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3786 = torch.aten.div.Scalar %3785, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3787 = torch.aten.add.Tensor %3786, %3702, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3788 = torch.prims.convert_element_type %3787, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3789 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_117, %result1_118 = torch.aten.var_mean.correction %3788, %3789, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3790 = torch.aten.add.Scalar %result0_117, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3791 = torch.aten.rsqrt %3790 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3792 = torch.aten.sub.Tensor %3787, %result1_118, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3793 = torch.aten.mul.Tensor %3792, %3791 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %3794 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3795 = torch.aten.mul.Tensor %3793, %3794 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %3796 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3797 = torch.aten.add.Tensor %3795, %3796, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3798 = torch.prims.convert_element_type %3797, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3799 = torch.prims.convert_element_type %result1_118, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3800 = torch.prims.convert_element_type %3791, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %3801 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3802 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3803 = torch.aten.view %3798, %3802 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3804 = torch_c.to_builtin_tensor %3803 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3805 = torch_c.to_builtin_tensor %3801 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3806 = tensor.empty() : tensor<2048x1280xf32>
    %3807 = linalg.fill ins(%cst : f32) outs(%3806 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3808 = tensor.empty() : tensor<2048x1280xf32>
    %3809 = linalg.fill ins(%cst : f32) outs(%3808 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3810:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3807, %3809, %3804, %3805, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3807, %3809)
    %3811 = arith.truncf %3810#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3812 = torch_c.from_builtin_tensor %3811 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3813 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3814 = torch.aten.view %3812, %3813 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %3815 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3816 = torch.aten.transpose.int %3815, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3817 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3818 = torch.aten.view %4, %3817 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3819 = torch.aten.mm %3818, %3816 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3820 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3821 = torch.aten.view %3819, %3820 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %3822 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3823 = torch.aten.transpose.int %3822, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3824 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3825 = torch.aten.view %4, %3824 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3826 = torch.aten.mm %3825, %3823 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %3827 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3828 = torch.aten.view %3826, %3827 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3829 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3830 = torch.aten.view %3814, %3829 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3831 = torch.aten.transpose.int %3830, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3832 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3833 = torch.aten.view %3821, %3832 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3834 = torch.aten.transpose.int %3833, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3835 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3836 = torch.aten.view %3828, %3835 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3837 = torch.aten.transpose.int %3836, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3838:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3831, %3834, %3837, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3839 = torch.aten.transpose.int %3838#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3840 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3841 = torch.aten.view %3839, %3840 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3842 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3843 = torch.aten.view %3841, %3842 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3844 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3845 = torch.aten.transpose.int %3844, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %3846 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3847 = torch.prims.convert_element_type %3846, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3848 = torch.prims.convert_element_type %3843, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3849 = torch.prims.convert_element_type %3845, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3850 = torch.aten.mm %3848, %3849 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3851 = torch.aten.mul.Scalar %3850, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3852 = torch.aten.mul.Scalar %3847, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3853 = torch.aten.add.Tensor %3851, %3852, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3854 = torch.prims.convert_element_type %3853, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3855 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3856 = torch.aten.view %3854, %3855 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3857 = torch.aten.div.Scalar %3856, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3858 = torch.aten.add.Tensor %3857, %3787, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3859 = torch.prims.convert_element_type %3858, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3860 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_119, %result1_120 = torch.aten.var_mean.correction %3859, %3860, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3861 = torch.aten.add.Scalar %result0_119, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3862 = torch.aten.rsqrt %3861 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3863 = torch.aten.sub.Tensor %3858, %result1_120, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3864 = torch.aten.mul.Tensor %3863, %3862 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %3865 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3866 = torch.aten.mul.Tensor %3864, %3865 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %3867 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3868 = torch.aten.add.Tensor %3866, %3867, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3869 = torch.prims.convert_element_type %3868, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3870 = torch.prims.convert_element_type %result1_120, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3871 = torch.prims.convert_element_type %3862, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3872 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3873 = torch.aten.view %3869, %3872 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3874 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %3875 = torch.aten.transpose.int %3874, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %3876 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %3877 = torch.prims.convert_element_type %3876, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %3878 = torch.prims.convert_element_type %3873, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3879 = torch.prims.convert_element_type %3875, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3880 = torch.aten.mm %3878, %3879 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %3881 = torch.aten.mul.Scalar %3880, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3882 = torch.aten.mul.Scalar %3877, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %3883 = torch.aten.add.Tensor %3881, %3882, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3884 = torch.prims.convert_element_type %3883, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3885 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3886 = torch.aten.view %3884, %3885 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3887 = torch.aten.slice.Tensor %3886, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3888 = torch.aten.slice.Tensor %3886, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3889 = torch.aten.gelu %3888, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3890 = torch.aten.mul.Tensor %3887, %3889 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3891 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3892 = torch.aten.view %3890, %3891 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %3893 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3894 = torch.aten.transpose.int %3893, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %3895 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3896 = torch.prims.convert_element_type %3895, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3897 = torch.prims.convert_element_type %3892, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3898 = torch.prims.convert_element_type %3894, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3899 = torch.aten.mm %3897, %3898 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3900 = torch.aten.mul.Scalar %3899, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3901 = torch.aten.mul.Scalar %3896, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3902 = torch.aten.add.Tensor %3900, %3901, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3903 = torch.prims.convert_element_type %3902, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3904 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3905 = torch.aten.view %3903, %3904 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3906 = torch.aten.add.Tensor %3905, %3858, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3907 = torch.prims.convert_element_type %3906, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3908 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_121, %result1_122 = torch.aten.var_mean.correction %3907, %3908, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3909 = torch.aten.add.Scalar %result0_121, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3910 = torch.aten.rsqrt %3909 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3911 = torch.aten.sub.Tensor %3906, %result1_122, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3912 = torch.aten.mul.Tensor %3911, %3910 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %3913 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3914 = torch.aten.mul.Tensor %3912, %3913 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %3915 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3916 = torch.aten.add.Tensor %3914, %3915, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3917 = torch.prims.convert_element_type %3916, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3918 = torch.prims.convert_element_type %result1_122, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3919 = torch.prims.convert_element_type %3910, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %3920 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3921 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3922 = torch.aten.view %3917, %3921 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3923 = torch_c.to_builtin_tensor %3922 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3924 = torch_c.to_builtin_tensor %3920 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3925 = tensor.empty() : tensor<2048x1280xf32>
    %3926 = linalg.fill ins(%cst : f32) outs(%3925 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3927 = tensor.empty() : tensor<2048x1280xf32>
    %3928 = linalg.fill ins(%cst : f32) outs(%3927 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3929:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3926, %3928, %3923, %3924, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3926, %3928)
    %3930 = arith.truncf %3929#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3931 = torch_c.from_builtin_tensor %3930 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3932 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3933 = torch.aten.view %3931, %3932 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %3934 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3935 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3936 = torch.aten.view %3917, %3935 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3937 = torch_c.to_builtin_tensor %3936 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3938 = torch_c.to_builtin_tensor %3934 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3939 = tensor.empty() : tensor<2048x1280xf32>
    %3940 = linalg.fill ins(%cst : f32) outs(%3939 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3941 = tensor.empty() : tensor<2048x1280xf32>
    %3942 = linalg.fill ins(%cst : f32) outs(%3941 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3943:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3940, %3942, %3937, %3938, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3940, %3942)
    %3944 = arith.truncf %3943#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3945 = torch_c.from_builtin_tensor %3944 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3946 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3947 = torch.aten.view %3945, %3946 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %3948 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3949 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3950 = torch.aten.view %3917, %3949 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3951 = torch_c.to_builtin_tensor %3950 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3952 = torch_c.to_builtin_tensor %3948 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3953 = tensor.empty() : tensor<2048x1280xf32>
    %3954 = linalg.fill ins(%cst : f32) outs(%3953 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3955 = tensor.empty() : tensor<2048x1280xf32>
    %3956 = linalg.fill ins(%cst : f32) outs(%3955 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3957:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3954, %3956, %3951, %3952, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3954, %3956)
    %3958 = arith.truncf %3957#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3959 = torch_c.from_builtin_tensor %3958 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3960 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3961 = torch.aten.view %3959, %3960 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3962 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3963 = torch.aten.view %3933, %3962 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3964 = torch.aten.transpose.int %3963, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3965 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3966 = torch.aten.view %3947, %3965 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3967 = torch.aten.transpose.int %3966, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3968 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3969 = torch.aten.view %3961, %3968 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3970 = torch.aten.transpose.int %3969, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3971:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3964, %3967, %3970, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3972 = torch.aten.transpose.int %3971#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3973 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3974 = torch.aten.view %3972, %3973 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3975 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3976 = torch.aten.view %3974, %3975 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3977 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3978 = torch.aten.transpose.int %3977, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %3979 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3980 = torch.prims.convert_element_type %3979, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3981 = torch.prims.convert_element_type %3976, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3982 = torch.prims.convert_element_type %3978, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3983 = torch.aten.mm %3981, %3982 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3984 = torch.aten.mul.Scalar %3983, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3985 = torch.aten.mul.Scalar %3980, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3986 = torch.aten.add.Tensor %3984, %3985, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3987 = torch.prims.convert_element_type %3986, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3988 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3989 = torch.aten.view %3987, %3988 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3990 = torch.aten.div.Scalar %3989, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3991 = torch.aten.add.Tensor %3990, %3906, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3992 = torch.prims.convert_element_type %3991, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3993 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_123, %result1_124 = torch.aten.var_mean.correction %3992, %3993, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3994 = torch.aten.add.Scalar %result0_123, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3995 = torch.aten.rsqrt %3994 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3996 = torch.aten.sub.Tensor %3991, %result1_124, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3997 = torch.aten.mul.Tensor %3996, %3995 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %3998 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3999 = torch.aten.mul.Tensor %3997, %3998 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %4000 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4001 = torch.aten.add.Tensor %3999, %4000, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4002 = torch.prims.convert_element_type %4001, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4003 = torch.prims.convert_element_type %result1_124, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4004 = torch.prims.convert_element_type %3995, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %4005 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4006 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4007 = torch.aten.view %4002, %4006 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4008 = torch_c.to_builtin_tensor %4007 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4009 = torch_c.to_builtin_tensor %4005 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4010 = tensor.empty() : tensor<2048x1280xf32>
    %4011 = linalg.fill ins(%cst : f32) outs(%4010 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4012 = tensor.empty() : tensor<2048x1280xf32>
    %4013 = linalg.fill ins(%cst : f32) outs(%4012 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4014:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4011, %4013, %4008, %4009, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4011, %4013)
    %4015 = arith.truncf %4014#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4016 = torch_c.from_builtin_tensor %4015 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4017 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4018 = torch.aten.view %4016, %4017 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %4019 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4020 = torch.aten.transpose.int %4019, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4021 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4022 = torch.aten.view %4, %4021 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4023 = torch.aten.mm %4022, %4020 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4024 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4025 = torch.aten.view %4023, %4024 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %4026 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4027 = torch.aten.transpose.int %4026, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4028 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4029 = torch.aten.view %4, %4028 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4030 = torch.aten.mm %4029, %4027 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4031 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4032 = torch.aten.view %4030, %4031 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4033 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4034 = torch.aten.view %4018, %4033 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4035 = torch.aten.transpose.int %4034, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4036 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4037 = torch.aten.view %4025, %4036 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4038 = torch.aten.transpose.int %4037, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4039 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4040 = torch.aten.view %4032, %4039 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4041 = torch.aten.transpose.int %4040, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4042:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4035, %4038, %4041, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4043 = torch.aten.transpose.int %4042#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4044 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4045 = torch.aten.view %4043, %4044 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4046 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4047 = torch.aten.view %4045, %4046 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4048 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4049 = torch.aten.transpose.int %4048, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %4050 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4051 = torch.prims.convert_element_type %4050, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4052 = torch.prims.convert_element_type %4047, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4053 = torch.prims.convert_element_type %4049, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4054 = torch.aten.mm %4052, %4053 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4055 = torch.aten.mul.Scalar %4054, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4056 = torch.aten.mul.Scalar %4051, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4057 = torch.aten.add.Tensor %4055, %4056, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4058 = torch.prims.convert_element_type %4057, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4059 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4060 = torch.aten.view %4058, %4059 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4061 = torch.aten.div.Scalar %4060, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4062 = torch.aten.add.Tensor %4061, %3991, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4063 = torch.prims.convert_element_type %4062, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4064 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_125, %result1_126 = torch.aten.var_mean.correction %4063, %4064, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4065 = torch.aten.add.Scalar %result0_125, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4066 = torch.aten.rsqrt %4065 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4067 = torch.aten.sub.Tensor %4062, %result1_126, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4068 = torch.aten.mul.Tensor %4067, %4066 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %4069 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4070 = torch.aten.mul.Tensor %4068, %4069 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %4071 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4072 = torch.aten.add.Tensor %4070, %4071, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4073 = torch.prims.convert_element_type %4072, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4074 = torch.prims.convert_element_type %result1_126, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4075 = torch.prims.convert_element_type %4066, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4076 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4077 = torch.aten.view %4073, %4076 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4078 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4079 = torch.aten.transpose.int %4078, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %4080 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4081 = torch.prims.convert_element_type %4080, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4082 = torch.prims.convert_element_type %4077, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4083 = torch.prims.convert_element_type %4079, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4084 = torch.aten.mm %4082, %4083 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4085 = torch.aten.mul.Scalar %4084, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4086 = torch.aten.mul.Scalar %4081, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4087 = torch.aten.add.Tensor %4085, %4086, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4088 = torch.prims.convert_element_type %4087, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4089 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4090 = torch.aten.view %4088, %4089 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4091 = torch.aten.slice.Tensor %4090, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4092 = torch.aten.slice.Tensor %4090, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4093 = torch.aten.gelu %4092, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4094 = torch.aten.mul.Tensor %4091, %4093 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4095 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4096 = torch.aten.view %4094, %4095 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %4097 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4098 = torch.aten.transpose.int %4097, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %4099 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4100 = torch.prims.convert_element_type %4099, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4101 = torch.prims.convert_element_type %4096, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4102 = torch.prims.convert_element_type %4098, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4103 = torch.aten.mm %4101, %4102 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4104 = torch.aten.mul.Scalar %4103, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4105 = torch.aten.mul.Scalar %4100, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4106 = torch.aten.add.Tensor %4104, %4105, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4107 = torch.prims.convert_element_type %4106, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4108 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4109 = torch.aten.view %4107, %4108 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4110 = torch.aten.add.Tensor %4109, %4062, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4111 = torch.prims.convert_element_type %4110, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4112 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_127, %result1_128 = torch.aten.var_mean.correction %4111, %4112, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4113 = torch.aten.add.Scalar %result0_127, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4114 = torch.aten.rsqrt %4113 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4115 = torch.aten.sub.Tensor %4110, %result1_128, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4116 = torch.aten.mul.Tensor %4115, %4114 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %4117 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4118 = torch.aten.mul.Tensor %4116, %4117 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %4119 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4120 = torch.aten.add.Tensor %4118, %4119, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4121 = torch.prims.convert_element_type %4120, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4122 = torch.prims.convert_element_type %result1_128, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4123 = torch.prims.convert_element_type %4114, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %4124 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4125 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4126 = torch.aten.view %4121, %4125 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4127 = torch_c.to_builtin_tensor %4126 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4128 = torch_c.to_builtin_tensor %4124 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4129 = tensor.empty() : tensor<2048x1280xf32>
    %4130 = linalg.fill ins(%cst : f32) outs(%4129 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4131 = tensor.empty() : tensor<2048x1280xf32>
    %4132 = linalg.fill ins(%cst : f32) outs(%4131 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4133:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4130, %4132, %4127, %4128, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4130, %4132)
    %4134 = arith.truncf %4133#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4135 = torch_c.from_builtin_tensor %4134 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4136 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4137 = torch.aten.view %4135, %4136 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %4138 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4139 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4140 = torch.aten.view %4121, %4139 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4141 = torch_c.to_builtin_tensor %4140 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4142 = torch_c.to_builtin_tensor %4138 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4143 = tensor.empty() : tensor<2048x1280xf32>
    %4144 = linalg.fill ins(%cst : f32) outs(%4143 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4145 = tensor.empty() : tensor<2048x1280xf32>
    %4146 = linalg.fill ins(%cst : f32) outs(%4145 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4147:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4144, %4146, %4141, %4142, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4144, %4146)
    %4148 = arith.truncf %4147#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4149 = torch_c.from_builtin_tensor %4148 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4150 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4151 = torch.aten.view %4149, %4150 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %4152 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4153 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4154 = torch.aten.view %4121, %4153 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4155 = torch_c.to_builtin_tensor %4154 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4156 = torch_c.to_builtin_tensor %4152 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4157 = tensor.empty() : tensor<2048x1280xf32>
    %4158 = linalg.fill ins(%cst : f32) outs(%4157 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4159 = tensor.empty() : tensor<2048x1280xf32>
    %4160 = linalg.fill ins(%cst : f32) outs(%4159 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4161:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4158, %4160, %4155, %4156, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4158, %4160)
    %4162 = arith.truncf %4161#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4163 = torch_c.from_builtin_tensor %4162 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4164 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4165 = torch.aten.view %4163, %4164 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4166 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4167 = torch.aten.view %4137, %4166 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4168 = torch.aten.transpose.int %4167, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4169 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4170 = torch.aten.view %4151, %4169 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4171 = torch.aten.transpose.int %4170, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4172 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4173 = torch.aten.view %4165, %4172 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4174 = torch.aten.transpose.int %4173, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4175:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4168, %4171, %4174, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4176 = torch.aten.transpose.int %4175#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4177 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4178 = torch.aten.view %4176, %4177 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4179 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4180 = torch.aten.view %4178, %4179 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4181 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4182 = torch.aten.transpose.int %4181, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %4183 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4184 = torch.prims.convert_element_type %4183, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4185 = torch.prims.convert_element_type %4180, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4186 = torch.prims.convert_element_type %4182, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4187 = torch.aten.mm %4185, %4186 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4188 = torch.aten.mul.Scalar %4187, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4189 = torch.aten.mul.Scalar %4184, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4190 = torch.aten.add.Tensor %4188, %4189, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4191 = torch.prims.convert_element_type %4190, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4192 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4193 = torch.aten.view %4191, %4192 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4194 = torch.aten.div.Scalar %4193, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4195 = torch.aten.add.Tensor %4194, %4110, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4196 = torch.prims.convert_element_type %4195, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4197 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_129, %result1_130 = torch.aten.var_mean.correction %4196, %4197, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4198 = torch.aten.add.Scalar %result0_129, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4199 = torch.aten.rsqrt %4198 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4200 = torch.aten.sub.Tensor %4195, %result1_130, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4201 = torch.aten.mul.Tensor %4200, %4199 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %4202 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4203 = torch.aten.mul.Tensor %4201, %4202 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %4204 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4205 = torch.aten.add.Tensor %4203, %4204, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4206 = torch.prims.convert_element_type %4205, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4207 = torch.prims.convert_element_type %result1_130, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4208 = torch.prims.convert_element_type %4199, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %4209 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4210 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4211 = torch.aten.view %4206, %4210 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4212 = torch_c.to_builtin_tensor %4211 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4213 = torch_c.to_builtin_tensor %4209 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4214 = tensor.empty() : tensor<2048x1280xf32>
    %4215 = linalg.fill ins(%cst : f32) outs(%4214 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4216 = tensor.empty() : tensor<2048x1280xf32>
    %4217 = linalg.fill ins(%cst : f32) outs(%4216 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4218:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4215, %4217, %4212, %4213, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4215, %4217)
    %4219 = arith.truncf %4218#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4220 = torch_c.from_builtin_tensor %4219 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4221 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4222 = torch.aten.view %4220, %4221 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %4223 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4224 = torch.aten.transpose.int %4223, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4225 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4226 = torch.aten.view %4, %4225 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4227 = torch.aten.mm %4226, %4224 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4228 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4229 = torch.aten.view %4227, %4228 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %4230 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4231 = torch.aten.transpose.int %4230, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4232 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4233 = torch.aten.view %4, %4232 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4234 = torch.aten.mm %4233, %4231 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4235 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4236 = torch.aten.view %4234, %4235 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4237 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4238 = torch.aten.view %4222, %4237 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4239 = torch.aten.transpose.int %4238, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4240 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4241 = torch.aten.view %4229, %4240 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4242 = torch.aten.transpose.int %4241, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4243 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4244 = torch.aten.view %4236, %4243 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4245 = torch.aten.transpose.int %4244, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4246:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4239, %4242, %4245, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4247 = torch.aten.transpose.int %4246#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4248 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4249 = torch.aten.view %4247, %4248 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4250 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4251 = torch.aten.view %4249, %4250 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4252 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4253 = torch.aten.transpose.int %4252, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %4254 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4255 = torch.prims.convert_element_type %4254, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4256 = torch.prims.convert_element_type %4251, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4257 = torch.prims.convert_element_type %4253, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4258 = torch.aten.mm %4256, %4257 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4259 = torch.aten.mul.Scalar %4258, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4260 = torch.aten.mul.Scalar %4255, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4261 = torch.aten.add.Tensor %4259, %4260, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4262 = torch.prims.convert_element_type %4261, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4263 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4264 = torch.aten.view %4262, %4263 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4265 = torch.aten.div.Scalar %4264, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4266 = torch.aten.add.Tensor %4265, %4195, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4267 = torch.prims.convert_element_type %4266, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4268 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_131, %result1_132 = torch.aten.var_mean.correction %4267, %4268, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4269 = torch.aten.add.Scalar %result0_131, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4270 = torch.aten.rsqrt %4269 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4271 = torch.aten.sub.Tensor %4266, %result1_132, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4272 = torch.aten.mul.Tensor %4271, %4270 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %4273 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4274 = torch.aten.mul.Tensor %4272, %4273 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %4275 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4276 = torch.aten.add.Tensor %4274, %4275, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4277 = torch.prims.convert_element_type %4276, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4278 = torch.prims.convert_element_type %result1_132, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4279 = torch.prims.convert_element_type %4270, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4280 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4281 = torch.aten.view %4277, %4280 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4282 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4283 = torch.aten.transpose.int %4282, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %4284 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4285 = torch.prims.convert_element_type %4284, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4286 = torch.prims.convert_element_type %4281, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4287 = torch.prims.convert_element_type %4283, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4288 = torch.aten.mm %4286, %4287 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4289 = torch.aten.mul.Scalar %4288, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4290 = torch.aten.mul.Scalar %4285, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4291 = torch.aten.add.Tensor %4289, %4290, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4292 = torch.prims.convert_element_type %4291, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4293 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4294 = torch.aten.view %4292, %4293 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4295 = torch.aten.slice.Tensor %4294, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4296 = torch.aten.slice.Tensor %4294, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4297 = torch.aten.gelu %4296, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4298 = torch.aten.mul.Tensor %4295, %4297 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4299 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4300 = torch.aten.view %4298, %4299 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %4301 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4302 = torch.aten.transpose.int %4301, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %4303 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4304 = torch.prims.convert_element_type %4303, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4305 = torch.prims.convert_element_type %4300, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4306 = torch.prims.convert_element_type %4302, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4307 = torch.aten.mm %4305, %4306 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4308 = torch.aten.mul.Scalar %4307, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4309 = torch.aten.mul.Scalar %4304, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4310 = torch.aten.add.Tensor %4308, %4309, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4311 = torch.prims.convert_element_type %4310, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4312 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4313 = torch.aten.view %4311, %4312 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4314 = torch.aten.add.Tensor %4313, %4266, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4315 = torch.prims.convert_element_type %4314, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4316 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_133, %result1_134 = torch.aten.var_mean.correction %4315, %4316, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4317 = torch.aten.add.Scalar %result0_133, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4318 = torch.aten.rsqrt %4317 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4319 = torch.aten.sub.Tensor %4314, %result1_134, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4320 = torch.aten.mul.Tensor %4319, %4318 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %4321 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4322 = torch.aten.mul.Tensor %4320, %4321 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %4323 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4324 = torch.aten.add.Tensor %4322, %4323, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4325 = torch.prims.convert_element_type %4324, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4326 = torch.prims.convert_element_type %result1_134, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4327 = torch.prims.convert_element_type %4318, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %4328 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4329 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4330 = torch.aten.view %4325, %4329 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4331 = torch_c.to_builtin_tensor %4330 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4332 = torch_c.to_builtin_tensor %4328 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4333 = tensor.empty() : tensor<2048x1280xf32>
    %4334 = linalg.fill ins(%cst : f32) outs(%4333 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4335 = tensor.empty() : tensor<2048x1280xf32>
    %4336 = linalg.fill ins(%cst : f32) outs(%4335 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4337:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4334, %4336, %4331, %4332, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4334, %4336)
    %4338 = arith.truncf %4337#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4339 = torch_c.from_builtin_tensor %4338 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4340 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4341 = torch.aten.view %4339, %4340 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %4342 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4343 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4344 = torch.aten.view %4325, %4343 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4345 = torch_c.to_builtin_tensor %4344 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4346 = torch_c.to_builtin_tensor %4342 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4347 = tensor.empty() : tensor<2048x1280xf32>
    %4348 = linalg.fill ins(%cst : f32) outs(%4347 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4349 = tensor.empty() : tensor<2048x1280xf32>
    %4350 = linalg.fill ins(%cst : f32) outs(%4349 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4351:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4348, %4350, %4345, %4346, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4348, %4350)
    %4352 = arith.truncf %4351#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4353 = torch_c.from_builtin_tensor %4352 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4354 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4355 = torch.aten.view %4353, %4354 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %4356 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4357 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4358 = torch.aten.view %4325, %4357 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4359 = torch_c.to_builtin_tensor %4358 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4360 = torch_c.to_builtin_tensor %4356 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4361 = tensor.empty() : tensor<2048x1280xf32>
    %4362 = linalg.fill ins(%cst : f32) outs(%4361 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4363 = tensor.empty() : tensor<2048x1280xf32>
    %4364 = linalg.fill ins(%cst : f32) outs(%4363 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4365:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4362, %4364, %4359, %4360, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4362, %4364)
    %4366 = arith.truncf %4365#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4367 = torch_c.from_builtin_tensor %4366 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4368 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4369 = torch.aten.view %4367, %4368 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4370 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4371 = torch.aten.view %4341, %4370 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4372 = torch.aten.transpose.int %4371, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4373 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4374 = torch.aten.view %4355, %4373 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4375 = torch.aten.transpose.int %4374, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4376 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4377 = torch.aten.view %4369, %4376 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4378 = torch.aten.transpose.int %4377, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4379:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4372, %4375, %4378, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4380 = torch.aten.transpose.int %4379#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4381 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4382 = torch.aten.view %4380, %4381 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4383 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4384 = torch.aten.view %4382, %4383 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4385 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4386 = torch.aten.transpose.int %4385, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %4387 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4388 = torch.prims.convert_element_type %4387, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4389 = torch.prims.convert_element_type %4384, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4390 = torch.prims.convert_element_type %4386, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4391 = torch.aten.mm %4389, %4390 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4392 = torch.aten.mul.Scalar %4391, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4393 = torch.aten.mul.Scalar %4388, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4394 = torch.aten.add.Tensor %4392, %4393, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4395 = torch.prims.convert_element_type %4394, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4396 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4397 = torch.aten.view %4395, %4396 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4398 = torch.aten.div.Scalar %4397, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4399 = torch.aten.add.Tensor %4398, %4314, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4400 = torch.prims.convert_element_type %4399, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4401 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_135, %result1_136 = torch.aten.var_mean.correction %4400, %4401, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4402 = torch.aten.add.Scalar %result0_135, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4403 = torch.aten.rsqrt %4402 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4404 = torch.aten.sub.Tensor %4399, %result1_136, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4405 = torch.aten.mul.Tensor %4404, %4403 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %4406 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4407 = torch.aten.mul.Tensor %4405, %4406 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %4408 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4409 = torch.aten.add.Tensor %4407, %4408, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4410 = torch.prims.convert_element_type %4409, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4411 = torch.prims.convert_element_type %result1_136, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4412 = torch.prims.convert_element_type %4403, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %4413 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4414 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4415 = torch.aten.view %4410, %4414 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4416 = torch_c.to_builtin_tensor %4415 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4417 = torch_c.to_builtin_tensor %4413 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4418 = tensor.empty() : tensor<2048x1280xf32>
    %4419 = linalg.fill ins(%cst : f32) outs(%4418 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4420 = tensor.empty() : tensor<2048x1280xf32>
    %4421 = linalg.fill ins(%cst : f32) outs(%4420 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4422:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4419, %4421, %4416, %4417, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4419, %4421)
    %4423 = arith.truncf %4422#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4424 = torch_c.from_builtin_tensor %4423 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4425 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4426 = torch.aten.view %4424, %4425 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %4427 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4428 = torch.aten.transpose.int %4427, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4429 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4430 = torch.aten.view %4, %4429 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4431 = torch.aten.mm %4430, %4428 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4432 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4433 = torch.aten.view %4431, %4432 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %4434 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4435 = torch.aten.transpose.int %4434, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4436 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4437 = torch.aten.view %4, %4436 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4438 = torch.aten.mm %4437, %4435 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4439 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4440 = torch.aten.view %4438, %4439 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4441 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4442 = torch.aten.view %4426, %4441 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4443 = torch.aten.transpose.int %4442, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4444 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4445 = torch.aten.view %4433, %4444 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4446 = torch.aten.transpose.int %4445, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4447 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4448 = torch.aten.view %4440, %4447 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4449 = torch.aten.transpose.int %4448, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4450:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4443, %4446, %4449, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4451 = torch.aten.transpose.int %4450#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4452 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4453 = torch.aten.view %4451, %4452 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4454 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4455 = torch.aten.view %4453, %4454 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4456 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4457 = torch.aten.transpose.int %4456, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %4458 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4459 = torch.prims.convert_element_type %4458, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4460 = torch.prims.convert_element_type %4455, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4461 = torch.prims.convert_element_type %4457, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4462 = torch.aten.mm %4460, %4461 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4463 = torch.aten.mul.Scalar %4462, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4464 = torch.aten.mul.Scalar %4459, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4465 = torch.aten.add.Tensor %4463, %4464, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4466 = torch.prims.convert_element_type %4465, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4467 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4468 = torch.aten.view %4466, %4467 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4469 = torch.aten.div.Scalar %4468, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4470 = torch.aten.add.Tensor %4469, %4399, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4471 = torch.prims.convert_element_type %4470, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4472 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_137, %result1_138 = torch.aten.var_mean.correction %4471, %4472, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4473 = torch.aten.add.Scalar %result0_137, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4474 = torch.aten.rsqrt %4473 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4475 = torch.aten.sub.Tensor %4470, %result1_138, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4476 = torch.aten.mul.Tensor %4475, %4474 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %4477 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4478 = torch.aten.mul.Tensor %4476, %4477 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %4479 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4480 = torch.aten.add.Tensor %4478, %4479, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4481 = torch.prims.convert_element_type %4480, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4482 = torch.prims.convert_element_type %result1_138, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4483 = torch.prims.convert_element_type %4474, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4484 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4485 = torch.aten.view %4481, %4484 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4486 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4487 = torch.aten.transpose.int %4486, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %4488 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4489 = torch.prims.convert_element_type %4488, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4490 = torch.prims.convert_element_type %4485, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4491 = torch.prims.convert_element_type %4487, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4492 = torch.aten.mm %4490, %4491 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4493 = torch.aten.mul.Scalar %4492, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4494 = torch.aten.mul.Scalar %4489, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4495 = torch.aten.add.Tensor %4493, %4494, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4496 = torch.prims.convert_element_type %4495, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4497 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4498 = torch.aten.view %4496, %4497 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4499 = torch.aten.slice.Tensor %4498, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4500 = torch.aten.slice.Tensor %4498, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4501 = torch.aten.gelu %4500, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4502 = torch.aten.mul.Tensor %4499, %4501 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4503 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4504 = torch.aten.view %4502, %4503 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %4505 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4506 = torch.aten.transpose.int %4505, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %4507 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4508 = torch.prims.convert_element_type %4507, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4509 = torch.prims.convert_element_type %4504, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4510 = torch.prims.convert_element_type %4506, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4511 = torch.aten.mm %4509, %4510 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4512 = torch.aten.mul.Scalar %4511, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4513 = torch.aten.mul.Scalar %4508, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4514 = torch.aten.add.Tensor %4512, %4513, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4515 = torch.prims.convert_element_type %4514, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4516 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4517 = torch.aten.view %4515, %4516 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4518 = torch.aten.add.Tensor %4517, %4470, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4519 = torch.prims.convert_element_type %4518, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4520 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_139, %result1_140 = torch.aten.var_mean.correction %4519, %4520, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4521 = torch.aten.add.Scalar %result0_139, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4522 = torch.aten.rsqrt %4521 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4523 = torch.aten.sub.Tensor %4518, %result1_140, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4524 = torch.aten.mul.Tensor %4523, %4522 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %4525 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4526 = torch.aten.mul.Tensor %4524, %4525 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %4527 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4528 = torch.aten.add.Tensor %4526, %4527, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4529 = torch.prims.convert_element_type %4528, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4530 = torch.prims.convert_element_type %result1_140, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4531 = torch.prims.convert_element_type %4522, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %4532 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4533 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4534 = torch.aten.view %4529, %4533 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4535 = torch_c.to_builtin_tensor %4534 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4536 = torch_c.to_builtin_tensor %4532 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4537 = tensor.empty() : tensor<2048x1280xf32>
    %4538 = linalg.fill ins(%cst : f32) outs(%4537 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4539 = tensor.empty() : tensor<2048x1280xf32>
    %4540 = linalg.fill ins(%cst : f32) outs(%4539 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4541:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4538, %4540, %4535, %4536, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4538, %4540)
    %4542 = arith.truncf %4541#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4543 = torch_c.from_builtin_tensor %4542 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4544 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4545 = torch.aten.view %4543, %4544 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %4546 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4547 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4548 = torch.aten.view %4529, %4547 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4549 = torch_c.to_builtin_tensor %4548 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4550 = torch_c.to_builtin_tensor %4546 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4551 = tensor.empty() : tensor<2048x1280xf32>
    %4552 = linalg.fill ins(%cst : f32) outs(%4551 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4553 = tensor.empty() : tensor<2048x1280xf32>
    %4554 = linalg.fill ins(%cst : f32) outs(%4553 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4555:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4552, %4554, %4549, %4550, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4552, %4554)
    %4556 = arith.truncf %4555#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4557 = torch_c.from_builtin_tensor %4556 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4558 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4559 = torch.aten.view %4557, %4558 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %4560 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4561 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4562 = torch.aten.view %4529, %4561 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4563 = torch_c.to_builtin_tensor %4562 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4564 = torch_c.to_builtin_tensor %4560 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4565 = tensor.empty() : tensor<2048x1280xf32>
    %4566 = linalg.fill ins(%cst : f32) outs(%4565 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4567 = tensor.empty() : tensor<2048x1280xf32>
    %4568 = linalg.fill ins(%cst : f32) outs(%4567 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4569:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4566, %4568, %4563, %4564, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4566, %4568)
    %4570 = arith.truncf %4569#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4571 = torch_c.from_builtin_tensor %4570 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4572 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4573 = torch.aten.view %4571, %4572 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4574 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4575 = torch.aten.view %4545, %4574 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4576 = torch.aten.transpose.int %4575, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4577 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4578 = torch.aten.view %4559, %4577 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4579 = torch.aten.transpose.int %4578, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4580 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4581 = torch.aten.view %4573, %4580 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4582 = torch.aten.transpose.int %4581, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4583:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4576, %4579, %4582, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4584 = torch.aten.transpose.int %4583#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4585 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4586 = torch.aten.view %4584, %4585 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4587 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4588 = torch.aten.view %4586, %4587 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4589 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4590 = torch.aten.transpose.int %4589, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %4591 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4592 = torch.prims.convert_element_type %4591, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4593 = torch.prims.convert_element_type %4588, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4594 = torch.prims.convert_element_type %4590, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4595 = torch.aten.mm %4593, %4594 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4596 = torch.aten.mul.Scalar %4595, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4597 = torch.aten.mul.Scalar %4592, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4598 = torch.aten.add.Tensor %4596, %4597, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4599 = torch.prims.convert_element_type %4598, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4600 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4601 = torch.aten.view %4599, %4600 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4602 = torch.aten.div.Scalar %4601, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4603 = torch.aten.add.Tensor %4602, %4518, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4604 = torch.prims.convert_element_type %4603, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4605 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_141, %result1_142 = torch.aten.var_mean.correction %4604, %4605, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4606 = torch.aten.add.Scalar %result0_141, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4607 = torch.aten.rsqrt %4606 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4608 = torch.aten.sub.Tensor %4603, %result1_142, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4609 = torch.aten.mul.Tensor %4608, %4607 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %4610 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4611 = torch.aten.mul.Tensor %4609, %4610 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %4612 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4613 = torch.aten.add.Tensor %4611, %4612, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4614 = torch.prims.convert_element_type %4613, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4615 = torch.prims.convert_element_type %result1_142, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4616 = torch.prims.convert_element_type %4607, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %4617 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4618 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4619 = torch.aten.view %4614, %4618 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4620 = torch_c.to_builtin_tensor %4619 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4621 = torch_c.to_builtin_tensor %4617 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4622 = tensor.empty() : tensor<2048x1280xf32>
    %4623 = linalg.fill ins(%cst : f32) outs(%4622 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4624 = tensor.empty() : tensor<2048x1280xf32>
    %4625 = linalg.fill ins(%cst : f32) outs(%4624 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4626:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4623, %4625, %4620, %4621, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4623, %4625)
    %4627 = arith.truncf %4626#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4628 = torch_c.from_builtin_tensor %4627 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4629 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4630 = torch.aten.view %4628, %4629 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %4631 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4632 = torch.aten.transpose.int %4631, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4633 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4634 = torch.aten.view %4, %4633 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4635 = torch.aten.mm %4634, %4632 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4636 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4637 = torch.aten.view %4635, %4636 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %4638 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4639 = torch.aten.transpose.int %4638, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4640 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4641 = torch.aten.view %4, %4640 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4642 = torch.aten.mm %4641, %4639 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4643 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4644 = torch.aten.view %4642, %4643 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4645 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4646 = torch.aten.view %4630, %4645 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4647 = torch.aten.transpose.int %4646, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4648 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4649 = torch.aten.view %4637, %4648 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4650 = torch.aten.transpose.int %4649, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4651 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4652 = torch.aten.view %4644, %4651 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4653 = torch.aten.transpose.int %4652, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4654:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4647, %4650, %4653, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4655 = torch.aten.transpose.int %4654#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4656 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4657 = torch.aten.view %4655, %4656 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4658 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4659 = torch.aten.view %4657, %4658 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4660 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4661 = torch.aten.transpose.int %4660, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %4662 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4663 = torch.prims.convert_element_type %4662, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4664 = torch.prims.convert_element_type %4659, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4665 = torch.prims.convert_element_type %4661, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4666 = torch.aten.mm %4664, %4665 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4667 = torch.aten.mul.Scalar %4666, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4668 = torch.aten.mul.Scalar %4663, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4669 = torch.aten.add.Tensor %4667, %4668, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4670 = torch.prims.convert_element_type %4669, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4671 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4672 = torch.aten.view %4670, %4671 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4673 = torch.aten.div.Scalar %4672, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4674 = torch.aten.add.Tensor %4673, %4603, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4675 = torch.prims.convert_element_type %4674, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4676 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_143, %result1_144 = torch.aten.var_mean.correction %4675, %4676, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4677 = torch.aten.add.Scalar %result0_143, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4678 = torch.aten.rsqrt %4677 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4679 = torch.aten.sub.Tensor %4674, %result1_144, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4680 = torch.aten.mul.Tensor %4679, %4678 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %4681 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4682 = torch.aten.mul.Tensor %4680, %4681 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %4683 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4684 = torch.aten.add.Tensor %4682, %4683, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4685 = torch.prims.convert_element_type %4684, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4686 = torch.prims.convert_element_type %result1_144, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4687 = torch.prims.convert_element_type %4678, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4688 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4689 = torch.aten.view %4685, %4688 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4690 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4691 = torch.aten.transpose.int %4690, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %4692 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4693 = torch.prims.convert_element_type %4692, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4694 = torch.prims.convert_element_type %4689, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4695 = torch.prims.convert_element_type %4691, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4696 = torch.aten.mm %4694, %4695 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4697 = torch.aten.mul.Scalar %4696, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4698 = torch.aten.mul.Scalar %4693, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4699 = torch.aten.add.Tensor %4697, %4698, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4700 = torch.prims.convert_element_type %4699, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4701 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4702 = torch.aten.view %4700, %4701 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4703 = torch.aten.slice.Tensor %4702, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4704 = torch.aten.slice.Tensor %4702, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4705 = torch.aten.gelu %4704, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4706 = torch.aten.mul.Tensor %4703, %4705 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4707 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4708 = torch.aten.view %4706, %4707 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %4709 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4710 = torch.aten.transpose.int %4709, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %4711 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4712 = torch.prims.convert_element_type %4711, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4713 = torch.prims.convert_element_type %4708, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4714 = torch.prims.convert_element_type %4710, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4715 = torch.aten.mm %4713, %4714 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4716 = torch.aten.mul.Scalar %4715, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4717 = torch.aten.mul.Scalar %4712, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4718 = torch.aten.add.Tensor %4716, %4717, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4719 = torch.prims.convert_element_type %4718, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4720 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4721 = torch.aten.view %4719, %4720 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4722 = torch.aten.add.Tensor %4721, %4674, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4723 = torch.prims.convert_element_type %4722, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4724 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_145, %result1_146 = torch.aten.var_mean.correction %4723, %4724, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4725 = torch.aten.add.Scalar %result0_145, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4726 = torch.aten.rsqrt %4725 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4727 = torch.aten.sub.Tensor %4722, %result1_146, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4728 = torch.aten.mul.Tensor %4727, %4726 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %4729 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4730 = torch.aten.mul.Tensor %4728, %4729 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %4731 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4732 = torch.aten.add.Tensor %4730, %4731, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4733 = torch.prims.convert_element_type %4732, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4734 = torch.prims.convert_element_type %result1_146, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4735 = torch.prims.convert_element_type %4726, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %4736 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4737 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4738 = torch.aten.view %4733, %4737 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4739 = torch_c.to_builtin_tensor %4738 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4740 = torch_c.to_builtin_tensor %4736 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4741 = tensor.empty() : tensor<2048x1280xf32>
    %4742 = linalg.fill ins(%cst : f32) outs(%4741 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4743 = tensor.empty() : tensor<2048x1280xf32>
    %4744 = linalg.fill ins(%cst : f32) outs(%4743 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4745:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4742, %4744, %4739, %4740, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4742, %4744)
    %4746 = arith.truncf %4745#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4747 = torch_c.from_builtin_tensor %4746 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4748 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4749 = torch.aten.view %4747, %4748 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %4750 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4751 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4752 = torch.aten.view %4733, %4751 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4753 = torch_c.to_builtin_tensor %4752 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4754 = torch_c.to_builtin_tensor %4750 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4755 = tensor.empty() : tensor<2048x1280xf32>
    %4756 = linalg.fill ins(%cst : f32) outs(%4755 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4757 = tensor.empty() : tensor<2048x1280xf32>
    %4758 = linalg.fill ins(%cst : f32) outs(%4757 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4759:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4756, %4758, %4753, %4754, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4756, %4758)
    %4760 = arith.truncf %4759#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4761 = torch_c.from_builtin_tensor %4760 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4762 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4763 = torch.aten.view %4761, %4762 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %4764 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4765 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4766 = torch.aten.view %4733, %4765 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4767 = torch_c.to_builtin_tensor %4766 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4768 = torch_c.to_builtin_tensor %4764 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4769 = tensor.empty() : tensor<2048x1280xf32>
    %4770 = linalg.fill ins(%cst : f32) outs(%4769 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4771 = tensor.empty() : tensor<2048x1280xf32>
    %4772 = linalg.fill ins(%cst : f32) outs(%4771 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4773:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4770, %4772, %4767, %4768, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4770, %4772)
    %4774 = arith.truncf %4773#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4775 = torch_c.from_builtin_tensor %4774 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4776 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4777 = torch.aten.view %4775, %4776 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4778 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4779 = torch.aten.view %4749, %4778 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4780 = torch.aten.transpose.int %4779, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4781 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4782 = torch.aten.view %4763, %4781 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4783 = torch.aten.transpose.int %4782, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4784 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4785 = torch.aten.view %4777, %4784 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4786 = torch.aten.transpose.int %4785, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4787:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4780, %4783, %4786, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4788 = torch.aten.transpose.int %4787#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4789 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4790 = torch.aten.view %4788, %4789 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4791 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4792 = torch.aten.view %4790, %4791 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4793 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4794 = torch.aten.transpose.int %4793, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %4795 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4796 = torch.prims.convert_element_type %4795, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4797 = torch.prims.convert_element_type %4792, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4798 = torch.prims.convert_element_type %4794, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4799 = torch.aten.mm %4797, %4798 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4800 = torch.aten.mul.Scalar %4799, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4801 = torch.aten.mul.Scalar %4796, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4802 = torch.aten.add.Tensor %4800, %4801, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4803 = torch.prims.convert_element_type %4802, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4804 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4805 = torch.aten.view %4803, %4804 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4806 = torch.aten.div.Scalar %4805, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4807 = torch.aten.add.Tensor %4806, %4722, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4808 = torch.prims.convert_element_type %4807, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4809 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_147, %result1_148 = torch.aten.var_mean.correction %4808, %4809, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4810 = torch.aten.add.Scalar %result0_147, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4811 = torch.aten.rsqrt %4810 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4812 = torch.aten.sub.Tensor %4807, %result1_148, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4813 = torch.aten.mul.Tensor %4812, %4811 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %4814 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4815 = torch.aten.mul.Tensor %4813, %4814 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %4816 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4817 = torch.aten.add.Tensor %4815, %4816, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4818 = torch.prims.convert_element_type %4817, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4819 = torch.prims.convert_element_type %result1_148, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4820 = torch.prims.convert_element_type %4811, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %4821 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4822 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4823 = torch.aten.view %4818, %4822 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4824 = torch_c.to_builtin_tensor %4823 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4825 = torch_c.to_builtin_tensor %4821 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4826 = tensor.empty() : tensor<2048x1280xf32>
    %4827 = linalg.fill ins(%cst : f32) outs(%4826 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4828 = tensor.empty() : tensor<2048x1280xf32>
    %4829 = linalg.fill ins(%cst : f32) outs(%4828 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4830:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4827, %4829, %4824, %4825, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4827, %4829)
    %4831 = arith.truncf %4830#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4832 = torch_c.from_builtin_tensor %4831 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4833 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4834 = torch.aten.view %4832, %4833 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %4835 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4836 = torch.aten.transpose.int %4835, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4837 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4838 = torch.aten.view %4, %4837 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4839 = torch.aten.mm %4838, %4836 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4840 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4841 = torch.aten.view %4839, %4840 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %4842 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4843 = torch.aten.transpose.int %4842, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4844 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4845 = torch.aten.view %4, %4844 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4846 = torch.aten.mm %4845, %4843 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %4847 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4848 = torch.aten.view %4846, %4847 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4849 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4850 = torch.aten.view %4834, %4849 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4851 = torch.aten.transpose.int %4850, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4852 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4853 = torch.aten.view %4841, %4852 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4854 = torch.aten.transpose.int %4853, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4855 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4856 = torch.aten.view %4848, %4855 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4857 = torch.aten.transpose.int %4856, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4858:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4851, %4854, %4857, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4859 = torch.aten.transpose.int %4858#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4860 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4861 = torch.aten.view %4859, %4860 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4862 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4863 = torch.aten.view %4861, %4862 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4864 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4865 = torch.aten.transpose.int %4864, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %4866 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4867 = torch.prims.convert_element_type %4866, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4868 = torch.prims.convert_element_type %4863, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4869 = torch.prims.convert_element_type %4865, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4870 = torch.aten.mm %4868, %4869 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4871 = torch.aten.mul.Scalar %4870, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4872 = torch.aten.mul.Scalar %4867, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4873 = torch.aten.add.Tensor %4871, %4872, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4874 = torch.prims.convert_element_type %4873, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4875 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4876 = torch.aten.view %4874, %4875 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4877 = torch.aten.div.Scalar %4876, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4878 = torch.aten.add.Tensor %4877, %4807, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4879 = torch.prims.convert_element_type %4878, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4880 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_149, %result1_150 = torch.aten.var_mean.correction %4879, %4880, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4881 = torch.aten.add.Scalar %result0_149, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4882 = torch.aten.rsqrt %4881 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4883 = torch.aten.sub.Tensor %4878, %result1_150, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4884 = torch.aten.mul.Tensor %4883, %4882 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %4885 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4886 = torch.aten.mul.Tensor %4884, %4885 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %4887 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4888 = torch.aten.add.Tensor %4886, %4887, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4889 = torch.prims.convert_element_type %4888, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4890 = torch.prims.convert_element_type %result1_150, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4891 = torch.prims.convert_element_type %4882, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4892 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4893 = torch.aten.view %4889, %4892 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4894 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4895 = torch.aten.transpose.int %4894, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %4896 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4897 = torch.prims.convert_element_type %4896, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4898 = torch.prims.convert_element_type %4893, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4899 = torch.prims.convert_element_type %4895, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4900 = torch.aten.mm %4898, %4899 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4901 = torch.aten.mul.Scalar %4900, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4902 = torch.aten.mul.Scalar %4897, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4903 = torch.aten.add.Tensor %4901, %4902, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4904 = torch.prims.convert_element_type %4903, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4905 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4906 = torch.aten.view %4904, %4905 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4907 = torch.aten.slice.Tensor %4906, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4908 = torch.aten.slice.Tensor %4906, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4909 = torch.aten.gelu %4908, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4910 = torch.aten.mul.Tensor %4907, %4909 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4911 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4912 = torch.aten.view %4910, %4911 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %4913 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4914 = torch.aten.transpose.int %4913, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %4915 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4916 = torch.prims.convert_element_type %4915, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4917 = torch.prims.convert_element_type %4912, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4918 = torch.prims.convert_element_type %4914, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4919 = torch.aten.mm %4917, %4918 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4920 = torch.aten.mul.Scalar %4919, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4921 = torch.aten.mul.Scalar %4916, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4922 = torch.aten.add.Tensor %4920, %4921, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4923 = torch.prims.convert_element_type %4922, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4924 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4925 = torch.aten.view %4923, %4924 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4926 = torch.aten.add.Tensor %4925, %4878, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4927 = torch.prims.convert_element_type %4926, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4928 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_151, %result1_152 = torch.aten.var_mean.correction %4927, %4928, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4929 = torch.aten.add.Scalar %result0_151, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4930 = torch.aten.rsqrt %4929 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4931 = torch.aten.sub.Tensor %4926, %result1_152, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4932 = torch.aten.mul.Tensor %4931, %4930 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %4933 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4934 = torch.aten.mul.Tensor %4932, %4933 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %4935 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4936 = torch.aten.add.Tensor %4934, %4935, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4937 = torch.prims.convert_element_type %4936, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4938 = torch.prims.convert_element_type %result1_152, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4939 = torch.prims.convert_element_type %4930, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %4940 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4941 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4942 = torch.aten.view %4937, %4941 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4943 = torch_c.to_builtin_tensor %4942 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4944 = torch_c.to_builtin_tensor %4940 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4945 = tensor.empty() : tensor<2048x1280xf32>
    %4946 = linalg.fill ins(%cst : f32) outs(%4945 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4947 = tensor.empty() : tensor<2048x1280xf32>
    %4948 = linalg.fill ins(%cst : f32) outs(%4947 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4949:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4946, %4948, %4943, %4944, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4946, %4948)
    %4950 = arith.truncf %4949#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4951 = torch_c.from_builtin_tensor %4950 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4952 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4953 = torch.aten.view %4951, %4952 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %4954 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4955 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4956 = torch.aten.view %4937, %4955 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4957 = torch_c.to_builtin_tensor %4956 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4958 = torch_c.to_builtin_tensor %4954 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4959 = tensor.empty() : tensor<2048x1280xf32>
    %4960 = linalg.fill ins(%cst : f32) outs(%4959 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4961 = tensor.empty() : tensor<2048x1280xf32>
    %4962 = linalg.fill ins(%cst : f32) outs(%4961 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4963:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4960, %4962, %4957, %4958, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4960, %4962)
    %4964 = arith.truncf %4963#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4965 = torch_c.from_builtin_tensor %4964 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4966 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4967 = torch.aten.view %4965, %4966 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %4968 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4969 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4970 = torch.aten.view %4937, %4969 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4971 = torch_c.to_builtin_tensor %4970 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4972 = torch_c.to_builtin_tensor %4968 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4973 = tensor.empty() : tensor<2048x1280xf32>
    %4974 = linalg.fill ins(%cst : f32) outs(%4973 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4975 = tensor.empty() : tensor<2048x1280xf32>
    %4976 = linalg.fill ins(%cst : f32) outs(%4975 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4977:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4974, %4976, %4971, %4972, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4974, %4976)
    %4978 = arith.truncf %4977#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4979 = torch_c.from_builtin_tensor %4978 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4980 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4981 = torch.aten.view %4979, %4980 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4982 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4983 = torch.aten.view %4953, %4982 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4984 = torch.aten.transpose.int %4983, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4985 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4986 = torch.aten.view %4967, %4985 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4987 = torch.aten.transpose.int %4986, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4988 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4989 = torch.aten.view %4981, %4988 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4990 = torch.aten.transpose.int %4989, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4991:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4984, %4987, %4990, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4992 = torch.aten.transpose.int %4991#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4993 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4994 = torch.aten.view %4992, %4993 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4995 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4996 = torch.aten.view %4994, %4995 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4997 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4998 = torch.aten.transpose.int %4997, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %4999 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5000 = torch.prims.convert_element_type %4999, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5001 = torch.prims.convert_element_type %4996, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5002 = torch.prims.convert_element_type %4998, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5003 = torch.aten.mm %5001, %5002 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5004 = torch.aten.mul.Scalar %5003, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5005 = torch.aten.mul.Scalar %5000, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5006 = torch.aten.add.Tensor %5004, %5005, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5007 = torch.prims.convert_element_type %5006, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5008 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5009 = torch.aten.view %5007, %5008 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5010 = torch.aten.div.Scalar %5009, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5011 = torch.aten.add.Tensor %5010, %4926, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5012 = torch.prims.convert_element_type %5011, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5013 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_153, %result1_154 = torch.aten.var_mean.correction %5012, %5013, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5014 = torch.aten.add.Scalar %result0_153, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5015 = torch.aten.rsqrt %5014 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5016 = torch.aten.sub.Tensor %5011, %result1_154, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5017 = torch.aten.mul.Tensor %5016, %5015 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %5018 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5019 = torch.aten.mul.Tensor %5017, %5018 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %5020 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5021 = torch.aten.add.Tensor %5019, %5020, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5022 = torch.prims.convert_element_type %5021, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5023 = torch.prims.convert_element_type %result1_154, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5024 = torch.prims.convert_element_type %5015, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %5025 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5026 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5027 = torch.aten.view %5022, %5026 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5028 = torch_c.to_builtin_tensor %5027 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5029 = torch_c.to_builtin_tensor %5025 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5030 = tensor.empty() : tensor<2048x1280xf32>
    %5031 = linalg.fill ins(%cst : f32) outs(%5030 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5032 = tensor.empty() : tensor<2048x1280xf32>
    %5033 = linalg.fill ins(%cst : f32) outs(%5032 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5034:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5031, %5033, %5028, %5029, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5031, %5033)
    %5035 = arith.truncf %5034#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5036 = torch_c.from_builtin_tensor %5035 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5037 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5038 = torch.aten.view %5036, %5037 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %5039 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5040 = torch.aten.transpose.int %5039, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5041 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5042 = torch.aten.view %4, %5041 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5043 = torch.aten.mm %5042, %5040 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5044 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5045 = torch.aten.view %5043, %5044 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %5046 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5047 = torch.aten.transpose.int %5046, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5048 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5049 = torch.aten.view %4, %5048 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5050 = torch.aten.mm %5049, %5047 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5051 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5052 = torch.aten.view %5050, %5051 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5053 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5054 = torch.aten.view %5038, %5053 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5055 = torch.aten.transpose.int %5054, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5056 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5057 = torch.aten.view %5045, %5056 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5058 = torch.aten.transpose.int %5057, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5059 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5060 = torch.aten.view %5052, %5059 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5061 = torch.aten.transpose.int %5060, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5062:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5055, %5058, %5061, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5063 = torch.aten.transpose.int %5062#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5064 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5065 = torch.aten.view %5063, %5064 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5066 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5067 = torch.aten.view %5065, %5066 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5068 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5069 = torch.aten.transpose.int %5068, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %5070 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5071 = torch.prims.convert_element_type %5070, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5072 = torch.prims.convert_element_type %5067, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5073 = torch.prims.convert_element_type %5069, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5074 = torch.aten.mm %5072, %5073 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5075 = torch.aten.mul.Scalar %5074, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5076 = torch.aten.mul.Scalar %5071, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5077 = torch.aten.add.Tensor %5075, %5076, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5078 = torch.prims.convert_element_type %5077, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5079 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5080 = torch.aten.view %5078, %5079 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5081 = torch.aten.div.Scalar %5080, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5082 = torch.aten.add.Tensor %5081, %5011, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5083 = torch.prims.convert_element_type %5082, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5084 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_155, %result1_156 = torch.aten.var_mean.correction %5083, %5084, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5085 = torch.aten.add.Scalar %result0_155, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5086 = torch.aten.rsqrt %5085 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5087 = torch.aten.sub.Tensor %5082, %result1_156, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5088 = torch.aten.mul.Tensor %5087, %5086 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %5089 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5090 = torch.aten.mul.Tensor %5088, %5089 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %5091 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5092 = torch.aten.add.Tensor %5090, %5091, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5093 = torch.prims.convert_element_type %5092, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5094 = torch.prims.convert_element_type %result1_156, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5095 = torch.prims.convert_element_type %5086, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5096 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5097 = torch.aten.view %5093, %5096 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5098 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5099 = torch.aten.transpose.int %5098, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %5100 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5101 = torch.prims.convert_element_type %5100, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5102 = torch.prims.convert_element_type %5097, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5103 = torch.prims.convert_element_type %5099, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5104 = torch.aten.mm %5102, %5103 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5105 = torch.aten.mul.Scalar %5104, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5106 = torch.aten.mul.Scalar %5101, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5107 = torch.aten.add.Tensor %5105, %5106, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5108 = torch.prims.convert_element_type %5107, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5109 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5110 = torch.aten.view %5108, %5109 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5111 = torch.aten.slice.Tensor %5110, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5112 = torch.aten.slice.Tensor %5110, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5113 = torch.aten.gelu %5112, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5114 = torch.aten.mul.Tensor %5111, %5113 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5115 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5116 = torch.aten.view %5114, %5115 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %5117 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5118 = torch.aten.transpose.int %5117, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %5119 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5120 = torch.prims.convert_element_type %5119, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5121 = torch.prims.convert_element_type %5116, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5122 = torch.prims.convert_element_type %5118, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5123 = torch.aten.mm %5121, %5122 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5124 = torch.aten.mul.Scalar %5123, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5125 = torch.aten.mul.Scalar %5120, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5126 = torch.aten.add.Tensor %5124, %5125, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5127 = torch.prims.convert_element_type %5126, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5128 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5129 = torch.aten.view %5127, %5128 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5130 = torch.aten.add.Tensor %5129, %5082, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5131 = torch.prims.convert_element_type %5130, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5132 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_157, %result1_158 = torch.aten.var_mean.correction %5131, %5132, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5133 = torch.aten.add.Scalar %result0_157, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5134 = torch.aten.rsqrt %5133 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5135 = torch.aten.sub.Tensor %5130, %result1_158, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5136 = torch.aten.mul.Tensor %5135, %5134 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %5137 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5138 = torch.aten.mul.Tensor %5136, %5137 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %5139 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5140 = torch.aten.add.Tensor %5138, %5139, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5141 = torch.prims.convert_element_type %5140, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5142 = torch.prims.convert_element_type %result1_158, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5143 = torch.prims.convert_element_type %5134, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %5144 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5145 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5146 = torch.aten.view %5141, %5145 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5147 = torch_c.to_builtin_tensor %5146 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5148 = torch_c.to_builtin_tensor %5144 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5149 = tensor.empty() : tensor<2048x1280xf32>
    %5150 = linalg.fill ins(%cst : f32) outs(%5149 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5151 = tensor.empty() : tensor<2048x1280xf32>
    %5152 = linalg.fill ins(%cst : f32) outs(%5151 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5153:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5150, %5152, %5147, %5148, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5150, %5152)
    %5154 = arith.truncf %5153#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5155 = torch_c.from_builtin_tensor %5154 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5156 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5157 = torch.aten.view %5155, %5156 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %5158 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5159 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5160 = torch.aten.view %5141, %5159 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5161 = torch_c.to_builtin_tensor %5160 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5162 = torch_c.to_builtin_tensor %5158 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5163 = tensor.empty() : tensor<2048x1280xf32>
    %5164 = linalg.fill ins(%cst : f32) outs(%5163 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5165 = tensor.empty() : tensor<2048x1280xf32>
    %5166 = linalg.fill ins(%cst : f32) outs(%5165 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5167:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5164, %5166, %5161, %5162, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5164, %5166)
    %5168 = arith.truncf %5167#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5169 = torch_c.from_builtin_tensor %5168 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5170 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5171 = torch.aten.view %5169, %5170 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %5172 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5173 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5174 = torch.aten.view %5141, %5173 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5175 = torch_c.to_builtin_tensor %5174 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5176 = torch_c.to_builtin_tensor %5172 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5177 = tensor.empty() : tensor<2048x1280xf32>
    %5178 = linalg.fill ins(%cst : f32) outs(%5177 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5179 = tensor.empty() : tensor<2048x1280xf32>
    %5180 = linalg.fill ins(%cst : f32) outs(%5179 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5181:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5178, %5180, %5175, %5176, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5178, %5180)
    %5182 = arith.truncf %5181#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5183 = torch_c.from_builtin_tensor %5182 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5184 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5185 = torch.aten.view %5183, %5184 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5186 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5187 = torch.aten.view %5157, %5186 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5188 = torch.aten.transpose.int %5187, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5189 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5190 = torch.aten.view %5171, %5189 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5191 = torch.aten.transpose.int %5190, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5192 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5193 = torch.aten.view %5185, %5192 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5194 = torch.aten.transpose.int %5193, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5195:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5188, %5191, %5194, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5196 = torch.aten.transpose.int %5195#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5197 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5198 = torch.aten.view %5196, %5197 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5199 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5200 = torch.aten.view %5198, %5199 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5201 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5202 = torch.aten.transpose.int %5201, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %5203 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5204 = torch.prims.convert_element_type %5203, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5205 = torch.prims.convert_element_type %5200, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5206 = torch.prims.convert_element_type %5202, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5207 = torch.aten.mm %5205, %5206 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5208 = torch.aten.mul.Scalar %5207, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5209 = torch.aten.mul.Scalar %5204, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5210 = torch.aten.add.Tensor %5208, %5209, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5211 = torch.prims.convert_element_type %5210, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5212 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5213 = torch.aten.view %5211, %5212 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5214 = torch.aten.div.Scalar %5213, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5215 = torch.aten.add.Tensor %5214, %5130, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5216 = torch.prims.convert_element_type %5215, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5217 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_159, %result1_160 = torch.aten.var_mean.correction %5216, %5217, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5218 = torch.aten.add.Scalar %result0_159, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5219 = torch.aten.rsqrt %5218 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5220 = torch.aten.sub.Tensor %5215, %result1_160, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5221 = torch.aten.mul.Tensor %5220, %5219 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %5222 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5223 = torch.aten.mul.Tensor %5221, %5222 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %5224 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5225 = torch.aten.add.Tensor %5223, %5224, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5226 = torch.prims.convert_element_type %5225, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5227 = torch.prims.convert_element_type %result1_160, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5228 = torch.prims.convert_element_type %5219, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %5229 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5230 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5231 = torch.aten.view %5226, %5230 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5232 = torch_c.to_builtin_tensor %5231 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5233 = torch_c.to_builtin_tensor %5229 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5234 = tensor.empty() : tensor<2048x1280xf32>
    %5235 = linalg.fill ins(%cst : f32) outs(%5234 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5236 = tensor.empty() : tensor<2048x1280xf32>
    %5237 = linalg.fill ins(%cst : f32) outs(%5236 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5238:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5235, %5237, %5232, %5233, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5235, %5237)
    %5239 = arith.truncf %5238#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5240 = torch_c.from_builtin_tensor %5239 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5241 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5242 = torch.aten.view %5240, %5241 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %5243 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5244 = torch.aten.transpose.int %5243, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5245 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5246 = torch.aten.view %4, %5245 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5247 = torch.aten.mm %5246, %5244 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5248 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5249 = torch.aten.view %5247, %5248 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %5250 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5251 = torch.aten.transpose.int %5250, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5252 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5253 = torch.aten.view %4, %5252 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5254 = torch.aten.mm %5253, %5251 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5255 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5256 = torch.aten.view %5254, %5255 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5257 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5258 = torch.aten.view %5242, %5257 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5259 = torch.aten.transpose.int %5258, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5260 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5261 = torch.aten.view %5249, %5260 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5262 = torch.aten.transpose.int %5261, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5263 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5264 = torch.aten.view %5256, %5263 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5265 = torch.aten.transpose.int %5264, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5266:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5259, %5262, %5265, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5267 = torch.aten.transpose.int %5266#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5268 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5269 = torch.aten.view %5267, %5268 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5270 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5271 = torch.aten.view %5269, %5270 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5272 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5273 = torch.aten.transpose.int %5272, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %5274 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5275 = torch.prims.convert_element_type %5274, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5276 = torch.prims.convert_element_type %5271, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5277 = torch.prims.convert_element_type %5273, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5278 = torch.aten.mm %5276, %5277 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5279 = torch.aten.mul.Scalar %5278, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5280 = torch.aten.mul.Scalar %5275, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5281 = torch.aten.add.Tensor %5279, %5280, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5282 = torch.prims.convert_element_type %5281, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5283 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5284 = torch.aten.view %5282, %5283 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5285 = torch.aten.div.Scalar %5284, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5286 = torch.aten.add.Tensor %5285, %5215, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5287 = torch.prims.convert_element_type %5286, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5288 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_161, %result1_162 = torch.aten.var_mean.correction %5287, %5288, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5289 = torch.aten.add.Scalar %result0_161, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5290 = torch.aten.rsqrt %5289 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5291 = torch.aten.sub.Tensor %5286, %result1_162, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5292 = torch.aten.mul.Tensor %5291, %5290 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %5293 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5294 = torch.aten.mul.Tensor %5292, %5293 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %5295 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5296 = torch.aten.add.Tensor %5294, %5295, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5297 = torch.prims.convert_element_type %5296, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5298 = torch.prims.convert_element_type %result1_162, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5299 = torch.prims.convert_element_type %5290, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5300 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5301 = torch.aten.view %5297, %5300 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5302 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5303 = torch.aten.transpose.int %5302, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %5304 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5305 = torch.prims.convert_element_type %5304, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5306 = torch.prims.convert_element_type %5301, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5307 = torch.prims.convert_element_type %5303, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5308 = torch.aten.mm %5306, %5307 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5309 = torch.aten.mul.Scalar %5308, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5310 = torch.aten.mul.Scalar %5305, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5311 = torch.aten.add.Tensor %5309, %5310, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5312 = torch.prims.convert_element_type %5311, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5313 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5314 = torch.aten.view %5312, %5313 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5315 = torch.aten.slice.Tensor %5314, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5316 = torch.aten.slice.Tensor %5314, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5317 = torch.aten.gelu %5316, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5318 = torch.aten.mul.Tensor %5315, %5317 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5319 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5320 = torch.aten.view %5318, %5319 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %5321 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5322 = torch.aten.transpose.int %5321, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %5323 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5324 = torch.prims.convert_element_type %5323, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5325 = torch.prims.convert_element_type %5320, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5326 = torch.prims.convert_element_type %5322, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5327 = torch.aten.mm %5325, %5326 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5328 = torch.aten.mul.Scalar %5327, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5329 = torch.aten.mul.Scalar %5324, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5330 = torch.aten.add.Tensor %5328, %5329, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5331 = torch.prims.convert_element_type %5330, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5332 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5333 = torch.aten.view %5331, %5332 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5334 = torch.aten.add.Tensor %5333, %5286, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5335 = torch.prims.convert_element_type %5334, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5336 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_163, %result1_164 = torch.aten.var_mean.correction %5335, %5336, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5337 = torch.aten.add.Scalar %result0_163, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5338 = torch.aten.rsqrt %5337 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5339 = torch.aten.sub.Tensor %5334, %result1_164, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5340 = torch.aten.mul.Tensor %5339, %5338 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %5341 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5342 = torch.aten.mul.Tensor %5340, %5341 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %5343 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5344 = torch.aten.add.Tensor %5342, %5343, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5345 = torch.prims.convert_element_type %5344, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5346 = torch.prims.convert_element_type %result1_164, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5347 = torch.prims.convert_element_type %5338, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %5348 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5349 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5350 = torch.aten.view %5345, %5349 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5351 = torch_c.to_builtin_tensor %5350 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5352 = torch_c.to_builtin_tensor %5348 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5353 = tensor.empty() : tensor<2048x1280xf32>
    %5354 = linalg.fill ins(%cst : f32) outs(%5353 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5355 = tensor.empty() : tensor<2048x1280xf32>
    %5356 = linalg.fill ins(%cst : f32) outs(%5355 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5357:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5354, %5356, %5351, %5352, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5354, %5356)
    %5358 = arith.truncf %5357#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5359 = torch_c.from_builtin_tensor %5358 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5360 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5361 = torch.aten.view %5359, %5360 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %5362 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5363 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5364 = torch.aten.view %5345, %5363 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5365 = torch_c.to_builtin_tensor %5364 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5366 = torch_c.to_builtin_tensor %5362 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5367 = tensor.empty() : tensor<2048x1280xf32>
    %5368 = linalg.fill ins(%cst : f32) outs(%5367 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5369 = tensor.empty() : tensor<2048x1280xf32>
    %5370 = linalg.fill ins(%cst : f32) outs(%5369 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5371:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5368, %5370, %5365, %5366, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5368, %5370)
    %5372 = arith.truncf %5371#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5373 = torch_c.from_builtin_tensor %5372 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5374 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5375 = torch.aten.view %5373, %5374 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %5376 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5377 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5378 = torch.aten.view %5345, %5377 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5379 = torch_c.to_builtin_tensor %5378 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5380 = torch_c.to_builtin_tensor %5376 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5381 = tensor.empty() : tensor<2048x1280xf32>
    %5382 = linalg.fill ins(%cst : f32) outs(%5381 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5383 = tensor.empty() : tensor<2048x1280xf32>
    %5384 = linalg.fill ins(%cst : f32) outs(%5383 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5385:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5382, %5384, %5379, %5380, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5382, %5384)
    %5386 = arith.truncf %5385#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5387 = torch_c.from_builtin_tensor %5386 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5388 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5389 = torch.aten.view %5387, %5388 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5390 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5391 = torch.aten.view %5361, %5390 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5392 = torch.aten.transpose.int %5391, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5393 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5394 = torch.aten.view %5375, %5393 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5395 = torch.aten.transpose.int %5394, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5396 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5397 = torch.aten.view %5389, %5396 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5398 = torch.aten.transpose.int %5397, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5399:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5392, %5395, %5398, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5400 = torch.aten.transpose.int %5399#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5401 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5402 = torch.aten.view %5400, %5401 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5403 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5404 = torch.aten.view %5402, %5403 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5405 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5406 = torch.aten.transpose.int %5405, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %5407 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5408 = torch.prims.convert_element_type %5407, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5409 = torch.prims.convert_element_type %5404, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5410 = torch.prims.convert_element_type %5406, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5411 = torch.aten.mm %5409, %5410 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5412 = torch.aten.mul.Scalar %5411, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5413 = torch.aten.mul.Scalar %5408, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5414 = torch.aten.add.Tensor %5412, %5413, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5415 = torch.prims.convert_element_type %5414, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5416 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5417 = torch.aten.view %5415, %5416 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5418 = torch.aten.div.Scalar %5417, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5419 = torch.aten.add.Tensor %5418, %5334, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5420 = torch.prims.convert_element_type %5419, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5421 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_165, %result1_166 = torch.aten.var_mean.correction %5420, %5421, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5422 = torch.aten.add.Scalar %result0_165, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5423 = torch.aten.rsqrt %5422 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5424 = torch.aten.sub.Tensor %5419, %result1_166, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5425 = torch.aten.mul.Tensor %5424, %5423 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %5426 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5427 = torch.aten.mul.Tensor %5425, %5426 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %5428 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5429 = torch.aten.add.Tensor %5427, %5428, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5430 = torch.prims.convert_element_type %5429, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5431 = torch.prims.convert_element_type %result1_166, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5432 = torch.prims.convert_element_type %5423, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %5433 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5434 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5435 = torch.aten.view %5430, %5434 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5436 = torch_c.to_builtin_tensor %5435 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5437 = torch_c.to_builtin_tensor %5433 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5438 = tensor.empty() : tensor<2048x1280xf32>
    %5439 = linalg.fill ins(%cst : f32) outs(%5438 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5440 = tensor.empty() : tensor<2048x1280xf32>
    %5441 = linalg.fill ins(%cst : f32) outs(%5440 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5442:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5439, %5441, %5436, %5437, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5439, %5441)
    %5443 = arith.truncf %5442#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5444 = torch_c.from_builtin_tensor %5443 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5445 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5446 = torch.aten.view %5444, %5445 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %5447 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5448 = torch.aten.transpose.int %5447, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5449 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5450 = torch.aten.view %4, %5449 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5451 = torch.aten.mm %5450, %5448 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5452 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5453 = torch.aten.view %5451, %5452 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %5454 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5455 = torch.aten.transpose.int %5454, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5456 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5457 = torch.aten.view %4, %5456 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5458 = torch.aten.mm %5457, %5455 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5459 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5460 = torch.aten.view %5458, %5459 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5461 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5462 = torch.aten.view %5446, %5461 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5463 = torch.aten.transpose.int %5462, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5464 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5465 = torch.aten.view %5453, %5464 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5466 = torch.aten.transpose.int %5465, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5467 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5468 = torch.aten.view %5460, %5467 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5469 = torch.aten.transpose.int %5468, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5470:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5463, %5466, %5469, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5471 = torch.aten.transpose.int %5470#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5472 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5473 = torch.aten.view %5471, %5472 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5474 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5475 = torch.aten.view %5473, %5474 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5476 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5477 = torch.aten.transpose.int %5476, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %5478 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5479 = torch.prims.convert_element_type %5478, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5480 = torch.prims.convert_element_type %5475, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5481 = torch.prims.convert_element_type %5477, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5482 = torch.aten.mm %5480, %5481 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5483 = torch.aten.mul.Scalar %5482, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5484 = torch.aten.mul.Scalar %5479, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5485 = torch.aten.add.Tensor %5483, %5484, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5486 = torch.prims.convert_element_type %5485, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5487 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5488 = torch.aten.view %5486, %5487 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5489 = torch.aten.div.Scalar %5488, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5490 = torch.aten.add.Tensor %5489, %5419, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5491 = torch.prims.convert_element_type %5490, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5492 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_167, %result1_168 = torch.aten.var_mean.correction %5491, %5492, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5493 = torch.aten.add.Scalar %result0_167, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5494 = torch.aten.rsqrt %5493 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5495 = torch.aten.sub.Tensor %5490, %result1_168, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5496 = torch.aten.mul.Tensor %5495, %5494 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %5497 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5498 = torch.aten.mul.Tensor %5496, %5497 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %5499 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5500 = torch.aten.add.Tensor %5498, %5499, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5501 = torch.prims.convert_element_type %5500, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5502 = torch.prims.convert_element_type %result1_168, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5503 = torch.prims.convert_element_type %5494, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5504 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5505 = torch.aten.view %5501, %5504 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5506 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5507 = torch.aten.transpose.int %5506, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %5508 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5509 = torch.prims.convert_element_type %5508, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5510 = torch.prims.convert_element_type %5505, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5511 = torch.prims.convert_element_type %5507, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5512 = torch.aten.mm %5510, %5511 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5513 = torch.aten.mul.Scalar %5512, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5514 = torch.aten.mul.Scalar %5509, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5515 = torch.aten.add.Tensor %5513, %5514, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5516 = torch.prims.convert_element_type %5515, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5517 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5518 = torch.aten.view %5516, %5517 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5519 = torch.aten.slice.Tensor %5518, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5520 = torch.aten.slice.Tensor %5518, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5521 = torch.aten.gelu %5520, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5522 = torch.aten.mul.Tensor %5519, %5521 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5523 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5524 = torch.aten.view %5522, %5523 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %5525 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5526 = torch.aten.transpose.int %5525, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %5527 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5528 = torch.prims.convert_element_type %5527, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5529 = torch.prims.convert_element_type %5524, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5530 = torch.prims.convert_element_type %5526, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5531 = torch.aten.mm %5529, %5530 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5532 = torch.aten.mul.Scalar %5531, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5533 = torch.aten.mul.Scalar %5528, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5534 = torch.aten.add.Tensor %5532, %5533, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5535 = torch.prims.convert_element_type %5534, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5536 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5537 = torch.aten.view %5535, %5536 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5538 = torch.aten.add.Tensor %5537, %5490, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5539 = torch.prims.convert_element_type %5538, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5540 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_169, %result1_170 = torch.aten.var_mean.correction %5539, %5540, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5541 = torch.aten.add.Scalar %result0_169, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5542 = torch.aten.rsqrt %5541 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5543 = torch.aten.sub.Tensor %5538, %result1_170, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5544 = torch.aten.mul.Tensor %5543, %5542 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %5545 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5546 = torch.aten.mul.Tensor %5544, %5545 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %5547 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5548 = torch.aten.add.Tensor %5546, %5547, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5549 = torch.prims.convert_element_type %5548, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5550 = torch.prims.convert_element_type %result1_170, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5551 = torch.prims.convert_element_type %5542, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %5552 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5553 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5554 = torch.aten.view %5549, %5553 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5555 = torch_c.to_builtin_tensor %5554 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5556 = torch_c.to_builtin_tensor %5552 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5557 = tensor.empty() : tensor<2048x1280xf32>
    %5558 = linalg.fill ins(%cst : f32) outs(%5557 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5559 = tensor.empty() : tensor<2048x1280xf32>
    %5560 = linalg.fill ins(%cst : f32) outs(%5559 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5561:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5558, %5560, %5555, %5556, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5558, %5560)
    %5562 = arith.truncf %5561#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5563 = torch_c.from_builtin_tensor %5562 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5564 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5565 = torch.aten.view %5563, %5564 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %5566 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5567 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5568 = torch.aten.view %5549, %5567 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5569 = torch_c.to_builtin_tensor %5568 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5570 = torch_c.to_builtin_tensor %5566 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5571 = tensor.empty() : tensor<2048x1280xf32>
    %5572 = linalg.fill ins(%cst : f32) outs(%5571 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5573 = tensor.empty() : tensor<2048x1280xf32>
    %5574 = linalg.fill ins(%cst : f32) outs(%5573 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5575:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5572, %5574, %5569, %5570, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5572, %5574)
    %5576 = arith.truncf %5575#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5577 = torch_c.from_builtin_tensor %5576 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5578 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5579 = torch.aten.view %5577, %5578 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %5580 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5581 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5582 = torch.aten.view %5549, %5581 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5583 = torch_c.to_builtin_tensor %5582 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5584 = torch_c.to_builtin_tensor %5580 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5585 = tensor.empty() : tensor<2048x1280xf32>
    %5586 = linalg.fill ins(%cst : f32) outs(%5585 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5587 = tensor.empty() : tensor<2048x1280xf32>
    %5588 = linalg.fill ins(%cst : f32) outs(%5587 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5589:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5586, %5588, %5583, %5584, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5586, %5588)
    %5590 = arith.truncf %5589#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5591 = torch_c.from_builtin_tensor %5590 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5592 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5593 = torch.aten.view %5591, %5592 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5594 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5595 = torch.aten.view %5565, %5594 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5596 = torch.aten.transpose.int %5595, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5597 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5598 = torch.aten.view %5579, %5597 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5599 = torch.aten.transpose.int %5598, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5600 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5601 = torch.aten.view %5593, %5600 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5602 = torch.aten.transpose.int %5601, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5603:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5596, %5599, %5602, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5604 = torch.aten.transpose.int %5603#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5605 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5606 = torch.aten.view %5604, %5605 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5607 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5608 = torch.aten.view %5606, %5607 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5609 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5610 = torch.aten.transpose.int %5609, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %5611 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5612 = torch.prims.convert_element_type %5611, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5613 = torch.prims.convert_element_type %5608, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5614 = torch.prims.convert_element_type %5610, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5615 = torch.aten.mm %5613, %5614 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5616 = torch.aten.mul.Scalar %5615, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5617 = torch.aten.mul.Scalar %5612, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5618 = torch.aten.add.Tensor %5616, %5617, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5619 = torch.prims.convert_element_type %5618, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5620 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5621 = torch.aten.view %5619, %5620 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5622 = torch.aten.div.Scalar %5621, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5623 = torch.aten.add.Tensor %5622, %5538, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5624 = torch.prims.convert_element_type %5623, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5625 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_171, %result1_172 = torch.aten.var_mean.correction %5624, %5625, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5626 = torch.aten.add.Scalar %result0_171, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5627 = torch.aten.rsqrt %5626 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5628 = torch.aten.sub.Tensor %5623, %result1_172, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5629 = torch.aten.mul.Tensor %5628, %5627 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %5630 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5631 = torch.aten.mul.Tensor %5629, %5630 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %5632 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5633 = torch.aten.add.Tensor %5631, %5632, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5634 = torch.prims.convert_element_type %5633, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5635 = torch.prims.convert_element_type %result1_172, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5636 = torch.prims.convert_element_type %5627, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %5637 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5638 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5639 = torch.aten.view %5634, %5638 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5640 = torch_c.to_builtin_tensor %5639 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5641 = torch_c.to_builtin_tensor %5637 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5642 = tensor.empty() : tensor<2048x1280xf32>
    %5643 = linalg.fill ins(%cst : f32) outs(%5642 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5644 = tensor.empty() : tensor<2048x1280xf32>
    %5645 = linalg.fill ins(%cst : f32) outs(%5644 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5646:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5643, %5645, %5640, %5641, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5643, %5645)
    %5647 = arith.truncf %5646#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5648 = torch_c.from_builtin_tensor %5647 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5649 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5650 = torch.aten.view %5648, %5649 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %5651 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5652 = torch.aten.transpose.int %5651, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5653 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5654 = torch.aten.view %4, %5653 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5655 = torch.aten.mm %5654, %5652 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5656 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5657 = torch.aten.view %5655, %5656 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %5658 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5659 = torch.aten.transpose.int %5658, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5660 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5661 = torch.aten.view %4, %5660 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5662 = torch.aten.mm %5661, %5659 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %5663 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5664 = torch.aten.view %5662, %5663 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5665 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5666 = torch.aten.view %5650, %5665 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5667 = torch.aten.transpose.int %5666, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5668 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5669 = torch.aten.view %5657, %5668 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5670 = torch.aten.transpose.int %5669, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5671 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5672 = torch.aten.view %5664, %5671 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5673 = torch.aten.transpose.int %5672, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5674:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5667, %5670, %5673, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5675 = torch.aten.transpose.int %5674#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5676 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5677 = torch.aten.view %5675, %5676 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5678 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5679 = torch.aten.view %5677, %5678 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5680 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5681 = torch.aten.transpose.int %5680, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %5682 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5683 = torch.prims.convert_element_type %5682, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5684 = torch.prims.convert_element_type %5679, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5685 = torch.prims.convert_element_type %5681, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5686 = torch.aten.mm %5684, %5685 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5687 = torch.aten.mul.Scalar %5686, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5688 = torch.aten.mul.Scalar %5683, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5689 = torch.aten.add.Tensor %5687, %5688, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5690 = torch.prims.convert_element_type %5689, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5691 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5692 = torch.aten.view %5690, %5691 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5693 = torch.aten.div.Scalar %5692, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5694 = torch.aten.add.Tensor %5693, %5623, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5695 = torch.prims.convert_element_type %5694, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5696 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_173, %result1_174 = torch.aten.var_mean.correction %5695, %5696, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5697 = torch.aten.add.Scalar %result0_173, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5698 = torch.aten.rsqrt %5697 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5699 = torch.aten.sub.Tensor %5694, %result1_174, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5700 = torch.aten.mul.Tensor %5699, %5698 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %5701 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5702 = torch.aten.mul.Tensor %5700, %5701 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %5703 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5704 = torch.aten.add.Tensor %5702, %5703, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5705 = torch.prims.convert_element_type %5704, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5706 = torch.prims.convert_element_type %result1_174, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5707 = torch.prims.convert_element_type %5698, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5708 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5709 = torch.aten.view %5705, %5708 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5710 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5711 = torch.aten.transpose.int %5710, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %5712 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5713 = torch.prims.convert_element_type %5712, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5714 = torch.prims.convert_element_type %5709, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5715 = torch.prims.convert_element_type %5711, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5716 = torch.aten.mm %5714, %5715 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5717 = torch.aten.mul.Scalar %5716, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5718 = torch.aten.mul.Scalar %5713, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5719 = torch.aten.add.Tensor %5717, %5718, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5720 = torch.prims.convert_element_type %5719, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5721 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5722 = torch.aten.view %5720, %5721 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5723 = torch.aten.slice.Tensor %5722, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5724 = torch.aten.slice.Tensor %5722, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5725 = torch.aten.gelu %5724, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5726 = torch.aten.mul.Tensor %5723, %5725 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5727 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5728 = torch.aten.view %5726, %5727 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %5729 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5730 = torch.aten.transpose.int %5729, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %5731 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5732 = torch.prims.convert_element_type %5731, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5733 = torch.prims.convert_element_type %5728, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5734 = torch.prims.convert_element_type %5730, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5735 = torch.aten.mm %5733, %5734 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5736 = torch.aten.mul.Scalar %5735, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5737 = torch.aten.mul.Scalar %5732, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5738 = torch.aten.add.Tensor %5736, %5737, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5739 = torch.prims.convert_element_type %5738, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5740 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5741 = torch.aten.view %5739, %5740 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5742 = torch.aten.add.Tensor %5741, %5694, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5743 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5744 = torch.aten.view %5742, %5743 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_out.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_out.weight : tensor<1280x1280xf16>
    %5745 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5746 = torch.aten.transpose.int %5745, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_out.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_out.bias : tensor<1280xf16>
    %5747 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5748 = torch.prims.convert_element_type %5747, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5749 = torch.prims.convert_element_type %5744, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5750 = torch.prims.convert_element_type %5746, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5751 = torch.aten.mm %5749, %5750 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5752 = torch.aten.mul.Scalar %5751, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5753 = torch.aten.mul.Scalar %5748, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5754 = torch.aten.add.Tensor %5752, %5753, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5755 = torch.prims.convert_element_type %5754, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5756 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5757 = torch.aten.view %5755, %5756 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5758 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5759 = torch.aten.view %5757, %5758 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %5760 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5761 = torch.aten.permute %5759, %5760 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %5762 = torch.aten.add.Tensor %5761, %3651, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5763 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5764 = torch.aten.view %5762, %5763 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %5765 = torch.prims.convert_element_type %5764, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %5766 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_175, %result1_176 = torch.aten.var_mean.correction %5765, %5766, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %5767 = torch.aten.add.Scalar %result0_175, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %5768 = torch.aten.rsqrt %5767 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %5769 = torch.aten.sub.Tensor %5764, %result1_176, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %5770 = torch.aten.mul.Tensor %5769, %5768 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %5771 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5772 = torch.aten.view %5770, %5771 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.0.norm1.bias = util.global.load @_params.unet.mid_block.resnets.0.norm1.bias : tensor<1280xf16>
    %5773 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5774 = torch.aten.unsqueeze %5773, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %5775 = torch.aten.unsqueeze %5774, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %5776 = torch.aten.unsqueeze %5775, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.0.norm1.weight = util.global.load @_params.unet.mid_block.resnets.0.norm1.weight : tensor<1280xf16>
    %5777 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5778 = torch.aten.unsqueeze %5777, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %5779 = torch.aten.unsqueeze %5778, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %5780 = torch.aten.unsqueeze %5779, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %5781 = torch.aten.mul.Tensor %5772, %5780 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %5782 = torch.aten.add.Tensor %5781, %5776, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %5783 = torch.prims.convert_element_type %5782, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5784 = torch.prims.convert_element_type %result1_176, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %5785 = torch.prims.convert_element_type %5768, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %5786 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %5787 = torch.prims.squeeze %5784, %5786 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %5788 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %5789 = torch.prims.squeeze %5787, %5788 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %5790 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %5791 = torch.prims.squeeze %5785, %5790 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %5792 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %5793 = torch.prims.squeeze %5791, %5792 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %5794 = torch.aten.silu %5783 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.0.conv1.weight = util.global.load @_params.unet.mid_block.resnets.0.conv1.weight : tensor<1280x1280x3x3xf16>
    %5795 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.0.conv1.bias = util.global.load @_params.unet.mid_block.resnets.0.conv1.bias : tensor<1280xf16>
    %5796 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5797 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %5798 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %5799 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %5800 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %5801 = torch.aten.convolution %5794, %5795, %5796, %5797, %5798, %5799, %false, %5800, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5802 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.mid_block.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.mid_block.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %5803 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5804 = torch.aten.transpose.int %5803, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.mid_block.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %5805 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5806 = torch.prims.convert_element_type %5805, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5807 = torch.prims.convert_element_type %5802, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %5808 = torch.prims.convert_element_type %5804, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5809 = torch.aten.mm %5807, %5808 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %5810 = torch.aten.mul.Scalar %5809, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %5811 = torch.aten.mul.Scalar %5806, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5812 = torch.aten.add.Tensor %5810, %5811, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %5813 = torch.prims.convert_element_type %5812, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %5814 = torch.aten.unsqueeze %5813, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %5815 = torch.aten.unsqueeze %5814, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %5816 = torch.aten.add.Tensor %5801, %5815, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5817 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5818 = torch.aten.view %5816, %5817 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %5819 = torch.prims.convert_element_type %5818, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %5820 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_177, %result1_178 = torch.aten.var_mean.correction %5819, %5820, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %5821 = torch.aten.add.Scalar %result0_177, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %5822 = torch.aten.rsqrt %5821 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %5823 = torch.aten.sub.Tensor %5818, %result1_178, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %5824 = torch.aten.mul.Tensor %5823, %5822 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %5825 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5826 = torch.aten.view %5824, %5825 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.0.norm2.bias = util.global.load @_params.unet.mid_block.resnets.0.norm2.bias : tensor<1280xf16>
    %5827 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5828 = torch.aten.unsqueeze %5827, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %5829 = torch.aten.unsqueeze %5828, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %5830 = torch.aten.unsqueeze %5829, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.0.norm2.weight = util.global.load @_params.unet.mid_block.resnets.0.norm2.weight : tensor<1280xf16>
    %5831 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5832 = torch.aten.unsqueeze %5831, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %5833 = torch.aten.unsqueeze %5832, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %5834 = torch.aten.unsqueeze %5833, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %5835 = torch.aten.mul.Tensor %5826, %5834 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %5836 = torch.aten.add.Tensor %5835, %5830, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %5837 = torch.prims.convert_element_type %5836, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5838 = torch.prims.convert_element_type %result1_178, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %5839 = torch.prims.convert_element_type %5822, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %5840 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %5841 = torch.prims.squeeze %5838, %5840 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %5842 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %5843 = torch.prims.squeeze %5841, %5842 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %5844 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %5845 = torch.prims.squeeze %5839, %5844 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %5846 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %5847 = torch.prims.squeeze %5845, %5846 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %5848 = torch.aten.silu %5837 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.0.conv2.weight = util.global.load @_params.unet.mid_block.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %5849 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.0.conv2.bias = util.global.load @_params.unet.mid_block.resnets.0.conv2.bias : tensor<1280xf16>
    %5850 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5851 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %5852 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %5853 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %5854 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %5855 = torch.aten.convolution %5848, %5849, %5850, %5851, %5852, %5853, %false, %5854, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5856 = torch.aten.add.Tensor %5762, %5855, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5857 = torch.aten.div.Scalar %5856, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5858 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5859 = torch.aten.view %5857, %5858 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %5860 = torch.prims.convert_element_type %5859, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %5861 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_179, %result1_180 = torch.aten.var_mean.correction %5860, %5861, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %5862 = torch.aten.add.Scalar %result0_179, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %5863 = torch.aten.rsqrt %5862 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %5864 = torch.aten.sub.Tensor %5859, %result1_180, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %5865 = torch.aten.mul.Tensor %5864, %5863 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %5866 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5867 = torch.aten.view %5865, %5866 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.attentions.0.norm.bias = util.global.load @_params.unet.mid_block.attentions.0.norm.bias : tensor<1280xf16>
    %5868 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5869 = torch.aten.unsqueeze %5868, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %5870 = torch.aten.unsqueeze %5869, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %5871 = torch.aten.unsqueeze %5870, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.attentions.0.norm.weight = util.global.load @_params.unet.mid_block.attentions.0.norm.weight : tensor<1280xf16>
    %5872 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5873 = torch.aten.unsqueeze %5872, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %5874 = torch.aten.unsqueeze %5873, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %5875 = torch.aten.unsqueeze %5874, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %5876 = torch.aten.mul.Tensor %5867, %5875 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %5877 = torch.aten.add.Tensor %5876, %5871, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %5878 = torch.prims.convert_element_type %5877, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %5879 = torch.prims.convert_element_type %result1_180, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %5880 = torch.prims.convert_element_type %5863, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %5881 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %5882 = torch.prims.squeeze %5879, %5881 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %5883 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %5884 = torch.prims.squeeze %5882, %5883 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %5885 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %5886 = torch.prims.squeeze %5880, %5885 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %5887 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %5888 = torch.prims.squeeze %5886, %5887 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %5889 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5890 = torch.aten.permute %5878, %5889 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %5891 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5892 = torch.aten.view %5890, %5891 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_in.weight = util.global.load @_params.unet.mid_block.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %5893 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5894 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5895 = torch.aten._unsafe_view %5892, %5894 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5896 = torch_c.to_builtin_tensor %5895 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5897 = torch_c.to_builtin_tensor %5893 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5898 = tensor.empty() : tensor<2048x1280xf32>
    %5899 = linalg.fill ins(%cst : f32) outs(%5898 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5900 = tensor.empty() : tensor<2048x1280xf32>
    %5901 = linalg.fill ins(%cst : f32) outs(%5900 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5902:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5899, %5901, %5896, %5897, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5899, %5901)
    %5903 = arith.truncf %5902#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5904 = torch_c.from_builtin_tensor %5903 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5905 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5906 = torch.aten.view %5904, %5905 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_in.bias = util.global.load @_params.unet.mid_block.attentions.0.proj_in.bias : tensor<1280xf16>
    %5907 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5908 = torch.aten.add.Tensor %5906, %5907, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5909 = torch.prims.convert_element_type %5908, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5910 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_181, %result1_182 = torch.aten.var_mean.correction %5909, %5910, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5911 = torch.aten.add.Scalar %result0_181, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5912 = torch.aten.rsqrt %5911 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5913 = torch.aten.sub.Tensor %5908, %result1_182, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5914 = torch.aten.mul.Tensor %5913, %5912 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %5915 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5916 = torch.aten.mul.Tensor %5914, %5915 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %5917 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5918 = torch.aten.add.Tensor %5916, %5917, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5919 = torch.prims.convert_element_type %5918, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5920 = torch.prims.convert_element_type %result1_182, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5921 = torch.prims.convert_element_type %5912, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %5922 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5923 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5924 = torch.aten.view %5919, %5923 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5925 = torch_c.to_builtin_tensor %5924 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5926 = torch_c.to_builtin_tensor %5922 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5927 = tensor.empty() : tensor<2048x1280xf32>
    %5928 = linalg.fill ins(%cst : f32) outs(%5927 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5929 = tensor.empty() : tensor<2048x1280xf32>
    %5930 = linalg.fill ins(%cst : f32) outs(%5929 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5931:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5928, %5930, %5925, %5926, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5928, %5930)
    %5932 = arith.truncf %5931#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5933 = torch_c.from_builtin_tensor %5932 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5934 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5935 = torch.aten.view %5933, %5934 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %5936 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5937 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5938 = torch.aten.view %5919, %5937 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5939 = torch_c.to_builtin_tensor %5938 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5940 = torch_c.to_builtin_tensor %5936 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5941 = tensor.empty() : tensor<2048x1280xf32>
    %5942 = linalg.fill ins(%cst : f32) outs(%5941 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5943 = tensor.empty() : tensor<2048x1280xf32>
    %5944 = linalg.fill ins(%cst : f32) outs(%5943 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5945:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5942, %5944, %5939, %5940, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5942, %5944)
    %5946 = arith.truncf %5945#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5947 = torch_c.from_builtin_tensor %5946 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5948 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5949 = torch.aten.view %5947, %5948 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %5950 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5951 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5952 = torch.aten.view %5919, %5951 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5953 = torch_c.to_builtin_tensor %5952 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5954 = torch_c.to_builtin_tensor %5950 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5955 = tensor.empty() : tensor<2048x1280xf32>
    %5956 = linalg.fill ins(%cst : f32) outs(%5955 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5957 = tensor.empty() : tensor<2048x1280xf32>
    %5958 = linalg.fill ins(%cst : f32) outs(%5957 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5959:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5956, %5958, %5953, %5954, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5956, %5958)
    %5960 = arith.truncf %5959#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5961 = torch_c.from_builtin_tensor %5960 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5962 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5963 = torch.aten.view %5961, %5962 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5964 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5965 = torch.aten.view %5935, %5964 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5966 = torch.aten.transpose.int %5965, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5967 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5968 = torch.aten.view %5949, %5967 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5969 = torch.aten.transpose.int %5968, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5970 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5971 = torch.aten.view %5963, %5970 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5972 = torch.aten.transpose.int %5971, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5973:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5966, %5969, %5972, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5974 = torch.aten.transpose.int %5973#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5975 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5976 = torch.aten.view %5974, %5975 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5977 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5978 = torch.aten.view %5976, %5977 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5979 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5980 = torch.aten.transpose.int %5979, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %5981 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5982 = torch.prims.convert_element_type %5981, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5983 = torch.prims.convert_element_type %5978, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5984 = torch.prims.convert_element_type %5980, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5985 = torch.aten.mm %5983, %5984 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5986 = torch.aten.mul.Scalar %5985, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5987 = torch.aten.mul.Scalar %5982, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5988 = torch.aten.add.Tensor %5986, %5987, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5989 = torch.prims.convert_element_type %5988, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5990 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5991 = torch.aten.view %5989, %5990 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5992 = torch.aten.div.Scalar %5991, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5993 = torch.aten.add.Tensor %5992, %5908, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5994 = torch.prims.convert_element_type %5993, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5995 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_183, %result1_184 = torch.aten.var_mean.correction %5994, %5995, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5996 = torch.aten.add.Scalar %result0_183, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5997 = torch.aten.rsqrt %5996 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5998 = torch.aten.sub.Tensor %5993, %result1_184, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5999 = torch.aten.mul.Tensor %5998, %5997 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %6000 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6001 = torch.aten.mul.Tensor %5999, %6000 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %6002 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6003 = torch.aten.add.Tensor %6001, %6002, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6004 = torch.prims.convert_element_type %6003, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6005 = torch.prims.convert_element_type %result1_184, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6006 = torch.prims.convert_element_type %5997, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %6007 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6008 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6009 = torch.aten.view %6004, %6008 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6010 = torch_c.to_builtin_tensor %6009 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6011 = torch_c.to_builtin_tensor %6007 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6012 = tensor.empty() : tensor<2048x1280xf32>
    %6013 = linalg.fill ins(%cst : f32) outs(%6012 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6014 = tensor.empty() : tensor<2048x1280xf32>
    %6015 = linalg.fill ins(%cst : f32) outs(%6014 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6016:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6013, %6015, %6010, %6011, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6013, %6015)
    %6017 = arith.truncf %6016#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6018 = torch_c.from_builtin_tensor %6017 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6019 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6020 = torch.aten.view %6018, %6019 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %6021 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6022 = torch.aten.transpose.int %6021, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6023 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6024 = torch.aten.view %4, %6023 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6025 = torch.aten.mm %6024, %6022 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6026 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6027 = torch.aten.view %6025, %6026 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %6028 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6029 = torch.aten.transpose.int %6028, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6030 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6031 = torch.aten.view %4, %6030 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6032 = torch.aten.mm %6031, %6029 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6033 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6034 = torch.aten.view %6032, %6033 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6035 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6036 = torch.aten.view %6020, %6035 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6037 = torch.aten.transpose.int %6036, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6038 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6039 = torch.aten.view %6027, %6038 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6040 = torch.aten.transpose.int %6039, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6041 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6042 = torch.aten.view %6034, %6041 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6043 = torch.aten.transpose.int %6042, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6044:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6037, %6040, %6043, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6045 = torch.aten.transpose.int %6044#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6046 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6047 = torch.aten.view %6045, %6046 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6048 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6049 = torch.aten.view %6047, %6048 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6050 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6051 = torch.aten.transpose.int %6050, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %6052 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6053 = torch.prims.convert_element_type %6052, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6054 = torch.prims.convert_element_type %6049, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6055 = torch.prims.convert_element_type %6051, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6056 = torch.aten.mm %6054, %6055 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6057 = torch.aten.mul.Scalar %6056, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6058 = torch.aten.mul.Scalar %6053, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6059 = torch.aten.add.Tensor %6057, %6058, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6060 = torch.prims.convert_element_type %6059, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6061 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6062 = torch.aten.view %6060, %6061 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6063 = torch.aten.div.Scalar %6062, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6064 = torch.aten.add.Tensor %6063, %5993, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6065 = torch.prims.convert_element_type %6064, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6066 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_185, %result1_186 = torch.aten.var_mean.correction %6065, %6066, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6067 = torch.aten.add.Scalar %result0_185, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6068 = torch.aten.rsqrt %6067 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6069 = torch.aten.sub.Tensor %6064, %result1_186, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6070 = torch.aten.mul.Tensor %6069, %6068 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %6071 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6072 = torch.aten.mul.Tensor %6070, %6071 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %6073 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6074 = torch.aten.add.Tensor %6072, %6073, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6075 = torch.prims.convert_element_type %6074, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6076 = torch.prims.convert_element_type %result1_186, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6077 = torch.prims.convert_element_type %6068, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6078 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6079 = torch.aten.view %6075, %6078 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6080 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6081 = torch.aten.transpose.int %6080, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %6082 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6083 = torch.prims.convert_element_type %6082, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6084 = torch.prims.convert_element_type %6079, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6085 = torch.prims.convert_element_type %6081, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6086 = torch.aten.mm %6084, %6085 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6087 = torch.aten.mul.Scalar %6086, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6088 = torch.aten.mul.Scalar %6083, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6089 = torch.aten.add.Tensor %6087, %6088, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6090 = torch.prims.convert_element_type %6089, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6091 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6092 = torch.aten.view %6090, %6091 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6093 = torch.aten.slice.Tensor %6092, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6094 = torch.aten.slice.Tensor %6092, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6095 = torch.aten.gelu %6094, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6096 = torch.aten.mul.Tensor %6093, %6095 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6097 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6098 = torch.aten.view %6096, %6097 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %6099 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6100 = torch.aten.transpose.int %6099, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %6101 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6102 = torch.prims.convert_element_type %6101, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6103 = torch.prims.convert_element_type %6098, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6104 = torch.prims.convert_element_type %6100, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6105 = torch.aten.mm %6103, %6104 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6106 = torch.aten.mul.Scalar %6105, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6107 = torch.aten.mul.Scalar %6102, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6108 = torch.aten.add.Tensor %6106, %6107, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6109 = torch.prims.convert_element_type %6108, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6110 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6111 = torch.aten.view %6109, %6110 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6112 = torch.aten.add.Tensor %6111, %6064, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6113 = torch.prims.convert_element_type %6112, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6114 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_187, %result1_188 = torch.aten.var_mean.correction %6113, %6114, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6115 = torch.aten.add.Scalar %result0_187, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6116 = torch.aten.rsqrt %6115 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6117 = torch.aten.sub.Tensor %6112, %result1_188, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6118 = torch.aten.mul.Tensor %6117, %6116 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %6119 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6120 = torch.aten.mul.Tensor %6118, %6119 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %6121 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6122 = torch.aten.add.Tensor %6120, %6121, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6123 = torch.prims.convert_element_type %6122, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6124 = torch.prims.convert_element_type %result1_188, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6125 = torch.prims.convert_element_type %6116, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %6126 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6127 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6128 = torch.aten.view %6123, %6127 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6129 = torch_c.to_builtin_tensor %6128 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6130 = torch_c.to_builtin_tensor %6126 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6131 = tensor.empty() : tensor<2048x1280xf32>
    %6132 = linalg.fill ins(%cst : f32) outs(%6131 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6133 = tensor.empty() : tensor<2048x1280xf32>
    %6134 = linalg.fill ins(%cst : f32) outs(%6133 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6135:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6132, %6134, %6129, %6130, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6132, %6134)
    %6136 = arith.truncf %6135#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6137 = torch_c.from_builtin_tensor %6136 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6138 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6139 = torch.aten.view %6137, %6138 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %6140 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6141 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6142 = torch.aten.view %6123, %6141 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6143 = torch_c.to_builtin_tensor %6142 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6144 = torch_c.to_builtin_tensor %6140 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6145 = tensor.empty() : tensor<2048x1280xf32>
    %6146 = linalg.fill ins(%cst : f32) outs(%6145 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6147 = tensor.empty() : tensor<2048x1280xf32>
    %6148 = linalg.fill ins(%cst : f32) outs(%6147 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6149:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6146, %6148, %6143, %6144, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6146, %6148)
    %6150 = arith.truncf %6149#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6151 = torch_c.from_builtin_tensor %6150 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6152 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6153 = torch.aten.view %6151, %6152 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %6154 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6155 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6156 = torch.aten.view %6123, %6155 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6157 = torch_c.to_builtin_tensor %6156 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6158 = torch_c.to_builtin_tensor %6154 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6159 = tensor.empty() : tensor<2048x1280xf32>
    %6160 = linalg.fill ins(%cst : f32) outs(%6159 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6161 = tensor.empty() : tensor<2048x1280xf32>
    %6162 = linalg.fill ins(%cst : f32) outs(%6161 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6163:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6160, %6162, %6157, %6158, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6160, %6162)
    %6164 = arith.truncf %6163#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6165 = torch_c.from_builtin_tensor %6164 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6166 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6167 = torch.aten.view %6165, %6166 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6168 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6169 = torch.aten.view %6139, %6168 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6170 = torch.aten.transpose.int %6169, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6171 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6172 = torch.aten.view %6153, %6171 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6173 = torch.aten.transpose.int %6172, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6174 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6175 = torch.aten.view %6167, %6174 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6176 = torch.aten.transpose.int %6175, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6177:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6170, %6173, %6176, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6178 = torch.aten.transpose.int %6177#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6179 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6180 = torch.aten.view %6178, %6179 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6181 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6182 = torch.aten.view %6180, %6181 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6183 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6184 = torch.aten.transpose.int %6183, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %6185 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6186 = torch.prims.convert_element_type %6185, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6187 = torch.prims.convert_element_type %6182, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6188 = torch.prims.convert_element_type %6184, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6189 = torch.aten.mm %6187, %6188 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6190 = torch.aten.mul.Scalar %6189, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6191 = torch.aten.mul.Scalar %6186, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6192 = torch.aten.add.Tensor %6190, %6191, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6193 = torch.prims.convert_element_type %6192, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6194 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6195 = torch.aten.view %6193, %6194 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6196 = torch.aten.div.Scalar %6195, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6197 = torch.aten.add.Tensor %6196, %6112, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6198 = torch.prims.convert_element_type %6197, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6199 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_189, %result1_190 = torch.aten.var_mean.correction %6198, %6199, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6200 = torch.aten.add.Scalar %result0_189, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6201 = torch.aten.rsqrt %6200 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6202 = torch.aten.sub.Tensor %6197, %result1_190, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6203 = torch.aten.mul.Tensor %6202, %6201 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %6204 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6205 = torch.aten.mul.Tensor %6203, %6204 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %6206 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6207 = torch.aten.add.Tensor %6205, %6206, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6208 = torch.prims.convert_element_type %6207, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6209 = torch.prims.convert_element_type %result1_190, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6210 = torch.prims.convert_element_type %6201, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %6211 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6212 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6213 = torch.aten.view %6208, %6212 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6214 = torch_c.to_builtin_tensor %6213 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6215 = torch_c.to_builtin_tensor %6211 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6216 = tensor.empty() : tensor<2048x1280xf32>
    %6217 = linalg.fill ins(%cst : f32) outs(%6216 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6218 = tensor.empty() : tensor<2048x1280xf32>
    %6219 = linalg.fill ins(%cst : f32) outs(%6218 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6220:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6217, %6219, %6214, %6215, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6217, %6219)
    %6221 = arith.truncf %6220#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6222 = torch_c.from_builtin_tensor %6221 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6223 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6224 = torch.aten.view %6222, %6223 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %6225 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6226 = torch.aten.transpose.int %6225, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6227 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6228 = torch.aten.view %4, %6227 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6229 = torch.aten.mm %6228, %6226 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6230 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6231 = torch.aten.view %6229, %6230 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %6232 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6233 = torch.aten.transpose.int %6232, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6234 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6235 = torch.aten.view %4, %6234 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6236 = torch.aten.mm %6235, %6233 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6237 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6238 = torch.aten.view %6236, %6237 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6239 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6240 = torch.aten.view %6224, %6239 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6241 = torch.aten.transpose.int %6240, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6242 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6243 = torch.aten.view %6231, %6242 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6244 = torch.aten.transpose.int %6243, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6245 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6246 = torch.aten.view %6238, %6245 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6247 = torch.aten.transpose.int %6246, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6248:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6241, %6244, %6247, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6249 = torch.aten.transpose.int %6248#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6250 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6251 = torch.aten.view %6249, %6250 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6252 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6253 = torch.aten.view %6251, %6252 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6254 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6255 = torch.aten.transpose.int %6254, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %6256 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6257 = torch.prims.convert_element_type %6256, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6258 = torch.prims.convert_element_type %6253, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6259 = torch.prims.convert_element_type %6255, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6260 = torch.aten.mm %6258, %6259 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6261 = torch.aten.mul.Scalar %6260, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6262 = torch.aten.mul.Scalar %6257, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6263 = torch.aten.add.Tensor %6261, %6262, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6264 = torch.prims.convert_element_type %6263, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6265 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6266 = torch.aten.view %6264, %6265 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6267 = torch.aten.div.Scalar %6266, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6268 = torch.aten.add.Tensor %6267, %6197, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6269 = torch.prims.convert_element_type %6268, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6270 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_191, %result1_192 = torch.aten.var_mean.correction %6269, %6270, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6271 = torch.aten.add.Scalar %result0_191, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6272 = torch.aten.rsqrt %6271 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6273 = torch.aten.sub.Tensor %6268, %result1_192, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6274 = torch.aten.mul.Tensor %6273, %6272 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %6275 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6276 = torch.aten.mul.Tensor %6274, %6275 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %6277 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6278 = torch.aten.add.Tensor %6276, %6277, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6279 = torch.prims.convert_element_type %6278, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6280 = torch.prims.convert_element_type %result1_192, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6281 = torch.prims.convert_element_type %6272, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6282 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6283 = torch.aten.view %6279, %6282 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6284 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6285 = torch.aten.transpose.int %6284, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %6286 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6287 = torch.prims.convert_element_type %6286, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6288 = torch.prims.convert_element_type %6283, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6289 = torch.prims.convert_element_type %6285, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6290 = torch.aten.mm %6288, %6289 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6291 = torch.aten.mul.Scalar %6290, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6292 = torch.aten.mul.Scalar %6287, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6293 = torch.aten.add.Tensor %6291, %6292, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6294 = torch.prims.convert_element_type %6293, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6295 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6296 = torch.aten.view %6294, %6295 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6297 = torch.aten.slice.Tensor %6296, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6298 = torch.aten.slice.Tensor %6296, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6299 = torch.aten.gelu %6298, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6300 = torch.aten.mul.Tensor %6297, %6299 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6301 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6302 = torch.aten.view %6300, %6301 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %6303 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6304 = torch.aten.transpose.int %6303, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %6305 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6306 = torch.prims.convert_element_type %6305, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6307 = torch.prims.convert_element_type %6302, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6308 = torch.prims.convert_element_type %6304, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6309 = torch.aten.mm %6307, %6308 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6310 = torch.aten.mul.Scalar %6309, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6311 = torch.aten.mul.Scalar %6306, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6312 = torch.aten.add.Tensor %6310, %6311, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6313 = torch.prims.convert_element_type %6312, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6314 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6315 = torch.aten.view %6313, %6314 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6316 = torch.aten.add.Tensor %6315, %6268, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6317 = torch.prims.convert_element_type %6316, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6318 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_193, %result1_194 = torch.aten.var_mean.correction %6317, %6318, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6319 = torch.aten.add.Scalar %result0_193, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6320 = torch.aten.rsqrt %6319 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6321 = torch.aten.sub.Tensor %6316, %result1_194, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6322 = torch.aten.mul.Tensor %6321, %6320 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %6323 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6324 = torch.aten.mul.Tensor %6322, %6323 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %6325 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6326 = torch.aten.add.Tensor %6324, %6325, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6327 = torch.prims.convert_element_type %6326, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6328 = torch.prims.convert_element_type %result1_194, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6329 = torch.prims.convert_element_type %6320, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %6330 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6331 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6332 = torch.aten.view %6327, %6331 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6333 = torch_c.to_builtin_tensor %6332 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6334 = torch_c.to_builtin_tensor %6330 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6335 = tensor.empty() : tensor<2048x1280xf32>
    %6336 = linalg.fill ins(%cst : f32) outs(%6335 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6337 = tensor.empty() : tensor<2048x1280xf32>
    %6338 = linalg.fill ins(%cst : f32) outs(%6337 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6339:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6336, %6338, %6333, %6334, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6336, %6338)
    %6340 = arith.truncf %6339#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6341 = torch_c.from_builtin_tensor %6340 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6342 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6343 = torch.aten.view %6341, %6342 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %6344 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6345 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6346 = torch.aten.view %6327, %6345 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6347 = torch_c.to_builtin_tensor %6346 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6348 = torch_c.to_builtin_tensor %6344 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6349 = tensor.empty() : tensor<2048x1280xf32>
    %6350 = linalg.fill ins(%cst : f32) outs(%6349 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6351 = tensor.empty() : tensor<2048x1280xf32>
    %6352 = linalg.fill ins(%cst : f32) outs(%6351 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6353:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6350, %6352, %6347, %6348, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6350, %6352)
    %6354 = arith.truncf %6353#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6355 = torch_c.from_builtin_tensor %6354 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6356 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6357 = torch.aten.view %6355, %6356 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %6358 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6359 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6360 = torch.aten.view %6327, %6359 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6361 = torch_c.to_builtin_tensor %6360 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6362 = torch_c.to_builtin_tensor %6358 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6363 = tensor.empty() : tensor<2048x1280xf32>
    %6364 = linalg.fill ins(%cst : f32) outs(%6363 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6365 = tensor.empty() : tensor<2048x1280xf32>
    %6366 = linalg.fill ins(%cst : f32) outs(%6365 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6367:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6364, %6366, %6361, %6362, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6364, %6366)
    %6368 = arith.truncf %6367#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6369 = torch_c.from_builtin_tensor %6368 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6370 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6371 = torch.aten.view %6369, %6370 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6372 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6373 = torch.aten.view %6343, %6372 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6374 = torch.aten.transpose.int %6373, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6375 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6376 = torch.aten.view %6357, %6375 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6377 = torch.aten.transpose.int %6376, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6378 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6379 = torch.aten.view %6371, %6378 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6380 = torch.aten.transpose.int %6379, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6381:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6374, %6377, %6380, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6382 = torch.aten.transpose.int %6381#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6383 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6384 = torch.aten.view %6382, %6383 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6385 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6386 = torch.aten.view %6384, %6385 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6387 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6388 = torch.aten.transpose.int %6387, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %6389 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6390 = torch.prims.convert_element_type %6389, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6391 = torch.prims.convert_element_type %6386, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6392 = torch.prims.convert_element_type %6388, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6393 = torch.aten.mm %6391, %6392 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6394 = torch.aten.mul.Scalar %6393, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6395 = torch.aten.mul.Scalar %6390, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6396 = torch.aten.add.Tensor %6394, %6395, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6397 = torch.prims.convert_element_type %6396, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6398 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6399 = torch.aten.view %6397, %6398 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6400 = torch.aten.div.Scalar %6399, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6401 = torch.aten.add.Tensor %6400, %6316, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6402 = torch.prims.convert_element_type %6401, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6403 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_195, %result1_196 = torch.aten.var_mean.correction %6402, %6403, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6404 = torch.aten.add.Scalar %result0_195, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6405 = torch.aten.rsqrt %6404 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6406 = torch.aten.sub.Tensor %6401, %result1_196, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6407 = torch.aten.mul.Tensor %6406, %6405 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %6408 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6409 = torch.aten.mul.Tensor %6407, %6408 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %6410 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6411 = torch.aten.add.Tensor %6409, %6410, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6412 = torch.prims.convert_element_type %6411, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6413 = torch.prims.convert_element_type %result1_196, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6414 = torch.prims.convert_element_type %6405, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %6415 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6416 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6417 = torch.aten.view %6412, %6416 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6418 = torch_c.to_builtin_tensor %6417 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6419 = torch_c.to_builtin_tensor %6415 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6420 = tensor.empty() : tensor<2048x1280xf32>
    %6421 = linalg.fill ins(%cst : f32) outs(%6420 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6422 = tensor.empty() : tensor<2048x1280xf32>
    %6423 = linalg.fill ins(%cst : f32) outs(%6422 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6424:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6421, %6423, %6418, %6419, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6421, %6423)
    %6425 = arith.truncf %6424#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6426 = torch_c.from_builtin_tensor %6425 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6427 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6428 = torch.aten.view %6426, %6427 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %6429 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6430 = torch.aten.transpose.int %6429, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6431 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6432 = torch.aten.view %4, %6431 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6433 = torch.aten.mm %6432, %6430 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6434 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6435 = torch.aten.view %6433, %6434 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %6436 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6437 = torch.aten.transpose.int %6436, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6438 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6439 = torch.aten.view %4, %6438 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6440 = torch.aten.mm %6439, %6437 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6441 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6442 = torch.aten.view %6440, %6441 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6443 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6444 = torch.aten.view %6428, %6443 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6445 = torch.aten.transpose.int %6444, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6446 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6447 = torch.aten.view %6435, %6446 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6448 = torch.aten.transpose.int %6447, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6449 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6450 = torch.aten.view %6442, %6449 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6451 = torch.aten.transpose.int %6450, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6452:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6445, %6448, %6451, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6453 = torch.aten.transpose.int %6452#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6454 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6455 = torch.aten.view %6453, %6454 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6456 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6457 = torch.aten.view %6455, %6456 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6458 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6459 = torch.aten.transpose.int %6458, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %6460 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6461 = torch.prims.convert_element_type %6460, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6462 = torch.prims.convert_element_type %6457, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6463 = torch.prims.convert_element_type %6459, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6464 = torch.aten.mm %6462, %6463 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6465 = torch.aten.mul.Scalar %6464, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6466 = torch.aten.mul.Scalar %6461, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6467 = torch.aten.add.Tensor %6465, %6466, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6468 = torch.prims.convert_element_type %6467, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6469 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6470 = torch.aten.view %6468, %6469 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6471 = torch.aten.div.Scalar %6470, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6472 = torch.aten.add.Tensor %6471, %6401, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6473 = torch.prims.convert_element_type %6472, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6474 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_197, %result1_198 = torch.aten.var_mean.correction %6473, %6474, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6475 = torch.aten.add.Scalar %result0_197, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6476 = torch.aten.rsqrt %6475 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6477 = torch.aten.sub.Tensor %6472, %result1_198, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6478 = torch.aten.mul.Tensor %6477, %6476 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %6479 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6480 = torch.aten.mul.Tensor %6478, %6479 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %6481 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6482 = torch.aten.add.Tensor %6480, %6481, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6483 = torch.prims.convert_element_type %6482, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6484 = torch.prims.convert_element_type %result1_198, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6485 = torch.prims.convert_element_type %6476, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6486 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6487 = torch.aten.view %6483, %6486 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6488 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6489 = torch.aten.transpose.int %6488, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %6490 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6491 = torch.prims.convert_element_type %6490, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6492 = torch.prims.convert_element_type %6487, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6493 = torch.prims.convert_element_type %6489, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6494 = torch.aten.mm %6492, %6493 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6495 = torch.aten.mul.Scalar %6494, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6496 = torch.aten.mul.Scalar %6491, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6497 = torch.aten.add.Tensor %6495, %6496, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6498 = torch.prims.convert_element_type %6497, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6499 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6500 = torch.aten.view %6498, %6499 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6501 = torch.aten.slice.Tensor %6500, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6502 = torch.aten.slice.Tensor %6500, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6503 = torch.aten.gelu %6502, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6504 = torch.aten.mul.Tensor %6501, %6503 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6505 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6506 = torch.aten.view %6504, %6505 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %6507 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6508 = torch.aten.transpose.int %6507, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %6509 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6510 = torch.prims.convert_element_type %6509, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6511 = torch.prims.convert_element_type %6506, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6512 = torch.prims.convert_element_type %6508, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6513 = torch.aten.mm %6511, %6512 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6514 = torch.aten.mul.Scalar %6513, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6515 = torch.aten.mul.Scalar %6510, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6516 = torch.aten.add.Tensor %6514, %6515, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6517 = torch.prims.convert_element_type %6516, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6518 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6519 = torch.aten.view %6517, %6518 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6520 = torch.aten.add.Tensor %6519, %6472, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6521 = torch.prims.convert_element_type %6520, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6522 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_199, %result1_200 = torch.aten.var_mean.correction %6521, %6522, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6523 = torch.aten.add.Scalar %result0_199, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6524 = torch.aten.rsqrt %6523 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6525 = torch.aten.sub.Tensor %6520, %result1_200, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6526 = torch.aten.mul.Tensor %6525, %6524 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %6527 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6528 = torch.aten.mul.Tensor %6526, %6527 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %6529 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6530 = torch.aten.add.Tensor %6528, %6529, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6531 = torch.prims.convert_element_type %6530, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6532 = torch.prims.convert_element_type %result1_200, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6533 = torch.prims.convert_element_type %6524, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %6534 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6535 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6536 = torch.aten.view %6531, %6535 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6537 = torch_c.to_builtin_tensor %6536 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6538 = torch_c.to_builtin_tensor %6534 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6539 = tensor.empty() : tensor<2048x1280xf32>
    %6540 = linalg.fill ins(%cst : f32) outs(%6539 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6541 = tensor.empty() : tensor<2048x1280xf32>
    %6542 = linalg.fill ins(%cst : f32) outs(%6541 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6543:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6540, %6542, %6537, %6538, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6540, %6542)
    %6544 = arith.truncf %6543#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6545 = torch_c.from_builtin_tensor %6544 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6546 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6547 = torch.aten.view %6545, %6546 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %6548 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6549 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6550 = torch.aten.view %6531, %6549 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6551 = torch_c.to_builtin_tensor %6550 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6552 = torch_c.to_builtin_tensor %6548 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6553 = tensor.empty() : tensor<2048x1280xf32>
    %6554 = linalg.fill ins(%cst : f32) outs(%6553 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6555 = tensor.empty() : tensor<2048x1280xf32>
    %6556 = linalg.fill ins(%cst : f32) outs(%6555 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6557:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6554, %6556, %6551, %6552, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6554, %6556)
    %6558 = arith.truncf %6557#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6559 = torch_c.from_builtin_tensor %6558 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6560 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6561 = torch.aten.view %6559, %6560 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %6562 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6563 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6564 = torch.aten.view %6531, %6563 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6565 = torch_c.to_builtin_tensor %6564 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6566 = torch_c.to_builtin_tensor %6562 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6567 = tensor.empty() : tensor<2048x1280xf32>
    %6568 = linalg.fill ins(%cst : f32) outs(%6567 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6569 = tensor.empty() : tensor<2048x1280xf32>
    %6570 = linalg.fill ins(%cst : f32) outs(%6569 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6571:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6568, %6570, %6565, %6566, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6568, %6570)
    %6572 = arith.truncf %6571#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6573 = torch_c.from_builtin_tensor %6572 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6574 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6575 = torch.aten.view %6573, %6574 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6576 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6577 = torch.aten.view %6547, %6576 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6578 = torch.aten.transpose.int %6577, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6579 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6580 = torch.aten.view %6561, %6579 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6581 = torch.aten.transpose.int %6580, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6582 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6583 = torch.aten.view %6575, %6582 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6584 = torch.aten.transpose.int %6583, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6585:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6578, %6581, %6584, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6586 = torch.aten.transpose.int %6585#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6587 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6588 = torch.aten.view %6586, %6587 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6589 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6590 = torch.aten.view %6588, %6589 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6591 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6592 = torch.aten.transpose.int %6591, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %6593 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6594 = torch.prims.convert_element_type %6593, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6595 = torch.prims.convert_element_type %6590, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6596 = torch.prims.convert_element_type %6592, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6597 = torch.aten.mm %6595, %6596 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6598 = torch.aten.mul.Scalar %6597, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6599 = torch.aten.mul.Scalar %6594, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6600 = torch.aten.add.Tensor %6598, %6599, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6601 = torch.prims.convert_element_type %6600, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6602 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6603 = torch.aten.view %6601, %6602 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6604 = torch.aten.div.Scalar %6603, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6605 = torch.aten.add.Tensor %6604, %6520, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6606 = torch.prims.convert_element_type %6605, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6607 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_201, %result1_202 = torch.aten.var_mean.correction %6606, %6607, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6608 = torch.aten.add.Scalar %result0_201, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6609 = torch.aten.rsqrt %6608 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6610 = torch.aten.sub.Tensor %6605, %result1_202, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6611 = torch.aten.mul.Tensor %6610, %6609 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %6612 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6613 = torch.aten.mul.Tensor %6611, %6612 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %6614 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6615 = torch.aten.add.Tensor %6613, %6614, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6616 = torch.prims.convert_element_type %6615, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6617 = torch.prims.convert_element_type %result1_202, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6618 = torch.prims.convert_element_type %6609, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %6619 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6620 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6621 = torch.aten.view %6616, %6620 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6622 = torch_c.to_builtin_tensor %6621 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6623 = torch_c.to_builtin_tensor %6619 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6624 = tensor.empty() : tensor<2048x1280xf32>
    %6625 = linalg.fill ins(%cst : f32) outs(%6624 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6626 = tensor.empty() : tensor<2048x1280xf32>
    %6627 = linalg.fill ins(%cst : f32) outs(%6626 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6628:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6625, %6627, %6622, %6623, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6625, %6627)
    %6629 = arith.truncf %6628#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6630 = torch_c.from_builtin_tensor %6629 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6631 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6632 = torch.aten.view %6630, %6631 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %6633 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6634 = torch.aten.transpose.int %6633, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6635 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6636 = torch.aten.view %4, %6635 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6637 = torch.aten.mm %6636, %6634 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6638 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6639 = torch.aten.view %6637, %6638 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %6640 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6641 = torch.aten.transpose.int %6640, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6642 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6643 = torch.aten.view %4, %6642 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6644 = torch.aten.mm %6643, %6641 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6645 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6646 = torch.aten.view %6644, %6645 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6647 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6648 = torch.aten.view %6632, %6647 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6649 = torch.aten.transpose.int %6648, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6650 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6651 = torch.aten.view %6639, %6650 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6652 = torch.aten.transpose.int %6651, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6653 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6654 = torch.aten.view %6646, %6653 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6655 = torch.aten.transpose.int %6654, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6656:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6649, %6652, %6655, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6657 = torch.aten.transpose.int %6656#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6658 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6659 = torch.aten.view %6657, %6658 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6660 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6661 = torch.aten.view %6659, %6660 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6662 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6663 = torch.aten.transpose.int %6662, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %6664 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6665 = torch.prims.convert_element_type %6664, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6666 = torch.prims.convert_element_type %6661, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6667 = torch.prims.convert_element_type %6663, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6668 = torch.aten.mm %6666, %6667 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6669 = torch.aten.mul.Scalar %6668, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6670 = torch.aten.mul.Scalar %6665, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6671 = torch.aten.add.Tensor %6669, %6670, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6672 = torch.prims.convert_element_type %6671, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6673 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6674 = torch.aten.view %6672, %6673 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6675 = torch.aten.div.Scalar %6674, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6676 = torch.aten.add.Tensor %6675, %6605, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6677 = torch.prims.convert_element_type %6676, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6678 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_203, %result1_204 = torch.aten.var_mean.correction %6677, %6678, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6679 = torch.aten.add.Scalar %result0_203, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6680 = torch.aten.rsqrt %6679 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6681 = torch.aten.sub.Tensor %6676, %result1_204, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6682 = torch.aten.mul.Tensor %6681, %6680 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %6683 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6684 = torch.aten.mul.Tensor %6682, %6683 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %6685 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6686 = torch.aten.add.Tensor %6684, %6685, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6687 = torch.prims.convert_element_type %6686, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6688 = torch.prims.convert_element_type %result1_204, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6689 = torch.prims.convert_element_type %6680, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6690 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6691 = torch.aten.view %6687, %6690 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6692 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6693 = torch.aten.transpose.int %6692, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %6694 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6695 = torch.prims.convert_element_type %6694, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6696 = torch.prims.convert_element_type %6691, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6697 = torch.prims.convert_element_type %6693, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6698 = torch.aten.mm %6696, %6697 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6699 = torch.aten.mul.Scalar %6698, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6700 = torch.aten.mul.Scalar %6695, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6701 = torch.aten.add.Tensor %6699, %6700, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6702 = torch.prims.convert_element_type %6701, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6703 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6704 = torch.aten.view %6702, %6703 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6705 = torch.aten.slice.Tensor %6704, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6706 = torch.aten.slice.Tensor %6704, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6707 = torch.aten.gelu %6706, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6708 = torch.aten.mul.Tensor %6705, %6707 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6709 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6710 = torch.aten.view %6708, %6709 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %6711 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6712 = torch.aten.transpose.int %6711, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %6713 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6714 = torch.prims.convert_element_type %6713, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6715 = torch.prims.convert_element_type %6710, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6716 = torch.prims.convert_element_type %6712, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6717 = torch.aten.mm %6715, %6716 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6718 = torch.aten.mul.Scalar %6717, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6719 = torch.aten.mul.Scalar %6714, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6720 = torch.aten.add.Tensor %6718, %6719, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6721 = torch.prims.convert_element_type %6720, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6722 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6723 = torch.aten.view %6721, %6722 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6724 = torch.aten.add.Tensor %6723, %6676, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6725 = torch.prims.convert_element_type %6724, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6726 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_205, %result1_206 = torch.aten.var_mean.correction %6725, %6726, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6727 = torch.aten.add.Scalar %result0_205, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6728 = torch.aten.rsqrt %6727 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6729 = torch.aten.sub.Tensor %6724, %result1_206, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6730 = torch.aten.mul.Tensor %6729, %6728 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %6731 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6732 = torch.aten.mul.Tensor %6730, %6731 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %6733 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6734 = torch.aten.add.Tensor %6732, %6733, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6735 = torch.prims.convert_element_type %6734, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6736 = torch.prims.convert_element_type %result1_206, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6737 = torch.prims.convert_element_type %6728, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %6738 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6739 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6740 = torch.aten.view %6735, %6739 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6741 = torch_c.to_builtin_tensor %6740 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6742 = torch_c.to_builtin_tensor %6738 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6743 = tensor.empty() : tensor<2048x1280xf32>
    %6744 = linalg.fill ins(%cst : f32) outs(%6743 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6745 = tensor.empty() : tensor<2048x1280xf32>
    %6746 = linalg.fill ins(%cst : f32) outs(%6745 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6747:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6744, %6746, %6741, %6742, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6744, %6746)
    %6748 = arith.truncf %6747#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6749 = torch_c.from_builtin_tensor %6748 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6750 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6751 = torch.aten.view %6749, %6750 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %6752 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6753 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6754 = torch.aten.view %6735, %6753 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6755 = torch_c.to_builtin_tensor %6754 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6756 = torch_c.to_builtin_tensor %6752 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6757 = tensor.empty() : tensor<2048x1280xf32>
    %6758 = linalg.fill ins(%cst : f32) outs(%6757 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6759 = tensor.empty() : tensor<2048x1280xf32>
    %6760 = linalg.fill ins(%cst : f32) outs(%6759 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6761:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6758, %6760, %6755, %6756, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6758, %6760)
    %6762 = arith.truncf %6761#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6763 = torch_c.from_builtin_tensor %6762 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6764 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6765 = torch.aten.view %6763, %6764 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %6766 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6767 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6768 = torch.aten.view %6735, %6767 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6769 = torch_c.to_builtin_tensor %6768 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6770 = torch_c.to_builtin_tensor %6766 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6771 = tensor.empty() : tensor<2048x1280xf32>
    %6772 = linalg.fill ins(%cst : f32) outs(%6771 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6773 = tensor.empty() : tensor<2048x1280xf32>
    %6774 = linalg.fill ins(%cst : f32) outs(%6773 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6775:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6772, %6774, %6769, %6770, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6772, %6774)
    %6776 = arith.truncf %6775#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6777 = torch_c.from_builtin_tensor %6776 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6778 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6779 = torch.aten.view %6777, %6778 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6780 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6781 = torch.aten.view %6751, %6780 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6782 = torch.aten.transpose.int %6781, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6783 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6784 = torch.aten.view %6765, %6783 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6785 = torch.aten.transpose.int %6784, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6786 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6787 = torch.aten.view %6779, %6786 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6788 = torch.aten.transpose.int %6787, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6789:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6782, %6785, %6788, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6790 = torch.aten.transpose.int %6789#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6791 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6792 = torch.aten.view %6790, %6791 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6793 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6794 = torch.aten.view %6792, %6793 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6795 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6796 = torch.aten.transpose.int %6795, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %6797 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6798 = torch.prims.convert_element_type %6797, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6799 = torch.prims.convert_element_type %6794, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6800 = torch.prims.convert_element_type %6796, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6801 = torch.aten.mm %6799, %6800 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6802 = torch.aten.mul.Scalar %6801, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6803 = torch.aten.mul.Scalar %6798, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6804 = torch.aten.add.Tensor %6802, %6803, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6805 = torch.prims.convert_element_type %6804, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6806 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6807 = torch.aten.view %6805, %6806 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6808 = torch.aten.div.Scalar %6807, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6809 = torch.aten.add.Tensor %6808, %6724, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6810 = torch.prims.convert_element_type %6809, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6811 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_207, %result1_208 = torch.aten.var_mean.correction %6810, %6811, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6812 = torch.aten.add.Scalar %result0_207, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6813 = torch.aten.rsqrt %6812 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6814 = torch.aten.sub.Tensor %6809, %result1_208, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6815 = torch.aten.mul.Tensor %6814, %6813 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %6816 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6817 = torch.aten.mul.Tensor %6815, %6816 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %6818 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6819 = torch.aten.add.Tensor %6817, %6818, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6820 = torch.prims.convert_element_type %6819, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6821 = torch.prims.convert_element_type %result1_208, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6822 = torch.prims.convert_element_type %6813, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %6823 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6824 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6825 = torch.aten.view %6820, %6824 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6826 = torch_c.to_builtin_tensor %6825 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6827 = torch_c.to_builtin_tensor %6823 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6828 = tensor.empty() : tensor<2048x1280xf32>
    %6829 = linalg.fill ins(%cst : f32) outs(%6828 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6830 = tensor.empty() : tensor<2048x1280xf32>
    %6831 = linalg.fill ins(%cst : f32) outs(%6830 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6832:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6829, %6831, %6826, %6827, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6829, %6831)
    %6833 = arith.truncf %6832#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6834 = torch_c.from_builtin_tensor %6833 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6835 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6836 = torch.aten.view %6834, %6835 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %6837 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6838 = torch.aten.transpose.int %6837, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6839 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6840 = torch.aten.view %4, %6839 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6841 = torch.aten.mm %6840, %6838 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6842 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6843 = torch.aten.view %6841, %6842 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %6844 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6845 = torch.aten.transpose.int %6844, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6846 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6847 = torch.aten.view %4, %6846 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6848 = torch.aten.mm %6847, %6845 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %6849 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6850 = torch.aten.view %6848, %6849 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6851 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6852 = torch.aten.view %6836, %6851 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6853 = torch.aten.transpose.int %6852, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6854 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6855 = torch.aten.view %6843, %6854 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6856 = torch.aten.transpose.int %6855, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6857 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6858 = torch.aten.view %6850, %6857 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6859 = torch.aten.transpose.int %6858, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6860:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6853, %6856, %6859, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6861 = torch.aten.transpose.int %6860#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6862 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6863 = torch.aten.view %6861, %6862 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6864 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6865 = torch.aten.view %6863, %6864 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6866 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6867 = torch.aten.transpose.int %6866, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %6868 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6869 = torch.prims.convert_element_type %6868, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6870 = torch.prims.convert_element_type %6865, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6871 = torch.prims.convert_element_type %6867, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6872 = torch.aten.mm %6870, %6871 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6873 = torch.aten.mul.Scalar %6872, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6874 = torch.aten.mul.Scalar %6869, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6875 = torch.aten.add.Tensor %6873, %6874, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6876 = torch.prims.convert_element_type %6875, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6877 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6878 = torch.aten.view %6876, %6877 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6879 = torch.aten.div.Scalar %6878, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6880 = torch.aten.add.Tensor %6879, %6809, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6881 = torch.prims.convert_element_type %6880, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6882 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_209, %result1_210 = torch.aten.var_mean.correction %6881, %6882, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6883 = torch.aten.add.Scalar %result0_209, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6884 = torch.aten.rsqrt %6883 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6885 = torch.aten.sub.Tensor %6880, %result1_210, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6886 = torch.aten.mul.Tensor %6885, %6884 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %6887 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6888 = torch.aten.mul.Tensor %6886, %6887 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %6889 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6890 = torch.aten.add.Tensor %6888, %6889, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6891 = torch.prims.convert_element_type %6890, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6892 = torch.prims.convert_element_type %result1_210, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6893 = torch.prims.convert_element_type %6884, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6894 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6895 = torch.aten.view %6891, %6894 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6896 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6897 = torch.aten.transpose.int %6896, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %6898 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6899 = torch.prims.convert_element_type %6898, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6900 = torch.prims.convert_element_type %6895, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6901 = torch.prims.convert_element_type %6897, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6902 = torch.aten.mm %6900, %6901 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6903 = torch.aten.mul.Scalar %6902, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6904 = torch.aten.mul.Scalar %6899, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6905 = torch.aten.add.Tensor %6903, %6904, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6906 = torch.prims.convert_element_type %6905, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6907 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6908 = torch.aten.view %6906, %6907 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6909 = torch.aten.slice.Tensor %6908, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6910 = torch.aten.slice.Tensor %6908, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6911 = torch.aten.gelu %6910, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6912 = torch.aten.mul.Tensor %6909, %6911 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6913 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6914 = torch.aten.view %6912, %6913 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %6915 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6916 = torch.aten.transpose.int %6915, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %6917 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6918 = torch.prims.convert_element_type %6917, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6919 = torch.prims.convert_element_type %6914, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6920 = torch.prims.convert_element_type %6916, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6921 = torch.aten.mm %6919, %6920 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6922 = torch.aten.mul.Scalar %6921, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6923 = torch.aten.mul.Scalar %6918, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6924 = torch.aten.add.Tensor %6922, %6923, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6925 = torch.prims.convert_element_type %6924, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6926 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6927 = torch.aten.view %6925, %6926 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6928 = torch.aten.add.Tensor %6927, %6880, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6929 = torch.prims.convert_element_type %6928, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6930 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_211, %result1_212 = torch.aten.var_mean.correction %6929, %6930, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6931 = torch.aten.add.Scalar %result0_211, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6932 = torch.aten.rsqrt %6931 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6933 = torch.aten.sub.Tensor %6928, %result1_212, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6934 = torch.aten.mul.Tensor %6933, %6932 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %6935 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6936 = torch.aten.mul.Tensor %6934, %6935 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %6937 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6938 = torch.aten.add.Tensor %6936, %6937, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6939 = torch.prims.convert_element_type %6938, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6940 = torch.prims.convert_element_type %result1_212, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6941 = torch.prims.convert_element_type %6932, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %6942 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6943 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6944 = torch.aten.view %6939, %6943 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6945 = torch_c.to_builtin_tensor %6944 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6946 = torch_c.to_builtin_tensor %6942 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6947 = tensor.empty() : tensor<2048x1280xf32>
    %6948 = linalg.fill ins(%cst : f32) outs(%6947 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6949 = tensor.empty() : tensor<2048x1280xf32>
    %6950 = linalg.fill ins(%cst : f32) outs(%6949 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6951:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6948, %6950, %6945, %6946, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6948, %6950)
    %6952 = arith.truncf %6951#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6953 = torch_c.from_builtin_tensor %6952 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6954 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6955 = torch.aten.view %6953, %6954 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %6956 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6957 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6958 = torch.aten.view %6939, %6957 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6959 = torch_c.to_builtin_tensor %6958 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6960 = torch_c.to_builtin_tensor %6956 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6961 = tensor.empty() : tensor<2048x1280xf32>
    %6962 = linalg.fill ins(%cst : f32) outs(%6961 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6963 = tensor.empty() : tensor<2048x1280xf32>
    %6964 = linalg.fill ins(%cst : f32) outs(%6963 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6965:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6962, %6964, %6959, %6960, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6962, %6964)
    %6966 = arith.truncf %6965#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6967 = torch_c.from_builtin_tensor %6966 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6968 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6969 = torch.aten.view %6967, %6968 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %6970 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6971 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6972 = torch.aten.view %6939, %6971 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6973 = torch_c.to_builtin_tensor %6972 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6974 = torch_c.to_builtin_tensor %6970 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6975 = tensor.empty() : tensor<2048x1280xf32>
    %6976 = linalg.fill ins(%cst : f32) outs(%6975 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6977 = tensor.empty() : tensor<2048x1280xf32>
    %6978 = linalg.fill ins(%cst : f32) outs(%6977 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6979:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6976, %6978, %6973, %6974, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6976, %6978)
    %6980 = arith.truncf %6979#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6981 = torch_c.from_builtin_tensor %6980 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6982 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6983 = torch.aten.view %6981, %6982 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6984 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6985 = torch.aten.view %6955, %6984 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6986 = torch.aten.transpose.int %6985, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6987 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6988 = torch.aten.view %6969, %6987 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6989 = torch.aten.transpose.int %6988, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6990 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6991 = torch.aten.view %6983, %6990 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6992 = torch.aten.transpose.int %6991, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6993:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6986, %6989, %6992, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6994 = torch.aten.transpose.int %6993#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6995 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6996 = torch.aten.view %6994, %6995 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6997 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6998 = torch.aten.view %6996, %6997 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6999 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7000 = torch.aten.transpose.int %6999, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %7001 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7002 = torch.prims.convert_element_type %7001, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7003 = torch.prims.convert_element_type %6998, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7004 = torch.prims.convert_element_type %7000, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7005 = torch.aten.mm %7003, %7004 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7006 = torch.aten.mul.Scalar %7005, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7007 = torch.aten.mul.Scalar %7002, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7008 = torch.aten.add.Tensor %7006, %7007, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7009 = torch.prims.convert_element_type %7008, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7010 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7011 = torch.aten.view %7009, %7010 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7012 = torch.aten.div.Scalar %7011, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7013 = torch.aten.add.Tensor %7012, %6928, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7014 = torch.prims.convert_element_type %7013, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7015 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_213, %result1_214 = torch.aten.var_mean.correction %7014, %7015, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7016 = torch.aten.add.Scalar %result0_213, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7017 = torch.aten.rsqrt %7016 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7018 = torch.aten.sub.Tensor %7013, %result1_214, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7019 = torch.aten.mul.Tensor %7018, %7017 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %7020 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7021 = torch.aten.mul.Tensor %7019, %7020 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %7022 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7023 = torch.aten.add.Tensor %7021, %7022, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7024 = torch.prims.convert_element_type %7023, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7025 = torch.prims.convert_element_type %result1_214, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7026 = torch.prims.convert_element_type %7017, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %7027 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7028 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7029 = torch.aten.view %7024, %7028 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7030 = torch_c.to_builtin_tensor %7029 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7031 = torch_c.to_builtin_tensor %7027 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7032 = tensor.empty() : tensor<2048x1280xf32>
    %7033 = linalg.fill ins(%cst : f32) outs(%7032 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7034 = tensor.empty() : tensor<2048x1280xf32>
    %7035 = linalg.fill ins(%cst : f32) outs(%7034 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7036:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7033, %7035, %7030, %7031, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7033, %7035)
    %7037 = arith.truncf %7036#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7038 = torch_c.from_builtin_tensor %7037 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7039 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7040 = torch.aten.view %7038, %7039 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %7041 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7042 = torch.aten.transpose.int %7041, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7043 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7044 = torch.aten.view %4, %7043 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7045 = torch.aten.mm %7044, %7042 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7046 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7047 = torch.aten.view %7045, %7046 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %7048 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7049 = torch.aten.transpose.int %7048, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7050 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7051 = torch.aten.view %4, %7050 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7052 = torch.aten.mm %7051, %7049 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7053 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7054 = torch.aten.view %7052, %7053 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7055 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7056 = torch.aten.view %7040, %7055 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7057 = torch.aten.transpose.int %7056, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7058 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7059 = torch.aten.view %7047, %7058 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7060 = torch.aten.transpose.int %7059, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7061 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7062 = torch.aten.view %7054, %7061 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7063 = torch.aten.transpose.int %7062, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7064:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7057, %7060, %7063, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7065 = torch.aten.transpose.int %7064#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7066 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7067 = torch.aten.view %7065, %7066 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7068 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7069 = torch.aten.view %7067, %7068 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7070 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7071 = torch.aten.transpose.int %7070, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %7072 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7073 = torch.prims.convert_element_type %7072, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7074 = torch.prims.convert_element_type %7069, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7075 = torch.prims.convert_element_type %7071, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7076 = torch.aten.mm %7074, %7075 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7077 = torch.aten.mul.Scalar %7076, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7078 = torch.aten.mul.Scalar %7073, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7079 = torch.aten.add.Tensor %7077, %7078, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7080 = torch.prims.convert_element_type %7079, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7081 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7082 = torch.aten.view %7080, %7081 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7083 = torch.aten.div.Scalar %7082, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7084 = torch.aten.add.Tensor %7083, %7013, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7085 = torch.prims.convert_element_type %7084, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7086 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_215, %result1_216 = torch.aten.var_mean.correction %7085, %7086, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7087 = torch.aten.add.Scalar %result0_215, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7088 = torch.aten.rsqrt %7087 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7089 = torch.aten.sub.Tensor %7084, %result1_216, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7090 = torch.aten.mul.Tensor %7089, %7088 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %7091 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7092 = torch.aten.mul.Tensor %7090, %7091 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %7093 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7094 = torch.aten.add.Tensor %7092, %7093, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7095 = torch.prims.convert_element_type %7094, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7096 = torch.prims.convert_element_type %result1_216, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7097 = torch.prims.convert_element_type %7088, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7098 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7099 = torch.aten.view %7095, %7098 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7100 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7101 = torch.aten.transpose.int %7100, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %7102 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7103 = torch.prims.convert_element_type %7102, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7104 = torch.prims.convert_element_type %7099, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7105 = torch.prims.convert_element_type %7101, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7106 = torch.aten.mm %7104, %7105 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7107 = torch.aten.mul.Scalar %7106, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7108 = torch.aten.mul.Scalar %7103, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7109 = torch.aten.add.Tensor %7107, %7108, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7110 = torch.prims.convert_element_type %7109, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7111 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7112 = torch.aten.view %7110, %7111 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7113 = torch.aten.slice.Tensor %7112, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7114 = torch.aten.slice.Tensor %7112, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7115 = torch.aten.gelu %7114, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7116 = torch.aten.mul.Tensor %7113, %7115 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7117 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7118 = torch.aten.view %7116, %7117 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %7119 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7120 = torch.aten.transpose.int %7119, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %7121 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7122 = torch.prims.convert_element_type %7121, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7123 = torch.prims.convert_element_type %7118, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7124 = torch.prims.convert_element_type %7120, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7125 = torch.aten.mm %7123, %7124 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7126 = torch.aten.mul.Scalar %7125, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7127 = torch.aten.mul.Scalar %7122, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7128 = torch.aten.add.Tensor %7126, %7127, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7129 = torch.prims.convert_element_type %7128, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7130 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7131 = torch.aten.view %7129, %7130 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7132 = torch.aten.add.Tensor %7131, %7084, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7133 = torch.prims.convert_element_type %7132, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7134 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_217, %result1_218 = torch.aten.var_mean.correction %7133, %7134, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7135 = torch.aten.add.Scalar %result0_217, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7136 = torch.aten.rsqrt %7135 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7137 = torch.aten.sub.Tensor %7132, %result1_218, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7138 = torch.aten.mul.Tensor %7137, %7136 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %7139 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7140 = torch.aten.mul.Tensor %7138, %7139 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %7141 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7142 = torch.aten.add.Tensor %7140, %7141, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7143 = torch.prims.convert_element_type %7142, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7144 = torch.prims.convert_element_type %result1_218, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7145 = torch.prims.convert_element_type %7136, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %7146 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7147 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7148 = torch.aten.view %7143, %7147 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7149 = torch_c.to_builtin_tensor %7148 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7150 = torch_c.to_builtin_tensor %7146 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7151 = tensor.empty() : tensor<2048x1280xf32>
    %7152 = linalg.fill ins(%cst : f32) outs(%7151 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7153 = tensor.empty() : tensor<2048x1280xf32>
    %7154 = linalg.fill ins(%cst : f32) outs(%7153 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7155:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7152, %7154, %7149, %7150, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7152, %7154)
    %7156 = arith.truncf %7155#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7157 = torch_c.from_builtin_tensor %7156 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7158 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7159 = torch.aten.view %7157, %7158 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %7160 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7161 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7162 = torch.aten.view %7143, %7161 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7163 = torch_c.to_builtin_tensor %7162 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7164 = torch_c.to_builtin_tensor %7160 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7165 = tensor.empty() : tensor<2048x1280xf32>
    %7166 = linalg.fill ins(%cst : f32) outs(%7165 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7167 = tensor.empty() : tensor<2048x1280xf32>
    %7168 = linalg.fill ins(%cst : f32) outs(%7167 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7169:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7166, %7168, %7163, %7164, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7166, %7168)
    %7170 = arith.truncf %7169#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7171 = torch_c.from_builtin_tensor %7170 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7172 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7173 = torch.aten.view %7171, %7172 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %7174 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7175 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7176 = torch.aten.view %7143, %7175 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7177 = torch_c.to_builtin_tensor %7176 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7178 = torch_c.to_builtin_tensor %7174 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7179 = tensor.empty() : tensor<2048x1280xf32>
    %7180 = linalg.fill ins(%cst : f32) outs(%7179 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7181 = tensor.empty() : tensor<2048x1280xf32>
    %7182 = linalg.fill ins(%cst : f32) outs(%7181 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7183:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7180, %7182, %7177, %7178, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7180, %7182)
    %7184 = arith.truncf %7183#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7185 = torch_c.from_builtin_tensor %7184 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7186 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7187 = torch.aten.view %7185, %7186 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7188 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7189 = torch.aten.view %7159, %7188 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7190 = torch.aten.transpose.int %7189, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7191 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7192 = torch.aten.view %7173, %7191 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7193 = torch.aten.transpose.int %7192, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7194 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7195 = torch.aten.view %7187, %7194 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7196 = torch.aten.transpose.int %7195, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7197:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7190, %7193, %7196, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7198 = torch.aten.transpose.int %7197#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7199 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7200 = torch.aten.view %7198, %7199 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7201 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7202 = torch.aten.view %7200, %7201 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7203 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7204 = torch.aten.transpose.int %7203, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %7205 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7206 = torch.prims.convert_element_type %7205, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7207 = torch.prims.convert_element_type %7202, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7208 = torch.prims.convert_element_type %7204, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7209 = torch.aten.mm %7207, %7208 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7210 = torch.aten.mul.Scalar %7209, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7211 = torch.aten.mul.Scalar %7206, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7212 = torch.aten.add.Tensor %7210, %7211, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7213 = torch.prims.convert_element_type %7212, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7214 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7215 = torch.aten.view %7213, %7214 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7216 = torch.aten.div.Scalar %7215, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7217 = torch.aten.add.Tensor %7216, %7132, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7218 = torch.prims.convert_element_type %7217, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7219 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_219, %result1_220 = torch.aten.var_mean.correction %7218, %7219, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7220 = torch.aten.add.Scalar %result0_219, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7221 = torch.aten.rsqrt %7220 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7222 = torch.aten.sub.Tensor %7217, %result1_220, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7223 = torch.aten.mul.Tensor %7222, %7221 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %7224 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7225 = torch.aten.mul.Tensor %7223, %7224 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %7226 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7227 = torch.aten.add.Tensor %7225, %7226, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7228 = torch.prims.convert_element_type %7227, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7229 = torch.prims.convert_element_type %result1_220, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7230 = torch.prims.convert_element_type %7221, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %7231 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7232 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7233 = torch.aten.view %7228, %7232 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7234 = torch_c.to_builtin_tensor %7233 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7235 = torch_c.to_builtin_tensor %7231 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7236 = tensor.empty() : tensor<2048x1280xf32>
    %7237 = linalg.fill ins(%cst : f32) outs(%7236 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7238 = tensor.empty() : tensor<2048x1280xf32>
    %7239 = linalg.fill ins(%cst : f32) outs(%7238 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7240:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7237, %7239, %7234, %7235, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7237, %7239)
    %7241 = arith.truncf %7240#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7242 = torch_c.from_builtin_tensor %7241 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7243 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7244 = torch.aten.view %7242, %7243 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %7245 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7246 = torch.aten.transpose.int %7245, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7247 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7248 = torch.aten.view %4, %7247 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7249 = torch.aten.mm %7248, %7246 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7250 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7251 = torch.aten.view %7249, %7250 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %7252 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7253 = torch.aten.transpose.int %7252, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7254 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7255 = torch.aten.view %4, %7254 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7256 = torch.aten.mm %7255, %7253 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7257 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7258 = torch.aten.view %7256, %7257 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7259 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7260 = torch.aten.view %7244, %7259 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7261 = torch.aten.transpose.int %7260, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7262 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7263 = torch.aten.view %7251, %7262 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7264 = torch.aten.transpose.int %7263, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7265 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7266 = torch.aten.view %7258, %7265 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7267 = torch.aten.transpose.int %7266, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7268:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7261, %7264, %7267, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7269 = torch.aten.transpose.int %7268#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7270 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7271 = torch.aten.view %7269, %7270 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7272 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7273 = torch.aten.view %7271, %7272 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7274 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7275 = torch.aten.transpose.int %7274, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %7276 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7277 = torch.prims.convert_element_type %7276, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7278 = torch.prims.convert_element_type %7273, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7279 = torch.prims.convert_element_type %7275, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7280 = torch.aten.mm %7278, %7279 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7281 = torch.aten.mul.Scalar %7280, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7282 = torch.aten.mul.Scalar %7277, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7283 = torch.aten.add.Tensor %7281, %7282, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7284 = torch.prims.convert_element_type %7283, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7285 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7286 = torch.aten.view %7284, %7285 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7287 = torch.aten.div.Scalar %7286, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7288 = torch.aten.add.Tensor %7287, %7217, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7289 = torch.prims.convert_element_type %7288, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7290 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_221, %result1_222 = torch.aten.var_mean.correction %7289, %7290, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7291 = torch.aten.add.Scalar %result0_221, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7292 = torch.aten.rsqrt %7291 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7293 = torch.aten.sub.Tensor %7288, %result1_222, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7294 = torch.aten.mul.Tensor %7293, %7292 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %7295 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7296 = torch.aten.mul.Tensor %7294, %7295 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %7297 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7298 = torch.aten.add.Tensor %7296, %7297, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7299 = torch.prims.convert_element_type %7298, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7300 = torch.prims.convert_element_type %result1_222, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7301 = torch.prims.convert_element_type %7292, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7302 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7303 = torch.aten.view %7299, %7302 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7304 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7305 = torch.aten.transpose.int %7304, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %7306 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7307 = torch.prims.convert_element_type %7306, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7308 = torch.prims.convert_element_type %7303, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7309 = torch.prims.convert_element_type %7305, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7310 = torch.aten.mm %7308, %7309 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7311 = torch.aten.mul.Scalar %7310, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7312 = torch.aten.mul.Scalar %7307, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7313 = torch.aten.add.Tensor %7311, %7312, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7314 = torch.prims.convert_element_type %7313, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7315 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7316 = torch.aten.view %7314, %7315 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7317 = torch.aten.slice.Tensor %7316, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7318 = torch.aten.slice.Tensor %7316, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7319 = torch.aten.gelu %7318, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7320 = torch.aten.mul.Tensor %7317, %7319 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7321 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7322 = torch.aten.view %7320, %7321 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %7323 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7324 = torch.aten.transpose.int %7323, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %7325 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7326 = torch.prims.convert_element_type %7325, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7327 = torch.prims.convert_element_type %7322, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7328 = torch.prims.convert_element_type %7324, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7329 = torch.aten.mm %7327, %7328 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7330 = torch.aten.mul.Scalar %7329, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7331 = torch.aten.mul.Scalar %7326, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7332 = torch.aten.add.Tensor %7330, %7331, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7333 = torch.prims.convert_element_type %7332, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7334 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7335 = torch.aten.view %7333, %7334 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7336 = torch.aten.add.Tensor %7335, %7288, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7337 = torch.prims.convert_element_type %7336, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7338 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_223, %result1_224 = torch.aten.var_mean.correction %7337, %7338, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7339 = torch.aten.add.Scalar %result0_223, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7340 = torch.aten.rsqrt %7339 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7341 = torch.aten.sub.Tensor %7336, %result1_224, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7342 = torch.aten.mul.Tensor %7341, %7340 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %7343 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7344 = torch.aten.mul.Tensor %7342, %7343 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %7345 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7346 = torch.aten.add.Tensor %7344, %7345, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7347 = torch.prims.convert_element_type %7346, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7348 = torch.prims.convert_element_type %result1_224, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7349 = torch.prims.convert_element_type %7340, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %7350 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7351 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7352 = torch.aten.view %7347, %7351 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7353 = torch_c.to_builtin_tensor %7352 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7354 = torch_c.to_builtin_tensor %7350 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7355 = tensor.empty() : tensor<2048x1280xf32>
    %7356 = linalg.fill ins(%cst : f32) outs(%7355 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7357 = tensor.empty() : tensor<2048x1280xf32>
    %7358 = linalg.fill ins(%cst : f32) outs(%7357 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7359:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7356, %7358, %7353, %7354, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7356, %7358)
    %7360 = arith.truncf %7359#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7361 = torch_c.from_builtin_tensor %7360 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7362 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7363 = torch.aten.view %7361, %7362 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %7364 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7365 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7366 = torch.aten.view %7347, %7365 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7367 = torch_c.to_builtin_tensor %7366 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7368 = torch_c.to_builtin_tensor %7364 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7369 = tensor.empty() : tensor<2048x1280xf32>
    %7370 = linalg.fill ins(%cst : f32) outs(%7369 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7371 = tensor.empty() : tensor<2048x1280xf32>
    %7372 = linalg.fill ins(%cst : f32) outs(%7371 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7373:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7370, %7372, %7367, %7368, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7370, %7372)
    %7374 = arith.truncf %7373#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7375 = torch_c.from_builtin_tensor %7374 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7376 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7377 = torch.aten.view %7375, %7376 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %7378 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7379 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7380 = torch.aten.view %7347, %7379 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7381 = torch_c.to_builtin_tensor %7380 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7382 = torch_c.to_builtin_tensor %7378 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7383 = tensor.empty() : tensor<2048x1280xf32>
    %7384 = linalg.fill ins(%cst : f32) outs(%7383 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7385 = tensor.empty() : tensor<2048x1280xf32>
    %7386 = linalg.fill ins(%cst : f32) outs(%7385 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7387:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7384, %7386, %7381, %7382, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7384, %7386)
    %7388 = arith.truncf %7387#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7389 = torch_c.from_builtin_tensor %7388 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7390 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7391 = torch.aten.view %7389, %7390 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7392 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7393 = torch.aten.view %7363, %7392 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7394 = torch.aten.transpose.int %7393, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7395 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7396 = torch.aten.view %7377, %7395 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7397 = torch.aten.transpose.int %7396, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7398 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7399 = torch.aten.view %7391, %7398 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7400 = torch.aten.transpose.int %7399, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7401:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7394, %7397, %7400, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7402 = torch.aten.transpose.int %7401#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7403 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7404 = torch.aten.view %7402, %7403 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7405 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7406 = torch.aten.view %7404, %7405 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7407 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7408 = torch.aten.transpose.int %7407, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %7409 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7410 = torch.prims.convert_element_type %7409, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7411 = torch.prims.convert_element_type %7406, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7412 = torch.prims.convert_element_type %7408, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7413 = torch.aten.mm %7411, %7412 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7414 = torch.aten.mul.Scalar %7413, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7415 = torch.aten.mul.Scalar %7410, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7416 = torch.aten.add.Tensor %7414, %7415, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7417 = torch.prims.convert_element_type %7416, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7418 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7419 = torch.aten.view %7417, %7418 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7420 = torch.aten.div.Scalar %7419, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7421 = torch.aten.add.Tensor %7420, %7336, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7422 = torch.prims.convert_element_type %7421, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7423 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_225, %result1_226 = torch.aten.var_mean.correction %7422, %7423, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7424 = torch.aten.add.Scalar %result0_225, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7425 = torch.aten.rsqrt %7424 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7426 = torch.aten.sub.Tensor %7421, %result1_226, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7427 = torch.aten.mul.Tensor %7426, %7425 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %7428 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7429 = torch.aten.mul.Tensor %7427, %7428 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %7430 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7431 = torch.aten.add.Tensor %7429, %7430, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7432 = torch.prims.convert_element_type %7431, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7433 = torch.prims.convert_element_type %result1_226, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7434 = torch.prims.convert_element_type %7425, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %7435 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7436 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7437 = torch.aten.view %7432, %7436 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7438 = torch_c.to_builtin_tensor %7437 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7439 = torch_c.to_builtin_tensor %7435 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7440 = tensor.empty() : tensor<2048x1280xf32>
    %7441 = linalg.fill ins(%cst : f32) outs(%7440 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7442 = tensor.empty() : tensor<2048x1280xf32>
    %7443 = linalg.fill ins(%cst : f32) outs(%7442 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7444:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7441, %7443, %7438, %7439, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7441, %7443)
    %7445 = arith.truncf %7444#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7446 = torch_c.from_builtin_tensor %7445 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7447 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7448 = torch.aten.view %7446, %7447 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %7449 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7450 = torch.aten.transpose.int %7449, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7451 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7452 = torch.aten.view %4, %7451 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7453 = torch.aten.mm %7452, %7450 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7454 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7455 = torch.aten.view %7453, %7454 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %7456 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7457 = torch.aten.transpose.int %7456, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7458 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7459 = torch.aten.view %4, %7458 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7460 = torch.aten.mm %7459, %7457 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7461 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7462 = torch.aten.view %7460, %7461 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7463 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7464 = torch.aten.view %7448, %7463 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7465 = torch.aten.transpose.int %7464, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7466 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7467 = torch.aten.view %7455, %7466 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7468 = torch.aten.transpose.int %7467, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7469 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7470 = torch.aten.view %7462, %7469 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7471 = torch.aten.transpose.int %7470, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7472:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7465, %7468, %7471, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7473 = torch.aten.transpose.int %7472#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7474 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7475 = torch.aten.view %7473, %7474 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7476 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7477 = torch.aten.view %7475, %7476 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7478 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7479 = torch.aten.transpose.int %7478, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %7480 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7481 = torch.prims.convert_element_type %7480, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7482 = torch.prims.convert_element_type %7477, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7483 = torch.prims.convert_element_type %7479, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7484 = torch.aten.mm %7482, %7483 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7485 = torch.aten.mul.Scalar %7484, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7486 = torch.aten.mul.Scalar %7481, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7487 = torch.aten.add.Tensor %7485, %7486, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7488 = torch.prims.convert_element_type %7487, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7489 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7490 = torch.aten.view %7488, %7489 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7491 = torch.aten.div.Scalar %7490, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7492 = torch.aten.add.Tensor %7491, %7421, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7493 = torch.prims.convert_element_type %7492, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7494 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_227, %result1_228 = torch.aten.var_mean.correction %7493, %7494, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7495 = torch.aten.add.Scalar %result0_227, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7496 = torch.aten.rsqrt %7495 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7497 = torch.aten.sub.Tensor %7492, %result1_228, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7498 = torch.aten.mul.Tensor %7497, %7496 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %7499 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7500 = torch.aten.mul.Tensor %7498, %7499 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %7501 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7502 = torch.aten.add.Tensor %7500, %7501, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7503 = torch.prims.convert_element_type %7502, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7504 = torch.prims.convert_element_type %result1_228, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7505 = torch.prims.convert_element_type %7496, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7506 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7507 = torch.aten.view %7503, %7506 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7508 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7509 = torch.aten.transpose.int %7508, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %7510 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7511 = torch.prims.convert_element_type %7510, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7512 = torch.prims.convert_element_type %7507, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7513 = torch.prims.convert_element_type %7509, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7514 = torch.aten.mm %7512, %7513 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7515 = torch.aten.mul.Scalar %7514, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7516 = torch.aten.mul.Scalar %7511, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7517 = torch.aten.add.Tensor %7515, %7516, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7518 = torch.prims.convert_element_type %7517, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7519 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7520 = torch.aten.view %7518, %7519 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7521 = torch.aten.slice.Tensor %7520, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7522 = torch.aten.slice.Tensor %7520, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7523 = torch.aten.gelu %7522, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7524 = torch.aten.mul.Tensor %7521, %7523 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7525 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7526 = torch.aten.view %7524, %7525 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %7527 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7528 = torch.aten.transpose.int %7527, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %7529 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7530 = torch.prims.convert_element_type %7529, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7531 = torch.prims.convert_element_type %7526, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7532 = torch.prims.convert_element_type %7528, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7533 = torch.aten.mm %7531, %7532 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7534 = torch.aten.mul.Scalar %7533, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7535 = torch.aten.mul.Scalar %7530, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7536 = torch.aten.add.Tensor %7534, %7535, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7537 = torch.prims.convert_element_type %7536, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7538 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7539 = torch.aten.view %7537, %7538 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7540 = torch.aten.add.Tensor %7539, %7492, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7541 = torch.prims.convert_element_type %7540, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7542 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_229, %result1_230 = torch.aten.var_mean.correction %7541, %7542, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7543 = torch.aten.add.Scalar %result0_229, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7544 = torch.aten.rsqrt %7543 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7545 = torch.aten.sub.Tensor %7540, %result1_230, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7546 = torch.aten.mul.Tensor %7545, %7544 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %7547 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7548 = torch.aten.mul.Tensor %7546, %7547 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %7549 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7550 = torch.aten.add.Tensor %7548, %7549, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7551 = torch.prims.convert_element_type %7550, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7552 = torch.prims.convert_element_type %result1_230, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7553 = torch.prims.convert_element_type %7544, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %7554 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7555 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7556 = torch.aten.view %7551, %7555 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7557 = torch_c.to_builtin_tensor %7556 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7558 = torch_c.to_builtin_tensor %7554 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7559 = tensor.empty() : tensor<2048x1280xf32>
    %7560 = linalg.fill ins(%cst : f32) outs(%7559 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7561 = tensor.empty() : tensor<2048x1280xf32>
    %7562 = linalg.fill ins(%cst : f32) outs(%7561 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7563:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7560, %7562, %7557, %7558, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7560, %7562)
    %7564 = arith.truncf %7563#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7565 = torch_c.from_builtin_tensor %7564 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7566 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7567 = torch.aten.view %7565, %7566 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %7568 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7569 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7570 = torch.aten.view %7551, %7569 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7571 = torch_c.to_builtin_tensor %7570 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7572 = torch_c.to_builtin_tensor %7568 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7573 = tensor.empty() : tensor<2048x1280xf32>
    %7574 = linalg.fill ins(%cst : f32) outs(%7573 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7575 = tensor.empty() : tensor<2048x1280xf32>
    %7576 = linalg.fill ins(%cst : f32) outs(%7575 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7577:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7574, %7576, %7571, %7572, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7574, %7576)
    %7578 = arith.truncf %7577#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7579 = torch_c.from_builtin_tensor %7578 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7580 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7581 = torch.aten.view %7579, %7580 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %7582 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7583 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7584 = torch.aten.view %7551, %7583 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7585 = torch_c.to_builtin_tensor %7584 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7586 = torch_c.to_builtin_tensor %7582 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7587 = tensor.empty() : tensor<2048x1280xf32>
    %7588 = linalg.fill ins(%cst : f32) outs(%7587 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7589 = tensor.empty() : tensor<2048x1280xf32>
    %7590 = linalg.fill ins(%cst : f32) outs(%7589 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7591:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7588, %7590, %7585, %7586, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7588, %7590)
    %7592 = arith.truncf %7591#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7593 = torch_c.from_builtin_tensor %7592 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7594 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7595 = torch.aten.view %7593, %7594 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7596 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7597 = torch.aten.view %7567, %7596 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7598 = torch.aten.transpose.int %7597, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7599 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7600 = torch.aten.view %7581, %7599 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7601 = torch.aten.transpose.int %7600, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7602 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7603 = torch.aten.view %7595, %7602 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7604 = torch.aten.transpose.int %7603, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7605:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7598, %7601, %7604, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7606 = torch.aten.transpose.int %7605#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7607 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7608 = torch.aten.view %7606, %7607 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7609 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7610 = torch.aten.view %7608, %7609 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7611 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7612 = torch.aten.transpose.int %7611, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %7613 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7614 = torch.prims.convert_element_type %7613, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7615 = torch.prims.convert_element_type %7610, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7616 = torch.prims.convert_element_type %7612, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7617 = torch.aten.mm %7615, %7616 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7618 = torch.aten.mul.Scalar %7617, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7619 = torch.aten.mul.Scalar %7614, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7620 = torch.aten.add.Tensor %7618, %7619, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7621 = torch.prims.convert_element_type %7620, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7622 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7623 = torch.aten.view %7621, %7622 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7624 = torch.aten.div.Scalar %7623, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7625 = torch.aten.add.Tensor %7624, %7540, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7626 = torch.prims.convert_element_type %7625, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7627 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_231, %result1_232 = torch.aten.var_mean.correction %7626, %7627, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7628 = torch.aten.add.Scalar %result0_231, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7629 = torch.aten.rsqrt %7628 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7630 = torch.aten.sub.Tensor %7625, %result1_232, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7631 = torch.aten.mul.Tensor %7630, %7629 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %7632 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7633 = torch.aten.mul.Tensor %7631, %7632 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %7634 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7635 = torch.aten.add.Tensor %7633, %7634, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7636 = torch.prims.convert_element_type %7635, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7637 = torch.prims.convert_element_type %result1_232, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7638 = torch.prims.convert_element_type %7629, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %7639 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7640 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7641 = torch.aten.view %7636, %7640 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7642 = torch_c.to_builtin_tensor %7641 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7643 = torch_c.to_builtin_tensor %7639 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7644 = tensor.empty() : tensor<2048x1280xf32>
    %7645 = linalg.fill ins(%cst : f32) outs(%7644 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7646 = tensor.empty() : tensor<2048x1280xf32>
    %7647 = linalg.fill ins(%cst : f32) outs(%7646 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7648:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7645, %7647, %7642, %7643, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7645, %7647)
    %7649 = arith.truncf %7648#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7650 = torch_c.from_builtin_tensor %7649 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7651 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7652 = torch.aten.view %7650, %7651 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %7653 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7654 = torch.aten.transpose.int %7653, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7655 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7656 = torch.aten.view %4, %7655 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7657 = torch.aten.mm %7656, %7654 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7658 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7659 = torch.aten.view %7657, %7658 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %7660 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7661 = torch.aten.transpose.int %7660, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7662 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7663 = torch.aten.view %4, %7662 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7664 = torch.aten.mm %7663, %7661 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7665 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7666 = torch.aten.view %7664, %7665 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7667 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7668 = torch.aten.view %7652, %7667 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7669 = torch.aten.transpose.int %7668, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7670 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7671 = torch.aten.view %7659, %7670 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7672 = torch.aten.transpose.int %7671, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7673 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7674 = torch.aten.view %7666, %7673 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7675 = torch.aten.transpose.int %7674, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7676:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7669, %7672, %7675, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7677 = torch.aten.transpose.int %7676#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7678 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7679 = torch.aten.view %7677, %7678 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7680 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7681 = torch.aten.view %7679, %7680 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7682 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7683 = torch.aten.transpose.int %7682, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %7684 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7685 = torch.prims.convert_element_type %7684, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7686 = torch.prims.convert_element_type %7681, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7687 = torch.prims.convert_element_type %7683, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7688 = torch.aten.mm %7686, %7687 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7689 = torch.aten.mul.Scalar %7688, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7690 = torch.aten.mul.Scalar %7685, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7691 = torch.aten.add.Tensor %7689, %7690, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7692 = torch.prims.convert_element_type %7691, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7693 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7694 = torch.aten.view %7692, %7693 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7695 = torch.aten.div.Scalar %7694, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7696 = torch.aten.add.Tensor %7695, %7625, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7697 = torch.prims.convert_element_type %7696, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7698 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_233, %result1_234 = torch.aten.var_mean.correction %7697, %7698, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7699 = torch.aten.add.Scalar %result0_233, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7700 = torch.aten.rsqrt %7699 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7701 = torch.aten.sub.Tensor %7696, %result1_234, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7702 = torch.aten.mul.Tensor %7701, %7700 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %7703 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7704 = torch.aten.mul.Tensor %7702, %7703 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %7705 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7706 = torch.aten.add.Tensor %7704, %7705, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7707 = torch.prims.convert_element_type %7706, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7708 = torch.prims.convert_element_type %result1_234, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7709 = torch.prims.convert_element_type %7700, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7710 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7711 = torch.aten.view %7707, %7710 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7712 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7713 = torch.aten.transpose.int %7712, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %7714 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7715 = torch.prims.convert_element_type %7714, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7716 = torch.prims.convert_element_type %7711, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7717 = torch.prims.convert_element_type %7713, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7718 = torch.aten.mm %7716, %7717 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7719 = torch.aten.mul.Scalar %7718, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7720 = torch.aten.mul.Scalar %7715, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7721 = torch.aten.add.Tensor %7719, %7720, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7722 = torch.prims.convert_element_type %7721, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7723 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7724 = torch.aten.view %7722, %7723 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7725 = torch.aten.slice.Tensor %7724, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7726 = torch.aten.slice.Tensor %7724, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7727 = torch.aten.gelu %7726, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7728 = torch.aten.mul.Tensor %7725, %7727 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7729 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7730 = torch.aten.view %7728, %7729 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %7731 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7732 = torch.aten.transpose.int %7731, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %7733 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7734 = torch.prims.convert_element_type %7733, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7735 = torch.prims.convert_element_type %7730, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7736 = torch.prims.convert_element_type %7732, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7737 = torch.aten.mm %7735, %7736 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7738 = torch.aten.mul.Scalar %7737, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7739 = torch.aten.mul.Scalar %7734, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7740 = torch.aten.add.Tensor %7738, %7739, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7741 = torch.prims.convert_element_type %7740, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7742 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7743 = torch.aten.view %7741, %7742 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7744 = torch.aten.add.Tensor %7743, %7696, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7745 = torch.prims.convert_element_type %7744, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7746 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_235, %result1_236 = torch.aten.var_mean.correction %7745, %7746, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7747 = torch.aten.add.Scalar %result0_235, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7748 = torch.aten.rsqrt %7747 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7749 = torch.aten.sub.Tensor %7744, %result1_236, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7750 = torch.aten.mul.Tensor %7749, %7748 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %7751 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7752 = torch.aten.mul.Tensor %7750, %7751 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %7753 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7754 = torch.aten.add.Tensor %7752, %7753, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7755 = torch.prims.convert_element_type %7754, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7756 = torch.prims.convert_element_type %result1_236, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7757 = torch.prims.convert_element_type %7748, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %7758 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7759 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7760 = torch.aten.view %7755, %7759 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7761 = torch_c.to_builtin_tensor %7760 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7762 = torch_c.to_builtin_tensor %7758 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7763 = tensor.empty() : tensor<2048x1280xf32>
    %7764 = linalg.fill ins(%cst : f32) outs(%7763 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7765 = tensor.empty() : tensor<2048x1280xf32>
    %7766 = linalg.fill ins(%cst : f32) outs(%7765 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7767:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7764, %7766, %7761, %7762, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7764, %7766)
    %7768 = arith.truncf %7767#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7769 = torch_c.from_builtin_tensor %7768 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7770 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7771 = torch.aten.view %7769, %7770 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %7772 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7773 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7774 = torch.aten.view %7755, %7773 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7775 = torch_c.to_builtin_tensor %7774 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7776 = torch_c.to_builtin_tensor %7772 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7777 = tensor.empty() : tensor<2048x1280xf32>
    %7778 = linalg.fill ins(%cst : f32) outs(%7777 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7779 = tensor.empty() : tensor<2048x1280xf32>
    %7780 = linalg.fill ins(%cst : f32) outs(%7779 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7781:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7778, %7780, %7775, %7776, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7778, %7780)
    %7782 = arith.truncf %7781#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7783 = torch_c.from_builtin_tensor %7782 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7784 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7785 = torch.aten.view %7783, %7784 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %7786 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7787 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7788 = torch.aten.view %7755, %7787 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7789 = torch_c.to_builtin_tensor %7788 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7790 = torch_c.to_builtin_tensor %7786 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7791 = tensor.empty() : tensor<2048x1280xf32>
    %7792 = linalg.fill ins(%cst : f32) outs(%7791 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7793 = tensor.empty() : tensor<2048x1280xf32>
    %7794 = linalg.fill ins(%cst : f32) outs(%7793 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7795:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7792, %7794, %7789, %7790, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7792, %7794)
    %7796 = arith.truncf %7795#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7797 = torch_c.from_builtin_tensor %7796 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7798 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7799 = torch.aten.view %7797, %7798 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7800 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7801 = torch.aten.view %7771, %7800 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7802 = torch.aten.transpose.int %7801, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7803 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7804 = torch.aten.view %7785, %7803 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7805 = torch.aten.transpose.int %7804, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7806 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7807 = torch.aten.view %7799, %7806 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7808 = torch.aten.transpose.int %7807, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7809:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7802, %7805, %7808, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7810 = torch.aten.transpose.int %7809#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7811 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7812 = torch.aten.view %7810, %7811 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7813 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7814 = torch.aten.view %7812, %7813 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7815 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7816 = torch.aten.transpose.int %7815, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %7817 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7818 = torch.prims.convert_element_type %7817, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7819 = torch.prims.convert_element_type %7814, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7820 = torch.prims.convert_element_type %7816, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7821 = torch.aten.mm %7819, %7820 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7822 = torch.aten.mul.Scalar %7821, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7823 = torch.aten.mul.Scalar %7818, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7824 = torch.aten.add.Tensor %7822, %7823, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7825 = torch.prims.convert_element_type %7824, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7826 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7827 = torch.aten.view %7825, %7826 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7828 = torch.aten.div.Scalar %7827, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7829 = torch.aten.add.Tensor %7828, %7744, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7830 = torch.prims.convert_element_type %7829, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7831 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_237, %result1_238 = torch.aten.var_mean.correction %7830, %7831, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7832 = torch.aten.add.Scalar %result0_237, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7833 = torch.aten.rsqrt %7832 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7834 = torch.aten.sub.Tensor %7829, %result1_238, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7835 = torch.aten.mul.Tensor %7834, %7833 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %7836 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7837 = torch.aten.mul.Tensor %7835, %7836 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %7838 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7839 = torch.aten.add.Tensor %7837, %7838, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7840 = torch.prims.convert_element_type %7839, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7841 = torch.prims.convert_element_type %result1_238, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7842 = torch.prims.convert_element_type %7833, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %7843 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7844 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7845 = torch.aten.view %7840, %7844 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7846 = torch_c.to_builtin_tensor %7845 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7847 = torch_c.to_builtin_tensor %7843 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7848 = tensor.empty() : tensor<2048x1280xf32>
    %7849 = linalg.fill ins(%cst : f32) outs(%7848 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7850 = tensor.empty() : tensor<2048x1280xf32>
    %7851 = linalg.fill ins(%cst : f32) outs(%7850 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7852:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7849, %7851, %7846, %7847, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7849, %7851)
    %7853 = arith.truncf %7852#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7854 = torch_c.from_builtin_tensor %7853 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7855 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7856 = torch.aten.view %7854, %7855 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %7857 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7858 = torch.aten.transpose.int %7857, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7859 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7860 = torch.aten.view %4, %7859 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7861 = torch.aten.mm %7860, %7858 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7862 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7863 = torch.aten.view %7861, %7862 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %7864 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7865 = torch.aten.transpose.int %7864, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7866 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7867 = torch.aten.view %4, %7866 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7868 = torch.aten.mm %7867, %7865 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %7869 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7870 = torch.aten.view %7868, %7869 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7871 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7872 = torch.aten.view %7856, %7871 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7873 = torch.aten.transpose.int %7872, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7874 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7875 = torch.aten.view %7863, %7874 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7876 = torch.aten.transpose.int %7875, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7877 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7878 = torch.aten.view %7870, %7877 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7879 = torch.aten.transpose.int %7878, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7880:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7873, %7876, %7879, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7881 = torch.aten.transpose.int %7880#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7882 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7883 = torch.aten.view %7881, %7882 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7884 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7885 = torch.aten.view %7883, %7884 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7886 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7887 = torch.aten.transpose.int %7886, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %7888 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7889 = torch.prims.convert_element_type %7888, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7890 = torch.prims.convert_element_type %7885, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7891 = torch.prims.convert_element_type %7887, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7892 = torch.aten.mm %7890, %7891 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7893 = torch.aten.mul.Scalar %7892, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7894 = torch.aten.mul.Scalar %7889, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7895 = torch.aten.add.Tensor %7893, %7894, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7896 = torch.prims.convert_element_type %7895, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7897 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7898 = torch.aten.view %7896, %7897 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7899 = torch.aten.div.Scalar %7898, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7900 = torch.aten.add.Tensor %7899, %7829, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7901 = torch.prims.convert_element_type %7900, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7902 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_239, %result1_240 = torch.aten.var_mean.correction %7901, %7902, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7903 = torch.aten.add.Scalar %result0_239, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7904 = torch.aten.rsqrt %7903 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7905 = torch.aten.sub.Tensor %7900, %result1_240, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7906 = torch.aten.mul.Tensor %7905, %7904 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %7907 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7908 = torch.aten.mul.Tensor %7906, %7907 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %7909 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7910 = torch.aten.add.Tensor %7908, %7909, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7911 = torch.prims.convert_element_type %7910, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7912 = torch.prims.convert_element_type %result1_240, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7913 = torch.prims.convert_element_type %7904, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7914 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7915 = torch.aten.view %7911, %7914 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7916 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7917 = torch.aten.transpose.int %7916, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %7918 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7919 = torch.prims.convert_element_type %7918, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7920 = torch.prims.convert_element_type %7915, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7921 = torch.prims.convert_element_type %7917, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7922 = torch.aten.mm %7920, %7921 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7923 = torch.aten.mul.Scalar %7922, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7924 = torch.aten.mul.Scalar %7919, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7925 = torch.aten.add.Tensor %7923, %7924, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7926 = torch.prims.convert_element_type %7925, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7927 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7928 = torch.aten.view %7926, %7927 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7929 = torch.aten.slice.Tensor %7928, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7930 = torch.aten.slice.Tensor %7928, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7931 = torch.aten.gelu %7930, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7932 = torch.aten.mul.Tensor %7929, %7931 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7933 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7934 = torch.aten.view %7932, %7933 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %7935 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7936 = torch.aten.transpose.int %7935, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %7937 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7938 = torch.prims.convert_element_type %7937, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7939 = torch.prims.convert_element_type %7934, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7940 = torch.prims.convert_element_type %7936, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7941 = torch.aten.mm %7939, %7940 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7942 = torch.aten.mul.Scalar %7941, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7943 = torch.aten.mul.Scalar %7938, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7944 = torch.aten.add.Tensor %7942, %7943, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7945 = torch.prims.convert_element_type %7944, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7946 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7947 = torch.aten.view %7945, %7946 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7948 = torch.aten.add.Tensor %7947, %7900, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7949 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7950 = torch.aten.view %7948, %7949 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_out.weight = util.global.load @_params.unet.mid_block.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %7951 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7952 = torch.aten.transpose.int %7951, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_out.bias = util.global.load @_params.unet.mid_block.attentions.0.proj_out.bias : tensor<1280xf16>
    %7953 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7954 = torch.prims.convert_element_type %7953, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7955 = torch.prims.convert_element_type %7950, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7956 = torch.prims.convert_element_type %7952, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7957 = torch.aten.mm %7955, %7956 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7958 = torch.aten.mul.Scalar %7957, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7959 = torch.aten.mul.Scalar %7954, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7960 = torch.aten.add.Tensor %7958, %7959, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7961 = torch.prims.convert_element_type %7960, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7962 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7963 = torch.aten.view %7961, %7962 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7964 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7965 = torch.aten.view %7963, %7964 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %7966 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7967 = torch.aten.permute %7965, %7966 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %7968 = torch.aten.add.Tensor %7967, %5857, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %7969 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7970 = torch.aten.view %7968, %7969 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %7971 = torch.prims.convert_element_type %7970, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %7972 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_241, %result1_242 = torch.aten.var_mean.correction %7971, %7972, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %7973 = torch.aten.add.Scalar %result0_241, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %7974 = torch.aten.rsqrt %7973 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %7975 = torch.aten.sub.Tensor %7970, %result1_242, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %7976 = torch.aten.mul.Tensor %7975, %7974 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %7977 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7978 = torch.aten.view %7976, %7977 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.1.norm1.bias = util.global.load @_params.unet.mid_block.resnets.1.norm1.bias : tensor<1280xf16>
    %7979 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7980 = torch.aten.unsqueeze %7979, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %7981 = torch.aten.unsqueeze %7980, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %7982 = torch.aten.unsqueeze %7981, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.1.norm1.weight = util.global.load @_params.unet.mid_block.resnets.1.norm1.weight : tensor<1280xf16>
    %7983 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7984 = torch.aten.unsqueeze %7983, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %7985 = torch.aten.unsqueeze %7984, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %7986 = torch.aten.unsqueeze %7985, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %7987 = torch.aten.mul.Tensor %7978, %7986 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %7988 = torch.aten.add.Tensor %7987, %7982, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %7989 = torch.prims.convert_element_type %7988, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %7990 = torch.prims.convert_element_type %result1_242, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %7991 = torch.prims.convert_element_type %7974, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %7992 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %7993 = torch.prims.squeeze %7990, %7992 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %7994 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %7995 = torch.prims.squeeze %7993, %7994 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %7996 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %7997 = torch.prims.squeeze %7991, %7996 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %7998 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %7999 = torch.prims.squeeze %7997, %7998 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8000 = torch.aten.silu %7989 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.1.conv1.weight = util.global.load @_params.unet.mid_block.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16>
    %8001 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.1.conv1.bias = util.global.load @_params.unet.mid_block.resnets.1.conv1.bias : tensor<1280xf16>
    %8002 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8003 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8004 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8005 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8006 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8007 = torch.aten.convolution %8000, %8001, %8002, %8003, %8004, %8005, %false, %8006, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8008 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.mid_block.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.mid_block.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %8009 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8010 = torch.aten.transpose.int %8009, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.mid_block.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %8011 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8012 = torch.prims.convert_element_type %8011, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8013 = torch.prims.convert_element_type %8008, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8014 = torch.prims.convert_element_type %8010, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8015 = torch.aten.mm %8013, %8014 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %8016 = torch.aten.mul.Scalar %8015, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8017 = torch.aten.mul.Scalar %8012, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8018 = torch.aten.add.Tensor %8016, %8017, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8019 = torch.prims.convert_element_type %8018, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %8020 = torch.aten.unsqueeze %8019, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %8021 = torch.aten.unsqueeze %8020, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %8022 = torch.aten.add.Tensor %8007, %8021, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8023 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8024 = torch.aten.view %8022, %8023 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %8025 = torch.prims.convert_element_type %8024, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8026 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_243, %result1_244 = torch.aten.var_mean.correction %8025, %8026, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8027 = torch.aten.add.Scalar %result0_243, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8028 = torch.aten.rsqrt %8027 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8029 = torch.aten.sub.Tensor %8024, %result1_244, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8030 = torch.aten.mul.Tensor %8029, %8028 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %8031 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8032 = torch.aten.view %8030, %8031 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.1.norm2.bias = util.global.load @_params.unet.mid_block.resnets.1.norm2.bias : tensor<1280xf16>
    %8033 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8034 = torch.aten.unsqueeze %8033, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8035 = torch.aten.unsqueeze %8034, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8036 = torch.aten.unsqueeze %8035, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.1.norm2.weight = util.global.load @_params.unet.mid_block.resnets.1.norm2.weight : tensor<1280xf16>
    %8037 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8038 = torch.aten.unsqueeze %8037, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8039 = torch.aten.unsqueeze %8038, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8040 = torch.aten.unsqueeze %8039, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %8041 = torch.aten.mul.Tensor %8032, %8040 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %8042 = torch.aten.add.Tensor %8041, %8036, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %8043 = torch.prims.convert_element_type %8042, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8044 = torch.prims.convert_element_type %result1_244, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8045 = torch.prims.convert_element_type %8028, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8046 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8047 = torch.prims.squeeze %8044, %8046 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8048 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8049 = torch.prims.squeeze %8047, %8048 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8050 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8051 = torch.prims.squeeze %8045, %8050 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8052 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8053 = torch.prims.squeeze %8051, %8052 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8054 = torch.aten.silu %8043 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.1.conv2.weight = util.global.load @_params.unet.mid_block.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %8055 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.1.conv2.bias = util.global.load @_params.unet.mid_block.resnets.1.conv2.bias : tensor<1280xf16>
    %8056 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8057 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8058 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8059 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8060 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8061 = torch.aten.convolution %8054, %8055, %8056, %8057, %8058, %8059, %false, %8060, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8062 = torch.aten.add.Tensor %7968, %8061, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8063 = torch.aten.div.Scalar %8062, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8064 = torch.prim.ListConstruct %8063, %5762 : (!torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>) -> !torch.list<vtensor>
    %8065 = torch.aten.cat %8064, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %8066 = torch.prim.ListConstruct %int2, %int32, %int80, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8067 = torch.aten.view %8065, %8066 : !torch.vtensor<[2,2560,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,80,1024],f16>
    %8068 = torch.prims.convert_element_type %8067, %int6 : !torch.vtensor<[2,32,80,1024],f16>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %8069 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_245, %result1_246 = torch.aten.var_mean.correction %8068, %8069, %int0, %true : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8070 = torch.aten.add.Scalar %result0_245, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8071 = torch.aten.rsqrt %8070 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8072 = torch.aten.sub.Tensor %8067, %result1_246, %int1 : !torch.vtensor<[2,32,80,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %8073 = torch.aten.mul.Tensor %8072, %8071 : !torch.vtensor<[2,32,80,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,80,1024],f32>
    %8074 = torch.prim.ListConstruct %int2, %int2560, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8075 = torch.aten.view %8073, %8074 : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,2560,32,32],f32>
    %_params.unet.up_blocks.0.resnets.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.norm1.bias : tensor<2560xf16>
    %8076 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm1.bias : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %8077 = torch.aten.unsqueeze %8076, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %8078 = torch.aten.unsqueeze %8077, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %8079 = torch.aten.unsqueeze %8078, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.norm1.weight : tensor<2560xf16>
    %8080 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm1.weight : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %8081 = torch.aten.unsqueeze %8080, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %8082 = torch.aten.unsqueeze %8081, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %8083 = torch.aten.unsqueeze %8082, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %8084 = torch.aten.mul.Tensor %8075, %8083 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16> -> !torch.vtensor<[2,2560,32,32],f32>
    %8085 = torch.aten.add.Tensor %8084, %8079, %int1 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16>, !torch.int -> !torch.vtensor<[2,2560,32,32],f32>
    %8086 = torch.prims.convert_element_type %8085, %int5 : !torch.vtensor<[2,2560,32,32],f32>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %8087 = torch.prims.convert_element_type %result1_246, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8088 = torch.prims.convert_element_type %8071, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8089 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8090 = torch.prims.squeeze %8087, %8089 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8091 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8092 = torch.prims.squeeze %8090, %8091 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8093 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8094 = torch.prims.squeeze %8088, %8093 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8095 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8096 = torch.prims.squeeze %8094, %8095 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8097 = torch.aten.silu %8086 : !torch.vtensor<[2,2560,32,32],f16> -> !torch.vtensor<[2,2560,32,32],f16>
    %_params.unet.up_blocks.0.resnets.0.conv1.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.conv1.weight : tensor<1280x2560x3x3xf16>
    %8098 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv1.weight : tensor<1280x2560x3x3xf16> -> !torch.vtensor<[1280,2560,3,3],f16>
    %_params.unet.up_blocks.0.resnets.0.conv1.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.conv1.bias : tensor<1280xf16>
    %8099 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8100 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8101 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8102 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8103 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8104 = torch.aten.convolution %8097, %8098, %8099, %8100, %8101, %8102, %false, %8103, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8105 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %8106 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8107 = torch.aten.transpose.int %8106, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %8108 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8109 = torch.prims.convert_element_type %8108, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8110 = torch.prims.convert_element_type %8105, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8111 = torch.prims.convert_element_type %8107, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8112 = torch.aten.mm %8110, %8111 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %8113 = torch.aten.mul.Scalar %8112, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8114 = torch.aten.mul.Scalar %8109, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8115 = torch.aten.add.Tensor %8113, %8114, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8116 = torch.prims.convert_element_type %8115, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %8117 = torch.aten.unsqueeze %8116, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %8118 = torch.aten.unsqueeze %8117, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %8119 = torch.aten.add.Tensor %8104, %8118, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8120 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8121 = torch.aten.view %8119, %8120 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %8122 = torch.prims.convert_element_type %8121, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8123 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_247, %result1_248 = torch.aten.var_mean.correction %8122, %8123, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8124 = torch.aten.add.Scalar %result0_247, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8125 = torch.aten.rsqrt %8124 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8126 = torch.aten.sub.Tensor %8121, %result1_248, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8127 = torch.aten.mul.Tensor %8126, %8125 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %8128 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8129 = torch.aten.view %8127, %8128 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.resnets.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.norm2.bias : tensor<1280xf16>
    %8130 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8131 = torch.aten.unsqueeze %8130, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8132 = torch.aten.unsqueeze %8131, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8133 = torch.aten.unsqueeze %8132, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.resnets.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.norm2.weight : tensor<1280xf16>
    %8134 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8135 = torch.aten.unsqueeze %8134, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8136 = torch.aten.unsqueeze %8135, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8137 = torch.aten.unsqueeze %8136, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %8138 = torch.aten.mul.Tensor %8129, %8137 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %8139 = torch.aten.add.Tensor %8138, %8133, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %8140 = torch.prims.convert_element_type %8139, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8141 = torch.prims.convert_element_type %result1_248, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8142 = torch.prims.convert_element_type %8125, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8143 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8144 = torch.prims.squeeze %8141, %8143 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8145 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8146 = torch.prims.squeeze %8144, %8145 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8147 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8148 = torch.prims.squeeze %8142, %8147 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8149 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8150 = torch.prims.squeeze %8148, %8149 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8151 = torch.aten.silu %8140 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.0.conv2.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %8152 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.resnets.0.conv2.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.conv2.bias : tensor<1280xf16>
    %8153 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8154 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8155 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8156 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8157 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8158 = torch.aten.convolution %8151, %8152, %8153, %8154, %8155, %8156, %false, %8157, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight : tensor<1280x2560x1x1xf16>
    %8159 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight : tensor<1280x2560x1x1xf16> -> !torch.vtensor<[1280,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias : tensor<1280xf16>
    %8160 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8161 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8162 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8163 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8164 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8165 = torch.aten.convolution %8065, %8159, %8160, %8161, %8162, %8163, %false, %8164, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8166 = torch.aten.add.Tensor %8165, %8158, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8167 = torch.aten.div.Scalar %8166, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %8168 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8169 = torch.aten.view %8167, %8168 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %8170 = torch.prims.convert_element_type %8169, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8171 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_249, %result1_250 = torch.aten.var_mean.correction %8170, %8171, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8172 = torch.aten.add.Scalar %result0_249, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8173 = torch.aten.rsqrt %8172 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8174 = torch.aten.sub.Tensor %8169, %result1_250, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8175 = torch.aten.mul.Tensor %8174, %8173 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %8176 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8177 = torch.aten.view %8175, %8176 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.attentions.0.norm.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.norm.bias : tensor<1280xf16>
    %8178 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8179 = torch.aten.unsqueeze %8178, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8180 = torch.aten.unsqueeze %8179, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8181 = torch.aten.unsqueeze %8180, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.attentions.0.norm.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.norm.weight : tensor<1280xf16>
    %8182 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8183 = torch.aten.unsqueeze %8182, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8184 = torch.aten.unsqueeze %8183, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8185 = torch.aten.unsqueeze %8184, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %8186 = torch.aten.mul.Tensor %8177, %8185 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %8187 = torch.aten.add.Tensor %8186, %8181, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %8188 = torch.prims.convert_element_type %8187, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8189 = torch.prims.convert_element_type %result1_250, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8190 = torch.prims.convert_element_type %8173, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8191 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8192 = torch.prims.squeeze %8189, %8191 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8193 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8194 = torch.prims.squeeze %8192, %8193 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8195 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8196 = torch.prims.squeeze %8190, %8195 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8197 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8198 = torch.prims.squeeze %8196, %8197 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8199 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8200 = torch.aten.permute %8188, %8199 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %8201 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8202 = torch.aten.view %8200, %8201 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_in.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %8203 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8204 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8205 = torch.aten._unsafe_view %8202, %8204 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8206 = torch_c.to_builtin_tensor %8205 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8207 = torch_c.to_builtin_tensor %8203 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8208 = tensor.empty() : tensor<2048x1280xf32>
    %8209 = linalg.fill ins(%cst : f32) outs(%8208 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8210 = tensor.empty() : tensor<2048x1280xf32>
    %8211 = linalg.fill ins(%cst : f32) outs(%8210 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8212:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8209, %8211, %8206, %8207, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8209, %8211)
    %8213 = arith.truncf %8212#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8214 = torch_c.from_builtin_tensor %8213 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8215 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8216 = torch.aten.view %8214, %8215 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_in.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_in.bias : tensor<1280xf16>
    %8217 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8218 = torch.aten.add.Tensor %8216, %8217, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8219 = torch.prims.convert_element_type %8218, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8220 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_251, %result1_252 = torch.aten.var_mean.correction %8219, %8220, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8221 = torch.aten.add.Scalar %result0_251, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8222 = torch.aten.rsqrt %8221 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8223 = torch.aten.sub.Tensor %8218, %result1_252, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8224 = torch.aten.mul.Tensor %8223, %8222 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %8225 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8226 = torch.aten.mul.Tensor %8224, %8225 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %8227 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8228 = torch.aten.add.Tensor %8226, %8227, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8229 = torch.prims.convert_element_type %8228, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8230 = torch.prims.convert_element_type %result1_252, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8231 = torch.prims.convert_element_type %8222, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %8232 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8233 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8234 = torch.aten.view %8229, %8233 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8235 = torch_c.to_builtin_tensor %8234 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8236 = torch_c.to_builtin_tensor %8232 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8237 = tensor.empty() : tensor<2048x1280xf32>
    %8238 = linalg.fill ins(%cst : f32) outs(%8237 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8239 = tensor.empty() : tensor<2048x1280xf32>
    %8240 = linalg.fill ins(%cst : f32) outs(%8239 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8241:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8238, %8240, %8235, %8236, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8238, %8240)
    %8242 = arith.truncf %8241#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8243 = torch_c.from_builtin_tensor %8242 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8244 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8245 = torch.aten.view %8243, %8244 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %8246 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8247 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8248 = torch.aten.view %8229, %8247 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8249 = torch_c.to_builtin_tensor %8248 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8250 = torch_c.to_builtin_tensor %8246 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8251 = tensor.empty() : tensor<2048x1280xf32>
    %8252 = linalg.fill ins(%cst : f32) outs(%8251 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8253 = tensor.empty() : tensor<2048x1280xf32>
    %8254 = linalg.fill ins(%cst : f32) outs(%8253 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8255:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8252, %8254, %8249, %8250, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8252, %8254)
    %8256 = arith.truncf %8255#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8257 = torch_c.from_builtin_tensor %8256 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8258 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8259 = torch.aten.view %8257, %8258 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %8260 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8261 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8262 = torch.aten.view %8229, %8261 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8263 = torch_c.to_builtin_tensor %8262 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8264 = torch_c.to_builtin_tensor %8260 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8265 = tensor.empty() : tensor<2048x1280xf32>
    %8266 = linalg.fill ins(%cst : f32) outs(%8265 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8267 = tensor.empty() : tensor<2048x1280xf32>
    %8268 = linalg.fill ins(%cst : f32) outs(%8267 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8269:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8266, %8268, %8263, %8264, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8266, %8268)
    %8270 = arith.truncf %8269#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8271 = torch_c.from_builtin_tensor %8270 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8272 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8273 = torch.aten.view %8271, %8272 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8274 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8275 = torch.aten.view %8245, %8274 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8276 = torch.aten.transpose.int %8275, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8277 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8278 = torch.aten.view %8259, %8277 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8279 = torch.aten.transpose.int %8278, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8280 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8281 = torch.aten.view %8273, %8280 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8282 = torch.aten.transpose.int %8281, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8283:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8276, %8279, %8282, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8284 = torch.aten.transpose.int %8283#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8285 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8286 = torch.aten.view %8284, %8285 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8287 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8288 = torch.aten.view %8286, %8287 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8289 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8290 = torch.aten.transpose.int %8289, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %8291 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8292 = torch.prims.convert_element_type %8291, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8293 = torch.prims.convert_element_type %8288, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8294 = torch.prims.convert_element_type %8290, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8295 = torch.aten.mm %8293, %8294 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8296 = torch.aten.mul.Scalar %8295, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8297 = torch.aten.mul.Scalar %8292, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8298 = torch.aten.add.Tensor %8296, %8297, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8299 = torch.prims.convert_element_type %8298, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8300 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8301 = torch.aten.view %8299, %8300 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8302 = torch.aten.div.Scalar %8301, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8303 = torch.aten.add.Tensor %8302, %8218, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8304 = torch.prims.convert_element_type %8303, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8305 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_253, %result1_254 = torch.aten.var_mean.correction %8304, %8305, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8306 = torch.aten.add.Scalar %result0_253, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8307 = torch.aten.rsqrt %8306 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8308 = torch.aten.sub.Tensor %8303, %result1_254, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8309 = torch.aten.mul.Tensor %8308, %8307 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %8310 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8311 = torch.aten.mul.Tensor %8309, %8310 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %8312 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8313 = torch.aten.add.Tensor %8311, %8312, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8314 = torch.prims.convert_element_type %8313, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8315 = torch.prims.convert_element_type %result1_254, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8316 = torch.prims.convert_element_type %8307, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %8317 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8318 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8319 = torch.aten.view %8314, %8318 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8320 = torch_c.to_builtin_tensor %8319 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8321 = torch_c.to_builtin_tensor %8317 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8322 = tensor.empty() : tensor<2048x1280xf32>
    %8323 = linalg.fill ins(%cst : f32) outs(%8322 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8324 = tensor.empty() : tensor<2048x1280xf32>
    %8325 = linalg.fill ins(%cst : f32) outs(%8324 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8326:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8323, %8325, %8320, %8321, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8323, %8325)
    %8327 = arith.truncf %8326#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8328 = torch_c.from_builtin_tensor %8327 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8329 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8330 = torch.aten.view %8328, %8329 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %8331 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8332 = torch.aten.transpose.int %8331, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8333 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8334 = torch.aten.view %4, %8333 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8335 = torch.aten.mm %8334, %8332 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8336 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8337 = torch.aten.view %8335, %8336 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %8338 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8339 = torch.aten.transpose.int %8338, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8340 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8341 = torch.aten.view %4, %8340 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8342 = torch.aten.mm %8341, %8339 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8343 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8344 = torch.aten.view %8342, %8343 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8345 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8346 = torch.aten.view %8330, %8345 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8347 = torch.aten.transpose.int %8346, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8348 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8349 = torch.aten.view %8337, %8348 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8350 = torch.aten.transpose.int %8349, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8351 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8352 = torch.aten.view %8344, %8351 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8353 = torch.aten.transpose.int %8352, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8354:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8347, %8350, %8353, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8355 = torch.aten.transpose.int %8354#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8356 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8357 = torch.aten.view %8355, %8356 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8358 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8359 = torch.aten.view %8357, %8358 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %8360 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8361 = torch.aten.transpose.int %8360, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %8362 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8363 = torch.prims.convert_element_type %8362, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8364 = torch.prims.convert_element_type %8359, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8365 = torch.prims.convert_element_type %8361, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8366 = torch.aten.mm %8364, %8365 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8367 = torch.aten.mul.Scalar %8366, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8368 = torch.aten.mul.Scalar %8363, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8369 = torch.aten.add.Tensor %8367, %8368, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8370 = torch.prims.convert_element_type %8369, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8371 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8372 = torch.aten.view %8370, %8371 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8373 = torch.aten.div.Scalar %8372, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8374 = torch.aten.add.Tensor %8373, %8303, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8375 = torch.prims.convert_element_type %8374, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8376 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_255, %result1_256 = torch.aten.var_mean.correction %8375, %8376, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8377 = torch.aten.add.Scalar %result0_255, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8378 = torch.aten.rsqrt %8377 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8379 = torch.aten.sub.Tensor %8374, %result1_256, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8380 = torch.aten.mul.Tensor %8379, %8378 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %8381 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8382 = torch.aten.mul.Tensor %8380, %8381 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %8383 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8384 = torch.aten.add.Tensor %8382, %8383, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8385 = torch.prims.convert_element_type %8384, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8386 = torch.prims.convert_element_type %result1_256, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8387 = torch.prims.convert_element_type %8378, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8388 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8389 = torch.aten.view %8385, %8388 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %8390 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %8391 = torch.aten.transpose.int %8390, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %8392 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %8393 = torch.prims.convert_element_type %8392, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %8394 = torch.prims.convert_element_type %8389, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8395 = torch.prims.convert_element_type %8391, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %8396 = torch.aten.mm %8394, %8395 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %8397 = torch.aten.mul.Scalar %8396, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8398 = torch.aten.mul.Scalar %8393, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %8399 = torch.aten.add.Tensor %8397, %8398, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8400 = torch.prims.convert_element_type %8399, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %8401 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8402 = torch.aten.view %8400, %8401 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %8403 = torch.aten.slice.Tensor %8402, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8404 = torch.aten.slice.Tensor %8402, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8405 = torch.aten.gelu %8404, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %8406 = torch.aten.mul.Tensor %8403, %8405 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %8407 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %8408 = torch.aten.view %8406, %8407 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %8409 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %8410 = torch.aten.transpose.int %8409, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %8411 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8412 = torch.prims.convert_element_type %8411, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8413 = torch.prims.convert_element_type %8408, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %8414 = torch.prims.convert_element_type %8410, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %8415 = torch.aten.mm %8413, %8414 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8416 = torch.aten.mul.Scalar %8415, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8417 = torch.aten.mul.Scalar %8412, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8418 = torch.aten.add.Tensor %8416, %8417, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8419 = torch.prims.convert_element_type %8418, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8420 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8421 = torch.aten.view %8419, %8420 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8422 = torch.aten.add.Tensor %8421, %8374, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8423 = torch.prims.convert_element_type %8422, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8424 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_257, %result1_258 = torch.aten.var_mean.correction %8423, %8424, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8425 = torch.aten.add.Scalar %result0_257, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8426 = torch.aten.rsqrt %8425 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8427 = torch.aten.sub.Tensor %8422, %result1_258, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8428 = torch.aten.mul.Tensor %8427, %8426 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %8429 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8430 = torch.aten.mul.Tensor %8428, %8429 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %8431 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8432 = torch.aten.add.Tensor %8430, %8431, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8433 = torch.prims.convert_element_type %8432, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8434 = torch.prims.convert_element_type %result1_258, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8435 = torch.prims.convert_element_type %8426, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %8436 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8437 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8438 = torch.aten.view %8433, %8437 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8439 = torch_c.to_builtin_tensor %8438 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8440 = torch_c.to_builtin_tensor %8436 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8441 = tensor.empty() : tensor<2048x1280xf32>
    %8442 = linalg.fill ins(%cst : f32) outs(%8441 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8443 = tensor.empty() : tensor<2048x1280xf32>
    %8444 = linalg.fill ins(%cst : f32) outs(%8443 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8445:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8442, %8444, %8439, %8440, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8442, %8444)
    %8446 = arith.truncf %8445#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8447 = torch_c.from_builtin_tensor %8446 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8448 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8449 = torch.aten.view %8447, %8448 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %8450 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8451 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8452 = torch.aten.view %8433, %8451 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8453 = torch_c.to_builtin_tensor %8452 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8454 = torch_c.to_builtin_tensor %8450 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8455 = tensor.empty() : tensor<2048x1280xf32>
    %8456 = linalg.fill ins(%cst : f32) outs(%8455 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8457 = tensor.empty() : tensor<2048x1280xf32>
    %8458 = linalg.fill ins(%cst : f32) outs(%8457 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8459:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8456, %8458, %8453, %8454, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8456, %8458)
    %8460 = arith.truncf %8459#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8461 = torch_c.from_builtin_tensor %8460 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8462 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8463 = torch.aten.view %8461, %8462 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %8464 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8465 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8466 = torch.aten.view %8433, %8465 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8467 = torch_c.to_builtin_tensor %8466 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8468 = torch_c.to_builtin_tensor %8464 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8469 = tensor.empty() : tensor<2048x1280xf32>
    %8470 = linalg.fill ins(%cst : f32) outs(%8469 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8471 = tensor.empty() : tensor<2048x1280xf32>
    %8472 = linalg.fill ins(%cst : f32) outs(%8471 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8473:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8470, %8472, %8467, %8468, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8470, %8472)
    %8474 = arith.truncf %8473#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8475 = torch_c.from_builtin_tensor %8474 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8476 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8477 = torch.aten.view %8475, %8476 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8478 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8479 = torch.aten.view %8449, %8478 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8480 = torch.aten.transpose.int %8479, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8481 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8482 = torch.aten.view %8463, %8481 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8483 = torch.aten.transpose.int %8482, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8484 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8485 = torch.aten.view %8477, %8484 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8486 = torch.aten.transpose.int %8485, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8487:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8480, %8483, %8486, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8488 = torch.aten.transpose.int %8487#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8489 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8490 = torch.aten.view %8488, %8489 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8491 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8492 = torch.aten.view %8490, %8491 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8493 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8494 = torch.aten.transpose.int %8493, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %8495 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8496 = torch.prims.convert_element_type %8495, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8497 = torch.prims.convert_element_type %8492, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8498 = torch.prims.convert_element_type %8494, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8499 = torch.aten.mm %8497, %8498 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8500 = torch.aten.mul.Scalar %8499, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8501 = torch.aten.mul.Scalar %8496, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8502 = torch.aten.add.Tensor %8500, %8501, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8503 = torch.prims.convert_element_type %8502, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8504 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8505 = torch.aten.view %8503, %8504 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8506 = torch.aten.div.Scalar %8505, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8507 = torch.aten.add.Tensor %8506, %8422, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8508 = torch.prims.convert_element_type %8507, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8509 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_259, %result1_260 = torch.aten.var_mean.correction %8508, %8509, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8510 = torch.aten.add.Scalar %result0_259, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8511 = torch.aten.rsqrt %8510 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8512 = torch.aten.sub.Tensor %8507, %result1_260, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8513 = torch.aten.mul.Tensor %8512, %8511 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %8514 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8515 = torch.aten.mul.Tensor %8513, %8514 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %8516 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8517 = torch.aten.add.Tensor %8515, %8516, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8518 = torch.prims.convert_element_type %8517, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8519 = torch.prims.convert_element_type %result1_260, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8520 = torch.prims.convert_element_type %8511, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %8521 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8522 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8523 = torch.aten.view %8518, %8522 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8524 = torch_c.to_builtin_tensor %8523 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8525 = torch_c.to_builtin_tensor %8521 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8526 = tensor.empty() : tensor<2048x1280xf32>
    %8527 = linalg.fill ins(%cst : f32) outs(%8526 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8528 = tensor.empty() : tensor<2048x1280xf32>
    %8529 = linalg.fill ins(%cst : f32) outs(%8528 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8530:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8527, %8529, %8524, %8525, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8527, %8529)
    %8531 = arith.truncf %8530#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8532 = torch_c.from_builtin_tensor %8531 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8533 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8534 = torch.aten.view %8532, %8533 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %8535 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8536 = torch.aten.transpose.int %8535, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8537 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8538 = torch.aten.view %4, %8537 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8539 = torch.aten.mm %8538, %8536 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8540 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8541 = torch.aten.view %8539, %8540 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %8542 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8543 = torch.aten.transpose.int %8542, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8544 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8545 = torch.aten.view %4, %8544 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8546 = torch.aten.mm %8545, %8543 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8547 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8548 = torch.aten.view %8546, %8547 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8549 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8550 = torch.aten.view %8534, %8549 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8551 = torch.aten.transpose.int %8550, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8552 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8553 = torch.aten.view %8541, %8552 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8554 = torch.aten.transpose.int %8553, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8555 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8556 = torch.aten.view %8548, %8555 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8557 = torch.aten.transpose.int %8556, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8558:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8551, %8554, %8557, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8559 = torch.aten.transpose.int %8558#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8560 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8561 = torch.aten.view %8559, %8560 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8562 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8563 = torch.aten.view %8561, %8562 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %8564 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8565 = torch.aten.transpose.int %8564, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %8566 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8567 = torch.prims.convert_element_type %8566, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8568 = torch.prims.convert_element_type %8563, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8569 = torch.prims.convert_element_type %8565, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8570 = torch.aten.mm %8568, %8569 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8571 = torch.aten.mul.Scalar %8570, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8572 = torch.aten.mul.Scalar %8567, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8573 = torch.aten.add.Tensor %8571, %8572, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8574 = torch.prims.convert_element_type %8573, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8575 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8576 = torch.aten.view %8574, %8575 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8577 = torch.aten.div.Scalar %8576, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8578 = torch.aten.add.Tensor %8577, %8507, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8579 = torch.prims.convert_element_type %8578, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8580 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_261, %result1_262 = torch.aten.var_mean.correction %8579, %8580, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8581 = torch.aten.add.Scalar %result0_261, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8582 = torch.aten.rsqrt %8581 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8583 = torch.aten.sub.Tensor %8578, %result1_262, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8584 = torch.aten.mul.Tensor %8583, %8582 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %8585 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8586 = torch.aten.mul.Tensor %8584, %8585 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %8587 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8588 = torch.aten.add.Tensor %8586, %8587, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8589 = torch.prims.convert_element_type %8588, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8590 = torch.prims.convert_element_type %result1_262, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8591 = torch.prims.convert_element_type %8582, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8592 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8593 = torch.aten.view %8589, %8592 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %8594 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %8595 = torch.aten.transpose.int %8594, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %8596 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %8597 = torch.prims.convert_element_type %8596, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %8598 = torch.prims.convert_element_type %8593, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8599 = torch.prims.convert_element_type %8595, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %8600 = torch.aten.mm %8598, %8599 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %8601 = torch.aten.mul.Scalar %8600, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8602 = torch.aten.mul.Scalar %8597, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %8603 = torch.aten.add.Tensor %8601, %8602, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8604 = torch.prims.convert_element_type %8603, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %8605 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8606 = torch.aten.view %8604, %8605 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %8607 = torch.aten.slice.Tensor %8606, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8608 = torch.aten.slice.Tensor %8606, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8609 = torch.aten.gelu %8608, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %8610 = torch.aten.mul.Tensor %8607, %8609 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %8611 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %8612 = torch.aten.view %8610, %8611 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %8613 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %8614 = torch.aten.transpose.int %8613, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %8615 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8616 = torch.prims.convert_element_type %8615, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8617 = torch.prims.convert_element_type %8612, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %8618 = torch.prims.convert_element_type %8614, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %8619 = torch.aten.mm %8617, %8618 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8620 = torch.aten.mul.Scalar %8619, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8621 = torch.aten.mul.Scalar %8616, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8622 = torch.aten.add.Tensor %8620, %8621, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8623 = torch.prims.convert_element_type %8622, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8624 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8625 = torch.aten.view %8623, %8624 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8626 = torch.aten.add.Tensor %8625, %8578, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8627 = torch.prims.convert_element_type %8626, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8628 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_263, %result1_264 = torch.aten.var_mean.correction %8627, %8628, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8629 = torch.aten.add.Scalar %result0_263, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8630 = torch.aten.rsqrt %8629 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8631 = torch.aten.sub.Tensor %8626, %result1_264, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8632 = torch.aten.mul.Tensor %8631, %8630 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %8633 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8634 = torch.aten.mul.Tensor %8632, %8633 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %8635 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8636 = torch.aten.add.Tensor %8634, %8635, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8637 = torch.prims.convert_element_type %8636, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8638 = torch.prims.convert_element_type %result1_264, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8639 = torch.prims.convert_element_type %8630, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %8640 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8641 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8642 = torch.aten.view %8637, %8641 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8643 = torch_c.to_builtin_tensor %8642 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8644 = torch_c.to_builtin_tensor %8640 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8645 = tensor.empty() : tensor<2048x1280xf32>
    %8646 = linalg.fill ins(%cst : f32) outs(%8645 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8647 = tensor.empty() : tensor<2048x1280xf32>
    %8648 = linalg.fill ins(%cst : f32) outs(%8647 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8649:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8646, %8648, %8643, %8644, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8646, %8648)
    %8650 = arith.truncf %8649#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8651 = torch_c.from_builtin_tensor %8650 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8652 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8653 = torch.aten.view %8651, %8652 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %8654 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8655 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8656 = torch.aten.view %8637, %8655 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8657 = torch_c.to_builtin_tensor %8656 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8658 = torch_c.to_builtin_tensor %8654 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8659 = tensor.empty() : tensor<2048x1280xf32>
    %8660 = linalg.fill ins(%cst : f32) outs(%8659 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8661 = tensor.empty() : tensor<2048x1280xf32>
    %8662 = linalg.fill ins(%cst : f32) outs(%8661 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8663:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8660, %8662, %8657, %8658, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8660, %8662)
    %8664 = arith.truncf %8663#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8665 = torch_c.from_builtin_tensor %8664 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8666 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8667 = torch.aten.view %8665, %8666 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %8668 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8669 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8670 = torch.aten.view %8637, %8669 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8671 = torch_c.to_builtin_tensor %8670 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8672 = torch_c.to_builtin_tensor %8668 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8673 = tensor.empty() : tensor<2048x1280xf32>
    %8674 = linalg.fill ins(%cst : f32) outs(%8673 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8675 = tensor.empty() : tensor<2048x1280xf32>
    %8676 = linalg.fill ins(%cst : f32) outs(%8675 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8677:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8674, %8676, %8671, %8672, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8674, %8676)
    %8678 = arith.truncf %8677#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8679 = torch_c.from_builtin_tensor %8678 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8680 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8681 = torch.aten.view %8679, %8680 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8682 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8683 = torch.aten.view %8653, %8682 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8684 = torch.aten.transpose.int %8683, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8685 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8686 = torch.aten.view %8667, %8685 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8687 = torch.aten.transpose.int %8686, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8688 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8689 = torch.aten.view %8681, %8688 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8690 = torch.aten.transpose.int %8689, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8691:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8684, %8687, %8690, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8692 = torch.aten.transpose.int %8691#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8693 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8694 = torch.aten.view %8692, %8693 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8695 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8696 = torch.aten.view %8694, %8695 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8697 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8698 = torch.aten.transpose.int %8697, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %8699 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8700 = torch.prims.convert_element_type %8699, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8701 = torch.prims.convert_element_type %8696, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8702 = torch.prims.convert_element_type %8698, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8703 = torch.aten.mm %8701, %8702 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8704 = torch.aten.mul.Scalar %8703, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8705 = torch.aten.mul.Scalar %8700, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8706 = torch.aten.add.Tensor %8704, %8705, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8707 = torch.prims.convert_element_type %8706, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8708 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8709 = torch.aten.view %8707, %8708 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8710 = torch.aten.div.Scalar %8709, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8711 = torch.aten.add.Tensor %8710, %8626, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8712 = torch.prims.convert_element_type %8711, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8713 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_265, %result1_266 = torch.aten.var_mean.correction %8712, %8713, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8714 = torch.aten.add.Scalar %result0_265, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8715 = torch.aten.rsqrt %8714 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8716 = torch.aten.sub.Tensor %8711, %result1_266, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8717 = torch.aten.mul.Tensor %8716, %8715 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %8718 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8719 = torch.aten.mul.Tensor %8717, %8718 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %8720 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8721 = torch.aten.add.Tensor %8719, %8720, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8722 = torch.prims.convert_element_type %8721, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8723 = torch.prims.convert_element_type %result1_266, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8724 = torch.prims.convert_element_type %8715, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %8725 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8726 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8727 = torch.aten.view %8722, %8726 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8728 = torch_c.to_builtin_tensor %8727 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8729 = torch_c.to_builtin_tensor %8725 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8730 = tensor.empty() : tensor<2048x1280xf32>
    %8731 = linalg.fill ins(%cst : f32) outs(%8730 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8732 = tensor.empty() : tensor<2048x1280xf32>
    %8733 = linalg.fill ins(%cst : f32) outs(%8732 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8734:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8731, %8733, %8728, %8729, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8731, %8733)
    %8735 = arith.truncf %8734#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8736 = torch_c.from_builtin_tensor %8735 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8737 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8738 = torch.aten.view %8736, %8737 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %8739 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8740 = torch.aten.transpose.int %8739, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8741 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8742 = torch.aten.view %4, %8741 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8743 = torch.aten.mm %8742, %8740 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8744 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8745 = torch.aten.view %8743, %8744 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %8746 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8747 = torch.aten.transpose.int %8746, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8748 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8749 = torch.aten.view %4, %8748 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8750 = torch.aten.mm %8749, %8747 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8751 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8752 = torch.aten.view %8750, %8751 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8753 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8754 = torch.aten.view %8738, %8753 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8755 = torch.aten.transpose.int %8754, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8756 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8757 = torch.aten.view %8745, %8756 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8758 = torch.aten.transpose.int %8757, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8759 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8760 = torch.aten.view %8752, %8759 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8761 = torch.aten.transpose.int %8760, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8762:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8755, %8758, %8761, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8763 = torch.aten.transpose.int %8762#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8764 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8765 = torch.aten.view %8763, %8764 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8766 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8767 = torch.aten.view %8765, %8766 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %8768 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8769 = torch.aten.transpose.int %8768, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %8770 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8771 = torch.prims.convert_element_type %8770, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8772 = torch.prims.convert_element_type %8767, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8773 = torch.prims.convert_element_type %8769, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8774 = torch.aten.mm %8772, %8773 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8775 = torch.aten.mul.Scalar %8774, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8776 = torch.aten.mul.Scalar %8771, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8777 = torch.aten.add.Tensor %8775, %8776, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8778 = torch.prims.convert_element_type %8777, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8779 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8780 = torch.aten.view %8778, %8779 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8781 = torch.aten.div.Scalar %8780, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8782 = torch.aten.add.Tensor %8781, %8711, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8783 = torch.prims.convert_element_type %8782, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8784 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_267, %result1_268 = torch.aten.var_mean.correction %8783, %8784, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8785 = torch.aten.add.Scalar %result0_267, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8786 = torch.aten.rsqrt %8785 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8787 = torch.aten.sub.Tensor %8782, %result1_268, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8788 = torch.aten.mul.Tensor %8787, %8786 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %8789 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8790 = torch.aten.mul.Tensor %8788, %8789 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %8791 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8792 = torch.aten.add.Tensor %8790, %8791, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8793 = torch.prims.convert_element_type %8792, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8794 = torch.prims.convert_element_type %result1_268, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8795 = torch.prims.convert_element_type %8786, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8796 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8797 = torch.aten.view %8793, %8796 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %8798 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %8799 = torch.aten.transpose.int %8798, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %8800 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %8801 = torch.prims.convert_element_type %8800, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %8802 = torch.prims.convert_element_type %8797, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8803 = torch.prims.convert_element_type %8799, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %8804 = torch.aten.mm %8802, %8803 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %8805 = torch.aten.mul.Scalar %8804, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8806 = torch.aten.mul.Scalar %8801, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %8807 = torch.aten.add.Tensor %8805, %8806, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8808 = torch.prims.convert_element_type %8807, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %8809 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8810 = torch.aten.view %8808, %8809 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %8811 = torch.aten.slice.Tensor %8810, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8812 = torch.aten.slice.Tensor %8810, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8813 = torch.aten.gelu %8812, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %8814 = torch.aten.mul.Tensor %8811, %8813 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %8815 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %8816 = torch.aten.view %8814, %8815 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %8817 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %8818 = torch.aten.transpose.int %8817, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %8819 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8820 = torch.prims.convert_element_type %8819, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8821 = torch.prims.convert_element_type %8816, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %8822 = torch.prims.convert_element_type %8818, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %8823 = torch.aten.mm %8821, %8822 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8824 = torch.aten.mul.Scalar %8823, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8825 = torch.aten.mul.Scalar %8820, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8826 = torch.aten.add.Tensor %8824, %8825, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8827 = torch.prims.convert_element_type %8826, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8828 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8829 = torch.aten.view %8827, %8828 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8830 = torch.aten.add.Tensor %8829, %8782, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8831 = torch.prims.convert_element_type %8830, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8832 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_269, %result1_270 = torch.aten.var_mean.correction %8831, %8832, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8833 = torch.aten.add.Scalar %result0_269, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8834 = torch.aten.rsqrt %8833 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8835 = torch.aten.sub.Tensor %8830, %result1_270, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8836 = torch.aten.mul.Tensor %8835, %8834 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %8837 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8838 = torch.aten.mul.Tensor %8836, %8837 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %8839 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8840 = torch.aten.add.Tensor %8838, %8839, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8841 = torch.prims.convert_element_type %8840, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8842 = torch.prims.convert_element_type %result1_270, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8843 = torch.prims.convert_element_type %8834, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %8844 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8845 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8846 = torch.aten.view %8841, %8845 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8847 = torch_c.to_builtin_tensor %8846 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8848 = torch_c.to_builtin_tensor %8844 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8849 = tensor.empty() : tensor<2048x1280xf32>
    %8850 = linalg.fill ins(%cst : f32) outs(%8849 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8851 = tensor.empty() : tensor<2048x1280xf32>
    %8852 = linalg.fill ins(%cst : f32) outs(%8851 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8853:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8850, %8852, %8847, %8848, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8850, %8852)
    %8854 = arith.truncf %8853#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8855 = torch_c.from_builtin_tensor %8854 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8856 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8857 = torch.aten.view %8855, %8856 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %8858 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8859 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8860 = torch.aten.view %8841, %8859 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8861 = torch_c.to_builtin_tensor %8860 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8862 = torch_c.to_builtin_tensor %8858 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8863 = tensor.empty() : tensor<2048x1280xf32>
    %8864 = linalg.fill ins(%cst : f32) outs(%8863 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8865 = tensor.empty() : tensor<2048x1280xf32>
    %8866 = linalg.fill ins(%cst : f32) outs(%8865 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8867:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8864, %8866, %8861, %8862, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8864, %8866)
    %8868 = arith.truncf %8867#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8869 = torch_c.from_builtin_tensor %8868 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8870 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8871 = torch.aten.view %8869, %8870 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %8872 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8873 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8874 = torch.aten.view %8841, %8873 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8875 = torch_c.to_builtin_tensor %8874 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8876 = torch_c.to_builtin_tensor %8872 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8877 = tensor.empty() : tensor<2048x1280xf32>
    %8878 = linalg.fill ins(%cst : f32) outs(%8877 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8879 = tensor.empty() : tensor<2048x1280xf32>
    %8880 = linalg.fill ins(%cst : f32) outs(%8879 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8881:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8878, %8880, %8875, %8876, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8878, %8880)
    %8882 = arith.truncf %8881#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8883 = torch_c.from_builtin_tensor %8882 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8884 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8885 = torch.aten.view %8883, %8884 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8886 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8887 = torch.aten.view %8857, %8886 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8888 = torch.aten.transpose.int %8887, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8889 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8890 = torch.aten.view %8871, %8889 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8891 = torch.aten.transpose.int %8890, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8892 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8893 = torch.aten.view %8885, %8892 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8894 = torch.aten.transpose.int %8893, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8895:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8888, %8891, %8894, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8896 = torch.aten.transpose.int %8895#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8897 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8898 = torch.aten.view %8896, %8897 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8899 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8900 = torch.aten.view %8898, %8899 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8901 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8902 = torch.aten.transpose.int %8901, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %8903 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8904 = torch.prims.convert_element_type %8903, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8905 = torch.prims.convert_element_type %8900, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8906 = torch.prims.convert_element_type %8902, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8907 = torch.aten.mm %8905, %8906 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8908 = torch.aten.mul.Scalar %8907, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8909 = torch.aten.mul.Scalar %8904, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8910 = torch.aten.add.Tensor %8908, %8909, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8911 = torch.prims.convert_element_type %8910, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8912 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8913 = torch.aten.view %8911, %8912 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8914 = torch.aten.div.Scalar %8913, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8915 = torch.aten.add.Tensor %8914, %8830, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8916 = torch.prims.convert_element_type %8915, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8917 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_271, %result1_272 = torch.aten.var_mean.correction %8916, %8917, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8918 = torch.aten.add.Scalar %result0_271, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8919 = torch.aten.rsqrt %8918 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8920 = torch.aten.sub.Tensor %8915, %result1_272, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8921 = torch.aten.mul.Tensor %8920, %8919 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %8922 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8923 = torch.aten.mul.Tensor %8921, %8922 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %8924 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8925 = torch.aten.add.Tensor %8923, %8924, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8926 = torch.prims.convert_element_type %8925, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8927 = torch.prims.convert_element_type %result1_272, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8928 = torch.prims.convert_element_type %8919, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %8929 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8930 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8931 = torch.aten.view %8926, %8930 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8932 = torch_c.to_builtin_tensor %8931 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8933 = torch_c.to_builtin_tensor %8929 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8934 = tensor.empty() : tensor<2048x1280xf32>
    %8935 = linalg.fill ins(%cst : f32) outs(%8934 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8936 = tensor.empty() : tensor<2048x1280xf32>
    %8937 = linalg.fill ins(%cst : f32) outs(%8936 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8938:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8935, %8937, %8932, %8933, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8935, %8937)
    %8939 = arith.truncf %8938#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8940 = torch_c.from_builtin_tensor %8939 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8941 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8942 = torch.aten.view %8940, %8941 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %8943 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8944 = torch.aten.transpose.int %8943, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8945 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8946 = torch.aten.view %4, %8945 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8947 = torch.aten.mm %8946, %8944 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8948 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8949 = torch.aten.view %8947, %8948 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %8950 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8951 = torch.aten.transpose.int %8950, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8952 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8953 = torch.aten.view %4, %8952 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8954 = torch.aten.mm %8953, %8951 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %8955 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8956 = torch.aten.view %8954, %8955 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8957 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8958 = torch.aten.view %8942, %8957 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8959 = torch.aten.transpose.int %8958, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8960 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8961 = torch.aten.view %8949, %8960 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8962 = torch.aten.transpose.int %8961, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8963 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8964 = torch.aten.view %8956, %8963 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8965 = torch.aten.transpose.int %8964, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8966:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8959, %8962, %8965, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8967 = torch.aten.transpose.int %8966#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8968 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8969 = torch.aten.view %8967, %8968 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8970 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8971 = torch.aten.view %8969, %8970 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %8972 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8973 = torch.aten.transpose.int %8972, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %8974 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8975 = torch.prims.convert_element_type %8974, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8976 = torch.prims.convert_element_type %8971, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8977 = torch.prims.convert_element_type %8973, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8978 = torch.aten.mm %8976, %8977 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8979 = torch.aten.mul.Scalar %8978, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8980 = torch.aten.mul.Scalar %8975, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8981 = torch.aten.add.Tensor %8979, %8980, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8982 = torch.prims.convert_element_type %8981, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8983 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8984 = torch.aten.view %8982, %8983 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8985 = torch.aten.div.Scalar %8984, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8986 = torch.aten.add.Tensor %8985, %8915, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8987 = torch.prims.convert_element_type %8986, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8988 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_273, %result1_274 = torch.aten.var_mean.correction %8987, %8988, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8989 = torch.aten.add.Scalar %result0_273, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8990 = torch.aten.rsqrt %8989 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8991 = torch.aten.sub.Tensor %8986, %result1_274, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8992 = torch.aten.mul.Tensor %8991, %8990 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %8993 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8994 = torch.aten.mul.Tensor %8992, %8993 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %8995 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8996 = torch.aten.add.Tensor %8994, %8995, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8997 = torch.prims.convert_element_type %8996, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8998 = torch.prims.convert_element_type %result1_274, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8999 = torch.prims.convert_element_type %8990, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9000 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9001 = torch.aten.view %8997, %9000 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9002 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9003 = torch.aten.transpose.int %9002, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %9004 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9005 = torch.prims.convert_element_type %9004, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9006 = torch.prims.convert_element_type %9001, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9007 = torch.prims.convert_element_type %9003, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9008 = torch.aten.mm %9006, %9007 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9009 = torch.aten.mul.Scalar %9008, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9010 = torch.aten.mul.Scalar %9005, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9011 = torch.aten.add.Tensor %9009, %9010, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9012 = torch.prims.convert_element_type %9011, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9013 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9014 = torch.aten.view %9012, %9013 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9015 = torch.aten.slice.Tensor %9014, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9016 = torch.aten.slice.Tensor %9014, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9017 = torch.aten.gelu %9016, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9018 = torch.aten.mul.Tensor %9015, %9017 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9019 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9020 = torch.aten.view %9018, %9019 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %9021 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9022 = torch.aten.transpose.int %9021, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %9023 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9024 = torch.prims.convert_element_type %9023, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9025 = torch.prims.convert_element_type %9020, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9026 = torch.prims.convert_element_type %9022, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9027 = torch.aten.mm %9025, %9026 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9028 = torch.aten.mul.Scalar %9027, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9029 = torch.aten.mul.Scalar %9024, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9030 = torch.aten.add.Tensor %9028, %9029, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9031 = torch.prims.convert_element_type %9030, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9032 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9033 = torch.aten.view %9031, %9032 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9034 = torch.aten.add.Tensor %9033, %8986, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9035 = torch.prims.convert_element_type %9034, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9036 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_275, %result1_276 = torch.aten.var_mean.correction %9035, %9036, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9037 = torch.aten.add.Scalar %result0_275, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9038 = torch.aten.rsqrt %9037 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9039 = torch.aten.sub.Tensor %9034, %result1_276, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9040 = torch.aten.mul.Tensor %9039, %9038 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %9041 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9042 = torch.aten.mul.Tensor %9040, %9041 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %9043 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9044 = torch.aten.add.Tensor %9042, %9043, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9045 = torch.prims.convert_element_type %9044, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9046 = torch.prims.convert_element_type %result1_276, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9047 = torch.prims.convert_element_type %9038, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %9048 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9049 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9050 = torch.aten.view %9045, %9049 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9051 = torch_c.to_builtin_tensor %9050 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9052 = torch_c.to_builtin_tensor %9048 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9053 = tensor.empty() : tensor<2048x1280xf32>
    %9054 = linalg.fill ins(%cst : f32) outs(%9053 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9055 = tensor.empty() : tensor<2048x1280xf32>
    %9056 = linalg.fill ins(%cst : f32) outs(%9055 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9057:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9054, %9056, %9051, %9052, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9054, %9056)
    %9058 = arith.truncf %9057#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9059 = torch_c.from_builtin_tensor %9058 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9060 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9061 = torch.aten.view %9059, %9060 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %9062 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9063 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9064 = torch.aten.view %9045, %9063 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9065 = torch_c.to_builtin_tensor %9064 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9066 = torch_c.to_builtin_tensor %9062 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9067 = tensor.empty() : tensor<2048x1280xf32>
    %9068 = linalg.fill ins(%cst : f32) outs(%9067 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9069 = tensor.empty() : tensor<2048x1280xf32>
    %9070 = linalg.fill ins(%cst : f32) outs(%9069 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9071:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9068, %9070, %9065, %9066, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9068, %9070)
    %9072 = arith.truncf %9071#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9073 = torch_c.from_builtin_tensor %9072 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9074 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9075 = torch.aten.view %9073, %9074 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %9076 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9077 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9078 = torch.aten.view %9045, %9077 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9079 = torch_c.to_builtin_tensor %9078 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9080 = torch_c.to_builtin_tensor %9076 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9081 = tensor.empty() : tensor<2048x1280xf32>
    %9082 = linalg.fill ins(%cst : f32) outs(%9081 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9083 = tensor.empty() : tensor<2048x1280xf32>
    %9084 = linalg.fill ins(%cst : f32) outs(%9083 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9085:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9082, %9084, %9079, %9080, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9082, %9084)
    %9086 = arith.truncf %9085#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9087 = torch_c.from_builtin_tensor %9086 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9088 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9089 = torch.aten.view %9087, %9088 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9090 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9091 = torch.aten.view %9061, %9090 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9092 = torch.aten.transpose.int %9091, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9093 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9094 = torch.aten.view %9075, %9093 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9095 = torch.aten.transpose.int %9094, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9096 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9097 = torch.aten.view %9089, %9096 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9098 = torch.aten.transpose.int %9097, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9099:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9092, %9095, %9098, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9100 = torch.aten.transpose.int %9099#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9101 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9102 = torch.aten.view %9100, %9101 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9103 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9104 = torch.aten.view %9102, %9103 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9105 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9106 = torch.aten.transpose.int %9105, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %9107 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9108 = torch.prims.convert_element_type %9107, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9109 = torch.prims.convert_element_type %9104, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9110 = torch.prims.convert_element_type %9106, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9111 = torch.aten.mm %9109, %9110 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9112 = torch.aten.mul.Scalar %9111, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9113 = torch.aten.mul.Scalar %9108, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9114 = torch.aten.add.Tensor %9112, %9113, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9115 = torch.prims.convert_element_type %9114, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9116 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9117 = torch.aten.view %9115, %9116 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9118 = torch.aten.div.Scalar %9117, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9119 = torch.aten.add.Tensor %9118, %9034, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9120 = torch.prims.convert_element_type %9119, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9121 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_277, %result1_278 = torch.aten.var_mean.correction %9120, %9121, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9122 = torch.aten.add.Scalar %result0_277, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9123 = torch.aten.rsqrt %9122 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9124 = torch.aten.sub.Tensor %9119, %result1_278, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9125 = torch.aten.mul.Tensor %9124, %9123 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %9126 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9127 = torch.aten.mul.Tensor %9125, %9126 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %9128 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9129 = torch.aten.add.Tensor %9127, %9128, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9130 = torch.prims.convert_element_type %9129, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9131 = torch.prims.convert_element_type %result1_278, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9132 = torch.prims.convert_element_type %9123, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %9133 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9134 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9135 = torch.aten.view %9130, %9134 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9136 = torch_c.to_builtin_tensor %9135 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9137 = torch_c.to_builtin_tensor %9133 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9138 = tensor.empty() : tensor<2048x1280xf32>
    %9139 = linalg.fill ins(%cst : f32) outs(%9138 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9140 = tensor.empty() : tensor<2048x1280xf32>
    %9141 = linalg.fill ins(%cst : f32) outs(%9140 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9142:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9139, %9141, %9136, %9137, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9139, %9141)
    %9143 = arith.truncf %9142#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9144 = torch_c.from_builtin_tensor %9143 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9145 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9146 = torch.aten.view %9144, %9145 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %9147 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9148 = torch.aten.transpose.int %9147, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9149 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9150 = torch.aten.view %4, %9149 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9151 = torch.aten.mm %9150, %9148 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9152 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9153 = torch.aten.view %9151, %9152 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %9154 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9155 = torch.aten.transpose.int %9154, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9156 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9157 = torch.aten.view %4, %9156 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9158 = torch.aten.mm %9157, %9155 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9159 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9160 = torch.aten.view %9158, %9159 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9161 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9162 = torch.aten.view %9146, %9161 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9163 = torch.aten.transpose.int %9162, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9164 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9165 = torch.aten.view %9153, %9164 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9166 = torch.aten.transpose.int %9165, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9167 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9168 = torch.aten.view %9160, %9167 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9169 = torch.aten.transpose.int %9168, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9170:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9163, %9166, %9169, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9171 = torch.aten.transpose.int %9170#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9172 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9173 = torch.aten.view %9171, %9172 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9174 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9175 = torch.aten.view %9173, %9174 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9176 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9177 = torch.aten.transpose.int %9176, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %9178 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9179 = torch.prims.convert_element_type %9178, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9180 = torch.prims.convert_element_type %9175, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9181 = torch.prims.convert_element_type %9177, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9182 = torch.aten.mm %9180, %9181 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9183 = torch.aten.mul.Scalar %9182, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9184 = torch.aten.mul.Scalar %9179, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9185 = torch.aten.add.Tensor %9183, %9184, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9186 = torch.prims.convert_element_type %9185, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9187 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9188 = torch.aten.view %9186, %9187 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9189 = torch.aten.div.Scalar %9188, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9190 = torch.aten.add.Tensor %9189, %9119, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9191 = torch.prims.convert_element_type %9190, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9192 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_279, %result1_280 = torch.aten.var_mean.correction %9191, %9192, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9193 = torch.aten.add.Scalar %result0_279, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9194 = torch.aten.rsqrt %9193 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9195 = torch.aten.sub.Tensor %9190, %result1_280, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9196 = torch.aten.mul.Tensor %9195, %9194 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %9197 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9198 = torch.aten.mul.Tensor %9196, %9197 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %9199 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9200 = torch.aten.add.Tensor %9198, %9199, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9201 = torch.prims.convert_element_type %9200, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9202 = torch.prims.convert_element_type %result1_280, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9203 = torch.prims.convert_element_type %9194, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9204 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9205 = torch.aten.view %9201, %9204 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9206 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9207 = torch.aten.transpose.int %9206, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %9208 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9209 = torch.prims.convert_element_type %9208, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9210 = torch.prims.convert_element_type %9205, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9211 = torch.prims.convert_element_type %9207, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9212 = torch.aten.mm %9210, %9211 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9213 = torch.aten.mul.Scalar %9212, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9214 = torch.aten.mul.Scalar %9209, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9215 = torch.aten.add.Tensor %9213, %9214, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9216 = torch.prims.convert_element_type %9215, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9217 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9218 = torch.aten.view %9216, %9217 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9219 = torch.aten.slice.Tensor %9218, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9220 = torch.aten.slice.Tensor %9218, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9221 = torch.aten.gelu %9220, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9222 = torch.aten.mul.Tensor %9219, %9221 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9223 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9224 = torch.aten.view %9222, %9223 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %9225 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9226 = torch.aten.transpose.int %9225, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %9227 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9228 = torch.prims.convert_element_type %9227, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9229 = torch.prims.convert_element_type %9224, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9230 = torch.prims.convert_element_type %9226, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9231 = torch.aten.mm %9229, %9230 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9232 = torch.aten.mul.Scalar %9231, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9233 = torch.aten.mul.Scalar %9228, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9234 = torch.aten.add.Tensor %9232, %9233, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9235 = torch.prims.convert_element_type %9234, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9236 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9237 = torch.aten.view %9235, %9236 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9238 = torch.aten.add.Tensor %9237, %9190, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9239 = torch.prims.convert_element_type %9238, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9240 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_281, %result1_282 = torch.aten.var_mean.correction %9239, %9240, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9241 = torch.aten.add.Scalar %result0_281, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9242 = torch.aten.rsqrt %9241 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9243 = torch.aten.sub.Tensor %9238, %result1_282, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9244 = torch.aten.mul.Tensor %9243, %9242 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %9245 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9246 = torch.aten.mul.Tensor %9244, %9245 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %9247 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9248 = torch.aten.add.Tensor %9246, %9247, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9249 = torch.prims.convert_element_type %9248, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9250 = torch.prims.convert_element_type %result1_282, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9251 = torch.prims.convert_element_type %9242, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %9252 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9253 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9254 = torch.aten.view %9249, %9253 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9255 = torch_c.to_builtin_tensor %9254 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9256 = torch_c.to_builtin_tensor %9252 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9257 = tensor.empty() : tensor<2048x1280xf32>
    %9258 = linalg.fill ins(%cst : f32) outs(%9257 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9259 = tensor.empty() : tensor<2048x1280xf32>
    %9260 = linalg.fill ins(%cst : f32) outs(%9259 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9261:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9258, %9260, %9255, %9256, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9258, %9260)
    %9262 = arith.truncf %9261#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9263 = torch_c.from_builtin_tensor %9262 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9264 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9265 = torch.aten.view %9263, %9264 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %9266 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9267 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9268 = torch.aten.view %9249, %9267 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9269 = torch_c.to_builtin_tensor %9268 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9270 = torch_c.to_builtin_tensor %9266 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9271 = tensor.empty() : tensor<2048x1280xf32>
    %9272 = linalg.fill ins(%cst : f32) outs(%9271 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9273 = tensor.empty() : tensor<2048x1280xf32>
    %9274 = linalg.fill ins(%cst : f32) outs(%9273 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9275:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9272, %9274, %9269, %9270, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9272, %9274)
    %9276 = arith.truncf %9275#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9277 = torch_c.from_builtin_tensor %9276 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9278 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9279 = torch.aten.view %9277, %9278 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %9280 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9281 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9282 = torch.aten.view %9249, %9281 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9283 = torch_c.to_builtin_tensor %9282 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9284 = torch_c.to_builtin_tensor %9280 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9285 = tensor.empty() : tensor<2048x1280xf32>
    %9286 = linalg.fill ins(%cst : f32) outs(%9285 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9287 = tensor.empty() : tensor<2048x1280xf32>
    %9288 = linalg.fill ins(%cst : f32) outs(%9287 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9289:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9286, %9288, %9283, %9284, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9286, %9288)
    %9290 = arith.truncf %9289#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9291 = torch_c.from_builtin_tensor %9290 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9292 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9293 = torch.aten.view %9291, %9292 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9294 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9295 = torch.aten.view %9265, %9294 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9296 = torch.aten.transpose.int %9295, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9297 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9298 = torch.aten.view %9279, %9297 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9299 = torch.aten.transpose.int %9298, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9300 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9301 = torch.aten.view %9293, %9300 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9302 = torch.aten.transpose.int %9301, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9303:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9296, %9299, %9302, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9304 = torch.aten.transpose.int %9303#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9305 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9306 = torch.aten.view %9304, %9305 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9307 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9308 = torch.aten.view %9306, %9307 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9309 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9310 = torch.aten.transpose.int %9309, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %9311 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9312 = torch.prims.convert_element_type %9311, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9313 = torch.prims.convert_element_type %9308, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9314 = torch.prims.convert_element_type %9310, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9315 = torch.aten.mm %9313, %9314 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9316 = torch.aten.mul.Scalar %9315, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9317 = torch.aten.mul.Scalar %9312, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9318 = torch.aten.add.Tensor %9316, %9317, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9319 = torch.prims.convert_element_type %9318, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9320 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9321 = torch.aten.view %9319, %9320 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9322 = torch.aten.div.Scalar %9321, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9323 = torch.aten.add.Tensor %9322, %9238, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9324 = torch.prims.convert_element_type %9323, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9325 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_283, %result1_284 = torch.aten.var_mean.correction %9324, %9325, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9326 = torch.aten.add.Scalar %result0_283, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9327 = torch.aten.rsqrt %9326 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9328 = torch.aten.sub.Tensor %9323, %result1_284, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9329 = torch.aten.mul.Tensor %9328, %9327 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %9330 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9331 = torch.aten.mul.Tensor %9329, %9330 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %9332 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9333 = torch.aten.add.Tensor %9331, %9332, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9334 = torch.prims.convert_element_type %9333, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9335 = torch.prims.convert_element_type %result1_284, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9336 = torch.prims.convert_element_type %9327, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %9337 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9338 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9339 = torch.aten.view %9334, %9338 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9340 = torch_c.to_builtin_tensor %9339 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9341 = torch_c.to_builtin_tensor %9337 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9342 = tensor.empty() : tensor<2048x1280xf32>
    %9343 = linalg.fill ins(%cst : f32) outs(%9342 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9344 = tensor.empty() : tensor<2048x1280xf32>
    %9345 = linalg.fill ins(%cst : f32) outs(%9344 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9346:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9343, %9345, %9340, %9341, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9343, %9345)
    %9347 = arith.truncf %9346#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9348 = torch_c.from_builtin_tensor %9347 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9349 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9350 = torch.aten.view %9348, %9349 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %9351 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9352 = torch.aten.transpose.int %9351, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9353 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9354 = torch.aten.view %4, %9353 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9355 = torch.aten.mm %9354, %9352 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9356 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9357 = torch.aten.view %9355, %9356 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %9358 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9359 = torch.aten.transpose.int %9358, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9360 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9361 = torch.aten.view %4, %9360 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9362 = torch.aten.mm %9361, %9359 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9363 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9364 = torch.aten.view %9362, %9363 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9365 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9366 = torch.aten.view %9350, %9365 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9367 = torch.aten.transpose.int %9366, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9368 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9369 = torch.aten.view %9357, %9368 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9370 = torch.aten.transpose.int %9369, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9371 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9372 = torch.aten.view %9364, %9371 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9373 = torch.aten.transpose.int %9372, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9374:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9367, %9370, %9373, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9375 = torch.aten.transpose.int %9374#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9376 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9377 = torch.aten.view %9375, %9376 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9378 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9379 = torch.aten.view %9377, %9378 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9380 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9381 = torch.aten.transpose.int %9380, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %9382 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9383 = torch.prims.convert_element_type %9382, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9384 = torch.prims.convert_element_type %9379, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9385 = torch.prims.convert_element_type %9381, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9386 = torch.aten.mm %9384, %9385 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9387 = torch.aten.mul.Scalar %9386, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9388 = torch.aten.mul.Scalar %9383, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9389 = torch.aten.add.Tensor %9387, %9388, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9390 = torch.prims.convert_element_type %9389, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9391 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9392 = torch.aten.view %9390, %9391 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9393 = torch.aten.div.Scalar %9392, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9394 = torch.aten.add.Tensor %9393, %9323, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9395 = torch.prims.convert_element_type %9394, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9396 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_285, %result1_286 = torch.aten.var_mean.correction %9395, %9396, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9397 = torch.aten.add.Scalar %result0_285, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9398 = torch.aten.rsqrt %9397 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9399 = torch.aten.sub.Tensor %9394, %result1_286, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9400 = torch.aten.mul.Tensor %9399, %9398 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %9401 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9402 = torch.aten.mul.Tensor %9400, %9401 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %9403 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9404 = torch.aten.add.Tensor %9402, %9403, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9405 = torch.prims.convert_element_type %9404, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9406 = torch.prims.convert_element_type %result1_286, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9407 = torch.prims.convert_element_type %9398, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9408 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9409 = torch.aten.view %9405, %9408 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9410 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9411 = torch.aten.transpose.int %9410, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %9412 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9413 = torch.prims.convert_element_type %9412, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9414 = torch.prims.convert_element_type %9409, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9415 = torch.prims.convert_element_type %9411, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9416 = torch.aten.mm %9414, %9415 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9417 = torch.aten.mul.Scalar %9416, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9418 = torch.aten.mul.Scalar %9413, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9419 = torch.aten.add.Tensor %9417, %9418, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9420 = torch.prims.convert_element_type %9419, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9421 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9422 = torch.aten.view %9420, %9421 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9423 = torch.aten.slice.Tensor %9422, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9424 = torch.aten.slice.Tensor %9422, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9425 = torch.aten.gelu %9424, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9426 = torch.aten.mul.Tensor %9423, %9425 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9427 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9428 = torch.aten.view %9426, %9427 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %9429 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9430 = torch.aten.transpose.int %9429, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %9431 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9432 = torch.prims.convert_element_type %9431, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9433 = torch.prims.convert_element_type %9428, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9434 = torch.prims.convert_element_type %9430, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9435 = torch.aten.mm %9433, %9434 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9436 = torch.aten.mul.Scalar %9435, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9437 = torch.aten.mul.Scalar %9432, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9438 = torch.aten.add.Tensor %9436, %9437, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9439 = torch.prims.convert_element_type %9438, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9440 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9441 = torch.aten.view %9439, %9440 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9442 = torch.aten.add.Tensor %9441, %9394, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9443 = torch.prims.convert_element_type %9442, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9444 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_287, %result1_288 = torch.aten.var_mean.correction %9443, %9444, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9445 = torch.aten.add.Scalar %result0_287, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9446 = torch.aten.rsqrt %9445 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9447 = torch.aten.sub.Tensor %9442, %result1_288, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9448 = torch.aten.mul.Tensor %9447, %9446 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %9449 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9450 = torch.aten.mul.Tensor %9448, %9449 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %9451 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9452 = torch.aten.add.Tensor %9450, %9451, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9453 = torch.prims.convert_element_type %9452, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9454 = torch.prims.convert_element_type %result1_288, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9455 = torch.prims.convert_element_type %9446, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %9456 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9457 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9458 = torch.aten.view %9453, %9457 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9459 = torch_c.to_builtin_tensor %9458 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9460 = torch_c.to_builtin_tensor %9456 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9461 = tensor.empty() : tensor<2048x1280xf32>
    %9462 = linalg.fill ins(%cst : f32) outs(%9461 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9463 = tensor.empty() : tensor<2048x1280xf32>
    %9464 = linalg.fill ins(%cst : f32) outs(%9463 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9465:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9462, %9464, %9459, %9460, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9462, %9464)
    %9466 = arith.truncf %9465#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9467 = torch_c.from_builtin_tensor %9466 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9468 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9469 = torch.aten.view %9467, %9468 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %9470 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9471 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9472 = torch.aten.view %9453, %9471 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9473 = torch_c.to_builtin_tensor %9472 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9474 = torch_c.to_builtin_tensor %9470 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9475 = tensor.empty() : tensor<2048x1280xf32>
    %9476 = linalg.fill ins(%cst : f32) outs(%9475 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9477 = tensor.empty() : tensor<2048x1280xf32>
    %9478 = linalg.fill ins(%cst : f32) outs(%9477 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9479:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9476, %9478, %9473, %9474, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9476, %9478)
    %9480 = arith.truncf %9479#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9481 = torch_c.from_builtin_tensor %9480 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9482 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9483 = torch.aten.view %9481, %9482 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %9484 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9485 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9486 = torch.aten.view %9453, %9485 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9487 = torch_c.to_builtin_tensor %9486 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9488 = torch_c.to_builtin_tensor %9484 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9489 = tensor.empty() : tensor<2048x1280xf32>
    %9490 = linalg.fill ins(%cst : f32) outs(%9489 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9491 = tensor.empty() : tensor<2048x1280xf32>
    %9492 = linalg.fill ins(%cst : f32) outs(%9491 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9493:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9490, %9492, %9487, %9488, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9490, %9492)
    %9494 = arith.truncf %9493#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9495 = torch_c.from_builtin_tensor %9494 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9496 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9497 = torch.aten.view %9495, %9496 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9498 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9499 = torch.aten.view %9469, %9498 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9500 = torch.aten.transpose.int %9499, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9501 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9502 = torch.aten.view %9483, %9501 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9503 = torch.aten.transpose.int %9502, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9504 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9505 = torch.aten.view %9497, %9504 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9506 = torch.aten.transpose.int %9505, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9507:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9500, %9503, %9506, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9508 = torch.aten.transpose.int %9507#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9509 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9510 = torch.aten.view %9508, %9509 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9511 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9512 = torch.aten.view %9510, %9511 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9513 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9514 = torch.aten.transpose.int %9513, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %9515 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9516 = torch.prims.convert_element_type %9515, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9517 = torch.prims.convert_element_type %9512, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9518 = torch.prims.convert_element_type %9514, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9519 = torch.aten.mm %9517, %9518 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9520 = torch.aten.mul.Scalar %9519, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9521 = torch.aten.mul.Scalar %9516, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9522 = torch.aten.add.Tensor %9520, %9521, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9523 = torch.prims.convert_element_type %9522, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9524 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9525 = torch.aten.view %9523, %9524 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9526 = torch.aten.div.Scalar %9525, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9527 = torch.aten.add.Tensor %9526, %9442, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9528 = torch.prims.convert_element_type %9527, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9529 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_289, %result1_290 = torch.aten.var_mean.correction %9528, %9529, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9530 = torch.aten.add.Scalar %result0_289, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9531 = torch.aten.rsqrt %9530 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9532 = torch.aten.sub.Tensor %9527, %result1_290, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9533 = torch.aten.mul.Tensor %9532, %9531 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %9534 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9535 = torch.aten.mul.Tensor %9533, %9534 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %9536 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9537 = torch.aten.add.Tensor %9535, %9536, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9538 = torch.prims.convert_element_type %9537, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9539 = torch.prims.convert_element_type %result1_290, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9540 = torch.prims.convert_element_type %9531, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %9541 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9542 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9543 = torch.aten.view %9538, %9542 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9544 = torch_c.to_builtin_tensor %9543 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9545 = torch_c.to_builtin_tensor %9541 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9546 = tensor.empty() : tensor<2048x1280xf32>
    %9547 = linalg.fill ins(%cst : f32) outs(%9546 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9548 = tensor.empty() : tensor<2048x1280xf32>
    %9549 = linalg.fill ins(%cst : f32) outs(%9548 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9550:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9547, %9549, %9544, %9545, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9547, %9549)
    %9551 = arith.truncf %9550#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9552 = torch_c.from_builtin_tensor %9551 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9553 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9554 = torch.aten.view %9552, %9553 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %9555 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9556 = torch.aten.transpose.int %9555, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9557 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9558 = torch.aten.view %4, %9557 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9559 = torch.aten.mm %9558, %9556 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9560 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9561 = torch.aten.view %9559, %9560 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %9562 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9563 = torch.aten.transpose.int %9562, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9564 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9565 = torch.aten.view %4, %9564 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9566 = torch.aten.mm %9565, %9563 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9567 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9568 = torch.aten.view %9566, %9567 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9569 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9570 = torch.aten.view %9554, %9569 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9571 = torch.aten.transpose.int %9570, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9572 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9573 = torch.aten.view %9561, %9572 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9574 = torch.aten.transpose.int %9573, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9575 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9576 = torch.aten.view %9568, %9575 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9577 = torch.aten.transpose.int %9576, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9578:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9571, %9574, %9577, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9579 = torch.aten.transpose.int %9578#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9580 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9581 = torch.aten.view %9579, %9580 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9582 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9583 = torch.aten.view %9581, %9582 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9584 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9585 = torch.aten.transpose.int %9584, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %9586 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9587 = torch.prims.convert_element_type %9586, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9588 = torch.prims.convert_element_type %9583, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9589 = torch.prims.convert_element_type %9585, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9590 = torch.aten.mm %9588, %9589 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9591 = torch.aten.mul.Scalar %9590, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9592 = torch.aten.mul.Scalar %9587, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9593 = torch.aten.add.Tensor %9591, %9592, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9594 = torch.prims.convert_element_type %9593, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9595 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9596 = torch.aten.view %9594, %9595 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9597 = torch.aten.div.Scalar %9596, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9598 = torch.aten.add.Tensor %9597, %9527, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9599 = torch.prims.convert_element_type %9598, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9600 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_291, %result1_292 = torch.aten.var_mean.correction %9599, %9600, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9601 = torch.aten.add.Scalar %result0_291, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9602 = torch.aten.rsqrt %9601 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9603 = torch.aten.sub.Tensor %9598, %result1_292, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9604 = torch.aten.mul.Tensor %9603, %9602 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %9605 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9606 = torch.aten.mul.Tensor %9604, %9605 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %9607 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9608 = torch.aten.add.Tensor %9606, %9607, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9609 = torch.prims.convert_element_type %9608, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9610 = torch.prims.convert_element_type %result1_292, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9611 = torch.prims.convert_element_type %9602, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9612 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9613 = torch.aten.view %9609, %9612 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9614 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9615 = torch.aten.transpose.int %9614, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %9616 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9617 = torch.prims.convert_element_type %9616, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9618 = torch.prims.convert_element_type %9613, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9619 = torch.prims.convert_element_type %9615, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9620 = torch.aten.mm %9618, %9619 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9621 = torch.aten.mul.Scalar %9620, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9622 = torch.aten.mul.Scalar %9617, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9623 = torch.aten.add.Tensor %9621, %9622, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9624 = torch.prims.convert_element_type %9623, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9625 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9626 = torch.aten.view %9624, %9625 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9627 = torch.aten.slice.Tensor %9626, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9628 = torch.aten.slice.Tensor %9626, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9629 = torch.aten.gelu %9628, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9630 = torch.aten.mul.Tensor %9627, %9629 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9631 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9632 = torch.aten.view %9630, %9631 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %9633 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9634 = torch.aten.transpose.int %9633, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %9635 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9636 = torch.prims.convert_element_type %9635, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9637 = torch.prims.convert_element_type %9632, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9638 = torch.prims.convert_element_type %9634, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9639 = torch.aten.mm %9637, %9638 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9640 = torch.aten.mul.Scalar %9639, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9641 = torch.aten.mul.Scalar %9636, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9642 = torch.aten.add.Tensor %9640, %9641, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9643 = torch.prims.convert_element_type %9642, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9644 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9645 = torch.aten.view %9643, %9644 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9646 = torch.aten.add.Tensor %9645, %9598, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9647 = torch.prims.convert_element_type %9646, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9648 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_293, %result1_294 = torch.aten.var_mean.correction %9647, %9648, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9649 = torch.aten.add.Scalar %result0_293, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9650 = torch.aten.rsqrt %9649 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9651 = torch.aten.sub.Tensor %9646, %result1_294, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9652 = torch.aten.mul.Tensor %9651, %9650 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %9653 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9654 = torch.aten.mul.Tensor %9652, %9653 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %9655 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9656 = torch.aten.add.Tensor %9654, %9655, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9657 = torch.prims.convert_element_type %9656, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9658 = torch.prims.convert_element_type %result1_294, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9659 = torch.prims.convert_element_type %9650, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %9660 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9661 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9662 = torch.aten.view %9657, %9661 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9663 = torch_c.to_builtin_tensor %9662 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9664 = torch_c.to_builtin_tensor %9660 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9665 = tensor.empty() : tensor<2048x1280xf32>
    %9666 = linalg.fill ins(%cst : f32) outs(%9665 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9667 = tensor.empty() : tensor<2048x1280xf32>
    %9668 = linalg.fill ins(%cst : f32) outs(%9667 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9669:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9666, %9668, %9663, %9664, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9666, %9668)
    %9670 = arith.truncf %9669#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9671 = torch_c.from_builtin_tensor %9670 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9672 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9673 = torch.aten.view %9671, %9672 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %9674 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9675 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9676 = torch.aten.view %9657, %9675 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9677 = torch_c.to_builtin_tensor %9676 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9678 = torch_c.to_builtin_tensor %9674 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9679 = tensor.empty() : tensor<2048x1280xf32>
    %9680 = linalg.fill ins(%cst : f32) outs(%9679 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9681 = tensor.empty() : tensor<2048x1280xf32>
    %9682 = linalg.fill ins(%cst : f32) outs(%9681 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9683:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9680, %9682, %9677, %9678, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9680, %9682)
    %9684 = arith.truncf %9683#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9685 = torch_c.from_builtin_tensor %9684 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9686 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9687 = torch.aten.view %9685, %9686 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %9688 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9689 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9690 = torch.aten.view %9657, %9689 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9691 = torch_c.to_builtin_tensor %9690 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9692 = torch_c.to_builtin_tensor %9688 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9693 = tensor.empty() : tensor<2048x1280xf32>
    %9694 = linalg.fill ins(%cst : f32) outs(%9693 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9695 = tensor.empty() : tensor<2048x1280xf32>
    %9696 = linalg.fill ins(%cst : f32) outs(%9695 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9697:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9694, %9696, %9691, %9692, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9694, %9696)
    %9698 = arith.truncf %9697#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9699 = torch_c.from_builtin_tensor %9698 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9700 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9701 = torch.aten.view %9699, %9700 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9702 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9703 = torch.aten.view %9673, %9702 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9704 = torch.aten.transpose.int %9703, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9705 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9706 = torch.aten.view %9687, %9705 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9707 = torch.aten.transpose.int %9706, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9708 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9709 = torch.aten.view %9701, %9708 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9710 = torch.aten.transpose.int %9709, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9711:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9704, %9707, %9710, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9712 = torch.aten.transpose.int %9711#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9713 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9714 = torch.aten.view %9712, %9713 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9715 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9716 = torch.aten.view %9714, %9715 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9717 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9718 = torch.aten.transpose.int %9717, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %9719 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9720 = torch.prims.convert_element_type %9719, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9721 = torch.prims.convert_element_type %9716, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9722 = torch.prims.convert_element_type %9718, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9723 = torch.aten.mm %9721, %9722 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9724 = torch.aten.mul.Scalar %9723, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9725 = torch.aten.mul.Scalar %9720, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9726 = torch.aten.add.Tensor %9724, %9725, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9727 = torch.prims.convert_element_type %9726, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9728 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9729 = torch.aten.view %9727, %9728 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9730 = torch.aten.div.Scalar %9729, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9731 = torch.aten.add.Tensor %9730, %9646, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9732 = torch.prims.convert_element_type %9731, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9733 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_295, %result1_296 = torch.aten.var_mean.correction %9732, %9733, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9734 = torch.aten.add.Scalar %result0_295, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9735 = torch.aten.rsqrt %9734 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9736 = torch.aten.sub.Tensor %9731, %result1_296, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9737 = torch.aten.mul.Tensor %9736, %9735 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %9738 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9739 = torch.aten.mul.Tensor %9737, %9738 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %9740 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9741 = torch.aten.add.Tensor %9739, %9740, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9742 = torch.prims.convert_element_type %9741, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9743 = torch.prims.convert_element_type %result1_296, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9744 = torch.prims.convert_element_type %9735, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %9745 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9746 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9747 = torch.aten.view %9742, %9746 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9748 = torch_c.to_builtin_tensor %9747 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9749 = torch_c.to_builtin_tensor %9745 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9750 = tensor.empty() : tensor<2048x1280xf32>
    %9751 = linalg.fill ins(%cst : f32) outs(%9750 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9752 = tensor.empty() : tensor<2048x1280xf32>
    %9753 = linalg.fill ins(%cst : f32) outs(%9752 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9754:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9751, %9753, %9748, %9749, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9751, %9753)
    %9755 = arith.truncf %9754#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9756 = torch_c.from_builtin_tensor %9755 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9757 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9758 = torch.aten.view %9756, %9757 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %9759 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9760 = torch.aten.transpose.int %9759, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9761 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9762 = torch.aten.view %4, %9761 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9763 = torch.aten.mm %9762, %9760 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9764 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9765 = torch.aten.view %9763, %9764 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %9766 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9767 = torch.aten.transpose.int %9766, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9768 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9769 = torch.aten.view %4, %9768 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9770 = torch.aten.mm %9769, %9767 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9771 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9772 = torch.aten.view %9770, %9771 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9773 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9774 = torch.aten.view %9758, %9773 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9775 = torch.aten.transpose.int %9774, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9776 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9777 = torch.aten.view %9765, %9776 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9778 = torch.aten.transpose.int %9777, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9779 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9780 = torch.aten.view %9772, %9779 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9781 = torch.aten.transpose.int %9780, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9782:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9775, %9778, %9781, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9783 = torch.aten.transpose.int %9782#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9784 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9785 = torch.aten.view %9783, %9784 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9786 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9787 = torch.aten.view %9785, %9786 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9788 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9789 = torch.aten.transpose.int %9788, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %9790 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9791 = torch.prims.convert_element_type %9790, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9792 = torch.prims.convert_element_type %9787, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9793 = torch.prims.convert_element_type %9789, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9794 = torch.aten.mm %9792, %9793 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9795 = torch.aten.mul.Scalar %9794, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9796 = torch.aten.mul.Scalar %9791, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9797 = torch.aten.add.Tensor %9795, %9796, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9798 = torch.prims.convert_element_type %9797, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9799 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9800 = torch.aten.view %9798, %9799 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9801 = torch.aten.div.Scalar %9800, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9802 = torch.aten.add.Tensor %9801, %9731, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9803 = torch.prims.convert_element_type %9802, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9804 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_297, %result1_298 = torch.aten.var_mean.correction %9803, %9804, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9805 = torch.aten.add.Scalar %result0_297, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9806 = torch.aten.rsqrt %9805 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9807 = torch.aten.sub.Tensor %9802, %result1_298, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9808 = torch.aten.mul.Tensor %9807, %9806 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %9809 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9810 = torch.aten.mul.Tensor %9808, %9809 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %9811 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9812 = torch.aten.add.Tensor %9810, %9811, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9813 = torch.prims.convert_element_type %9812, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9814 = torch.prims.convert_element_type %result1_298, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9815 = torch.prims.convert_element_type %9806, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9816 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9817 = torch.aten.view %9813, %9816 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9818 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9819 = torch.aten.transpose.int %9818, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %9820 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9821 = torch.prims.convert_element_type %9820, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9822 = torch.prims.convert_element_type %9817, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9823 = torch.prims.convert_element_type %9819, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9824 = torch.aten.mm %9822, %9823 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9825 = torch.aten.mul.Scalar %9824, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9826 = torch.aten.mul.Scalar %9821, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9827 = torch.aten.add.Tensor %9825, %9826, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9828 = torch.prims.convert_element_type %9827, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9829 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9830 = torch.aten.view %9828, %9829 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9831 = torch.aten.slice.Tensor %9830, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9832 = torch.aten.slice.Tensor %9830, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9833 = torch.aten.gelu %9832, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9834 = torch.aten.mul.Tensor %9831, %9833 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9835 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9836 = torch.aten.view %9834, %9835 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %9837 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9838 = torch.aten.transpose.int %9837, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %9839 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9840 = torch.prims.convert_element_type %9839, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9841 = torch.prims.convert_element_type %9836, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9842 = torch.prims.convert_element_type %9838, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9843 = torch.aten.mm %9841, %9842 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9844 = torch.aten.mul.Scalar %9843, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9845 = torch.aten.mul.Scalar %9840, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9846 = torch.aten.add.Tensor %9844, %9845, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9847 = torch.prims.convert_element_type %9846, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9848 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9849 = torch.aten.view %9847, %9848 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9850 = torch.aten.add.Tensor %9849, %9802, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9851 = torch.prims.convert_element_type %9850, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9852 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_299, %result1_300 = torch.aten.var_mean.correction %9851, %9852, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9853 = torch.aten.add.Scalar %result0_299, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9854 = torch.aten.rsqrt %9853 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9855 = torch.aten.sub.Tensor %9850, %result1_300, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9856 = torch.aten.mul.Tensor %9855, %9854 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %9857 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9858 = torch.aten.mul.Tensor %9856, %9857 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %9859 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9860 = torch.aten.add.Tensor %9858, %9859, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9861 = torch.prims.convert_element_type %9860, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9862 = torch.prims.convert_element_type %result1_300, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9863 = torch.prims.convert_element_type %9854, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %9864 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9865 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9866 = torch.aten.view %9861, %9865 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9867 = torch_c.to_builtin_tensor %9866 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9868 = torch_c.to_builtin_tensor %9864 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9869 = tensor.empty() : tensor<2048x1280xf32>
    %9870 = linalg.fill ins(%cst : f32) outs(%9869 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9871 = tensor.empty() : tensor<2048x1280xf32>
    %9872 = linalg.fill ins(%cst : f32) outs(%9871 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9873:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9870, %9872, %9867, %9868, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9870, %9872)
    %9874 = arith.truncf %9873#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9875 = torch_c.from_builtin_tensor %9874 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9876 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9877 = torch.aten.view %9875, %9876 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %9878 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9879 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9880 = torch.aten.view %9861, %9879 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9881 = torch_c.to_builtin_tensor %9880 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9882 = torch_c.to_builtin_tensor %9878 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9883 = tensor.empty() : tensor<2048x1280xf32>
    %9884 = linalg.fill ins(%cst : f32) outs(%9883 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9885 = tensor.empty() : tensor<2048x1280xf32>
    %9886 = linalg.fill ins(%cst : f32) outs(%9885 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9887:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9884, %9886, %9881, %9882, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9884, %9886)
    %9888 = arith.truncf %9887#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9889 = torch_c.from_builtin_tensor %9888 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9890 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9891 = torch.aten.view %9889, %9890 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %9892 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9893 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9894 = torch.aten.view %9861, %9893 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9895 = torch_c.to_builtin_tensor %9894 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9896 = torch_c.to_builtin_tensor %9892 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9897 = tensor.empty() : tensor<2048x1280xf32>
    %9898 = linalg.fill ins(%cst : f32) outs(%9897 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9899 = tensor.empty() : tensor<2048x1280xf32>
    %9900 = linalg.fill ins(%cst : f32) outs(%9899 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9901:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9898, %9900, %9895, %9896, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9898, %9900)
    %9902 = arith.truncf %9901#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9903 = torch_c.from_builtin_tensor %9902 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9904 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9905 = torch.aten.view %9903, %9904 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9906 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9907 = torch.aten.view %9877, %9906 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9908 = torch.aten.transpose.int %9907, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9909 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9910 = torch.aten.view %9891, %9909 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9911 = torch.aten.transpose.int %9910, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9912 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9913 = torch.aten.view %9905, %9912 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9914 = torch.aten.transpose.int %9913, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9915:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9908, %9911, %9914, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9916 = torch.aten.transpose.int %9915#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9917 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9918 = torch.aten.view %9916, %9917 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9919 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9920 = torch.aten.view %9918, %9919 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9921 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9922 = torch.aten.transpose.int %9921, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %9923 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9924 = torch.prims.convert_element_type %9923, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9925 = torch.prims.convert_element_type %9920, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9926 = torch.prims.convert_element_type %9922, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9927 = torch.aten.mm %9925, %9926 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9928 = torch.aten.mul.Scalar %9927, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9929 = torch.aten.mul.Scalar %9924, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9930 = torch.aten.add.Tensor %9928, %9929, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9931 = torch.prims.convert_element_type %9930, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9932 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9933 = torch.aten.view %9931, %9932 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9934 = torch.aten.div.Scalar %9933, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9935 = torch.aten.add.Tensor %9934, %9850, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9936 = torch.prims.convert_element_type %9935, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9937 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_301, %result1_302 = torch.aten.var_mean.correction %9936, %9937, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9938 = torch.aten.add.Scalar %result0_301, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9939 = torch.aten.rsqrt %9938 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9940 = torch.aten.sub.Tensor %9935, %result1_302, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9941 = torch.aten.mul.Tensor %9940, %9939 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %9942 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9943 = torch.aten.mul.Tensor %9941, %9942 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %9944 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9945 = torch.aten.add.Tensor %9943, %9944, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9946 = torch.prims.convert_element_type %9945, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9947 = torch.prims.convert_element_type %result1_302, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9948 = torch.prims.convert_element_type %9939, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %9949 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9950 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9951 = torch.aten.view %9946, %9950 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9952 = torch_c.to_builtin_tensor %9951 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9953 = torch_c.to_builtin_tensor %9949 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9954 = tensor.empty() : tensor<2048x1280xf32>
    %9955 = linalg.fill ins(%cst : f32) outs(%9954 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9956 = tensor.empty() : tensor<2048x1280xf32>
    %9957 = linalg.fill ins(%cst : f32) outs(%9956 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9958:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9955, %9957, %9952, %9953, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9955, %9957)
    %9959 = arith.truncf %9958#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9960 = torch_c.from_builtin_tensor %9959 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9961 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9962 = torch.aten.view %9960, %9961 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %9963 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9964 = torch.aten.transpose.int %9963, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9965 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9966 = torch.aten.view %4, %9965 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9967 = torch.aten.mm %9966, %9964 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9968 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9969 = torch.aten.view %9967, %9968 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %9970 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9971 = torch.aten.transpose.int %9970, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9972 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9973 = torch.aten.view %4, %9972 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9974 = torch.aten.mm %9973, %9971 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %9975 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9976 = torch.aten.view %9974, %9975 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9977 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9978 = torch.aten.view %9962, %9977 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9979 = torch.aten.transpose.int %9978, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9980 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9981 = torch.aten.view %9969, %9980 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9982 = torch.aten.transpose.int %9981, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9983 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9984 = torch.aten.view %9976, %9983 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9985 = torch.aten.transpose.int %9984, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9986:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9979, %9982, %9985, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9987 = torch.aten.transpose.int %9986#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9988 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9989 = torch.aten.view %9987, %9988 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9990 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9991 = torch.aten.view %9989, %9990 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9992 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9993 = torch.aten.transpose.int %9992, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %9994 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9995 = torch.prims.convert_element_type %9994, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9996 = torch.prims.convert_element_type %9991, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9997 = torch.prims.convert_element_type %9993, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9998 = torch.aten.mm %9996, %9997 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9999 = torch.aten.mul.Scalar %9998, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10000 = torch.aten.mul.Scalar %9995, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10001 = torch.aten.add.Tensor %9999, %10000, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10002 = torch.prims.convert_element_type %10001, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10003 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10004 = torch.aten.view %10002, %10003 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10005 = torch.aten.div.Scalar %10004, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10006 = torch.aten.add.Tensor %10005, %9935, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10007 = torch.prims.convert_element_type %10006, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10008 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_303, %result1_304 = torch.aten.var_mean.correction %10007, %10008, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10009 = torch.aten.add.Scalar %result0_303, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10010 = torch.aten.rsqrt %10009 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10011 = torch.aten.sub.Tensor %10006, %result1_304, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10012 = torch.aten.mul.Tensor %10011, %10010 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %10013 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10014 = torch.aten.mul.Tensor %10012, %10013 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %10015 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10016 = torch.aten.add.Tensor %10014, %10015, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10017 = torch.prims.convert_element_type %10016, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10018 = torch.prims.convert_element_type %result1_304, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10019 = torch.prims.convert_element_type %10010, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10020 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10021 = torch.aten.view %10017, %10020 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10022 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10023 = torch.aten.transpose.int %10022, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %10024 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10025 = torch.prims.convert_element_type %10024, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10026 = torch.prims.convert_element_type %10021, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10027 = torch.prims.convert_element_type %10023, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10028 = torch.aten.mm %10026, %10027 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10029 = torch.aten.mul.Scalar %10028, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10030 = torch.aten.mul.Scalar %10025, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10031 = torch.aten.add.Tensor %10029, %10030, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10032 = torch.prims.convert_element_type %10031, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10033 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10034 = torch.aten.view %10032, %10033 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10035 = torch.aten.slice.Tensor %10034, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10036 = torch.aten.slice.Tensor %10034, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10037 = torch.aten.gelu %10036, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10038 = torch.aten.mul.Tensor %10035, %10037 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10039 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10040 = torch.aten.view %10038, %10039 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %10041 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10042 = torch.aten.transpose.int %10041, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %10043 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10044 = torch.prims.convert_element_type %10043, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10045 = torch.prims.convert_element_type %10040, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10046 = torch.prims.convert_element_type %10042, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10047 = torch.aten.mm %10045, %10046 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10048 = torch.aten.mul.Scalar %10047, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10049 = torch.aten.mul.Scalar %10044, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10050 = torch.aten.add.Tensor %10048, %10049, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10051 = torch.prims.convert_element_type %10050, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10052 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10053 = torch.aten.view %10051, %10052 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10054 = torch.aten.add.Tensor %10053, %10006, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10055 = torch.prims.convert_element_type %10054, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10056 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_305, %result1_306 = torch.aten.var_mean.correction %10055, %10056, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10057 = torch.aten.add.Scalar %result0_305, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10058 = torch.aten.rsqrt %10057 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10059 = torch.aten.sub.Tensor %10054, %result1_306, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10060 = torch.aten.mul.Tensor %10059, %10058 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %10061 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10062 = torch.aten.mul.Tensor %10060, %10061 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %10063 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10064 = torch.aten.add.Tensor %10062, %10063, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10065 = torch.prims.convert_element_type %10064, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10066 = torch.prims.convert_element_type %result1_306, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10067 = torch.prims.convert_element_type %10058, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %10068 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10069 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10070 = torch.aten.view %10065, %10069 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10071 = torch_c.to_builtin_tensor %10070 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10072 = torch_c.to_builtin_tensor %10068 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10073 = tensor.empty() : tensor<2048x1280xf32>
    %10074 = linalg.fill ins(%cst : f32) outs(%10073 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10075 = tensor.empty() : tensor<2048x1280xf32>
    %10076 = linalg.fill ins(%cst : f32) outs(%10075 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10077:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10074, %10076, %10071, %10072, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10074, %10076)
    %10078 = arith.truncf %10077#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10079 = torch_c.from_builtin_tensor %10078 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10080 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10081 = torch.aten.view %10079, %10080 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %10082 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10083 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10084 = torch.aten.view %10065, %10083 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10085 = torch_c.to_builtin_tensor %10084 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10086 = torch_c.to_builtin_tensor %10082 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10087 = tensor.empty() : tensor<2048x1280xf32>
    %10088 = linalg.fill ins(%cst : f32) outs(%10087 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10089 = tensor.empty() : tensor<2048x1280xf32>
    %10090 = linalg.fill ins(%cst : f32) outs(%10089 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10091:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10088, %10090, %10085, %10086, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10088, %10090)
    %10092 = arith.truncf %10091#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10093 = torch_c.from_builtin_tensor %10092 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10094 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10095 = torch.aten.view %10093, %10094 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %10096 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10097 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10098 = torch.aten.view %10065, %10097 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10099 = torch_c.to_builtin_tensor %10098 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10100 = torch_c.to_builtin_tensor %10096 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10101 = tensor.empty() : tensor<2048x1280xf32>
    %10102 = linalg.fill ins(%cst : f32) outs(%10101 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10103 = tensor.empty() : tensor<2048x1280xf32>
    %10104 = linalg.fill ins(%cst : f32) outs(%10103 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10105:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10102, %10104, %10099, %10100, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10102, %10104)
    %10106 = arith.truncf %10105#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10107 = torch_c.from_builtin_tensor %10106 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10108 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10109 = torch.aten.view %10107, %10108 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10110 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10111 = torch.aten.view %10081, %10110 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10112 = torch.aten.transpose.int %10111, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10113 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10114 = torch.aten.view %10095, %10113 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10115 = torch.aten.transpose.int %10114, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10116 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10117 = torch.aten.view %10109, %10116 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10118 = torch.aten.transpose.int %10117, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10119:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10112, %10115, %10118, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10120 = torch.aten.transpose.int %10119#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10121 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10122 = torch.aten.view %10120, %10121 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10123 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10124 = torch.aten.view %10122, %10123 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10125 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10126 = torch.aten.transpose.int %10125, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %10127 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10128 = torch.prims.convert_element_type %10127, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10129 = torch.prims.convert_element_type %10124, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10130 = torch.prims.convert_element_type %10126, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10131 = torch.aten.mm %10129, %10130 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10132 = torch.aten.mul.Scalar %10131, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10133 = torch.aten.mul.Scalar %10128, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10134 = torch.aten.add.Tensor %10132, %10133, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10135 = torch.prims.convert_element_type %10134, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10136 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10137 = torch.aten.view %10135, %10136 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10138 = torch.aten.div.Scalar %10137, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10139 = torch.aten.add.Tensor %10138, %10054, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10140 = torch.prims.convert_element_type %10139, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10141 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_307, %result1_308 = torch.aten.var_mean.correction %10140, %10141, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10142 = torch.aten.add.Scalar %result0_307, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10143 = torch.aten.rsqrt %10142 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10144 = torch.aten.sub.Tensor %10139, %result1_308, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10145 = torch.aten.mul.Tensor %10144, %10143 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %10146 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10147 = torch.aten.mul.Tensor %10145, %10146 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %10148 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10149 = torch.aten.add.Tensor %10147, %10148, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10150 = torch.prims.convert_element_type %10149, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10151 = torch.prims.convert_element_type %result1_308, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10152 = torch.prims.convert_element_type %10143, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %10153 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10154 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10155 = torch.aten.view %10150, %10154 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10156 = torch_c.to_builtin_tensor %10155 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10157 = torch_c.to_builtin_tensor %10153 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10158 = tensor.empty() : tensor<2048x1280xf32>
    %10159 = linalg.fill ins(%cst : f32) outs(%10158 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10160 = tensor.empty() : tensor<2048x1280xf32>
    %10161 = linalg.fill ins(%cst : f32) outs(%10160 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10162:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10159, %10161, %10156, %10157, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10159, %10161)
    %10163 = arith.truncf %10162#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10164 = torch_c.from_builtin_tensor %10163 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10165 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10166 = torch.aten.view %10164, %10165 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %10167 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10168 = torch.aten.transpose.int %10167, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10169 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10170 = torch.aten.view %4, %10169 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10171 = torch.aten.mm %10170, %10168 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10172 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10173 = torch.aten.view %10171, %10172 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %10174 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10175 = torch.aten.transpose.int %10174, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10176 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10177 = torch.aten.view %4, %10176 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10178 = torch.aten.mm %10177, %10175 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10179 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10180 = torch.aten.view %10178, %10179 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10181 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10182 = torch.aten.view %10166, %10181 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10183 = torch.aten.transpose.int %10182, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10184 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10185 = torch.aten.view %10173, %10184 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10186 = torch.aten.transpose.int %10185, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10187 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10188 = torch.aten.view %10180, %10187 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10189 = torch.aten.transpose.int %10188, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10190:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10183, %10186, %10189, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10191 = torch.aten.transpose.int %10190#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10192 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10193 = torch.aten.view %10191, %10192 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10194 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10195 = torch.aten.view %10193, %10194 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10196 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10197 = torch.aten.transpose.int %10196, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %10198 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10199 = torch.prims.convert_element_type %10198, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10200 = torch.prims.convert_element_type %10195, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10201 = torch.prims.convert_element_type %10197, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10202 = torch.aten.mm %10200, %10201 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10203 = torch.aten.mul.Scalar %10202, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10204 = torch.aten.mul.Scalar %10199, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10205 = torch.aten.add.Tensor %10203, %10204, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10206 = torch.prims.convert_element_type %10205, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10207 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10208 = torch.aten.view %10206, %10207 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10209 = torch.aten.div.Scalar %10208, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10210 = torch.aten.add.Tensor %10209, %10139, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10211 = torch.prims.convert_element_type %10210, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10212 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_309, %result1_310 = torch.aten.var_mean.correction %10211, %10212, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10213 = torch.aten.add.Scalar %result0_309, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10214 = torch.aten.rsqrt %10213 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10215 = torch.aten.sub.Tensor %10210, %result1_310, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10216 = torch.aten.mul.Tensor %10215, %10214 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %10217 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10218 = torch.aten.mul.Tensor %10216, %10217 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %10219 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10220 = torch.aten.add.Tensor %10218, %10219, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10221 = torch.prims.convert_element_type %10220, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10222 = torch.prims.convert_element_type %result1_310, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10223 = torch.prims.convert_element_type %10214, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10224 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10225 = torch.aten.view %10221, %10224 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10226 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10227 = torch.aten.transpose.int %10226, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %10228 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10229 = torch.prims.convert_element_type %10228, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10230 = torch.prims.convert_element_type %10225, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10231 = torch.prims.convert_element_type %10227, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10232 = torch.aten.mm %10230, %10231 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10233 = torch.aten.mul.Scalar %10232, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10234 = torch.aten.mul.Scalar %10229, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10235 = torch.aten.add.Tensor %10233, %10234, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10236 = torch.prims.convert_element_type %10235, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10237 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10238 = torch.aten.view %10236, %10237 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10239 = torch.aten.slice.Tensor %10238, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10240 = torch.aten.slice.Tensor %10238, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10241 = torch.aten.gelu %10240, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10242 = torch.aten.mul.Tensor %10239, %10241 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10243 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10244 = torch.aten.view %10242, %10243 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %10245 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10246 = torch.aten.transpose.int %10245, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %10247 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10248 = torch.prims.convert_element_type %10247, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10249 = torch.prims.convert_element_type %10244, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10250 = torch.prims.convert_element_type %10246, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10251 = torch.aten.mm %10249, %10250 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10252 = torch.aten.mul.Scalar %10251, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10253 = torch.aten.mul.Scalar %10248, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10254 = torch.aten.add.Tensor %10252, %10253, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10255 = torch.prims.convert_element_type %10254, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10256 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10257 = torch.aten.view %10255, %10256 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10258 = torch.aten.add.Tensor %10257, %10210, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10259 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10260 = torch.aten.view %10258, %10259 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_out.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %10261 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10262 = torch.aten.transpose.int %10261, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_out.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_out.bias : tensor<1280xf16>
    %10263 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10264 = torch.prims.convert_element_type %10263, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10265 = torch.prims.convert_element_type %10260, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10266 = torch.prims.convert_element_type %10262, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10267 = torch.aten.mm %10265, %10266 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10268 = torch.aten.mul.Scalar %10267, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10269 = torch.aten.mul.Scalar %10264, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10270 = torch.aten.add.Tensor %10268, %10269, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10271 = torch.prims.convert_element_type %10270, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10272 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10273 = torch.aten.view %10271, %10272 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10274 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10275 = torch.aten.view %10273, %10274 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %10276 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10277 = torch.aten.permute %10275, %10276 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %10278 = torch.aten.add.Tensor %10277, %8167, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10279 = torch.prim.ListConstruct %10278, %3556 : (!torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>) -> !torch.list<vtensor>
    %10280 = torch.aten.cat %10279, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %10281 = torch.prim.ListConstruct %int2, %int32, %int80, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10282 = torch.aten.view %10280, %10281 : !torch.vtensor<[2,2560,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,80,1024],f16>
    %10283 = torch.prims.convert_element_type %10282, %int6 : !torch.vtensor<[2,32,80,1024],f16>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %10284 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_311, %result1_312 = torch.aten.var_mean.correction %10283, %10284, %int0, %true : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %10285 = torch.aten.add.Scalar %result0_311, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %10286 = torch.aten.rsqrt %10285 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %10287 = torch.aten.sub.Tensor %10282, %result1_312, %int1 : !torch.vtensor<[2,32,80,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %10288 = torch.aten.mul.Tensor %10287, %10286 : !torch.vtensor<[2,32,80,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,80,1024],f32>
    %10289 = torch.prim.ListConstruct %int2, %int2560, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10290 = torch.aten.view %10288, %10289 : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,2560,32,32],f32>
    %_params.unet.up_blocks.0.resnets.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.norm1.bias : tensor<2560xf16>
    %10291 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm1.bias : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %10292 = torch.aten.unsqueeze %10291, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %10293 = torch.aten.unsqueeze %10292, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %10294 = torch.aten.unsqueeze %10293, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.norm1.weight : tensor<2560xf16>
    %10295 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm1.weight : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %10296 = torch.aten.unsqueeze %10295, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %10297 = torch.aten.unsqueeze %10296, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %10298 = torch.aten.unsqueeze %10297, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %10299 = torch.aten.mul.Tensor %10290, %10298 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16> -> !torch.vtensor<[2,2560,32,32],f32>
    %10300 = torch.aten.add.Tensor %10299, %10294, %int1 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16>, !torch.int -> !torch.vtensor<[2,2560,32,32],f32>
    %10301 = torch.prims.convert_element_type %10300, %int5 : !torch.vtensor<[2,2560,32,32],f32>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %10302 = torch.prims.convert_element_type %result1_312, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10303 = torch.prims.convert_element_type %10286, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10304 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10305 = torch.prims.squeeze %10302, %10304 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10306 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10307 = torch.prims.squeeze %10305, %10306 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10308 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10309 = torch.prims.squeeze %10303, %10308 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10310 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10311 = torch.prims.squeeze %10309, %10310 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10312 = torch.aten.silu %10301 : !torch.vtensor<[2,2560,32,32],f16> -> !torch.vtensor<[2,2560,32,32],f16>
    %_params.unet.up_blocks.0.resnets.1.conv1.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.conv1.weight : tensor<1280x2560x3x3xf16>
    %10313 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv1.weight : tensor<1280x2560x3x3xf16> -> !torch.vtensor<[1280,2560,3,3],f16>
    %_params.unet.up_blocks.0.resnets.1.conv1.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.conv1.bias : tensor<1280xf16>
    %10314 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10315 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10316 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10317 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10318 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10319 = torch.aten.convolution %10312, %10313, %10314, %10315, %10316, %10317, %false, %10318, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10320 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %10321 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10322 = torch.aten.transpose.int %10321, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %10323 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10324 = torch.prims.convert_element_type %10323, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10325 = torch.prims.convert_element_type %10320, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %10326 = torch.prims.convert_element_type %10322, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10327 = torch.aten.mm %10325, %10326 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %10328 = torch.aten.mul.Scalar %10327, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %10329 = torch.aten.mul.Scalar %10324, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10330 = torch.aten.add.Tensor %10328, %10329, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %10331 = torch.prims.convert_element_type %10330, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %10332 = torch.aten.unsqueeze %10331, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %10333 = torch.aten.unsqueeze %10332, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %10334 = torch.aten.add.Tensor %10319, %10333, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10335 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10336 = torch.aten.view %10334, %10335 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %10337 = torch.prims.convert_element_type %10336, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10338 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_313, %result1_314 = torch.aten.var_mean.correction %10337, %10338, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %10339 = torch.aten.add.Scalar %result0_313, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %10340 = torch.aten.rsqrt %10339 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %10341 = torch.aten.sub.Tensor %10336, %result1_314, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10342 = torch.aten.mul.Tensor %10341, %10340 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %10343 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10344 = torch.aten.view %10342, %10343 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.resnets.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.norm2.bias : tensor<1280xf16>
    %10345 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10346 = torch.aten.unsqueeze %10345, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10347 = torch.aten.unsqueeze %10346, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10348 = torch.aten.unsqueeze %10347, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.resnets.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.norm2.weight : tensor<1280xf16>
    %10349 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10350 = torch.aten.unsqueeze %10349, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10351 = torch.aten.unsqueeze %10350, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10352 = torch.aten.unsqueeze %10351, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %10353 = torch.aten.mul.Tensor %10344, %10352 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %10354 = torch.aten.add.Tensor %10353, %10348, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %10355 = torch.prims.convert_element_type %10354, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10356 = torch.prims.convert_element_type %result1_314, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10357 = torch.prims.convert_element_type %10340, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10358 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10359 = torch.prims.squeeze %10356, %10358 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10360 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10361 = torch.prims.squeeze %10359, %10360 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10362 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10363 = torch.prims.squeeze %10357, %10362 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10364 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10365 = torch.prims.squeeze %10363, %10364 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10366 = torch.aten.silu %10355 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.1.conv2.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %10367 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.resnets.1.conv2.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.conv2.bias : tensor<1280xf16>
    %10368 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10369 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10370 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10371 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10372 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10373 = torch.aten.convolution %10366, %10367, %10368, %10369, %10370, %10371, %false, %10372, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight : tensor<1280x2560x1x1xf16>
    %10374 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight : tensor<1280x2560x1x1xf16> -> !torch.vtensor<[1280,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias : tensor<1280xf16>
    %10375 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10376 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10377 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10378 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10379 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10380 = torch.aten.convolution %10280, %10374, %10375, %10376, %10377, %10378, %false, %10379, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10381 = torch.aten.add.Tensor %10380, %10373, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10382 = torch.aten.div.Scalar %10381, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %10383 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10384 = torch.aten.view %10382, %10383 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %10385 = torch.prims.convert_element_type %10384, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10386 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_315, %result1_316 = torch.aten.var_mean.correction %10385, %10386, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %10387 = torch.aten.add.Scalar %result0_315, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %10388 = torch.aten.rsqrt %10387 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %10389 = torch.aten.sub.Tensor %10384, %result1_316, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10390 = torch.aten.mul.Tensor %10389, %10388 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %10391 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10392 = torch.aten.view %10390, %10391 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.attentions.1.norm.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.norm.bias : tensor<1280xf16>
    %10393 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10394 = torch.aten.unsqueeze %10393, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10395 = torch.aten.unsqueeze %10394, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10396 = torch.aten.unsqueeze %10395, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.attentions.1.norm.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.norm.weight : tensor<1280xf16>
    %10397 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10398 = torch.aten.unsqueeze %10397, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10399 = torch.aten.unsqueeze %10398, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10400 = torch.aten.unsqueeze %10399, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %10401 = torch.aten.mul.Tensor %10392, %10400 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %10402 = torch.aten.add.Tensor %10401, %10396, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %10403 = torch.prims.convert_element_type %10402, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10404 = torch.prims.convert_element_type %result1_316, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10405 = torch.prims.convert_element_type %10388, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10406 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10407 = torch.prims.squeeze %10404, %10406 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10408 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10409 = torch.prims.squeeze %10407, %10408 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10410 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10411 = torch.prims.squeeze %10405, %10410 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10412 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10413 = torch.prims.squeeze %10411, %10412 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10414 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10415 = torch.aten.permute %10403, %10414 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %10416 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10417 = torch.aten.view %10415, %10416 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_in.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_in.weight : tensor<1280x1280xf16>
    %10418 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10419 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10420 = torch.aten._unsafe_view %10417, %10419 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10421 = torch_c.to_builtin_tensor %10420 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10422 = torch_c.to_builtin_tensor %10418 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10423 = tensor.empty() : tensor<2048x1280xf32>
    %10424 = linalg.fill ins(%cst : f32) outs(%10423 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10425 = tensor.empty() : tensor<2048x1280xf32>
    %10426 = linalg.fill ins(%cst : f32) outs(%10425 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10427:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10424, %10426, %10421, %10422, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10424, %10426)
    %10428 = arith.truncf %10427#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10429 = torch_c.from_builtin_tensor %10428 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10430 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10431 = torch.aten.view %10429, %10430 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_in.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_in.bias : tensor<1280xf16>
    %10432 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10433 = torch.aten.add.Tensor %10431, %10432, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10434 = torch.prims.convert_element_type %10433, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10435 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_317, %result1_318 = torch.aten.var_mean.correction %10434, %10435, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10436 = torch.aten.add.Scalar %result0_317, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10437 = torch.aten.rsqrt %10436 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10438 = torch.aten.sub.Tensor %10433, %result1_318, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10439 = torch.aten.mul.Tensor %10438, %10437 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %10440 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10441 = torch.aten.mul.Tensor %10439, %10440 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %10442 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10443 = torch.aten.add.Tensor %10441, %10442, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10444 = torch.prims.convert_element_type %10443, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10445 = torch.prims.convert_element_type %result1_318, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10446 = torch.prims.convert_element_type %10437, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %10447 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10448 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10449 = torch.aten.view %10444, %10448 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10450 = torch_c.to_builtin_tensor %10449 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10451 = torch_c.to_builtin_tensor %10447 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10452 = tensor.empty() : tensor<2048x1280xf32>
    %10453 = linalg.fill ins(%cst : f32) outs(%10452 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10454 = tensor.empty() : tensor<2048x1280xf32>
    %10455 = linalg.fill ins(%cst : f32) outs(%10454 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10456:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10453, %10455, %10450, %10451, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10453, %10455)
    %10457 = arith.truncf %10456#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10458 = torch_c.from_builtin_tensor %10457 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10459 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10460 = torch.aten.view %10458, %10459 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %10461 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10462 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10463 = torch.aten.view %10444, %10462 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10464 = torch_c.to_builtin_tensor %10463 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10465 = torch_c.to_builtin_tensor %10461 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10466 = tensor.empty() : tensor<2048x1280xf32>
    %10467 = linalg.fill ins(%cst : f32) outs(%10466 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10468 = tensor.empty() : tensor<2048x1280xf32>
    %10469 = linalg.fill ins(%cst : f32) outs(%10468 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10470:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10467, %10469, %10464, %10465, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10467, %10469)
    %10471 = arith.truncf %10470#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10472 = torch_c.from_builtin_tensor %10471 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10473 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10474 = torch.aten.view %10472, %10473 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %10475 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10476 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10477 = torch.aten.view %10444, %10476 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10478 = torch_c.to_builtin_tensor %10477 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10479 = torch_c.to_builtin_tensor %10475 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10480 = tensor.empty() : tensor<2048x1280xf32>
    %10481 = linalg.fill ins(%cst : f32) outs(%10480 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10482 = tensor.empty() : tensor<2048x1280xf32>
    %10483 = linalg.fill ins(%cst : f32) outs(%10482 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10484:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10481, %10483, %10478, %10479, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10481, %10483)
    %10485 = arith.truncf %10484#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10486 = torch_c.from_builtin_tensor %10485 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10487 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10488 = torch.aten.view %10486, %10487 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10489 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10490 = torch.aten.view %10460, %10489 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10491 = torch.aten.transpose.int %10490, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10492 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10493 = torch.aten.view %10474, %10492 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10494 = torch.aten.transpose.int %10493, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10495 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10496 = torch.aten.view %10488, %10495 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10497 = torch.aten.transpose.int %10496, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10498:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10491, %10494, %10497, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10499 = torch.aten.transpose.int %10498#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10500 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10501 = torch.aten.view %10499, %10500 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10502 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10503 = torch.aten.view %10501, %10502 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10504 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10505 = torch.aten.transpose.int %10504, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %10506 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10507 = torch.prims.convert_element_type %10506, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10508 = torch.prims.convert_element_type %10503, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10509 = torch.prims.convert_element_type %10505, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10510 = torch.aten.mm %10508, %10509 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10511 = torch.aten.mul.Scalar %10510, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10512 = torch.aten.mul.Scalar %10507, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10513 = torch.aten.add.Tensor %10511, %10512, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10514 = torch.prims.convert_element_type %10513, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10515 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10516 = torch.aten.view %10514, %10515 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10517 = torch.aten.div.Scalar %10516, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10518 = torch.aten.add.Tensor %10517, %10433, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10519 = torch.prims.convert_element_type %10518, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10520 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_319, %result1_320 = torch.aten.var_mean.correction %10519, %10520, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10521 = torch.aten.add.Scalar %result0_319, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10522 = torch.aten.rsqrt %10521 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10523 = torch.aten.sub.Tensor %10518, %result1_320, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10524 = torch.aten.mul.Tensor %10523, %10522 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %10525 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10526 = torch.aten.mul.Tensor %10524, %10525 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %10527 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10528 = torch.aten.add.Tensor %10526, %10527, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10529 = torch.prims.convert_element_type %10528, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10530 = torch.prims.convert_element_type %result1_320, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10531 = torch.prims.convert_element_type %10522, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %10532 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10533 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10534 = torch.aten.view %10529, %10533 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10535 = torch_c.to_builtin_tensor %10534 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10536 = torch_c.to_builtin_tensor %10532 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10537 = tensor.empty() : tensor<2048x1280xf32>
    %10538 = linalg.fill ins(%cst : f32) outs(%10537 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10539 = tensor.empty() : tensor<2048x1280xf32>
    %10540 = linalg.fill ins(%cst : f32) outs(%10539 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10541:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10538, %10540, %10535, %10536, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10538, %10540)
    %10542 = arith.truncf %10541#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10543 = torch_c.from_builtin_tensor %10542 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10544 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10545 = torch.aten.view %10543, %10544 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %10546 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10547 = torch.aten.transpose.int %10546, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10548 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10549 = torch.aten.view %4, %10548 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10550 = torch.aten.mm %10549, %10547 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10551 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10552 = torch.aten.view %10550, %10551 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %10553 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10554 = torch.aten.transpose.int %10553, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10555 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10556 = torch.aten.view %4, %10555 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10557 = torch.aten.mm %10556, %10554 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10558 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10559 = torch.aten.view %10557, %10558 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10560 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10561 = torch.aten.view %10545, %10560 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10562 = torch.aten.transpose.int %10561, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10563 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10564 = torch.aten.view %10552, %10563 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10565 = torch.aten.transpose.int %10564, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10566 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10567 = torch.aten.view %10559, %10566 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10568 = torch.aten.transpose.int %10567, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10569:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10562, %10565, %10568, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10570 = torch.aten.transpose.int %10569#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10571 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10572 = torch.aten.view %10570, %10571 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10573 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10574 = torch.aten.view %10572, %10573 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10575 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10576 = torch.aten.transpose.int %10575, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %10577 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10578 = torch.prims.convert_element_type %10577, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10579 = torch.prims.convert_element_type %10574, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10580 = torch.prims.convert_element_type %10576, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10581 = torch.aten.mm %10579, %10580 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10582 = torch.aten.mul.Scalar %10581, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10583 = torch.aten.mul.Scalar %10578, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10584 = torch.aten.add.Tensor %10582, %10583, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10585 = torch.prims.convert_element_type %10584, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10586 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10587 = torch.aten.view %10585, %10586 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10588 = torch.aten.div.Scalar %10587, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10589 = torch.aten.add.Tensor %10588, %10518, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10590 = torch.prims.convert_element_type %10589, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10591 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_321, %result1_322 = torch.aten.var_mean.correction %10590, %10591, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10592 = torch.aten.add.Scalar %result0_321, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10593 = torch.aten.rsqrt %10592 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10594 = torch.aten.sub.Tensor %10589, %result1_322, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10595 = torch.aten.mul.Tensor %10594, %10593 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %10596 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10597 = torch.aten.mul.Tensor %10595, %10596 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %10598 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10599 = torch.aten.add.Tensor %10597, %10598, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10600 = torch.prims.convert_element_type %10599, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10601 = torch.prims.convert_element_type %result1_322, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10602 = torch.prims.convert_element_type %10593, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10603 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10604 = torch.aten.view %10600, %10603 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10605 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10606 = torch.aten.transpose.int %10605, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %10607 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10608 = torch.prims.convert_element_type %10607, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10609 = torch.prims.convert_element_type %10604, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10610 = torch.prims.convert_element_type %10606, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10611 = torch.aten.mm %10609, %10610 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10612 = torch.aten.mul.Scalar %10611, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10613 = torch.aten.mul.Scalar %10608, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10614 = torch.aten.add.Tensor %10612, %10613, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10615 = torch.prims.convert_element_type %10614, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10616 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10617 = torch.aten.view %10615, %10616 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10618 = torch.aten.slice.Tensor %10617, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10619 = torch.aten.slice.Tensor %10617, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10620 = torch.aten.gelu %10619, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10621 = torch.aten.mul.Tensor %10618, %10620 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10622 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10623 = torch.aten.view %10621, %10622 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %10624 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10625 = torch.aten.transpose.int %10624, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %10626 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10627 = torch.prims.convert_element_type %10626, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10628 = torch.prims.convert_element_type %10623, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10629 = torch.prims.convert_element_type %10625, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10630 = torch.aten.mm %10628, %10629 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10631 = torch.aten.mul.Scalar %10630, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10632 = torch.aten.mul.Scalar %10627, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10633 = torch.aten.add.Tensor %10631, %10632, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10634 = torch.prims.convert_element_type %10633, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10635 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10636 = torch.aten.view %10634, %10635 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10637 = torch.aten.add.Tensor %10636, %10589, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10638 = torch.prims.convert_element_type %10637, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10639 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_323, %result1_324 = torch.aten.var_mean.correction %10638, %10639, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10640 = torch.aten.add.Scalar %result0_323, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10641 = torch.aten.rsqrt %10640 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10642 = torch.aten.sub.Tensor %10637, %result1_324, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10643 = torch.aten.mul.Tensor %10642, %10641 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %10644 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10645 = torch.aten.mul.Tensor %10643, %10644 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %10646 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10647 = torch.aten.add.Tensor %10645, %10646, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10648 = torch.prims.convert_element_type %10647, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10649 = torch.prims.convert_element_type %result1_324, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10650 = torch.prims.convert_element_type %10641, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %10651 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10652 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10653 = torch.aten.view %10648, %10652 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10654 = torch_c.to_builtin_tensor %10653 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10655 = torch_c.to_builtin_tensor %10651 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10656 = tensor.empty() : tensor<2048x1280xf32>
    %10657 = linalg.fill ins(%cst : f32) outs(%10656 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10658 = tensor.empty() : tensor<2048x1280xf32>
    %10659 = linalg.fill ins(%cst : f32) outs(%10658 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10660:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10657, %10659, %10654, %10655, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10657, %10659)
    %10661 = arith.truncf %10660#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10662 = torch_c.from_builtin_tensor %10661 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10663 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10664 = torch.aten.view %10662, %10663 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %10665 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10666 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10667 = torch.aten.view %10648, %10666 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10668 = torch_c.to_builtin_tensor %10667 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10669 = torch_c.to_builtin_tensor %10665 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10670 = tensor.empty() : tensor<2048x1280xf32>
    %10671 = linalg.fill ins(%cst : f32) outs(%10670 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10672 = tensor.empty() : tensor<2048x1280xf32>
    %10673 = linalg.fill ins(%cst : f32) outs(%10672 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10674:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10671, %10673, %10668, %10669, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10671, %10673)
    %10675 = arith.truncf %10674#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10676 = torch_c.from_builtin_tensor %10675 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10677 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10678 = torch.aten.view %10676, %10677 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %10679 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10680 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10681 = torch.aten.view %10648, %10680 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10682 = torch_c.to_builtin_tensor %10681 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10683 = torch_c.to_builtin_tensor %10679 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10684 = tensor.empty() : tensor<2048x1280xf32>
    %10685 = linalg.fill ins(%cst : f32) outs(%10684 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10686 = tensor.empty() : tensor<2048x1280xf32>
    %10687 = linalg.fill ins(%cst : f32) outs(%10686 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10688:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10685, %10687, %10682, %10683, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10685, %10687)
    %10689 = arith.truncf %10688#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10690 = torch_c.from_builtin_tensor %10689 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10691 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10692 = torch.aten.view %10690, %10691 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10693 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10694 = torch.aten.view %10664, %10693 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10695 = torch.aten.transpose.int %10694, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10696 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10697 = torch.aten.view %10678, %10696 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10698 = torch.aten.transpose.int %10697, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10699 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10700 = torch.aten.view %10692, %10699 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10701 = torch.aten.transpose.int %10700, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10702:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10695, %10698, %10701, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10703 = torch.aten.transpose.int %10702#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10704 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10705 = torch.aten.view %10703, %10704 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10706 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10707 = torch.aten.view %10705, %10706 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10708 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10709 = torch.aten.transpose.int %10708, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %10710 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10711 = torch.prims.convert_element_type %10710, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10712 = torch.prims.convert_element_type %10707, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10713 = torch.prims.convert_element_type %10709, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10714 = torch.aten.mm %10712, %10713 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10715 = torch.aten.mul.Scalar %10714, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10716 = torch.aten.mul.Scalar %10711, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10717 = torch.aten.add.Tensor %10715, %10716, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10718 = torch.prims.convert_element_type %10717, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10719 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10720 = torch.aten.view %10718, %10719 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10721 = torch.aten.div.Scalar %10720, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10722 = torch.aten.add.Tensor %10721, %10637, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10723 = torch.prims.convert_element_type %10722, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10724 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_325, %result1_326 = torch.aten.var_mean.correction %10723, %10724, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10725 = torch.aten.add.Scalar %result0_325, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10726 = torch.aten.rsqrt %10725 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10727 = torch.aten.sub.Tensor %10722, %result1_326, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10728 = torch.aten.mul.Tensor %10727, %10726 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %10729 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10730 = torch.aten.mul.Tensor %10728, %10729 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %10731 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10732 = torch.aten.add.Tensor %10730, %10731, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10733 = torch.prims.convert_element_type %10732, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10734 = torch.prims.convert_element_type %result1_326, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10735 = torch.prims.convert_element_type %10726, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %10736 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10737 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10738 = torch.aten.view %10733, %10737 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10739 = torch_c.to_builtin_tensor %10738 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10740 = torch_c.to_builtin_tensor %10736 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10741 = tensor.empty() : tensor<2048x1280xf32>
    %10742 = linalg.fill ins(%cst : f32) outs(%10741 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10743 = tensor.empty() : tensor<2048x1280xf32>
    %10744 = linalg.fill ins(%cst : f32) outs(%10743 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10745:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10742, %10744, %10739, %10740, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10742, %10744)
    %10746 = arith.truncf %10745#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10747 = torch_c.from_builtin_tensor %10746 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10748 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10749 = torch.aten.view %10747, %10748 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %10750 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10751 = torch.aten.transpose.int %10750, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10752 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10753 = torch.aten.view %4, %10752 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10754 = torch.aten.mm %10753, %10751 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10755 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10756 = torch.aten.view %10754, %10755 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %10757 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10758 = torch.aten.transpose.int %10757, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10759 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10760 = torch.aten.view %4, %10759 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10761 = torch.aten.mm %10760, %10758 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10762 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10763 = torch.aten.view %10761, %10762 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10764 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10765 = torch.aten.view %10749, %10764 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10766 = torch.aten.transpose.int %10765, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10767 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10768 = torch.aten.view %10756, %10767 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10769 = torch.aten.transpose.int %10768, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10770 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10771 = torch.aten.view %10763, %10770 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10772 = torch.aten.transpose.int %10771, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10773:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10766, %10769, %10772, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10774 = torch.aten.transpose.int %10773#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10775 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10776 = torch.aten.view %10774, %10775 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10777 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10778 = torch.aten.view %10776, %10777 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10779 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10780 = torch.aten.transpose.int %10779, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %10781 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10782 = torch.prims.convert_element_type %10781, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10783 = torch.prims.convert_element_type %10778, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10784 = torch.prims.convert_element_type %10780, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10785 = torch.aten.mm %10783, %10784 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10786 = torch.aten.mul.Scalar %10785, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10787 = torch.aten.mul.Scalar %10782, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10788 = torch.aten.add.Tensor %10786, %10787, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10789 = torch.prims.convert_element_type %10788, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10790 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10791 = torch.aten.view %10789, %10790 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10792 = torch.aten.div.Scalar %10791, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10793 = torch.aten.add.Tensor %10792, %10722, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10794 = torch.prims.convert_element_type %10793, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10795 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_327, %result1_328 = torch.aten.var_mean.correction %10794, %10795, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10796 = torch.aten.add.Scalar %result0_327, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10797 = torch.aten.rsqrt %10796 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10798 = torch.aten.sub.Tensor %10793, %result1_328, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10799 = torch.aten.mul.Tensor %10798, %10797 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %10800 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10801 = torch.aten.mul.Tensor %10799, %10800 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %10802 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10803 = torch.aten.add.Tensor %10801, %10802, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10804 = torch.prims.convert_element_type %10803, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10805 = torch.prims.convert_element_type %result1_328, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10806 = torch.prims.convert_element_type %10797, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10807 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10808 = torch.aten.view %10804, %10807 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10809 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10810 = torch.aten.transpose.int %10809, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %10811 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10812 = torch.prims.convert_element_type %10811, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10813 = torch.prims.convert_element_type %10808, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10814 = torch.prims.convert_element_type %10810, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10815 = torch.aten.mm %10813, %10814 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10816 = torch.aten.mul.Scalar %10815, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10817 = torch.aten.mul.Scalar %10812, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10818 = torch.aten.add.Tensor %10816, %10817, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10819 = torch.prims.convert_element_type %10818, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10820 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10821 = torch.aten.view %10819, %10820 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10822 = torch.aten.slice.Tensor %10821, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10823 = torch.aten.slice.Tensor %10821, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10824 = torch.aten.gelu %10823, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10825 = torch.aten.mul.Tensor %10822, %10824 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10826 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10827 = torch.aten.view %10825, %10826 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %10828 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10829 = torch.aten.transpose.int %10828, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %10830 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10831 = torch.prims.convert_element_type %10830, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10832 = torch.prims.convert_element_type %10827, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10833 = torch.prims.convert_element_type %10829, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10834 = torch.aten.mm %10832, %10833 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10835 = torch.aten.mul.Scalar %10834, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10836 = torch.aten.mul.Scalar %10831, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10837 = torch.aten.add.Tensor %10835, %10836, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10838 = torch.prims.convert_element_type %10837, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10839 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10840 = torch.aten.view %10838, %10839 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10841 = torch.aten.add.Tensor %10840, %10793, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10842 = torch.prims.convert_element_type %10841, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10843 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_329, %result1_330 = torch.aten.var_mean.correction %10842, %10843, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10844 = torch.aten.add.Scalar %result0_329, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10845 = torch.aten.rsqrt %10844 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10846 = torch.aten.sub.Tensor %10841, %result1_330, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10847 = torch.aten.mul.Tensor %10846, %10845 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %10848 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10849 = torch.aten.mul.Tensor %10847, %10848 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %10850 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10851 = torch.aten.add.Tensor %10849, %10850, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10852 = torch.prims.convert_element_type %10851, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10853 = torch.prims.convert_element_type %result1_330, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10854 = torch.prims.convert_element_type %10845, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %10855 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10856 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10857 = torch.aten.view %10852, %10856 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10858 = torch_c.to_builtin_tensor %10857 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10859 = torch_c.to_builtin_tensor %10855 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10860 = tensor.empty() : tensor<2048x1280xf32>
    %10861 = linalg.fill ins(%cst : f32) outs(%10860 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10862 = tensor.empty() : tensor<2048x1280xf32>
    %10863 = linalg.fill ins(%cst : f32) outs(%10862 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10864:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10861, %10863, %10858, %10859, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10861, %10863)
    %10865 = arith.truncf %10864#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10866 = torch_c.from_builtin_tensor %10865 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10867 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10868 = torch.aten.view %10866, %10867 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %10869 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10870 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10871 = torch.aten.view %10852, %10870 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10872 = torch_c.to_builtin_tensor %10871 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10873 = torch_c.to_builtin_tensor %10869 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10874 = tensor.empty() : tensor<2048x1280xf32>
    %10875 = linalg.fill ins(%cst : f32) outs(%10874 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10876 = tensor.empty() : tensor<2048x1280xf32>
    %10877 = linalg.fill ins(%cst : f32) outs(%10876 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10878:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10875, %10877, %10872, %10873, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10875, %10877)
    %10879 = arith.truncf %10878#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10880 = torch_c.from_builtin_tensor %10879 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10881 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10882 = torch.aten.view %10880, %10881 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %10883 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10884 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10885 = torch.aten.view %10852, %10884 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10886 = torch_c.to_builtin_tensor %10885 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10887 = torch_c.to_builtin_tensor %10883 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10888 = tensor.empty() : tensor<2048x1280xf32>
    %10889 = linalg.fill ins(%cst : f32) outs(%10888 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10890 = tensor.empty() : tensor<2048x1280xf32>
    %10891 = linalg.fill ins(%cst : f32) outs(%10890 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10892:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10889, %10891, %10886, %10887, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10889, %10891)
    %10893 = arith.truncf %10892#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10894 = torch_c.from_builtin_tensor %10893 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10895 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10896 = torch.aten.view %10894, %10895 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10897 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10898 = torch.aten.view %10868, %10897 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10899 = torch.aten.transpose.int %10898, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10900 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10901 = torch.aten.view %10882, %10900 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10902 = torch.aten.transpose.int %10901, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10903 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10904 = torch.aten.view %10896, %10903 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10905 = torch.aten.transpose.int %10904, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10906:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10899, %10902, %10905, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10907 = torch.aten.transpose.int %10906#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10908 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10909 = torch.aten.view %10907, %10908 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10910 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10911 = torch.aten.view %10909, %10910 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10912 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10913 = torch.aten.transpose.int %10912, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %10914 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10915 = torch.prims.convert_element_type %10914, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10916 = torch.prims.convert_element_type %10911, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10917 = torch.prims.convert_element_type %10913, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10918 = torch.aten.mm %10916, %10917 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10919 = torch.aten.mul.Scalar %10918, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10920 = torch.aten.mul.Scalar %10915, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10921 = torch.aten.add.Tensor %10919, %10920, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10922 = torch.prims.convert_element_type %10921, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10923 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10924 = torch.aten.view %10922, %10923 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10925 = torch.aten.div.Scalar %10924, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10926 = torch.aten.add.Tensor %10925, %10841, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10927 = torch.prims.convert_element_type %10926, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10928 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_331, %result1_332 = torch.aten.var_mean.correction %10927, %10928, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10929 = torch.aten.add.Scalar %result0_331, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10930 = torch.aten.rsqrt %10929 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10931 = torch.aten.sub.Tensor %10926, %result1_332, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10932 = torch.aten.mul.Tensor %10931, %10930 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %10933 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10934 = torch.aten.mul.Tensor %10932, %10933 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %10935 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10936 = torch.aten.add.Tensor %10934, %10935, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10937 = torch.prims.convert_element_type %10936, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10938 = torch.prims.convert_element_type %result1_332, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10939 = torch.prims.convert_element_type %10930, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %10940 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10941 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10942 = torch.aten.view %10937, %10941 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10943 = torch_c.to_builtin_tensor %10942 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10944 = torch_c.to_builtin_tensor %10940 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10945 = tensor.empty() : tensor<2048x1280xf32>
    %10946 = linalg.fill ins(%cst : f32) outs(%10945 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10947 = tensor.empty() : tensor<2048x1280xf32>
    %10948 = linalg.fill ins(%cst : f32) outs(%10947 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10949:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10946, %10948, %10943, %10944, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10946, %10948)
    %10950 = arith.truncf %10949#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10951 = torch_c.from_builtin_tensor %10950 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10952 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10953 = torch.aten.view %10951, %10952 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %10954 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10955 = torch.aten.transpose.int %10954, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10956 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10957 = torch.aten.view %4, %10956 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10958 = torch.aten.mm %10957, %10955 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10959 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10960 = torch.aten.view %10958, %10959 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %10961 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10962 = torch.aten.transpose.int %10961, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10963 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10964 = torch.aten.view %4, %10963 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10965 = torch.aten.mm %10964, %10962 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %10966 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10967 = torch.aten.view %10965, %10966 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10968 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10969 = torch.aten.view %10953, %10968 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10970 = torch.aten.transpose.int %10969, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10971 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10972 = torch.aten.view %10960, %10971 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10973 = torch.aten.transpose.int %10972, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10974 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10975 = torch.aten.view %10967, %10974 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10976 = torch.aten.transpose.int %10975, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10977:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10970, %10973, %10976, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10978 = torch.aten.transpose.int %10977#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10979 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10980 = torch.aten.view %10978, %10979 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10981 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10982 = torch.aten.view %10980, %10981 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10983 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10984 = torch.aten.transpose.int %10983, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %10985 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10986 = torch.prims.convert_element_type %10985, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10987 = torch.prims.convert_element_type %10982, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10988 = torch.prims.convert_element_type %10984, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10989 = torch.aten.mm %10987, %10988 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10990 = torch.aten.mul.Scalar %10989, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10991 = torch.aten.mul.Scalar %10986, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10992 = torch.aten.add.Tensor %10990, %10991, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10993 = torch.prims.convert_element_type %10992, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10994 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10995 = torch.aten.view %10993, %10994 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10996 = torch.aten.div.Scalar %10995, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10997 = torch.aten.add.Tensor %10996, %10926, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10998 = torch.prims.convert_element_type %10997, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10999 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_333, %result1_334 = torch.aten.var_mean.correction %10998, %10999, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11000 = torch.aten.add.Scalar %result0_333, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11001 = torch.aten.rsqrt %11000 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11002 = torch.aten.sub.Tensor %10997, %result1_334, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11003 = torch.aten.mul.Tensor %11002, %11001 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %11004 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11005 = torch.aten.mul.Tensor %11003, %11004 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %11006 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11007 = torch.aten.add.Tensor %11005, %11006, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11008 = torch.prims.convert_element_type %11007, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11009 = torch.prims.convert_element_type %result1_334, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11010 = torch.prims.convert_element_type %11001, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11011 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11012 = torch.aten.view %11008, %11011 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11013 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11014 = torch.aten.transpose.int %11013, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %11015 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11016 = torch.prims.convert_element_type %11015, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11017 = torch.prims.convert_element_type %11012, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11018 = torch.prims.convert_element_type %11014, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11019 = torch.aten.mm %11017, %11018 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11020 = torch.aten.mul.Scalar %11019, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11021 = torch.aten.mul.Scalar %11016, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11022 = torch.aten.add.Tensor %11020, %11021, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11023 = torch.prims.convert_element_type %11022, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11024 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11025 = torch.aten.view %11023, %11024 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11026 = torch.aten.slice.Tensor %11025, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11027 = torch.aten.slice.Tensor %11025, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11028 = torch.aten.gelu %11027, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11029 = torch.aten.mul.Tensor %11026, %11028 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11030 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11031 = torch.aten.view %11029, %11030 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %11032 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11033 = torch.aten.transpose.int %11032, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %11034 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11035 = torch.prims.convert_element_type %11034, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11036 = torch.prims.convert_element_type %11031, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11037 = torch.prims.convert_element_type %11033, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11038 = torch.aten.mm %11036, %11037 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11039 = torch.aten.mul.Scalar %11038, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11040 = torch.aten.mul.Scalar %11035, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11041 = torch.aten.add.Tensor %11039, %11040, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11042 = torch.prims.convert_element_type %11041, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11043 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11044 = torch.aten.view %11042, %11043 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11045 = torch.aten.add.Tensor %11044, %10997, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11046 = torch.prims.convert_element_type %11045, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11047 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_335, %result1_336 = torch.aten.var_mean.correction %11046, %11047, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11048 = torch.aten.add.Scalar %result0_335, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11049 = torch.aten.rsqrt %11048 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11050 = torch.aten.sub.Tensor %11045, %result1_336, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11051 = torch.aten.mul.Tensor %11050, %11049 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %11052 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11053 = torch.aten.mul.Tensor %11051, %11052 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %11054 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11055 = torch.aten.add.Tensor %11053, %11054, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11056 = torch.prims.convert_element_type %11055, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11057 = torch.prims.convert_element_type %result1_336, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11058 = torch.prims.convert_element_type %11049, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %11059 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11060 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11061 = torch.aten.view %11056, %11060 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11062 = torch_c.to_builtin_tensor %11061 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11063 = torch_c.to_builtin_tensor %11059 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11064 = tensor.empty() : tensor<2048x1280xf32>
    %11065 = linalg.fill ins(%cst : f32) outs(%11064 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11066 = tensor.empty() : tensor<2048x1280xf32>
    %11067 = linalg.fill ins(%cst : f32) outs(%11066 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11068:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11065, %11067, %11062, %11063, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11065, %11067)
    %11069 = arith.truncf %11068#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11070 = torch_c.from_builtin_tensor %11069 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11071 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11072 = torch.aten.view %11070, %11071 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %11073 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11074 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11075 = torch.aten.view %11056, %11074 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11076 = torch_c.to_builtin_tensor %11075 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11077 = torch_c.to_builtin_tensor %11073 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11078 = tensor.empty() : tensor<2048x1280xf32>
    %11079 = linalg.fill ins(%cst : f32) outs(%11078 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11080 = tensor.empty() : tensor<2048x1280xf32>
    %11081 = linalg.fill ins(%cst : f32) outs(%11080 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11082:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11079, %11081, %11076, %11077, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11079, %11081)
    %11083 = arith.truncf %11082#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11084 = torch_c.from_builtin_tensor %11083 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11085 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11086 = torch.aten.view %11084, %11085 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %11087 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11088 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11089 = torch.aten.view %11056, %11088 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11090 = torch_c.to_builtin_tensor %11089 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11091 = torch_c.to_builtin_tensor %11087 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11092 = tensor.empty() : tensor<2048x1280xf32>
    %11093 = linalg.fill ins(%cst : f32) outs(%11092 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11094 = tensor.empty() : tensor<2048x1280xf32>
    %11095 = linalg.fill ins(%cst : f32) outs(%11094 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11096:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11093, %11095, %11090, %11091, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11093, %11095)
    %11097 = arith.truncf %11096#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11098 = torch_c.from_builtin_tensor %11097 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11099 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11100 = torch.aten.view %11098, %11099 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11101 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11102 = torch.aten.view %11072, %11101 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11103 = torch.aten.transpose.int %11102, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11104 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11105 = torch.aten.view %11086, %11104 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11106 = torch.aten.transpose.int %11105, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11107 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11108 = torch.aten.view %11100, %11107 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11109 = torch.aten.transpose.int %11108, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11110:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11103, %11106, %11109, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11111 = torch.aten.transpose.int %11110#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11112 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11113 = torch.aten.view %11111, %11112 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11114 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11115 = torch.aten.view %11113, %11114 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11116 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11117 = torch.aten.transpose.int %11116, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %11118 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11119 = torch.prims.convert_element_type %11118, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11120 = torch.prims.convert_element_type %11115, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11121 = torch.prims.convert_element_type %11117, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11122 = torch.aten.mm %11120, %11121 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11123 = torch.aten.mul.Scalar %11122, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11124 = torch.aten.mul.Scalar %11119, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11125 = torch.aten.add.Tensor %11123, %11124, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11126 = torch.prims.convert_element_type %11125, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11127 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11128 = torch.aten.view %11126, %11127 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11129 = torch.aten.div.Scalar %11128, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11130 = torch.aten.add.Tensor %11129, %11045, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11131 = torch.prims.convert_element_type %11130, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11132 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_337, %result1_338 = torch.aten.var_mean.correction %11131, %11132, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11133 = torch.aten.add.Scalar %result0_337, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11134 = torch.aten.rsqrt %11133 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11135 = torch.aten.sub.Tensor %11130, %result1_338, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11136 = torch.aten.mul.Tensor %11135, %11134 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %11137 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11138 = torch.aten.mul.Tensor %11136, %11137 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %11139 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11140 = torch.aten.add.Tensor %11138, %11139, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11141 = torch.prims.convert_element_type %11140, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11142 = torch.prims.convert_element_type %result1_338, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11143 = torch.prims.convert_element_type %11134, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %11144 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11145 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11146 = torch.aten.view %11141, %11145 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11147 = torch_c.to_builtin_tensor %11146 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11148 = torch_c.to_builtin_tensor %11144 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11149 = tensor.empty() : tensor<2048x1280xf32>
    %11150 = linalg.fill ins(%cst : f32) outs(%11149 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11151 = tensor.empty() : tensor<2048x1280xf32>
    %11152 = linalg.fill ins(%cst : f32) outs(%11151 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11153:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11150, %11152, %11147, %11148, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11150, %11152)
    %11154 = arith.truncf %11153#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11155 = torch_c.from_builtin_tensor %11154 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11156 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11157 = torch.aten.view %11155, %11156 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %11158 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11159 = torch.aten.transpose.int %11158, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11160 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11161 = torch.aten.view %4, %11160 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11162 = torch.aten.mm %11161, %11159 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11163 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11164 = torch.aten.view %11162, %11163 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %11165 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11166 = torch.aten.transpose.int %11165, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11167 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11168 = torch.aten.view %4, %11167 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11169 = torch.aten.mm %11168, %11166 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11170 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11171 = torch.aten.view %11169, %11170 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11172 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11173 = torch.aten.view %11157, %11172 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11174 = torch.aten.transpose.int %11173, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11175 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11176 = torch.aten.view %11164, %11175 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11177 = torch.aten.transpose.int %11176, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11178 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11179 = torch.aten.view %11171, %11178 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11180 = torch.aten.transpose.int %11179, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11181:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11174, %11177, %11180, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11182 = torch.aten.transpose.int %11181#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11183 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11184 = torch.aten.view %11182, %11183 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11185 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11186 = torch.aten.view %11184, %11185 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11187 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11188 = torch.aten.transpose.int %11187, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %11189 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11190 = torch.prims.convert_element_type %11189, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11191 = torch.prims.convert_element_type %11186, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11192 = torch.prims.convert_element_type %11188, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11193 = torch.aten.mm %11191, %11192 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11194 = torch.aten.mul.Scalar %11193, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11195 = torch.aten.mul.Scalar %11190, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11196 = torch.aten.add.Tensor %11194, %11195, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11197 = torch.prims.convert_element_type %11196, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11198 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11199 = torch.aten.view %11197, %11198 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11200 = torch.aten.div.Scalar %11199, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11201 = torch.aten.add.Tensor %11200, %11130, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11202 = torch.prims.convert_element_type %11201, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11203 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_339, %result1_340 = torch.aten.var_mean.correction %11202, %11203, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11204 = torch.aten.add.Scalar %result0_339, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11205 = torch.aten.rsqrt %11204 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11206 = torch.aten.sub.Tensor %11201, %result1_340, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11207 = torch.aten.mul.Tensor %11206, %11205 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %11208 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11209 = torch.aten.mul.Tensor %11207, %11208 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %11210 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11211 = torch.aten.add.Tensor %11209, %11210, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11212 = torch.prims.convert_element_type %11211, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11213 = torch.prims.convert_element_type %result1_340, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11214 = torch.prims.convert_element_type %11205, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11215 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11216 = torch.aten.view %11212, %11215 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11217 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11218 = torch.aten.transpose.int %11217, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %11219 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11220 = torch.prims.convert_element_type %11219, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11221 = torch.prims.convert_element_type %11216, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11222 = torch.prims.convert_element_type %11218, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11223 = torch.aten.mm %11221, %11222 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11224 = torch.aten.mul.Scalar %11223, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11225 = torch.aten.mul.Scalar %11220, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11226 = torch.aten.add.Tensor %11224, %11225, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11227 = torch.prims.convert_element_type %11226, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11228 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11229 = torch.aten.view %11227, %11228 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11230 = torch.aten.slice.Tensor %11229, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11231 = torch.aten.slice.Tensor %11229, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11232 = torch.aten.gelu %11231, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11233 = torch.aten.mul.Tensor %11230, %11232 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11234 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11235 = torch.aten.view %11233, %11234 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %11236 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11237 = torch.aten.transpose.int %11236, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %11238 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11239 = torch.prims.convert_element_type %11238, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11240 = torch.prims.convert_element_type %11235, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11241 = torch.prims.convert_element_type %11237, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11242 = torch.aten.mm %11240, %11241 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11243 = torch.aten.mul.Scalar %11242, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11244 = torch.aten.mul.Scalar %11239, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11245 = torch.aten.add.Tensor %11243, %11244, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11246 = torch.prims.convert_element_type %11245, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11247 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11248 = torch.aten.view %11246, %11247 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11249 = torch.aten.add.Tensor %11248, %11201, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11250 = torch.prims.convert_element_type %11249, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11251 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_341, %result1_342 = torch.aten.var_mean.correction %11250, %11251, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11252 = torch.aten.add.Scalar %result0_341, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11253 = torch.aten.rsqrt %11252 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11254 = torch.aten.sub.Tensor %11249, %result1_342, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11255 = torch.aten.mul.Tensor %11254, %11253 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %11256 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11257 = torch.aten.mul.Tensor %11255, %11256 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %11258 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11259 = torch.aten.add.Tensor %11257, %11258, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11260 = torch.prims.convert_element_type %11259, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11261 = torch.prims.convert_element_type %result1_342, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11262 = torch.prims.convert_element_type %11253, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %11263 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11264 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11265 = torch.aten.view %11260, %11264 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11266 = torch_c.to_builtin_tensor %11265 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11267 = torch_c.to_builtin_tensor %11263 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11268 = tensor.empty() : tensor<2048x1280xf32>
    %11269 = linalg.fill ins(%cst : f32) outs(%11268 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11270 = tensor.empty() : tensor<2048x1280xf32>
    %11271 = linalg.fill ins(%cst : f32) outs(%11270 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11272:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11269, %11271, %11266, %11267, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11269, %11271)
    %11273 = arith.truncf %11272#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11274 = torch_c.from_builtin_tensor %11273 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11275 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11276 = torch.aten.view %11274, %11275 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %11277 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11278 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11279 = torch.aten.view %11260, %11278 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11280 = torch_c.to_builtin_tensor %11279 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11281 = torch_c.to_builtin_tensor %11277 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11282 = tensor.empty() : tensor<2048x1280xf32>
    %11283 = linalg.fill ins(%cst : f32) outs(%11282 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11284 = tensor.empty() : tensor<2048x1280xf32>
    %11285 = linalg.fill ins(%cst : f32) outs(%11284 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11286:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11283, %11285, %11280, %11281, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11283, %11285)
    %11287 = arith.truncf %11286#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11288 = torch_c.from_builtin_tensor %11287 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11289 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11290 = torch.aten.view %11288, %11289 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %11291 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11292 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11293 = torch.aten.view %11260, %11292 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11294 = torch_c.to_builtin_tensor %11293 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11295 = torch_c.to_builtin_tensor %11291 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11296 = tensor.empty() : tensor<2048x1280xf32>
    %11297 = linalg.fill ins(%cst : f32) outs(%11296 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11298 = tensor.empty() : tensor<2048x1280xf32>
    %11299 = linalg.fill ins(%cst : f32) outs(%11298 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11300:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11297, %11299, %11294, %11295, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11297, %11299)
    %11301 = arith.truncf %11300#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11302 = torch_c.from_builtin_tensor %11301 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11303 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11304 = torch.aten.view %11302, %11303 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11305 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11306 = torch.aten.view %11276, %11305 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11307 = torch.aten.transpose.int %11306, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11308 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11309 = torch.aten.view %11290, %11308 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11310 = torch.aten.transpose.int %11309, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11311 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11312 = torch.aten.view %11304, %11311 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11313 = torch.aten.transpose.int %11312, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11314:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11307, %11310, %11313, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11315 = torch.aten.transpose.int %11314#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11316 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11317 = torch.aten.view %11315, %11316 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11318 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11319 = torch.aten.view %11317, %11318 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11320 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11321 = torch.aten.transpose.int %11320, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %11322 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11323 = torch.prims.convert_element_type %11322, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11324 = torch.prims.convert_element_type %11319, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11325 = torch.prims.convert_element_type %11321, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11326 = torch.aten.mm %11324, %11325 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11327 = torch.aten.mul.Scalar %11326, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11328 = torch.aten.mul.Scalar %11323, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11329 = torch.aten.add.Tensor %11327, %11328, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11330 = torch.prims.convert_element_type %11329, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11331 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11332 = torch.aten.view %11330, %11331 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11333 = torch.aten.div.Scalar %11332, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11334 = torch.aten.add.Tensor %11333, %11249, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11335 = torch.prims.convert_element_type %11334, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11336 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_343, %result1_344 = torch.aten.var_mean.correction %11335, %11336, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11337 = torch.aten.add.Scalar %result0_343, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11338 = torch.aten.rsqrt %11337 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11339 = torch.aten.sub.Tensor %11334, %result1_344, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11340 = torch.aten.mul.Tensor %11339, %11338 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %11341 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11342 = torch.aten.mul.Tensor %11340, %11341 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %11343 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11344 = torch.aten.add.Tensor %11342, %11343, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11345 = torch.prims.convert_element_type %11344, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11346 = torch.prims.convert_element_type %result1_344, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11347 = torch.prims.convert_element_type %11338, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %11348 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11349 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11350 = torch.aten.view %11345, %11349 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11351 = torch_c.to_builtin_tensor %11350 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11352 = torch_c.to_builtin_tensor %11348 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11353 = tensor.empty() : tensor<2048x1280xf32>
    %11354 = linalg.fill ins(%cst : f32) outs(%11353 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11355 = tensor.empty() : tensor<2048x1280xf32>
    %11356 = linalg.fill ins(%cst : f32) outs(%11355 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11357:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11354, %11356, %11351, %11352, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11354, %11356)
    %11358 = arith.truncf %11357#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11359 = torch_c.from_builtin_tensor %11358 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11360 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11361 = torch.aten.view %11359, %11360 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %11362 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11363 = torch.aten.transpose.int %11362, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11364 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11365 = torch.aten.view %4, %11364 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11366 = torch.aten.mm %11365, %11363 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11367 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11368 = torch.aten.view %11366, %11367 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %11369 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11370 = torch.aten.transpose.int %11369, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11371 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11372 = torch.aten.view %4, %11371 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11373 = torch.aten.mm %11372, %11370 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11374 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11375 = torch.aten.view %11373, %11374 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11376 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11377 = torch.aten.view %11361, %11376 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11378 = torch.aten.transpose.int %11377, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11379 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11380 = torch.aten.view %11368, %11379 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11381 = torch.aten.transpose.int %11380, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11382 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11383 = torch.aten.view %11375, %11382 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11384 = torch.aten.transpose.int %11383, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11385:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11378, %11381, %11384, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11386 = torch.aten.transpose.int %11385#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11387 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11388 = torch.aten.view %11386, %11387 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11389 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11390 = torch.aten.view %11388, %11389 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11391 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11392 = torch.aten.transpose.int %11391, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %11393 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11394 = torch.prims.convert_element_type %11393, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11395 = torch.prims.convert_element_type %11390, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11396 = torch.prims.convert_element_type %11392, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11397 = torch.aten.mm %11395, %11396 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11398 = torch.aten.mul.Scalar %11397, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11399 = torch.aten.mul.Scalar %11394, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11400 = torch.aten.add.Tensor %11398, %11399, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11401 = torch.prims.convert_element_type %11400, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11402 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11403 = torch.aten.view %11401, %11402 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11404 = torch.aten.div.Scalar %11403, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11405 = torch.aten.add.Tensor %11404, %11334, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11406 = torch.prims.convert_element_type %11405, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11407 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_345, %result1_346 = torch.aten.var_mean.correction %11406, %11407, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11408 = torch.aten.add.Scalar %result0_345, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11409 = torch.aten.rsqrt %11408 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11410 = torch.aten.sub.Tensor %11405, %result1_346, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11411 = torch.aten.mul.Tensor %11410, %11409 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %11412 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11413 = torch.aten.mul.Tensor %11411, %11412 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %11414 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11415 = torch.aten.add.Tensor %11413, %11414, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11416 = torch.prims.convert_element_type %11415, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11417 = torch.prims.convert_element_type %result1_346, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11418 = torch.prims.convert_element_type %11409, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11419 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11420 = torch.aten.view %11416, %11419 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11421 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11422 = torch.aten.transpose.int %11421, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %11423 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11424 = torch.prims.convert_element_type %11423, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11425 = torch.prims.convert_element_type %11420, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11426 = torch.prims.convert_element_type %11422, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11427 = torch.aten.mm %11425, %11426 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11428 = torch.aten.mul.Scalar %11427, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11429 = torch.aten.mul.Scalar %11424, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11430 = torch.aten.add.Tensor %11428, %11429, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11431 = torch.prims.convert_element_type %11430, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11432 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11433 = torch.aten.view %11431, %11432 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11434 = torch.aten.slice.Tensor %11433, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11435 = torch.aten.slice.Tensor %11433, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11436 = torch.aten.gelu %11435, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11437 = torch.aten.mul.Tensor %11434, %11436 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11438 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11439 = torch.aten.view %11437, %11438 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %11440 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11441 = torch.aten.transpose.int %11440, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %11442 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11443 = torch.prims.convert_element_type %11442, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11444 = torch.prims.convert_element_type %11439, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11445 = torch.prims.convert_element_type %11441, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11446 = torch.aten.mm %11444, %11445 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11447 = torch.aten.mul.Scalar %11446, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11448 = torch.aten.mul.Scalar %11443, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11449 = torch.aten.add.Tensor %11447, %11448, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11450 = torch.prims.convert_element_type %11449, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11451 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11452 = torch.aten.view %11450, %11451 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11453 = torch.aten.add.Tensor %11452, %11405, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11454 = torch.prims.convert_element_type %11453, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11455 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_347, %result1_348 = torch.aten.var_mean.correction %11454, %11455, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11456 = torch.aten.add.Scalar %result0_347, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11457 = torch.aten.rsqrt %11456 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11458 = torch.aten.sub.Tensor %11453, %result1_348, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11459 = torch.aten.mul.Tensor %11458, %11457 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %11460 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11461 = torch.aten.mul.Tensor %11459, %11460 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %11462 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11463 = torch.aten.add.Tensor %11461, %11462, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11464 = torch.prims.convert_element_type %11463, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11465 = torch.prims.convert_element_type %result1_348, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11466 = torch.prims.convert_element_type %11457, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %11467 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11468 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11469 = torch.aten.view %11464, %11468 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11470 = torch_c.to_builtin_tensor %11469 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11471 = torch_c.to_builtin_tensor %11467 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11472 = tensor.empty() : tensor<2048x1280xf32>
    %11473 = linalg.fill ins(%cst : f32) outs(%11472 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11474 = tensor.empty() : tensor<2048x1280xf32>
    %11475 = linalg.fill ins(%cst : f32) outs(%11474 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11476:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11473, %11475, %11470, %11471, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11473, %11475)
    %11477 = arith.truncf %11476#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11478 = torch_c.from_builtin_tensor %11477 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11479 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11480 = torch.aten.view %11478, %11479 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %11481 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11482 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11483 = torch.aten.view %11464, %11482 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11484 = torch_c.to_builtin_tensor %11483 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11485 = torch_c.to_builtin_tensor %11481 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11486 = tensor.empty() : tensor<2048x1280xf32>
    %11487 = linalg.fill ins(%cst : f32) outs(%11486 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11488 = tensor.empty() : tensor<2048x1280xf32>
    %11489 = linalg.fill ins(%cst : f32) outs(%11488 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11490:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11487, %11489, %11484, %11485, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11487, %11489)
    %11491 = arith.truncf %11490#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11492 = torch_c.from_builtin_tensor %11491 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11493 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11494 = torch.aten.view %11492, %11493 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %11495 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11496 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11497 = torch.aten.view %11464, %11496 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11498 = torch_c.to_builtin_tensor %11497 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11499 = torch_c.to_builtin_tensor %11495 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11500 = tensor.empty() : tensor<2048x1280xf32>
    %11501 = linalg.fill ins(%cst : f32) outs(%11500 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11502 = tensor.empty() : tensor<2048x1280xf32>
    %11503 = linalg.fill ins(%cst : f32) outs(%11502 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11504:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11501, %11503, %11498, %11499, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11501, %11503)
    %11505 = arith.truncf %11504#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11506 = torch_c.from_builtin_tensor %11505 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11507 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11508 = torch.aten.view %11506, %11507 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11509 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11510 = torch.aten.view %11480, %11509 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11511 = torch.aten.transpose.int %11510, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11512 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11513 = torch.aten.view %11494, %11512 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11514 = torch.aten.transpose.int %11513, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11515 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11516 = torch.aten.view %11508, %11515 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11517 = torch.aten.transpose.int %11516, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11518:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11511, %11514, %11517, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11519 = torch.aten.transpose.int %11518#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11520 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11521 = torch.aten.view %11519, %11520 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11522 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11523 = torch.aten.view %11521, %11522 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11524 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11525 = torch.aten.transpose.int %11524, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %11526 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11527 = torch.prims.convert_element_type %11526, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11528 = torch.prims.convert_element_type %11523, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11529 = torch.prims.convert_element_type %11525, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11530 = torch.aten.mm %11528, %11529 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11531 = torch.aten.mul.Scalar %11530, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11532 = torch.aten.mul.Scalar %11527, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11533 = torch.aten.add.Tensor %11531, %11532, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11534 = torch.prims.convert_element_type %11533, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11535 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11536 = torch.aten.view %11534, %11535 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11537 = torch.aten.div.Scalar %11536, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11538 = torch.aten.add.Tensor %11537, %11453, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11539 = torch.prims.convert_element_type %11538, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11540 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_349, %result1_350 = torch.aten.var_mean.correction %11539, %11540, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11541 = torch.aten.add.Scalar %result0_349, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11542 = torch.aten.rsqrt %11541 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11543 = torch.aten.sub.Tensor %11538, %result1_350, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11544 = torch.aten.mul.Tensor %11543, %11542 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %11545 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11546 = torch.aten.mul.Tensor %11544, %11545 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %11547 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11548 = torch.aten.add.Tensor %11546, %11547, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11549 = torch.prims.convert_element_type %11548, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11550 = torch.prims.convert_element_type %result1_350, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11551 = torch.prims.convert_element_type %11542, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %11552 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11553 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11554 = torch.aten.view %11549, %11553 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11555 = torch_c.to_builtin_tensor %11554 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11556 = torch_c.to_builtin_tensor %11552 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11557 = tensor.empty() : tensor<2048x1280xf32>
    %11558 = linalg.fill ins(%cst : f32) outs(%11557 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11559 = tensor.empty() : tensor<2048x1280xf32>
    %11560 = linalg.fill ins(%cst : f32) outs(%11559 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11561:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11558, %11560, %11555, %11556, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11558, %11560)
    %11562 = arith.truncf %11561#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11563 = torch_c.from_builtin_tensor %11562 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11564 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11565 = torch.aten.view %11563, %11564 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %11566 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11567 = torch.aten.transpose.int %11566, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11568 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11569 = torch.aten.view %4, %11568 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11570 = torch.aten.mm %11569, %11567 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11571 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11572 = torch.aten.view %11570, %11571 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %11573 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11574 = torch.aten.transpose.int %11573, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11575 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11576 = torch.aten.view %4, %11575 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11577 = torch.aten.mm %11576, %11574 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11578 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11579 = torch.aten.view %11577, %11578 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11580 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11581 = torch.aten.view %11565, %11580 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11582 = torch.aten.transpose.int %11581, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11583 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11584 = torch.aten.view %11572, %11583 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11585 = torch.aten.transpose.int %11584, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11586 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11587 = torch.aten.view %11579, %11586 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11588 = torch.aten.transpose.int %11587, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11589:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11582, %11585, %11588, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11590 = torch.aten.transpose.int %11589#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11591 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11592 = torch.aten.view %11590, %11591 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11593 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11594 = torch.aten.view %11592, %11593 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11595 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11596 = torch.aten.transpose.int %11595, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %11597 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11598 = torch.prims.convert_element_type %11597, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11599 = torch.prims.convert_element_type %11594, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11600 = torch.prims.convert_element_type %11596, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11601 = torch.aten.mm %11599, %11600 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11602 = torch.aten.mul.Scalar %11601, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11603 = torch.aten.mul.Scalar %11598, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11604 = torch.aten.add.Tensor %11602, %11603, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11605 = torch.prims.convert_element_type %11604, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11606 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11607 = torch.aten.view %11605, %11606 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11608 = torch.aten.div.Scalar %11607, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11609 = torch.aten.add.Tensor %11608, %11538, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11610 = torch.prims.convert_element_type %11609, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11611 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_351, %result1_352 = torch.aten.var_mean.correction %11610, %11611, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11612 = torch.aten.add.Scalar %result0_351, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11613 = torch.aten.rsqrt %11612 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11614 = torch.aten.sub.Tensor %11609, %result1_352, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11615 = torch.aten.mul.Tensor %11614, %11613 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %11616 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11617 = torch.aten.mul.Tensor %11615, %11616 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %11618 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11619 = torch.aten.add.Tensor %11617, %11618, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11620 = torch.prims.convert_element_type %11619, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11621 = torch.prims.convert_element_type %result1_352, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11622 = torch.prims.convert_element_type %11613, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11623 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11624 = torch.aten.view %11620, %11623 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11625 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11626 = torch.aten.transpose.int %11625, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %11627 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11628 = torch.prims.convert_element_type %11627, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11629 = torch.prims.convert_element_type %11624, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11630 = torch.prims.convert_element_type %11626, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11631 = torch.aten.mm %11629, %11630 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11632 = torch.aten.mul.Scalar %11631, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11633 = torch.aten.mul.Scalar %11628, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11634 = torch.aten.add.Tensor %11632, %11633, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11635 = torch.prims.convert_element_type %11634, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11636 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11637 = torch.aten.view %11635, %11636 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11638 = torch.aten.slice.Tensor %11637, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11639 = torch.aten.slice.Tensor %11637, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11640 = torch.aten.gelu %11639, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11641 = torch.aten.mul.Tensor %11638, %11640 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11642 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11643 = torch.aten.view %11641, %11642 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %11644 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11645 = torch.aten.transpose.int %11644, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %11646 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11647 = torch.prims.convert_element_type %11646, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11648 = torch.prims.convert_element_type %11643, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11649 = torch.prims.convert_element_type %11645, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11650 = torch.aten.mm %11648, %11649 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11651 = torch.aten.mul.Scalar %11650, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11652 = torch.aten.mul.Scalar %11647, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11653 = torch.aten.add.Tensor %11651, %11652, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11654 = torch.prims.convert_element_type %11653, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11655 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11656 = torch.aten.view %11654, %11655 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11657 = torch.aten.add.Tensor %11656, %11609, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11658 = torch.prims.convert_element_type %11657, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11659 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_353, %result1_354 = torch.aten.var_mean.correction %11658, %11659, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11660 = torch.aten.add.Scalar %result0_353, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11661 = torch.aten.rsqrt %11660 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11662 = torch.aten.sub.Tensor %11657, %result1_354, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11663 = torch.aten.mul.Tensor %11662, %11661 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %11664 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11665 = torch.aten.mul.Tensor %11663, %11664 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %11666 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11667 = torch.aten.add.Tensor %11665, %11666, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11668 = torch.prims.convert_element_type %11667, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11669 = torch.prims.convert_element_type %result1_354, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11670 = torch.prims.convert_element_type %11661, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %11671 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11672 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11673 = torch.aten.view %11668, %11672 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11674 = torch_c.to_builtin_tensor %11673 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11675 = torch_c.to_builtin_tensor %11671 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11676 = tensor.empty() : tensor<2048x1280xf32>
    %11677 = linalg.fill ins(%cst : f32) outs(%11676 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11678 = tensor.empty() : tensor<2048x1280xf32>
    %11679 = linalg.fill ins(%cst : f32) outs(%11678 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11680:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11677, %11679, %11674, %11675, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11677, %11679)
    %11681 = arith.truncf %11680#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11682 = torch_c.from_builtin_tensor %11681 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11683 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11684 = torch.aten.view %11682, %11683 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %11685 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11686 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11687 = torch.aten.view %11668, %11686 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11688 = torch_c.to_builtin_tensor %11687 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11689 = torch_c.to_builtin_tensor %11685 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11690 = tensor.empty() : tensor<2048x1280xf32>
    %11691 = linalg.fill ins(%cst : f32) outs(%11690 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11692 = tensor.empty() : tensor<2048x1280xf32>
    %11693 = linalg.fill ins(%cst : f32) outs(%11692 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11694:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11691, %11693, %11688, %11689, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11691, %11693)
    %11695 = arith.truncf %11694#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11696 = torch_c.from_builtin_tensor %11695 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11697 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11698 = torch.aten.view %11696, %11697 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %11699 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11700 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11701 = torch.aten.view %11668, %11700 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11702 = torch_c.to_builtin_tensor %11701 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11703 = torch_c.to_builtin_tensor %11699 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11704 = tensor.empty() : tensor<2048x1280xf32>
    %11705 = linalg.fill ins(%cst : f32) outs(%11704 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11706 = tensor.empty() : tensor<2048x1280xf32>
    %11707 = linalg.fill ins(%cst : f32) outs(%11706 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11708:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11705, %11707, %11702, %11703, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11705, %11707)
    %11709 = arith.truncf %11708#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11710 = torch_c.from_builtin_tensor %11709 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11711 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11712 = torch.aten.view %11710, %11711 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11713 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11714 = torch.aten.view %11684, %11713 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11715 = torch.aten.transpose.int %11714, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11716 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11717 = torch.aten.view %11698, %11716 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11718 = torch.aten.transpose.int %11717, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11719 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11720 = torch.aten.view %11712, %11719 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11721 = torch.aten.transpose.int %11720, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11722:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11715, %11718, %11721, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11723 = torch.aten.transpose.int %11722#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11724 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11725 = torch.aten.view %11723, %11724 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11726 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11727 = torch.aten.view %11725, %11726 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11728 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11729 = torch.aten.transpose.int %11728, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %11730 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11731 = torch.prims.convert_element_type %11730, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11732 = torch.prims.convert_element_type %11727, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11733 = torch.prims.convert_element_type %11729, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11734 = torch.aten.mm %11732, %11733 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11735 = torch.aten.mul.Scalar %11734, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11736 = torch.aten.mul.Scalar %11731, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11737 = torch.aten.add.Tensor %11735, %11736, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11738 = torch.prims.convert_element_type %11737, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11739 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11740 = torch.aten.view %11738, %11739 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11741 = torch.aten.div.Scalar %11740, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11742 = torch.aten.add.Tensor %11741, %11657, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11743 = torch.prims.convert_element_type %11742, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11744 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_355, %result1_356 = torch.aten.var_mean.correction %11743, %11744, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11745 = torch.aten.add.Scalar %result0_355, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11746 = torch.aten.rsqrt %11745 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11747 = torch.aten.sub.Tensor %11742, %result1_356, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11748 = torch.aten.mul.Tensor %11747, %11746 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %11749 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11750 = torch.aten.mul.Tensor %11748, %11749 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %11751 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11752 = torch.aten.add.Tensor %11750, %11751, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11753 = torch.prims.convert_element_type %11752, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11754 = torch.prims.convert_element_type %result1_356, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11755 = torch.prims.convert_element_type %11746, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %11756 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11757 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11758 = torch.aten.view %11753, %11757 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11759 = torch_c.to_builtin_tensor %11758 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11760 = torch_c.to_builtin_tensor %11756 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11761 = tensor.empty() : tensor<2048x1280xf32>
    %11762 = linalg.fill ins(%cst : f32) outs(%11761 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11763 = tensor.empty() : tensor<2048x1280xf32>
    %11764 = linalg.fill ins(%cst : f32) outs(%11763 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11765:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11762, %11764, %11759, %11760, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11762, %11764)
    %11766 = arith.truncf %11765#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11767 = torch_c.from_builtin_tensor %11766 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11768 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11769 = torch.aten.view %11767, %11768 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %11770 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11771 = torch.aten.transpose.int %11770, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11772 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11773 = torch.aten.view %4, %11772 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11774 = torch.aten.mm %11773, %11771 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11775 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11776 = torch.aten.view %11774, %11775 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %11777 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11778 = torch.aten.transpose.int %11777, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11779 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11780 = torch.aten.view %4, %11779 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11781 = torch.aten.mm %11780, %11778 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11782 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11783 = torch.aten.view %11781, %11782 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11784 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11785 = torch.aten.view %11769, %11784 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11786 = torch.aten.transpose.int %11785, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11787 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11788 = torch.aten.view %11776, %11787 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11789 = torch.aten.transpose.int %11788, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11790 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11791 = torch.aten.view %11783, %11790 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11792 = torch.aten.transpose.int %11791, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11793:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11786, %11789, %11792, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11794 = torch.aten.transpose.int %11793#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11795 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11796 = torch.aten.view %11794, %11795 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11797 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11798 = torch.aten.view %11796, %11797 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11799 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11800 = torch.aten.transpose.int %11799, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %11801 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11802 = torch.prims.convert_element_type %11801, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11803 = torch.prims.convert_element_type %11798, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11804 = torch.prims.convert_element_type %11800, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11805 = torch.aten.mm %11803, %11804 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11806 = torch.aten.mul.Scalar %11805, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11807 = torch.aten.mul.Scalar %11802, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11808 = torch.aten.add.Tensor %11806, %11807, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11809 = torch.prims.convert_element_type %11808, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11810 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11811 = torch.aten.view %11809, %11810 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11812 = torch.aten.div.Scalar %11811, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11813 = torch.aten.add.Tensor %11812, %11742, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11814 = torch.prims.convert_element_type %11813, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11815 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_357, %result1_358 = torch.aten.var_mean.correction %11814, %11815, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11816 = torch.aten.add.Scalar %result0_357, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11817 = torch.aten.rsqrt %11816 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11818 = torch.aten.sub.Tensor %11813, %result1_358, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11819 = torch.aten.mul.Tensor %11818, %11817 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %11820 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11821 = torch.aten.mul.Tensor %11819, %11820 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %11822 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11823 = torch.aten.add.Tensor %11821, %11822, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11824 = torch.prims.convert_element_type %11823, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11825 = torch.prims.convert_element_type %result1_358, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11826 = torch.prims.convert_element_type %11817, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11827 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11828 = torch.aten.view %11824, %11827 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11829 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11830 = torch.aten.transpose.int %11829, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %11831 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11832 = torch.prims.convert_element_type %11831, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11833 = torch.prims.convert_element_type %11828, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11834 = torch.prims.convert_element_type %11830, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11835 = torch.aten.mm %11833, %11834 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11836 = torch.aten.mul.Scalar %11835, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11837 = torch.aten.mul.Scalar %11832, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11838 = torch.aten.add.Tensor %11836, %11837, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11839 = torch.prims.convert_element_type %11838, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11840 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11841 = torch.aten.view %11839, %11840 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11842 = torch.aten.slice.Tensor %11841, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11843 = torch.aten.slice.Tensor %11841, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11844 = torch.aten.gelu %11843, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11845 = torch.aten.mul.Tensor %11842, %11844 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11846 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11847 = torch.aten.view %11845, %11846 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %11848 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11849 = torch.aten.transpose.int %11848, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %11850 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11851 = torch.prims.convert_element_type %11850, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11852 = torch.prims.convert_element_type %11847, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11853 = torch.prims.convert_element_type %11849, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11854 = torch.aten.mm %11852, %11853 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11855 = torch.aten.mul.Scalar %11854, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11856 = torch.aten.mul.Scalar %11851, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11857 = torch.aten.add.Tensor %11855, %11856, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11858 = torch.prims.convert_element_type %11857, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11859 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11860 = torch.aten.view %11858, %11859 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11861 = torch.aten.add.Tensor %11860, %11813, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11862 = torch.prims.convert_element_type %11861, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11863 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_359, %result1_360 = torch.aten.var_mean.correction %11862, %11863, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11864 = torch.aten.add.Scalar %result0_359, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11865 = torch.aten.rsqrt %11864 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11866 = torch.aten.sub.Tensor %11861, %result1_360, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11867 = torch.aten.mul.Tensor %11866, %11865 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %11868 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11869 = torch.aten.mul.Tensor %11867, %11868 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %11870 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11871 = torch.aten.add.Tensor %11869, %11870, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11872 = torch.prims.convert_element_type %11871, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11873 = torch.prims.convert_element_type %result1_360, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11874 = torch.prims.convert_element_type %11865, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %11875 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11876 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11877 = torch.aten.view %11872, %11876 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11878 = torch_c.to_builtin_tensor %11877 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11879 = torch_c.to_builtin_tensor %11875 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11880 = tensor.empty() : tensor<2048x1280xf32>
    %11881 = linalg.fill ins(%cst : f32) outs(%11880 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11882 = tensor.empty() : tensor<2048x1280xf32>
    %11883 = linalg.fill ins(%cst : f32) outs(%11882 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11884:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11881, %11883, %11878, %11879, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11881, %11883)
    %11885 = arith.truncf %11884#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11886 = torch_c.from_builtin_tensor %11885 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11887 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11888 = torch.aten.view %11886, %11887 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %11889 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11890 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11891 = torch.aten.view %11872, %11890 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11892 = torch_c.to_builtin_tensor %11891 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11893 = torch_c.to_builtin_tensor %11889 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11894 = tensor.empty() : tensor<2048x1280xf32>
    %11895 = linalg.fill ins(%cst : f32) outs(%11894 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11896 = tensor.empty() : tensor<2048x1280xf32>
    %11897 = linalg.fill ins(%cst : f32) outs(%11896 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11898:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11895, %11897, %11892, %11893, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11895, %11897)
    %11899 = arith.truncf %11898#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11900 = torch_c.from_builtin_tensor %11899 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11901 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11902 = torch.aten.view %11900, %11901 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %11903 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11904 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11905 = torch.aten.view %11872, %11904 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11906 = torch_c.to_builtin_tensor %11905 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11907 = torch_c.to_builtin_tensor %11903 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11908 = tensor.empty() : tensor<2048x1280xf32>
    %11909 = linalg.fill ins(%cst : f32) outs(%11908 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11910 = tensor.empty() : tensor<2048x1280xf32>
    %11911 = linalg.fill ins(%cst : f32) outs(%11910 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11912:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11909, %11911, %11906, %11907, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11909, %11911)
    %11913 = arith.truncf %11912#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11914 = torch_c.from_builtin_tensor %11913 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11915 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11916 = torch.aten.view %11914, %11915 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11917 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11918 = torch.aten.view %11888, %11917 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11919 = torch.aten.transpose.int %11918, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11920 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11921 = torch.aten.view %11902, %11920 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11922 = torch.aten.transpose.int %11921, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11923 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11924 = torch.aten.view %11916, %11923 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11925 = torch.aten.transpose.int %11924, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11926:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11919, %11922, %11925, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11927 = torch.aten.transpose.int %11926#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11928 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11929 = torch.aten.view %11927, %11928 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11930 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11931 = torch.aten.view %11929, %11930 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11932 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11933 = torch.aten.transpose.int %11932, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %11934 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11935 = torch.prims.convert_element_type %11934, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11936 = torch.prims.convert_element_type %11931, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11937 = torch.prims.convert_element_type %11933, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11938 = torch.aten.mm %11936, %11937 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11939 = torch.aten.mul.Scalar %11938, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11940 = torch.aten.mul.Scalar %11935, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11941 = torch.aten.add.Tensor %11939, %11940, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11942 = torch.prims.convert_element_type %11941, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11943 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11944 = torch.aten.view %11942, %11943 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11945 = torch.aten.div.Scalar %11944, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11946 = torch.aten.add.Tensor %11945, %11861, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11947 = torch.prims.convert_element_type %11946, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11948 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_361, %result1_362 = torch.aten.var_mean.correction %11947, %11948, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11949 = torch.aten.add.Scalar %result0_361, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11950 = torch.aten.rsqrt %11949 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11951 = torch.aten.sub.Tensor %11946, %result1_362, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11952 = torch.aten.mul.Tensor %11951, %11950 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %11953 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11954 = torch.aten.mul.Tensor %11952, %11953 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %11955 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11956 = torch.aten.add.Tensor %11954, %11955, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11957 = torch.prims.convert_element_type %11956, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11958 = torch.prims.convert_element_type %result1_362, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11959 = torch.prims.convert_element_type %11950, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %11960 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11961 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11962 = torch.aten.view %11957, %11961 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11963 = torch_c.to_builtin_tensor %11962 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11964 = torch_c.to_builtin_tensor %11960 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11965 = tensor.empty() : tensor<2048x1280xf32>
    %11966 = linalg.fill ins(%cst : f32) outs(%11965 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11967 = tensor.empty() : tensor<2048x1280xf32>
    %11968 = linalg.fill ins(%cst : f32) outs(%11967 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11969:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11966, %11968, %11963, %11964, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11966, %11968)
    %11970 = arith.truncf %11969#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11971 = torch_c.from_builtin_tensor %11970 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11972 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11973 = torch.aten.view %11971, %11972 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %11974 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11975 = torch.aten.transpose.int %11974, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11976 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11977 = torch.aten.view %4, %11976 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11978 = torch.aten.mm %11977, %11975 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11979 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11980 = torch.aten.view %11978, %11979 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %11981 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11982 = torch.aten.transpose.int %11981, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11983 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11984 = torch.aten.view %4, %11983 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11985 = torch.aten.mm %11984, %11982 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %11986 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11987 = torch.aten.view %11985, %11986 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11988 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11989 = torch.aten.view %11973, %11988 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11990 = torch.aten.transpose.int %11989, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11991 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11992 = torch.aten.view %11980, %11991 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11993 = torch.aten.transpose.int %11992, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11994 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11995 = torch.aten.view %11987, %11994 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11996 = torch.aten.transpose.int %11995, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11997:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11990, %11993, %11996, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11998 = torch.aten.transpose.int %11997#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11999 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12000 = torch.aten.view %11998, %11999 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12001 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12002 = torch.aten.view %12000, %12001 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12003 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12004 = torch.aten.transpose.int %12003, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %12005 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12006 = torch.prims.convert_element_type %12005, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12007 = torch.prims.convert_element_type %12002, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12008 = torch.prims.convert_element_type %12004, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12009 = torch.aten.mm %12007, %12008 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12010 = torch.aten.mul.Scalar %12009, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12011 = torch.aten.mul.Scalar %12006, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12012 = torch.aten.add.Tensor %12010, %12011, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12013 = torch.prims.convert_element_type %12012, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12014 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12015 = torch.aten.view %12013, %12014 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12016 = torch.aten.div.Scalar %12015, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12017 = torch.aten.add.Tensor %12016, %11946, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12018 = torch.prims.convert_element_type %12017, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12019 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_363, %result1_364 = torch.aten.var_mean.correction %12018, %12019, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12020 = torch.aten.add.Scalar %result0_363, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12021 = torch.aten.rsqrt %12020 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12022 = torch.aten.sub.Tensor %12017, %result1_364, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12023 = torch.aten.mul.Tensor %12022, %12021 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %12024 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12025 = torch.aten.mul.Tensor %12023, %12024 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %12026 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12027 = torch.aten.add.Tensor %12025, %12026, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12028 = torch.prims.convert_element_type %12027, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12029 = torch.prims.convert_element_type %result1_364, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12030 = torch.prims.convert_element_type %12021, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12031 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12032 = torch.aten.view %12028, %12031 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12033 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12034 = torch.aten.transpose.int %12033, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %12035 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12036 = torch.prims.convert_element_type %12035, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12037 = torch.prims.convert_element_type %12032, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12038 = torch.prims.convert_element_type %12034, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12039 = torch.aten.mm %12037, %12038 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12040 = torch.aten.mul.Scalar %12039, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12041 = torch.aten.mul.Scalar %12036, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12042 = torch.aten.add.Tensor %12040, %12041, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12043 = torch.prims.convert_element_type %12042, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12044 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12045 = torch.aten.view %12043, %12044 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12046 = torch.aten.slice.Tensor %12045, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12047 = torch.aten.slice.Tensor %12045, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12048 = torch.aten.gelu %12047, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12049 = torch.aten.mul.Tensor %12046, %12048 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12050 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12051 = torch.aten.view %12049, %12050 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %12052 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12053 = torch.aten.transpose.int %12052, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %12054 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12055 = torch.prims.convert_element_type %12054, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12056 = torch.prims.convert_element_type %12051, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12057 = torch.prims.convert_element_type %12053, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12058 = torch.aten.mm %12056, %12057 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12059 = torch.aten.mul.Scalar %12058, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12060 = torch.aten.mul.Scalar %12055, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12061 = torch.aten.add.Tensor %12059, %12060, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12062 = torch.prims.convert_element_type %12061, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12063 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12064 = torch.aten.view %12062, %12063 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12065 = torch.aten.add.Tensor %12064, %12017, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12066 = torch.prims.convert_element_type %12065, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12067 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_365, %result1_366 = torch.aten.var_mean.correction %12066, %12067, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12068 = torch.aten.add.Scalar %result0_365, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12069 = torch.aten.rsqrt %12068 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12070 = torch.aten.sub.Tensor %12065, %result1_366, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12071 = torch.aten.mul.Tensor %12070, %12069 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %12072 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12073 = torch.aten.mul.Tensor %12071, %12072 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %12074 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12075 = torch.aten.add.Tensor %12073, %12074, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12076 = torch.prims.convert_element_type %12075, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12077 = torch.prims.convert_element_type %result1_366, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12078 = torch.prims.convert_element_type %12069, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %12079 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12080 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12081 = torch.aten.view %12076, %12080 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12082 = torch_c.to_builtin_tensor %12081 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12083 = torch_c.to_builtin_tensor %12079 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12084 = tensor.empty() : tensor<2048x1280xf32>
    %12085 = linalg.fill ins(%cst : f32) outs(%12084 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12086 = tensor.empty() : tensor<2048x1280xf32>
    %12087 = linalg.fill ins(%cst : f32) outs(%12086 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12088:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12085, %12087, %12082, %12083, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12085, %12087)
    %12089 = arith.truncf %12088#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12090 = torch_c.from_builtin_tensor %12089 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12091 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12092 = torch.aten.view %12090, %12091 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %12093 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12094 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12095 = torch.aten.view %12076, %12094 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12096 = torch_c.to_builtin_tensor %12095 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12097 = torch_c.to_builtin_tensor %12093 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12098 = tensor.empty() : tensor<2048x1280xf32>
    %12099 = linalg.fill ins(%cst : f32) outs(%12098 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12100 = tensor.empty() : tensor<2048x1280xf32>
    %12101 = linalg.fill ins(%cst : f32) outs(%12100 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12102:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12099, %12101, %12096, %12097, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12099, %12101)
    %12103 = arith.truncf %12102#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12104 = torch_c.from_builtin_tensor %12103 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12105 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12106 = torch.aten.view %12104, %12105 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %12107 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12108 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12109 = torch.aten.view %12076, %12108 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12110 = torch_c.to_builtin_tensor %12109 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12111 = torch_c.to_builtin_tensor %12107 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12112 = tensor.empty() : tensor<2048x1280xf32>
    %12113 = linalg.fill ins(%cst : f32) outs(%12112 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12114 = tensor.empty() : tensor<2048x1280xf32>
    %12115 = linalg.fill ins(%cst : f32) outs(%12114 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12116:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12113, %12115, %12110, %12111, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12113, %12115)
    %12117 = arith.truncf %12116#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12118 = torch_c.from_builtin_tensor %12117 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12119 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12120 = torch.aten.view %12118, %12119 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12121 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12122 = torch.aten.view %12092, %12121 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12123 = torch.aten.transpose.int %12122, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12124 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12125 = torch.aten.view %12106, %12124 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12126 = torch.aten.transpose.int %12125, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12127 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12128 = torch.aten.view %12120, %12127 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12129 = torch.aten.transpose.int %12128, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12130:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12123, %12126, %12129, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12131 = torch.aten.transpose.int %12130#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12132 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12133 = torch.aten.view %12131, %12132 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12134 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12135 = torch.aten.view %12133, %12134 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12136 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12137 = torch.aten.transpose.int %12136, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %12138 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12139 = torch.prims.convert_element_type %12138, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12140 = torch.prims.convert_element_type %12135, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12141 = torch.prims.convert_element_type %12137, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12142 = torch.aten.mm %12140, %12141 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12143 = torch.aten.mul.Scalar %12142, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12144 = torch.aten.mul.Scalar %12139, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12145 = torch.aten.add.Tensor %12143, %12144, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12146 = torch.prims.convert_element_type %12145, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12147 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12148 = torch.aten.view %12146, %12147 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12149 = torch.aten.div.Scalar %12148, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12150 = torch.aten.add.Tensor %12149, %12065, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12151 = torch.prims.convert_element_type %12150, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12152 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_367, %result1_368 = torch.aten.var_mean.correction %12151, %12152, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12153 = torch.aten.add.Scalar %result0_367, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12154 = torch.aten.rsqrt %12153 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12155 = torch.aten.sub.Tensor %12150, %result1_368, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12156 = torch.aten.mul.Tensor %12155, %12154 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %12157 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12158 = torch.aten.mul.Tensor %12156, %12157 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %12159 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12160 = torch.aten.add.Tensor %12158, %12159, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12161 = torch.prims.convert_element_type %12160, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12162 = torch.prims.convert_element_type %result1_368, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12163 = torch.prims.convert_element_type %12154, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %12164 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12165 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12166 = torch.aten.view %12161, %12165 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12167 = torch_c.to_builtin_tensor %12166 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12168 = torch_c.to_builtin_tensor %12164 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12169 = tensor.empty() : tensor<2048x1280xf32>
    %12170 = linalg.fill ins(%cst : f32) outs(%12169 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12171 = tensor.empty() : tensor<2048x1280xf32>
    %12172 = linalg.fill ins(%cst : f32) outs(%12171 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12173:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12170, %12172, %12167, %12168, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12170, %12172)
    %12174 = arith.truncf %12173#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12175 = torch_c.from_builtin_tensor %12174 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12176 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12177 = torch.aten.view %12175, %12176 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %12178 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12179 = torch.aten.transpose.int %12178, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12180 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12181 = torch.aten.view %4, %12180 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12182 = torch.aten.mm %12181, %12179 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12183 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12184 = torch.aten.view %12182, %12183 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %12185 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12186 = torch.aten.transpose.int %12185, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12187 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12188 = torch.aten.view %4, %12187 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12189 = torch.aten.mm %12188, %12186 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12190 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12191 = torch.aten.view %12189, %12190 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12192 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12193 = torch.aten.view %12177, %12192 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12194 = torch.aten.transpose.int %12193, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12195 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12196 = torch.aten.view %12184, %12195 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12197 = torch.aten.transpose.int %12196, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12198 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12199 = torch.aten.view %12191, %12198 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12200 = torch.aten.transpose.int %12199, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12201:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12194, %12197, %12200, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12202 = torch.aten.transpose.int %12201#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12203 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12204 = torch.aten.view %12202, %12203 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12205 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12206 = torch.aten.view %12204, %12205 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12207 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12208 = torch.aten.transpose.int %12207, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %12209 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12210 = torch.prims.convert_element_type %12209, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12211 = torch.prims.convert_element_type %12206, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12212 = torch.prims.convert_element_type %12208, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12213 = torch.aten.mm %12211, %12212 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12214 = torch.aten.mul.Scalar %12213, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12215 = torch.aten.mul.Scalar %12210, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12216 = torch.aten.add.Tensor %12214, %12215, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12217 = torch.prims.convert_element_type %12216, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12218 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12219 = torch.aten.view %12217, %12218 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12220 = torch.aten.div.Scalar %12219, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12221 = torch.aten.add.Tensor %12220, %12150, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12222 = torch.prims.convert_element_type %12221, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12223 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_369, %result1_370 = torch.aten.var_mean.correction %12222, %12223, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12224 = torch.aten.add.Scalar %result0_369, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12225 = torch.aten.rsqrt %12224 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12226 = torch.aten.sub.Tensor %12221, %result1_370, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12227 = torch.aten.mul.Tensor %12226, %12225 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %12228 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12229 = torch.aten.mul.Tensor %12227, %12228 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %12230 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12231 = torch.aten.add.Tensor %12229, %12230, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12232 = torch.prims.convert_element_type %12231, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12233 = torch.prims.convert_element_type %result1_370, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12234 = torch.prims.convert_element_type %12225, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12235 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12236 = torch.aten.view %12232, %12235 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12237 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12238 = torch.aten.transpose.int %12237, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %12239 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12240 = torch.prims.convert_element_type %12239, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12241 = torch.prims.convert_element_type %12236, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12242 = torch.prims.convert_element_type %12238, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12243 = torch.aten.mm %12241, %12242 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12244 = torch.aten.mul.Scalar %12243, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12245 = torch.aten.mul.Scalar %12240, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12246 = torch.aten.add.Tensor %12244, %12245, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12247 = torch.prims.convert_element_type %12246, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12248 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12249 = torch.aten.view %12247, %12248 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12250 = torch.aten.slice.Tensor %12249, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12251 = torch.aten.slice.Tensor %12249, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12252 = torch.aten.gelu %12251, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12253 = torch.aten.mul.Tensor %12250, %12252 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12254 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12255 = torch.aten.view %12253, %12254 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %12256 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12257 = torch.aten.transpose.int %12256, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %12258 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12259 = torch.prims.convert_element_type %12258, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12260 = torch.prims.convert_element_type %12255, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12261 = torch.prims.convert_element_type %12257, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12262 = torch.aten.mm %12260, %12261 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12263 = torch.aten.mul.Scalar %12262, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12264 = torch.aten.mul.Scalar %12259, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12265 = torch.aten.add.Tensor %12263, %12264, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12266 = torch.prims.convert_element_type %12265, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12267 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12268 = torch.aten.view %12266, %12267 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12269 = torch.aten.add.Tensor %12268, %12221, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12270 = torch.prims.convert_element_type %12269, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12271 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_371, %result1_372 = torch.aten.var_mean.correction %12270, %12271, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12272 = torch.aten.add.Scalar %result0_371, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12273 = torch.aten.rsqrt %12272 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12274 = torch.aten.sub.Tensor %12269, %result1_372, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12275 = torch.aten.mul.Tensor %12274, %12273 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %12276 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12277 = torch.aten.mul.Tensor %12275, %12276 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %12278 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12279 = torch.aten.add.Tensor %12277, %12278, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12280 = torch.prims.convert_element_type %12279, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12281 = torch.prims.convert_element_type %result1_372, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12282 = torch.prims.convert_element_type %12273, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %12283 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12284 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12285 = torch.aten.view %12280, %12284 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12286 = torch_c.to_builtin_tensor %12285 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12287 = torch_c.to_builtin_tensor %12283 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12288 = tensor.empty() : tensor<2048x1280xf32>
    %12289 = linalg.fill ins(%cst : f32) outs(%12288 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12290 = tensor.empty() : tensor<2048x1280xf32>
    %12291 = linalg.fill ins(%cst : f32) outs(%12290 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12292:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12289, %12291, %12286, %12287, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12289, %12291)
    %12293 = arith.truncf %12292#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12294 = torch_c.from_builtin_tensor %12293 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12295 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12296 = torch.aten.view %12294, %12295 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %12297 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12298 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12299 = torch.aten.view %12280, %12298 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12300 = torch_c.to_builtin_tensor %12299 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12301 = torch_c.to_builtin_tensor %12297 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12302 = tensor.empty() : tensor<2048x1280xf32>
    %12303 = linalg.fill ins(%cst : f32) outs(%12302 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12304 = tensor.empty() : tensor<2048x1280xf32>
    %12305 = linalg.fill ins(%cst : f32) outs(%12304 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12306:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12303, %12305, %12300, %12301, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12303, %12305)
    %12307 = arith.truncf %12306#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12308 = torch_c.from_builtin_tensor %12307 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12309 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12310 = torch.aten.view %12308, %12309 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %12311 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12312 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12313 = torch.aten.view %12280, %12312 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12314 = torch_c.to_builtin_tensor %12313 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12315 = torch_c.to_builtin_tensor %12311 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12316 = tensor.empty() : tensor<2048x1280xf32>
    %12317 = linalg.fill ins(%cst : f32) outs(%12316 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12318 = tensor.empty() : tensor<2048x1280xf32>
    %12319 = linalg.fill ins(%cst : f32) outs(%12318 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12320:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12317, %12319, %12314, %12315, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12317, %12319)
    %12321 = arith.truncf %12320#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12322 = torch_c.from_builtin_tensor %12321 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12323 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12324 = torch.aten.view %12322, %12323 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12325 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12326 = torch.aten.view %12296, %12325 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12327 = torch.aten.transpose.int %12326, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12328 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12329 = torch.aten.view %12310, %12328 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12330 = torch.aten.transpose.int %12329, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12331 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12332 = torch.aten.view %12324, %12331 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12333 = torch.aten.transpose.int %12332, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12334:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12327, %12330, %12333, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12335 = torch.aten.transpose.int %12334#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12336 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12337 = torch.aten.view %12335, %12336 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12338 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12339 = torch.aten.view %12337, %12338 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12340 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12341 = torch.aten.transpose.int %12340, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %12342 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12343 = torch.prims.convert_element_type %12342, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12344 = torch.prims.convert_element_type %12339, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12345 = torch.prims.convert_element_type %12341, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12346 = torch.aten.mm %12344, %12345 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12347 = torch.aten.mul.Scalar %12346, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12348 = torch.aten.mul.Scalar %12343, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12349 = torch.aten.add.Tensor %12347, %12348, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12350 = torch.prims.convert_element_type %12349, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12351 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12352 = torch.aten.view %12350, %12351 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12353 = torch.aten.div.Scalar %12352, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12354 = torch.aten.add.Tensor %12353, %12269, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12355 = torch.prims.convert_element_type %12354, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12356 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_373, %result1_374 = torch.aten.var_mean.correction %12355, %12356, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12357 = torch.aten.add.Scalar %result0_373, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12358 = torch.aten.rsqrt %12357 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12359 = torch.aten.sub.Tensor %12354, %result1_374, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12360 = torch.aten.mul.Tensor %12359, %12358 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %12361 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12362 = torch.aten.mul.Tensor %12360, %12361 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %12363 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12364 = torch.aten.add.Tensor %12362, %12363, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12365 = torch.prims.convert_element_type %12364, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12366 = torch.prims.convert_element_type %result1_374, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12367 = torch.prims.convert_element_type %12358, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %12368 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12369 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12370 = torch.aten.view %12365, %12369 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12371 = torch_c.to_builtin_tensor %12370 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12372 = torch_c.to_builtin_tensor %12368 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12373 = tensor.empty() : tensor<2048x1280xf32>
    %12374 = linalg.fill ins(%cst : f32) outs(%12373 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12375 = tensor.empty() : tensor<2048x1280xf32>
    %12376 = linalg.fill ins(%cst : f32) outs(%12375 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12377:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12374, %12376, %12371, %12372, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12374, %12376)
    %12378 = arith.truncf %12377#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12379 = torch_c.from_builtin_tensor %12378 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12380 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12381 = torch.aten.view %12379, %12380 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %12382 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12383 = torch.aten.transpose.int %12382, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12384 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12385 = torch.aten.view %4, %12384 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12386 = torch.aten.mm %12385, %12383 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12387 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12388 = torch.aten.view %12386, %12387 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %12389 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12390 = torch.aten.transpose.int %12389, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12391 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12392 = torch.aten.view %4, %12391 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12393 = torch.aten.mm %12392, %12390 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12394 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12395 = torch.aten.view %12393, %12394 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12396 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12397 = torch.aten.view %12381, %12396 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12398 = torch.aten.transpose.int %12397, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12399 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12400 = torch.aten.view %12388, %12399 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12401 = torch.aten.transpose.int %12400, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12402 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12403 = torch.aten.view %12395, %12402 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12404 = torch.aten.transpose.int %12403, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12405:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12398, %12401, %12404, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12406 = torch.aten.transpose.int %12405#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12407 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12408 = torch.aten.view %12406, %12407 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12409 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12410 = torch.aten.view %12408, %12409 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12411 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12412 = torch.aten.transpose.int %12411, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %12413 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12414 = torch.prims.convert_element_type %12413, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12415 = torch.prims.convert_element_type %12410, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12416 = torch.prims.convert_element_type %12412, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12417 = torch.aten.mm %12415, %12416 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12418 = torch.aten.mul.Scalar %12417, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12419 = torch.aten.mul.Scalar %12414, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12420 = torch.aten.add.Tensor %12418, %12419, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12421 = torch.prims.convert_element_type %12420, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12422 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12423 = torch.aten.view %12421, %12422 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12424 = torch.aten.div.Scalar %12423, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12425 = torch.aten.add.Tensor %12424, %12354, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12426 = torch.prims.convert_element_type %12425, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12427 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_375, %result1_376 = torch.aten.var_mean.correction %12426, %12427, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12428 = torch.aten.add.Scalar %result0_375, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12429 = torch.aten.rsqrt %12428 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12430 = torch.aten.sub.Tensor %12425, %result1_376, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12431 = torch.aten.mul.Tensor %12430, %12429 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %12432 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12433 = torch.aten.mul.Tensor %12431, %12432 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %12434 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12435 = torch.aten.add.Tensor %12433, %12434, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12436 = torch.prims.convert_element_type %12435, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12437 = torch.prims.convert_element_type %result1_376, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12438 = torch.prims.convert_element_type %12429, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12439 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12440 = torch.aten.view %12436, %12439 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12441 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12442 = torch.aten.transpose.int %12441, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %12443 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12444 = torch.prims.convert_element_type %12443, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12445 = torch.prims.convert_element_type %12440, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12446 = torch.prims.convert_element_type %12442, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12447 = torch.aten.mm %12445, %12446 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12448 = torch.aten.mul.Scalar %12447, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12449 = torch.aten.mul.Scalar %12444, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12450 = torch.aten.add.Tensor %12448, %12449, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12451 = torch.prims.convert_element_type %12450, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12452 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12453 = torch.aten.view %12451, %12452 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12454 = torch.aten.slice.Tensor %12453, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12455 = torch.aten.slice.Tensor %12453, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12456 = torch.aten.gelu %12455, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12457 = torch.aten.mul.Tensor %12454, %12456 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12458 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12459 = torch.aten.view %12457, %12458 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %12460 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12461 = torch.aten.transpose.int %12460, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %12462 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12463 = torch.prims.convert_element_type %12462, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12464 = torch.prims.convert_element_type %12459, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12465 = torch.prims.convert_element_type %12461, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12466 = torch.aten.mm %12464, %12465 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12467 = torch.aten.mul.Scalar %12466, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12468 = torch.aten.mul.Scalar %12463, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12469 = torch.aten.add.Tensor %12467, %12468, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12470 = torch.prims.convert_element_type %12469, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12471 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12472 = torch.aten.view %12470, %12471 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12473 = torch.aten.add.Tensor %12472, %12425, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12474 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12475 = torch.aten.view %12473, %12474 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_out.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_out.weight : tensor<1280x1280xf16>
    %12476 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12477 = torch.aten.transpose.int %12476, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_out.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_out.bias : tensor<1280xf16>
    %12478 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12479 = torch.prims.convert_element_type %12478, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12480 = torch.prims.convert_element_type %12475, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12481 = torch.prims.convert_element_type %12477, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12482 = torch.aten.mm %12480, %12481 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12483 = torch.aten.mul.Scalar %12482, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12484 = torch.aten.mul.Scalar %12479, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12485 = torch.aten.add.Tensor %12483, %12484, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12486 = torch.prims.convert_element_type %12485, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12487 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12488 = torch.aten.view %12486, %12487 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12489 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12490 = torch.aten.view %12488, %12489 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %12491 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12492 = torch.aten.permute %12490, %12491 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %12493 = torch.aten.add.Tensor %12492, %10382, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %12494 = torch.prim.ListConstruct %12493, %1343 : (!torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,640,32,32],f16>) -> !torch.list<vtensor>
    %12495 = torch.aten.cat %12494, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,1920,32,32],f16>
    %12496 = torch.prim.ListConstruct %int2, %int32, %int60, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12497 = torch.aten.view %12495, %12496 : !torch.vtensor<[2,1920,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,60,1024],f16>
    %12498 = torch.prims.convert_element_type %12497, %int6 : !torch.vtensor<[2,32,60,1024],f16>, !torch.int -> !torch.vtensor<[2,32,60,1024],f32>
    %12499 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_377, %result1_378 = torch.aten.var_mean.correction %12498, %12499, %int0, %true : !torch.vtensor<[2,32,60,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %12500 = torch.aten.add.Scalar %result0_377, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %12501 = torch.aten.rsqrt %12500 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %12502 = torch.aten.sub.Tensor %12497, %result1_378, %int1 : !torch.vtensor<[2,32,60,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,60,1024],f32>
    %12503 = torch.aten.mul.Tensor %12502, %12501 : !torch.vtensor<[2,32,60,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,60,1024],f32>
    %12504 = torch.prim.ListConstruct %int2, %int1920, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12505 = torch.aten.view %12503, %12504 : !torch.vtensor<[2,32,60,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1920,32,32],f32>
    %_params.unet.up_blocks.0.resnets.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.norm1.bias : tensor<1920xf16>
    %12506 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm1.bias : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %12507 = torch.aten.unsqueeze %12506, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %12508 = torch.aten.unsqueeze %12507, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %12509 = torch.aten.unsqueeze %12508, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %_params.unet.up_blocks.0.resnets.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.norm1.weight : tensor<1920xf16>
    %12510 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm1.weight : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %12511 = torch.aten.unsqueeze %12510, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %12512 = torch.aten.unsqueeze %12511, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %12513 = torch.aten.unsqueeze %12512, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %12514 = torch.aten.mul.Tensor %12505, %12513 : !torch.vtensor<[2,1920,32,32],f32>, !torch.vtensor<[1,1920,1,1],f16> -> !torch.vtensor<[2,1920,32,32],f32>
    %12515 = torch.aten.add.Tensor %12514, %12509, %int1 : !torch.vtensor<[2,1920,32,32],f32>, !torch.vtensor<[1,1920,1,1],f16>, !torch.int -> !torch.vtensor<[2,1920,32,32],f32>
    %12516 = torch.prims.convert_element_type %12515, %int5 : !torch.vtensor<[2,1920,32,32],f32>, !torch.int -> !torch.vtensor<[2,1920,32,32],f16>
    %12517 = torch.prims.convert_element_type %result1_378, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %12518 = torch.prims.convert_element_type %12501, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %12519 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %12520 = torch.prims.squeeze %12517, %12519 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %12521 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %12522 = torch.prims.squeeze %12520, %12521 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %12523 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %12524 = torch.prims.squeeze %12518, %12523 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %12525 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %12526 = torch.prims.squeeze %12524, %12525 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %12527 = torch.aten.silu %12516 : !torch.vtensor<[2,1920,32,32],f16> -> !torch.vtensor<[2,1920,32,32],f16>
    %_params.unet.up_blocks.0.resnets.2.conv1.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.conv1.weight : tensor<1280x1920x3x3xf16>
    %12528 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv1.weight : tensor<1280x1920x3x3xf16> -> !torch.vtensor<[1280,1920,3,3],f16>
    %_params.unet.up_blocks.0.resnets.2.conv1.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.conv1.bias : tensor<1280xf16>
    %12529 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12530 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12531 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12532 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12533 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %12534 = torch.aten.convolution %12527, %12528, %12529, %12530, %12531, %12532, %false, %12533, %int1 : !torch.vtensor<[2,1920,32,32],f16>, !torch.vtensor<[1280,1920,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %12535 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight : tensor<1280x1280xf16>
    %12536 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12537 = torch.aten.transpose.int %12536, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias : tensor<1280xf16>
    %12538 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12539 = torch.prims.convert_element_type %12538, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12540 = torch.prims.convert_element_type %12535, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %12541 = torch.prims.convert_element_type %12537, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12542 = torch.aten.mm %12540, %12541 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %12543 = torch.aten.mul.Scalar %12542, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %12544 = torch.aten.mul.Scalar %12539, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12545 = torch.aten.add.Tensor %12543, %12544, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %12546 = torch.prims.convert_element_type %12545, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %12547 = torch.aten.unsqueeze %12546, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %12548 = torch.aten.unsqueeze %12547, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %12549 = torch.aten.add.Tensor %12534, %12548, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %12550 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12551 = torch.aten.view %12549, %12550 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %12552 = torch.prims.convert_element_type %12551, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %12553 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_379, %result1_380 = torch.aten.var_mean.correction %12552, %12553, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %12554 = torch.aten.add.Scalar %result0_379, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %12555 = torch.aten.rsqrt %12554 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %12556 = torch.aten.sub.Tensor %12551, %result1_380, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %12557 = torch.aten.mul.Tensor %12556, %12555 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %12558 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12559 = torch.aten.view %12557, %12558 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.resnets.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.norm2.bias : tensor<1280xf16>
    %12560 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12561 = torch.aten.unsqueeze %12560, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %12562 = torch.aten.unsqueeze %12561, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %12563 = torch.aten.unsqueeze %12562, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.resnets.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.norm2.weight : tensor<1280xf16>
    %12564 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12565 = torch.aten.unsqueeze %12564, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %12566 = torch.aten.unsqueeze %12565, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %12567 = torch.aten.unsqueeze %12566, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %12568 = torch.aten.mul.Tensor %12559, %12567 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %12569 = torch.aten.add.Tensor %12568, %12563, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %12570 = torch.prims.convert_element_type %12569, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %12571 = torch.prims.convert_element_type %result1_380, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %12572 = torch.prims.convert_element_type %12555, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %12573 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %12574 = torch.prims.squeeze %12571, %12573 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %12575 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %12576 = torch.prims.squeeze %12574, %12575 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %12577 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %12578 = torch.prims.squeeze %12572, %12577 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %12579 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %12580 = torch.prims.squeeze %12578, %12579 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %12581 = torch.aten.silu %12570 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.2.conv2.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.conv2.weight : tensor<1280x1280x3x3xf16>
    %12582 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.resnets.2.conv2.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.conv2.bias : tensor<1280xf16>
    %12583 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12584 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12585 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12586 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12587 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %12588 = torch.aten.convolution %12581, %12582, %12583, %12584, %12585, %12586, %false, %12587, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight : tensor<1280x1920x1x1xf16>
    %12589 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight : tensor<1280x1920x1x1xf16> -> !torch.vtensor<[1280,1920,1,1],f16>
    %_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias : tensor<1280xf16>
    %12590 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12591 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12592 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %12593 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %12594 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %12595 = torch.aten.convolution %12495, %12589, %12590, %12591, %12592, %12593, %false, %12594, %int1 : !torch.vtensor<[2,1920,32,32],f16>, !torch.vtensor<[1280,1920,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %12596 = torch.aten.add.Tensor %12595, %12588, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %12597 = torch.aten.div.Scalar %12596, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %12598 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12599 = torch.aten.view %12597, %12598 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %12600 = torch.prims.convert_element_type %12599, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %12601 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_381, %result1_382 = torch.aten.var_mean.correction %12600, %12601, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %12602 = torch.aten.add.Scalar %result0_381, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %12603 = torch.aten.rsqrt %12602 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %12604 = torch.aten.sub.Tensor %12599, %result1_382, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %12605 = torch.aten.mul.Tensor %12604, %12603 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %12606 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12607 = torch.aten.view %12605, %12606 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.attentions.2.norm.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.norm.bias : tensor<1280xf16>
    %12608 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12609 = torch.aten.unsqueeze %12608, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %12610 = torch.aten.unsqueeze %12609, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %12611 = torch.aten.unsqueeze %12610, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.attentions.2.norm.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.norm.weight : tensor<1280xf16>
    %12612 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12613 = torch.aten.unsqueeze %12612, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %12614 = torch.aten.unsqueeze %12613, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %12615 = torch.aten.unsqueeze %12614, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %12616 = torch.aten.mul.Tensor %12607, %12615 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %12617 = torch.aten.add.Tensor %12616, %12611, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %12618 = torch.prims.convert_element_type %12617, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %12619 = torch.prims.convert_element_type %result1_382, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %12620 = torch.prims.convert_element_type %12603, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %12621 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %12622 = torch.prims.squeeze %12619, %12621 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %12623 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %12624 = torch.prims.squeeze %12622, %12623 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %12625 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %12626 = torch.prims.squeeze %12620, %12625 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %12627 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %12628 = torch.prims.squeeze %12626, %12627 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %12629 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12630 = torch.aten.permute %12618, %12629 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %12631 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12632 = torch.aten.view %12630, %12631 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_in.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_in.weight : tensor<1280x1280xf16>
    %12633 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12634 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12635 = torch.aten._unsafe_view %12632, %12634 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12636 = torch_c.to_builtin_tensor %12635 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12637 = torch_c.to_builtin_tensor %12633 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12638 = tensor.empty() : tensor<2048x1280xf32>
    %12639 = linalg.fill ins(%cst : f32) outs(%12638 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12640 = tensor.empty() : tensor<2048x1280xf32>
    %12641 = linalg.fill ins(%cst : f32) outs(%12640 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12642:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12639, %12641, %12636, %12637, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12639, %12641)
    %12643 = arith.truncf %12642#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12644 = torch_c.from_builtin_tensor %12643 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12645 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12646 = torch.aten.view %12644, %12645 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_in.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_in.bias : tensor<1280xf16>
    %12647 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12648 = torch.aten.add.Tensor %12646, %12647, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12649 = torch.prims.convert_element_type %12648, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12650 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_383, %result1_384 = torch.aten.var_mean.correction %12649, %12650, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12651 = torch.aten.add.Scalar %result0_383, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12652 = torch.aten.rsqrt %12651 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12653 = torch.aten.sub.Tensor %12648, %result1_384, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12654 = torch.aten.mul.Tensor %12653, %12652 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %12655 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12656 = torch.aten.mul.Tensor %12654, %12655 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %12657 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12658 = torch.aten.add.Tensor %12656, %12657, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12659 = torch.prims.convert_element_type %12658, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12660 = torch.prims.convert_element_type %result1_384, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12661 = torch.prims.convert_element_type %12652, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %12662 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12663 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12664 = torch.aten.view %12659, %12663 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12665 = torch_c.to_builtin_tensor %12664 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12666 = torch_c.to_builtin_tensor %12662 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12667 = tensor.empty() : tensor<2048x1280xf32>
    %12668 = linalg.fill ins(%cst : f32) outs(%12667 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12669 = tensor.empty() : tensor<2048x1280xf32>
    %12670 = linalg.fill ins(%cst : f32) outs(%12669 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12671:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12668, %12670, %12665, %12666, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12668, %12670)
    %12672 = arith.truncf %12671#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12673 = torch_c.from_builtin_tensor %12672 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12674 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12675 = torch.aten.view %12673, %12674 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %12676 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12677 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12678 = torch.aten.view %12659, %12677 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12679 = torch_c.to_builtin_tensor %12678 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12680 = torch_c.to_builtin_tensor %12676 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12681 = tensor.empty() : tensor<2048x1280xf32>
    %12682 = linalg.fill ins(%cst : f32) outs(%12681 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12683 = tensor.empty() : tensor<2048x1280xf32>
    %12684 = linalg.fill ins(%cst : f32) outs(%12683 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12685:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12682, %12684, %12679, %12680, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12682, %12684)
    %12686 = arith.truncf %12685#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12687 = torch_c.from_builtin_tensor %12686 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12688 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12689 = torch.aten.view %12687, %12688 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %12690 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12691 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12692 = torch.aten.view %12659, %12691 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12693 = torch_c.to_builtin_tensor %12692 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12694 = torch_c.to_builtin_tensor %12690 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12695 = tensor.empty() : tensor<2048x1280xf32>
    %12696 = linalg.fill ins(%cst : f32) outs(%12695 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12697 = tensor.empty() : tensor<2048x1280xf32>
    %12698 = linalg.fill ins(%cst : f32) outs(%12697 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12699:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12696, %12698, %12693, %12694, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12696, %12698)
    %12700 = arith.truncf %12699#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12701 = torch_c.from_builtin_tensor %12700 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12702 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12703 = torch.aten.view %12701, %12702 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12704 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12705 = torch.aten.view %12675, %12704 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12706 = torch.aten.transpose.int %12705, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12707 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12708 = torch.aten.view %12689, %12707 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12709 = torch.aten.transpose.int %12708, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12710 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12711 = torch.aten.view %12703, %12710 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12712 = torch.aten.transpose.int %12711, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12713:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12706, %12709, %12712, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12714 = torch.aten.transpose.int %12713#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12715 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12716 = torch.aten.view %12714, %12715 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12717 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12718 = torch.aten.view %12716, %12717 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12719 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12720 = torch.aten.transpose.int %12719, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %12721 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12722 = torch.prims.convert_element_type %12721, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12723 = torch.prims.convert_element_type %12718, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12724 = torch.prims.convert_element_type %12720, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12725 = torch.aten.mm %12723, %12724 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12726 = torch.aten.mul.Scalar %12725, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12727 = torch.aten.mul.Scalar %12722, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12728 = torch.aten.add.Tensor %12726, %12727, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12729 = torch.prims.convert_element_type %12728, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12730 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12731 = torch.aten.view %12729, %12730 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12732 = torch.aten.div.Scalar %12731, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12733 = torch.aten.add.Tensor %12732, %12648, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12734 = torch.prims.convert_element_type %12733, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12735 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_385, %result1_386 = torch.aten.var_mean.correction %12734, %12735, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12736 = torch.aten.add.Scalar %result0_385, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12737 = torch.aten.rsqrt %12736 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12738 = torch.aten.sub.Tensor %12733, %result1_386, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12739 = torch.aten.mul.Tensor %12738, %12737 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %12740 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12741 = torch.aten.mul.Tensor %12739, %12740 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %12742 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12743 = torch.aten.add.Tensor %12741, %12742, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12744 = torch.prims.convert_element_type %12743, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12745 = torch.prims.convert_element_type %result1_386, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12746 = torch.prims.convert_element_type %12737, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %12747 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12748 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12749 = torch.aten.view %12744, %12748 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12750 = torch_c.to_builtin_tensor %12749 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12751 = torch_c.to_builtin_tensor %12747 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12752 = tensor.empty() : tensor<2048x1280xf32>
    %12753 = linalg.fill ins(%cst : f32) outs(%12752 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12754 = tensor.empty() : tensor<2048x1280xf32>
    %12755 = linalg.fill ins(%cst : f32) outs(%12754 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12756:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12753, %12755, %12750, %12751, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12753, %12755)
    %12757 = arith.truncf %12756#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12758 = torch_c.from_builtin_tensor %12757 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12759 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12760 = torch.aten.view %12758, %12759 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %12761 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12762 = torch.aten.transpose.int %12761, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12763 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12764 = torch.aten.view %4, %12763 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12765 = torch.aten.mm %12764, %12762 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12766 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12767 = torch.aten.view %12765, %12766 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %12768 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12769 = torch.aten.transpose.int %12768, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12770 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12771 = torch.aten.view %4, %12770 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12772 = torch.aten.mm %12771, %12769 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12773 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12774 = torch.aten.view %12772, %12773 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12775 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12776 = torch.aten.view %12760, %12775 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12777 = torch.aten.transpose.int %12776, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12778 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12779 = torch.aten.view %12767, %12778 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12780 = torch.aten.transpose.int %12779, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12781 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12782 = torch.aten.view %12774, %12781 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12783 = torch.aten.transpose.int %12782, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12784:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12777, %12780, %12783, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12785 = torch.aten.transpose.int %12784#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12786 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12787 = torch.aten.view %12785, %12786 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12788 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12789 = torch.aten.view %12787, %12788 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12790 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12791 = torch.aten.transpose.int %12790, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %12792 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12793 = torch.prims.convert_element_type %12792, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12794 = torch.prims.convert_element_type %12789, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12795 = torch.prims.convert_element_type %12791, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12796 = torch.aten.mm %12794, %12795 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12797 = torch.aten.mul.Scalar %12796, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12798 = torch.aten.mul.Scalar %12793, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12799 = torch.aten.add.Tensor %12797, %12798, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12800 = torch.prims.convert_element_type %12799, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12801 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12802 = torch.aten.view %12800, %12801 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12803 = torch.aten.div.Scalar %12802, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12804 = torch.aten.add.Tensor %12803, %12733, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12805 = torch.prims.convert_element_type %12804, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12806 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_387, %result1_388 = torch.aten.var_mean.correction %12805, %12806, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12807 = torch.aten.add.Scalar %result0_387, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12808 = torch.aten.rsqrt %12807 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12809 = torch.aten.sub.Tensor %12804, %result1_388, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12810 = torch.aten.mul.Tensor %12809, %12808 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %12811 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12812 = torch.aten.mul.Tensor %12810, %12811 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %12813 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12814 = torch.aten.add.Tensor %12812, %12813, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12815 = torch.prims.convert_element_type %12814, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12816 = torch.prims.convert_element_type %result1_388, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12817 = torch.prims.convert_element_type %12808, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12818 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12819 = torch.aten.view %12815, %12818 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12820 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12821 = torch.aten.transpose.int %12820, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %12822 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12823 = torch.prims.convert_element_type %12822, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12824 = torch.prims.convert_element_type %12819, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12825 = torch.prims.convert_element_type %12821, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12826 = torch.aten.mm %12824, %12825 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12827 = torch.aten.mul.Scalar %12826, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12828 = torch.aten.mul.Scalar %12823, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12829 = torch.aten.add.Tensor %12827, %12828, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12830 = torch.prims.convert_element_type %12829, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12831 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12832 = torch.aten.view %12830, %12831 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12833 = torch.aten.slice.Tensor %12832, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12834 = torch.aten.slice.Tensor %12832, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12835 = torch.aten.gelu %12834, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12836 = torch.aten.mul.Tensor %12833, %12835 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12837 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12838 = torch.aten.view %12836, %12837 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %12839 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12840 = torch.aten.transpose.int %12839, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %12841 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12842 = torch.prims.convert_element_type %12841, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12843 = torch.prims.convert_element_type %12838, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12844 = torch.prims.convert_element_type %12840, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12845 = torch.aten.mm %12843, %12844 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12846 = torch.aten.mul.Scalar %12845, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12847 = torch.aten.mul.Scalar %12842, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12848 = torch.aten.add.Tensor %12846, %12847, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12849 = torch.prims.convert_element_type %12848, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12850 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12851 = torch.aten.view %12849, %12850 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12852 = torch.aten.add.Tensor %12851, %12804, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12853 = torch.prims.convert_element_type %12852, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12854 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_389, %result1_390 = torch.aten.var_mean.correction %12853, %12854, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12855 = torch.aten.add.Scalar %result0_389, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12856 = torch.aten.rsqrt %12855 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12857 = torch.aten.sub.Tensor %12852, %result1_390, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12858 = torch.aten.mul.Tensor %12857, %12856 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %12859 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12860 = torch.aten.mul.Tensor %12858, %12859 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %12861 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12862 = torch.aten.add.Tensor %12860, %12861, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12863 = torch.prims.convert_element_type %12862, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12864 = torch.prims.convert_element_type %result1_390, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12865 = torch.prims.convert_element_type %12856, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %12866 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12867 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12868 = torch.aten.view %12863, %12867 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12869 = torch_c.to_builtin_tensor %12868 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12870 = torch_c.to_builtin_tensor %12866 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12871 = tensor.empty() : tensor<2048x1280xf32>
    %12872 = linalg.fill ins(%cst : f32) outs(%12871 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12873 = tensor.empty() : tensor<2048x1280xf32>
    %12874 = linalg.fill ins(%cst : f32) outs(%12873 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12875:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12872, %12874, %12869, %12870, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12872, %12874)
    %12876 = arith.truncf %12875#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12877 = torch_c.from_builtin_tensor %12876 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12878 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12879 = torch.aten.view %12877, %12878 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %12880 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12881 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12882 = torch.aten.view %12863, %12881 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12883 = torch_c.to_builtin_tensor %12882 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12884 = torch_c.to_builtin_tensor %12880 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12885 = tensor.empty() : tensor<2048x1280xf32>
    %12886 = linalg.fill ins(%cst : f32) outs(%12885 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12887 = tensor.empty() : tensor<2048x1280xf32>
    %12888 = linalg.fill ins(%cst : f32) outs(%12887 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12889:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12886, %12888, %12883, %12884, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12886, %12888)
    %12890 = arith.truncf %12889#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12891 = torch_c.from_builtin_tensor %12890 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12892 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12893 = torch.aten.view %12891, %12892 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %12894 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12895 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12896 = torch.aten.view %12863, %12895 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12897 = torch_c.to_builtin_tensor %12896 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12898 = torch_c.to_builtin_tensor %12894 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12899 = tensor.empty() : tensor<2048x1280xf32>
    %12900 = linalg.fill ins(%cst : f32) outs(%12899 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12901 = tensor.empty() : tensor<2048x1280xf32>
    %12902 = linalg.fill ins(%cst : f32) outs(%12901 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12903:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12900, %12902, %12897, %12898, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12900, %12902)
    %12904 = arith.truncf %12903#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12905 = torch_c.from_builtin_tensor %12904 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12906 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12907 = torch.aten.view %12905, %12906 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12908 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12909 = torch.aten.view %12879, %12908 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12910 = torch.aten.transpose.int %12909, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12911 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12912 = torch.aten.view %12893, %12911 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12913 = torch.aten.transpose.int %12912, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12914 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12915 = torch.aten.view %12907, %12914 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12916 = torch.aten.transpose.int %12915, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12917:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12910, %12913, %12916, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12918 = torch.aten.transpose.int %12917#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12919 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12920 = torch.aten.view %12918, %12919 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12921 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12922 = torch.aten.view %12920, %12921 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12923 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12924 = torch.aten.transpose.int %12923, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %12925 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12926 = torch.prims.convert_element_type %12925, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12927 = torch.prims.convert_element_type %12922, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12928 = torch.prims.convert_element_type %12924, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12929 = torch.aten.mm %12927, %12928 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12930 = torch.aten.mul.Scalar %12929, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12931 = torch.aten.mul.Scalar %12926, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12932 = torch.aten.add.Tensor %12930, %12931, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12933 = torch.prims.convert_element_type %12932, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12934 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12935 = torch.aten.view %12933, %12934 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12936 = torch.aten.div.Scalar %12935, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12937 = torch.aten.add.Tensor %12936, %12852, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12938 = torch.prims.convert_element_type %12937, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12939 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_391, %result1_392 = torch.aten.var_mean.correction %12938, %12939, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12940 = torch.aten.add.Scalar %result0_391, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12941 = torch.aten.rsqrt %12940 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12942 = torch.aten.sub.Tensor %12937, %result1_392, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12943 = torch.aten.mul.Tensor %12942, %12941 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %12944 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12945 = torch.aten.mul.Tensor %12943, %12944 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %12946 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12947 = torch.aten.add.Tensor %12945, %12946, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12948 = torch.prims.convert_element_type %12947, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12949 = torch.prims.convert_element_type %result1_392, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12950 = torch.prims.convert_element_type %12941, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %12951 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12952 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12953 = torch.aten.view %12948, %12952 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12954 = torch_c.to_builtin_tensor %12953 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12955 = torch_c.to_builtin_tensor %12951 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12956 = tensor.empty() : tensor<2048x1280xf32>
    %12957 = linalg.fill ins(%cst : f32) outs(%12956 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12958 = tensor.empty() : tensor<2048x1280xf32>
    %12959 = linalg.fill ins(%cst : f32) outs(%12958 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12960:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12957, %12959, %12954, %12955, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12957, %12959)
    %12961 = arith.truncf %12960#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12962 = torch_c.from_builtin_tensor %12961 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12963 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12964 = torch.aten.view %12962, %12963 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %12965 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12966 = torch.aten.transpose.int %12965, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12967 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12968 = torch.aten.view %4, %12967 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12969 = torch.aten.mm %12968, %12966 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12970 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12971 = torch.aten.view %12969, %12970 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %12972 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12973 = torch.aten.transpose.int %12972, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12974 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12975 = torch.aten.view %4, %12974 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12976 = torch.aten.mm %12975, %12973 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %12977 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12978 = torch.aten.view %12976, %12977 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12979 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12980 = torch.aten.view %12964, %12979 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12981 = torch.aten.transpose.int %12980, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12982 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12983 = torch.aten.view %12971, %12982 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12984 = torch.aten.transpose.int %12983, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12985 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12986 = torch.aten.view %12978, %12985 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12987 = torch.aten.transpose.int %12986, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12988:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12981, %12984, %12987, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12989 = torch.aten.transpose.int %12988#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12990 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12991 = torch.aten.view %12989, %12990 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12992 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12993 = torch.aten.view %12991, %12992 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12994 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12995 = torch.aten.transpose.int %12994, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %12996 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12997 = torch.prims.convert_element_type %12996, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12998 = torch.prims.convert_element_type %12993, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12999 = torch.prims.convert_element_type %12995, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13000 = torch.aten.mm %12998, %12999 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13001 = torch.aten.mul.Scalar %13000, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13002 = torch.aten.mul.Scalar %12997, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13003 = torch.aten.add.Tensor %13001, %13002, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13004 = torch.prims.convert_element_type %13003, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13005 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13006 = torch.aten.view %13004, %13005 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13007 = torch.aten.div.Scalar %13006, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13008 = torch.aten.add.Tensor %13007, %12937, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13009 = torch.prims.convert_element_type %13008, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13010 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_393, %result1_394 = torch.aten.var_mean.correction %13009, %13010, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13011 = torch.aten.add.Scalar %result0_393, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13012 = torch.aten.rsqrt %13011 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13013 = torch.aten.sub.Tensor %13008, %result1_394, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13014 = torch.aten.mul.Tensor %13013, %13012 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %13015 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13016 = torch.aten.mul.Tensor %13014, %13015 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %13017 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13018 = torch.aten.add.Tensor %13016, %13017, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13019 = torch.prims.convert_element_type %13018, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13020 = torch.prims.convert_element_type %result1_394, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13021 = torch.prims.convert_element_type %13012, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13022 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13023 = torch.aten.view %13019, %13022 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13024 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13025 = torch.aten.transpose.int %13024, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %13026 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13027 = torch.prims.convert_element_type %13026, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13028 = torch.prims.convert_element_type %13023, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13029 = torch.prims.convert_element_type %13025, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13030 = torch.aten.mm %13028, %13029 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13031 = torch.aten.mul.Scalar %13030, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13032 = torch.aten.mul.Scalar %13027, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13033 = torch.aten.add.Tensor %13031, %13032, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13034 = torch.prims.convert_element_type %13033, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13035 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13036 = torch.aten.view %13034, %13035 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13037 = torch.aten.slice.Tensor %13036, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13038 = torch.aten.slice.Tensor %13036, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13039 = torch.aten.gelu %13038, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13040 = torch.aten.mul.Tensor %13037, %13039 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13041 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13042 = torch.aten.view %13040, %13041 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %13043 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13044 = torch.aten.transpose.int %13043, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %13045 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13046 = torch.prims.convert_element_type %13045, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13047 = torch.prims.convert_element_type %13042, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13048 = torch.prims.convert_element_type %13044, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13049 = torch.aten.mm %13047, %13048 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13050 = torch.aten.mul.Scalar %13049, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13051 = torch.aten.mul.Scalar %13046, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13052 = torch.aten.add.Tensor %13050, %13051, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13053 = torch.prims.convert_element_type %13052, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13054 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13055 = torch.aten.view %13053, %13054 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13056 = torch.aten.add.Tensor %13055, %13008, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13057 = torch.prims.convert_element_type %13056, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13058 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_395, %result1_396 = torch.aten.var_mean.correction %13057, %13058, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13059 = torch.aten.add.Scalar %result0_395, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13060 = torch.aten.rsqrt %13059 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13061 = torch.aten.sub.Tensor %13056, %result1_396, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13062 = torch.aten.mul.Tensor %13061, %13060 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %13063 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13064 = torch.aten.mul.Tensor %13062, %13063 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %13065 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13066 = torch.aten.add.Tensor %13064, %13065, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13067 = torch.prims.convert_element_type %13066, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13068 = torch.prims.convert_element_type %result1_396, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13069 = torch.prims.convert_element_type %13060, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %13070 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13071 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13072 = torch.aten.view %13067, %13071 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13073 = torch_c.to_builtin_tensor %13072 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13074 = torch_c.to_builtin_tensor %13070 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13075 = tensor.empty() : tensor<2048x1280xf32>
    %13076 = linalg.fill ins(%cst : f32) outs(%13075 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13077 = tensor.empty() : tensor<2048x1280xf32>
    %13078 = linalg.fill ins(%cst : f32) outs(%13077 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13079:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13076, %13078, %13073, %13074, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13076, %13078)
    %13080 = arith.truncf %13079#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13081 = torch_c.from_builtin_tensor %13080 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13082 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13083 = torch.aten.view %13081, %13082 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %13084 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13085 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13086 = torch.aten.view %13067, %13085 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13087 = torch_c.to_builtin_tensor %13086 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13088 = torch_c.to_builtin_tensor %13084 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13089 = tensor.empty() : tensor<2048x1280xf32>
    %13090 = linalg.fill ins(%cst : f32) outs(%13089 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13091 = tensor.empty() : tensor<2048x1280xf32>
    %13092 = linalg.fill ins(%cst : f32) outs(%13091 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13093:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13090, %13092, %13087, %13088, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13090, %13092)
    %13094 = arith.truncf %13093#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13095 = torch_c.from_builtin_tensor %13094 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13096 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13097 = torch.aten.view %13095, %13096 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %13098 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13099 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13100 = torch.aten.view %13067, %13099 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13101 = torch_c.to_builtin_tensor %13100 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13102 = torch_c.to_builtin_tensor %13098 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13103 = tensor.empty() : tensor<2048x1280xf32>
    %13104 = linalg.fill ins(%cst : f32) outs(%13103 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13105 = tensor.empty() : tensor<2048x1280xf32>
    %13106 = linalg.fill ins(%cst : f32) outs(%13105 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13107:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13104, %13106, %13101, %13102, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13104, %13106)
    %13108 = arith.truncf %13107#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13109 = torch_c.from_builtin_tensor %13108 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13110 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13111 = torch.aten.view %13109, %13110 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13112 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13113 = torch.aten.view %13083, %13112 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13114 = torch.aten.transpose.int %13113, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13115 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13116 = torch.aten.view %13097, %13115 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13117 = torch.aten.transpose.int %13116, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13118 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13119 = torch.aten.view %13111, %13118 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13120 = torch.aten.transpose.int %13119, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13121:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13114, %13117, %13120, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13122 = torch.aten.transpose.int %13121#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13123 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13124 = torch.aten.view %13122, %13123 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13125 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13126 = torch.aten.view %13124, %13125 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13127 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13128 = torch.aten.transpose.int %13127, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %13129 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13130 = torch.prims.convert_element_type %13129, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13131 = torch.prims.convert_element_type %13126, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13132 = torch.prims.convert_element_type %13128, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13133 = torch.aten.mm %13131, %13132 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13134 = torch.aten.mul.Scalar %13133, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13135 = torch.aten.mul.Scalar %13130, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13136 = torch.aten.add.Tensor %13134, %13135, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13137 = torch.prims.convert_element_type %13136, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13138 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13139 = torch.aten.view %13137, %13138 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13140 = torch.aten.div.Scalar %13139, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13141 = torch.aten.add.Tensor %13140, %13056, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13142 = torch.prims.convert_element_type %13141, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13143 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_397, %result1_398 = torch.aten.var_mean.correction %13142, %13143, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13144 = torch.aten.add.Scalar %result0_397, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13145 = torch.aten.rsqrt %13144 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13146 = torch.aten.sub.Tensor %13141, %result1_398, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13147 = torch.aten.mul.Tensor %13146, %13145 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %13148 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13149 = torch.aten.mul.Tensor %13147, %13148 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %13150 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13151 = torch.aten.add.Tensor %13149, %13150, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13152 = torch.prims.convert_element_type %13151, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13153 = torch.prims.convert_element_type %result1_398, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13154 = torch.prims.convert_element_type %13145, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %13155 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13156 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13157 = torch.aten.view %13152, %13156 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13158 = torch_c.to_builtin_tensor %13157 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13159 = torch_c.to_builtin_tensor %13155 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13160 = tensor.empty() : tensor<2048x1280xf32>
    %13161 = linalg.fill ins(%cst : f32) outs(%13160 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13162 = tensor.empty() : tensor<2048x1280xf32>
    %13163 = linalg.fill ins(%cst : f32) outs(%13162 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13164:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13161, %13163, %13158, %13159, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13161, %13163)
    %13165 = arith.truncf %13164#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13166 = torch_c.from_builtin_tensor %13165 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13167 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13168 = torch.aten.view %13166, %13167 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %13169 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13170 = torch.aten.transpose.int %13169, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13171 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13172 = torch.aten.view %4, %13171 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13173 = torch.aten.mm %13172, %13170 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13174 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13175 = torch.aten.view %13173, %13174 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %13176 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13177 = torch.aten.transpose.int %13176, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13178 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13179 = torch.aten.view %4, %13178 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13180 = torch.aten.mm %13179, %13177 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13181 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13182 = torch.aten.view %13180, %13181 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13183 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13184 = torch.aten.view %13168, %13183 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13185 = torch.aten.transpose.int %13184, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13186 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13187 = torch.aten.view %13175, %13186 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13188 = torch.aten.transpose.int %13187, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13189 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13190 = torch.aten.view %13182, %13189 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13191 = torch.aten.transpose.int %13190, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13192:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13185, %13188, %13191, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13193 = torch.aten.transpose.int %13192#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13194 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13195 = torch.aten.view %13193, %13194 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13196 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13197 = torch.aten.view %13195, %13196 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13198 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13199 = torch.aten.transpose.int %13198, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %13200 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13201 = torch.prims.convert_element_type %13200, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13202 = torch.prims.convert_element_type %13197, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13203 = torch.prims.convert_element_type %13199, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13204 = torch.aten.mm %13202, %13203 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13205 = torch.aten.mul.Scalar %13204, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13206 = torch.aten.mul.Scalar %13201, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13207 = torch.aten.add.Tensor %13205, %13206, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13208 = torch.prims.convert_element_type %13207, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13209 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13210 = torch.aten.view %13208, %13209 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13211 = torch.aten.div.Scalar %13210, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13212 = torch.aten.add.Tensor %13211, %13141, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13213 = torch.prims.convert_element_type %13212, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13214 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_399, %result1_400 = torch.aten.var_mean.correction %13213, %13214, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13215 = torch.aten.add.Scalar %result0_399, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13216 = torch.aten.rsqrt %13215 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13217 = torch.aten.sub.Tensor %13212, %result1_400, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13218 = torch.aten.mul.Tensor %13217, %13216 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %13219 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13220 = torch.aten.mul.Tensor %13218, %13219 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %13221 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13222 = torch.aten.add.Tensor %13220, %13221, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13223 = torch.prims.convert_element_type %13222, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13224 = torch.prims.convert_element_type %result1_400, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13225 = torch.prims.convert_element_type %13216, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13226 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13227 = torch.aten.view %13223, %13226 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13228 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13229 = torch.aten.transpose.int %13228, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %13230 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13231 = torch.prims.convert_element_type %13230, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13232 = torch.prims.convert_element_type %13227, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13233 = torch.prims.convert_element_type %13229, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13234 = torch.aten.mm %13232, %13233 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13235 = torch.aten.mul.Scalar %13234, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13236 = torch.aten.mul.Scalar %13231, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13237 = torch.aten.add.Tensor %13235, %13236, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13238 = torch.prims.convert_element_type %13237, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13239 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13240 = torch.aten.view %13238, %13239 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13241 = torch.aten.slice.Tensor %13240, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13242 = torch.aten.slice.Tensor %13240, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13243 = torch.aten.gelu %13242, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13244 = torch.aten.mul.Tensor %13241, %13243 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13245 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13246 = torch.aten.view %13244, %13245 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %13247 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13248 = torch.aten.transpose.int %13247, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %13249 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13250 = torch.prims.convert_element_type %13249, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13251 = torch.prims.convert_element_type %13246, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13252 = torch.prims.convert_element_type %13248, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13253 = torch.aten.mm %13251, %13252 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13254 = torch.aten.mul.Scalar %13253, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13255 = torch.aten.mul.Scalar %13250, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13256 = torch.aten.add.Tensor %13254, %13255, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13257 = torch.prims.convert_element_type %13256, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13258 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13259 = torch.aten.view %13257, %13258 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13260 = torch.aten.add.Tensor %13259, %13212, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13261 = torch.prims.convert_element_type %13260, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13262 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_401, %result1_402 = torch.aten.var_mean.correction %13261, %13262, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13263 = torch.aten.add.Scalar %result0_401, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13264 = torch.aten.rsqrt %13263 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13265 = torch.aten.sub.Tensor %13260, %result1_402, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13266 = torch.aten.mul.Tensor %13265, %13264 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %13267 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13268 = torch.aten.mul.Tensor %13266, %13267 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %13269 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13270 = torch.aten.add.Tensor %13268, %13269, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13271 = torch.prims.convert_element_type %13270, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13272 = torch.prims.convert_element_type %result1_402, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13273 = torch.prims.convert_element_type %13264, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %13274 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13275 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13276 = torch.aten.view %13271, %13275 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13277 = torch_c.to_builtin_tensor %13276 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13278 = torch_c.to_builtin_tensor %13274 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13279 = tensor.empty() : tensor<2048x1280xf32>
    %13280 = linalg.fill ins(%cst : f32) outs(%13279 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13281 = tensor.empty() : tensor<2048x1280xf32>
    %13282 = linalg.fill ins(%cst : f32) outs(%13281 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13283:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13280, %13282, %13277, %13278, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13280, %13282)
    %13284 = arith.truncf %13283#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13285 = torch_c.from_builtin_tensor %13284 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13286 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13287 = torch.aten.view %13285, %13286 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %13288 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13289 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13290 = torch.aten.view %13271, %13289 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13291 = torch_c.to_builtin_tensor %13290 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13292 = torch_c.to_builtin_tensor %13288 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13293 = tensor.empty() : tensor<2048x1280xf32>
    %13294 = linalg.fill ins(%cst : f32) outs(%13293 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13295 = tensor.empty() : tensor<2048x1280xf32>
    %13296 = linalg.fill ins(%cst : f32) outs(%13295 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13297:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13294, %13296, %13291, %13292, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13294, %13296)
    %13298 = arith.truncf %13297#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13299 = torch_c.from_builtin_tensor %13298 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13300 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13301 = torch.aten.view %13299, %13300 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %13302 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13303 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13304 = torch.aten.view %13271, %13303 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13305 = torch_c.to_builtin_tensor %13304 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13306 = torch_c.to_builtin_tensor %13302 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13307 = tensor.empty() : tensor<2048x1280xf32>
    %13308 = linalg.fill ins(%cst : f32) outs(%13307 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13309 = tensor.empty() : tensor<2048x1280xf32>
    %13310 = linalg.fill ins(%cst : f32) outs(%13309 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13311:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13308, %13310, %13305, %13306, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13308, %13310)
    %13312 = arith.truncf %13311#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13313 = torch_c.from_builtin_tensor %13312 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13314 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13315 = torch.aten.view %13313, %13314 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13316 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13317 = torch.aten.view %13287, %13316 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13318 = torch.aten.transpose.int %13317, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13319 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13320 = torch.aten.view %13301, %13319 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13321 = torch.aten.transpose.int %13320, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13322 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13323 = torch.aten.view %13315, %13322 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13324 = torch.aten.transpose.int %13323, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13325:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13318, %13321, %13324, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13326 = torch.aten.transpose.int %13325#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13327 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13328 = torch.aten.view %13326, %13327 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13329 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13330 = torch.aten.view %13328, %13329 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13331 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13332 = torch.aten.transpose.int %13331, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %13333 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13334 = torch.prims.convert_element_type %13333, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13335 = torch.prims.convert_element_type %13330, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13336 = torch.prims.convert_element_type %13332, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13337 = torch.aten.mm %13335, %13336 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13338 = torch.aten.mul.Scalar %13337, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13339 = torch.aten.mul.Scalar %13334, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13340 = torch.aten.add.Tensor %13338, %13339, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13341 = torch.prims.convert_element_type %13340, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13342 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13343 = torch.aten.view %13341, %13342 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13344 = torch.aten.div.Scalar %13343, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13345 = torch.aten.add.Tensor %13344, %13260, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13346 = torch.prims.convert_element_type %13345, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13347 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_403, %result1_404 = torch.aten.var_mean.correction %13346, %13347, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13348 = torch.aten.add.Scalar %result0_403, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13349 = torch.aten.rsqrt %13348 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13350 = torch.aten.sub.Tensor %13345, %result1_404, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13351 = torch.aten.mul.Tensor %13350, %13349 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %13352 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13353 = torch.aten.mul.Tensor %13351, %13352 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %13354 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13355 = torch.aten.add.Tensor %13353, %13354, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13356 = torch.prims.convert_element_type %13355, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13357 = torch.prims.convert_element_type %result1_404, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13358 = torch.prims.convert_element_type %13349, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %13359 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13360 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13361 = torch.aten.view %13356, %13360 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13362 = torch_c.to_builtin_tensor %13361 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13363 = torch_c.to_builtin_tensor %13359 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13364 = tensor.empty() : tensor<2048x1280xf32>
    %13365 = linalg.fill ins(%cst : f32) outs(%13364 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13366 = tensor.empty() : tensor<2048x1280xf32>
    %13367 = linalg.fill ins(%cst : f32) outs(%13366 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13368:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13365, %13367, %13362, %13363, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13365, %13367)
    %13369 = arith.truncf %13368#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13370 = torch_c.from_builtin_tensor %13369 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13371 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13372 = torch.aten.view %13370, %13371 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %13373 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13374 = torch.aten.transpose.int %13373, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13375 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13376 = torch.aten.view %4, %13375 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13377 = torch.aten.mm %13376, %13374 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13378 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13379 = torch.aten.view %13377, %13378 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %13380 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13381 = torch.aten.transpose.int %13380, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13382 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13383 = torch.aten.view %4, %13382 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13384 = torch.aten.mm %13383, %13381 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13385 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13386 = torch.aten.view %13384, %13385 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13387 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13388 = torch.aten.view %13372, %13387 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13389 = torch.aten.transpose.int %13388, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13390 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13391 = torch.aten.view %13379, %13390 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13392 = torch.aten.transpose.int %13391, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13393 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13394 = torch.aten.view %13386, %13393 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13395 = torch.aten.transpose.int %13394, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13396:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13389, %13392, %13395, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13397 = torch.aten.transpose.int %13396#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13398 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13399 = torch.aten.view %13397, %13398 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13400 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13401 = torch.aten.view %13399, %13400 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13402 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13403 = torch.aten.transpose.int %13402, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %13404 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13405 = torch.prims.convert_element_type %13404, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13406 = torch.prims.convert_element_type %13401, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13407 = torch.prims.convert_element_type %13403, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13408 = torch.aten.mm %13406, %13407 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13409 = torch.aten.mul.Scalar %13408, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13410 = torch.aten.mul.Scalar %13405, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13411 = torch.aten.add.Tensor %13409, %13410, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13412 = torch.prims.convert_element_type %13411, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13413 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13414 = torch.aten.view %13412, %13413 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13415 = torch.aten.div.Scalar %13414, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13416 = torch.aten.add.Tensor %13415, %13345, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13417 = torch.prims.convert_element_type %13416, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13418 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_405, %result1_406 = torch.aten.var_mean.correction %13417, %13418, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13419 = torch.aten.add.Scalar %result0_405, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13420 = torch.aten.rsqrt %13419 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13421 = torch.aten.sub.Tensor %13416, %result1_406, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13422 = torch.aten.mul.Tensor %13421, %13420 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %13423 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13424 = torch.aten.mul.Tensor %13422, %13423 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %13425 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13426 = torch.aten.add.Tensor %13424, %13425, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13427 = torch.prims.convert_element_type %13426, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13428 = torch.prims.convert_element_type %result1_406, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13429 = torch.prims.convert_element_type %13420, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13430 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13431 = torch.aten.view %13427, %13430 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13432 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13433 = torch.aten.transpose.int %13432, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %13434 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13435 = torch.prims.convert_element_type %13434, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13436 = torch.prims.convert_element_type %13431, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13437 = torch.prims.convert_element_type %13433, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13438 = torch.aten.mm %13436, %13437 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13439 = torch.aten.mul.Scalar %13438, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13440 = torch.aten.mul.Scalar %13435, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13441 = torch.aten.add.Tensor %13439, %13440, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13442 = torch.prims.convert_element_type %13441, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13443 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13444 = torch.aten.view %13442, %13443 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13445 = torch.aten.slice.Tensor %13444, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13446 = torch.aten.slice.Tensor %13444, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13447 = torch.aten.gelu %13446, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13448 = torch.aten.mul.Tensor %13445, %13447 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13449 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13450 = torch.aten.view %13448, %13449 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %13451 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13452 = torch.aten.transpose.int %13451, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %13453 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13454 = torch.prims.convert_element_type %13453, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13455 = torch.prims.convert_element_type %13450, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13456 = torch.prims.convert_element_type %13452, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13457 = torch.aten.mm %13455, %13456 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13458 = torch.aten.mul.Scalar %13457, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13459 = torch.aten.mul.Scalar %13454, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13460 = torch.aten.add.Tensor %13458, %13459, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13461 = torch.prims.convert_element_type %13460, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13462 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13463 = torch.aten.view %13461, %13462 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13464 = torch.aten.add.Tensor %13463, %13416, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13465 = torch.prims.convert_element_type %13464, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13466 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_407, %result1_408 = torch.aten.var_mean.correction %13465, %13466, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13467 = torch.aten.add.Scalar %result0_407, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13468 = torch.aten.rsqrt %13467 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13469 = torch.aten.sub.Tensor %13464, %result1_408, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13470 = torch.aten.mul.Tensor %13469, %13468 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %13471 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13472 = torch.aten.mul.Tensor %13470, %13471 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %13473 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13474 = torch.aten.add.Tensor %13472, %13473, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13475 = torch.prims.convert_element_type %13474, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13476 = torch.prims.convert_element_type %result1_408, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13477 = torch.prims.convert_element_type %13468, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %13478 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13479 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13480 = torch.aten.view %13475, %13479 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13481 = torch_c.to_builtin_tensor %13480 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13482 = torch_c.to_builtin_tensor %13478 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13483 = tensor.empty() : tensor<2048x1280xf32>
    %13484 = linalg.fill ins(%cst : f32) outs(%13483 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13485 = tensor.empty() : tensor<2048x1280xf32>
    %13486 = linalg.fill ins(%cst : f32) outs(%13485 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13487:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13484, %13486, %13481, %13482, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13484, %13486)
    %13488 = arith.truncf %13487#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13489 = torch_c.from_builtin_tensor %13488 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13490 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13491 = torch.aten.view %13489, %13490 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %13492 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13493 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13494 = torch.aten.view %13475, %13493 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13495 = torch_c.to_builtin_tensor %13494 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13496 = torch_c.to_builtin_tensor %13492 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13497 = tensor.empty() : tensor<2048x1280xf32>
    %13498 = linalg.fill ins(%cst : f32) outs(%13497 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13499 = tensor.empty() : tensor<2048x1280xf32>
    %13500 = linalg.fill ins(%cst : f32) outs(%13499 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13501:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13498, %13500, %13495, %13496, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13498, %13500)
    %13502 = arith.truncf %13501#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13503 = torch_c.from_builtin_tensor %13502 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13504 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13505 = torch.aten.view %13503, %13504 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %13506 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13507 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13508 = torch.aten.view %13475, %13507 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13509 = torch_c.to_builtin_tensor %13508 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13510 = torch_c.to_builtin_tensor %13506 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13511 = tensor.empty() : tensor<2048x1280xf32>
    %13512 = linalg.fill ins(%cst : f32) outs(%13511 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13513 = tensor.empty() : tensor<2048x1280xf32>
    %13514 = linalg.fill ins(%cst : f32) outs(%13513 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13515:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13512, %13514, %13509, %13510, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13512, %13514)
    %13516 = arith.truncf %13515#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13517 = torch_c.from_builtin_tensor %13516 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13518 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13519 = torch.aten.view %13517, %13518 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13520 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13521 = torch.aten.view %13491, %13520 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13522 = torch.aten.transpose.int %13521, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13523 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13524 = torch.aten.view %13505, %13523 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13525 = torch.aten.transpose.int %13524, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13526 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13527 = torch.aten.view %13519, %13526 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13528 = torch.aten.transpose.int %13527, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13529:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13522, %13525, %13528, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13530 = torch.aten.transpose.int %13529#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13531 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13532 = torch.aten.view %13530, %13531 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13533 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13534 = torch.aten.view %13532, %13533 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13535 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13536 = torch.aten.transpose.int %13535, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %13537 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13538 = torch.prims.convert_element_type %13537, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13539 = torch.prims.convert_element_type %13534, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13540 = torch.prims.convert_element_type %13536, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13541 = torch.aten.mm %13539, %13540 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13542 = torch.aten.mul.Scalar %13541, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13543 = torch.aten.mul.Scalar %13538, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13544 = torch.aten.add.Tensor %13542, %13543, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13545 = torch.prims.convert_element_type %13544, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13546 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13547 = torch.aten.view %13545, %13546 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13548 = torch.aten.div.Scalar %13547, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13549 = torch.aten.add.Tensor %13548, %13464, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13550 = torch.prims.convert_element_type %13549, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13551 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_409, %result1_410 = torch.aten.var_mean.correction %13550, %13551, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13552 = torch.aten.add.Scalar %result0_409, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13553 = torch.aten.rsqrt %13552 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13554 = torch.aten.sub.Tensor %13549, %result1_410, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13555 = torch.aten.mul.Tensor %13554, %13553 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %13556 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13557 = torch.aten.mul.Tensor %13555, %13556 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %13558 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13559 = torch.aten.add.Tensor %13557, %13558, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13560 = torch.prims.convert_element_type %13559, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13561 = torch.prims.convert_element_type %result1_410, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13562 = torch.prims.convert_element_type %13553, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %13563 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13564 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13565 = torch.aten.view %13560, %13564 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13566 = torch_c.to_builtin_tensor %13565 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13567 = torch_c.to_builtin_tensor %13563 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13568 = tensor.empty() : tensor<2048x1280xf32>
    %13569 = linalg.fill ins(%cst : f32) outs(%13568 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13570 = tensor.empty() : tensor<2048x1280xf32>
    %13571 = linalg.fill ins(%cst : f32) outs(%13570 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13572:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13569, %13571, %13566, %13567, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13569, %13571)
    %13573 = arith.truncf %13572#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13574 = torch_c.from_builtin_tensor %13573 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13575 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13576 = torch.aten.view %13574, %13575 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %13577 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13578 = torch.aten.transpose.int %13577, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13579 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13580 = torch.aten.view %4, %13579 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13581 = torch.aten.mm %13580, %13578 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13582 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13583 = torch.aten.view %13581, %13582 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %13584 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13585 = torch.aten.transpose.int %13584, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13586 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13587 = torch.aten.view %4, %13586 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13588 = torch.aten.mm %13587, %13585 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13589 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13590 = torch.aten.view %13588, %13589 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13591 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13592 = torch.aten.view %13576, %13591 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13593 = torch.aten.transpose.int %13592, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13594 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13595 = torch.aten.view %13583, %13594 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13596 = torch.aten.transpose.int %13595, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13597 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13598 = torch.aten.view %13590, %13597 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13599 = torch.aten.transpose.int %13598, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13600:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13593, %13596, %13599, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13601 = torch.aten.transpose.int %13600#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13602 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13603 = torch.aten.view %13601, %13602 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13604 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13605 = torch.aten.view %13603, %13604 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13606 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13607 = torch.aten.transpose.int %13606, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %13608 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13609 = torch.prims.convert_element_type %13608, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13610 = torch.prims.convert_element_type %13605, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13611 = torch.prims.convert_element_type %13607, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13612 = torch.aten.mm %13610, %13611 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13613 = torch.aten.mul.Scalar %13612, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13614 = torch.aten.mul.Scalar %13609, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13615 = torch.aten.add.Tensor %13613, %13614, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13616 = torch.prims.convert_element_type %13615, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13617 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13618 = torch.aten.view %13616, %13617 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13619 = torch.aten.div.Scalar %13618, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13620 = torch.aten.add.Tensor %13619, %13549, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13621 = torch.prims.convert_element_type %13620, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13622 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_411, %result1_412 = torch.aten.var_mean.correction %13621, %13622, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13623 = torch.aten.add.Scalar %result0_411, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13624 = torch.aten.rsqrt %13623 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13625 = torch.aten.sub.Tensor %13620, %result1_412, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13626 = torch.aten.mul.Tensor %13625, %13624 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %13627 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13628 = torch.aten.mul.Tensor %13626, %13627 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %13629 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13630 = torch.aten.add.Tensor %13628, %13629, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13631 = torch.prims.convert_element_type %13630, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13632 = torch.prims.convert_element_type %result1_412, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13633 = torch.prims.convert_element_type %13624, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13634 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13635 = torch.aten.view %13631, %13634 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13636 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13637 = torch.aten.transpose.int %13636, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %13638 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13639 = torch.prims.convert_element_type %13638, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13640 = torch.prims.convert_element_type %13635, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13641 = torch.prims.convert_element_type %13637, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13642 = torch.aten.mm %13640, %13641 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13643 = torch.aten.mul.Scalar %13642, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13644 = torch.aten.mul.Scalar %13639, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13645 = torch.aten.add.Tensor %13643, %13644, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13646 = torch.prims.convert_element_type %13645, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13647 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13648 = torch.aten.view %13646, %13647 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13649 = torch.aten.slice.Tensor %13648, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13650 = torch.aten.slice.Tensor %13648, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13651 = torch.aten.gelu %13650, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13652 = torch.aten.mul.Tensor %13649, %13651 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13653 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13654 = torch.aten.view %13652, %13653 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %13655 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13656 = torch.aten.transpose.int %13655, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %13657 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13658 = torch.prims.convert_element_type %13657, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13659 = torch.prims.convert_element_type %13654, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13660 = torch.prims.convert_element_type %13656, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13661 = torch.aten.mm %13659, %13660 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13662 = torch.aten.mul.Scalar %13661, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13663 = torch.aten.mul.Scalar %13658, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13664 = torch.aten.add.Tensor %13662, %13663, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13665 = torch.prims.convert_element_type %13664, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13666 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13667 = torch.aten.view %13665, %13666 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13668 = torch.aten.add.Tensor %13667, %13620, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13669 = torch.prims.convert_element_type %13668, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13670 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_413, %result1_414 = torch.aten.var_mean.correction %13669, %13670, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13671 = torch.aten.add.Scalar %result0_413, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13672 = torch.aten.rsqrt %13671 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13673 = torch.aten.sub.Tensor %13668, %result1_414, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13674 = torch.aten.mul.Tensor %13673, %13672 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %13675 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13676 = torch.aten.mul.Tensor %13674, %13675 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %13677 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13678 = torch.aten.add.Tensor %13676, %13677, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13679 = torch.prims.convert_element_type %13678, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13680 = torch.prims.convert_element_type %result1_414, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13681 = torch.prims.convert_element_type %13672, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %13682 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13683 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13684 = torch.aten.view %13679, %13683 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13685 = torch_c.to_builtin_tensor %13684 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13686 = torch_c.to_builtin_tensor %13682 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13687 = tensor.empty() : tensor<2048x1280xf32>
    %13688 = linalg.fill ins(%cst : f32) outs(%13687 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13689 = tensor.empty() : tensor<2048x1280xf32>
    %13690 = linalg.fill ins(%cst : f32) outs(%13689 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13691:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13688, %13690, %13685, %13686, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13688, %13690)
    %13692 = arith.truncf %13691#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13693 = torch_c.from_builtin_tensor %13692 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13694 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13695 = torch.aten.view %13693, %13694 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %13696 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13697 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13698 = torch.aten.view %13679, %13697 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13699 = torch_c.to_builtin_tensor %13698 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13700 = torch_c.to_builtin_tensor %13696 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13701 = tensor.empty() : tensor<2048x1280xf32>
    %13702 = linalg.fill ins(%cst : f32) outs(%13701 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13703 = tensor.empty() : tensor<2048x1280xf32>
    %13704 = linalg.fill ins(%cst : f32) outs(%13703 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13705:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13702, %13704, %13699, %13700, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13702, %13704)
    %13706 = arith.truncf %13705#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13707 = torch_c.from_builtin_tensor %13706 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13708 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13709 = torch.aten.view %13707, %13708 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %13710 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13711 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13712 = torch.aten.view %13679, %13711 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13713 = torch_c.to_builtin_tensor %13712 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13714 = torch_c.to_builtin_tensor %13710 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13715 = tensor.empty() : tensor<2048x1280xf32>
    %13716 = linalg.fill ins(%cst : f32) outs(%13715 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13717 = tensor.empty() : tensor<2048x1280xf32>
    %13718 = linalg.fill ins(%cst : f32) outs(%13717 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13719:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13716, %13718, %13713, %13714, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13716, %13718)
    %13720 = arith.truncf %13719#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13721 = torch_c.from_builtin_tensor %13720 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13722 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13723 = torch.aten.view %13721, %13722 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13724 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13725 = torch.aten.view %13695, %13724 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13726 = torch.aten.transpose.int %13725, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13727 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13728 = torch.aten.view %13709, %13727 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13729 = torch.aten.transpose.int %13728, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13730 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13731 = torch.aten.view %13723, %13730 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13732 = torch.aten.transpose.int %13731, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13733:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13726, %13729, %13732, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13734 = torch.aten.transpose.int %13733#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13735 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13736 = torch.aten.view %13734, %13735 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13737 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13738 = torch.aten.view %13736, %13737 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13739 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13740 = torch.aten.transpose.int %13739, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %13741 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13742 = torch.prims.convert_element_type %13741, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13743 = torch.prims.convert_element_type %13738, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13744 = torch.prims.convert_element_type %13740, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13745 = torch.aten.mm %13743, %13744 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13746 = torch.aten.mul.Scalar %13745, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13747 = torch.aten.mul.Scalar %13742, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13748 = torch.aten.add.Tensor %13746, %13747, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13749 = torch.prims.convert_element_type %13748, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13750 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13751 = torch.aten.view %13749, %13750 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13752 = torch.aten.div.Scalar %13751, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13753 = torch.aten.add.Tensor %13752, %13668, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13754 = torch.prims.convert_element_type %13753, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13755 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_415, %result1_416 = torch.aten.var_mean.correction %13754, %13755, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13756 = torch.aten.add.Scalar %result0_415, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13757 = torch.aten.rsqrt %13756 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13758 = torch.aten.sub.Tensor %13753, %result1_416, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13759 = torch.aten.mul.Tensor %13758, %13757 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %13760 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13761 = torch.aten.mul.Tensor %13759, %13760 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %13762 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13763 = torch.aten.add.Tensor %13761, %13762, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13764 = torch.prims.convert_element_type %13763, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13765 = torch.prims.convert_element_type %result1_416, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13766 = torch.prims.convert_element_type %13757, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %13767 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13768 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13769 = torch.aten.view %13764, %13768 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13770 = torch_c.to_builtin_tensor %13769 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13771 = torch_c.to_builtin_tensor %13767 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13772 = tensor.empty() : tensor<2048x1280xf32>
    %13773 = linalg.fill ins(%cst : f32) outs(%13772 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13774 = tensor.empty() : tensor<2048x1280xf32>
    %13775 = linalg.fill ins(%cst : f32) outs(%13774 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13776:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13773, %13775, %13770, %13771, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13773, %13775)
    %13777 = arith.truncf %13776#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13778 = torch_c.from_builtin_tensor %13777 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13779 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13780 = torch.aten.view %13778, %13779 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %13781 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13782 = torch.aten.transpose.int %13781, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13783 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13784 = torch.aten.view %4, %13783 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13785 = torch.aten.mm %13784, %13782 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13786 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13787 = torch.aten.view %13785, %13786 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %13788 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13789 = torch.aten.transpose.int %13788, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13790 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13791 = torch.aten.view %4, %13790 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13792 = torch.aten.mm %13791, %13789 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13793 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13794 = torch.aten.view %13792, %13793 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13795 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13796 = torch.aten.view %13780, %13795 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13797 = torch.aten.transpose.int %13796, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13798 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13799 = torch.aten.view %13787, %13798 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13800 = torch.aten.transpose.int %13799, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13801 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13802 = torch.aten.view %13794, %13801 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13803 = torch.aten.transpose.int %13802, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13804:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13797, %13800, %13803, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13805 = torch.aten.transpose.int %13804#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13806 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13807 = torch.aten.view %13805, %13806 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13808 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13809 = torch.aten.view %13807, %13808 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13810 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13811 = torch.aten.transpose.int %13810, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %13812 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13813 = torch.prims.convert_element_type %13812, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13814 = torch.prims.convert_element_type %13809, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13815 = torch.prims.convert_element_type %13811, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13816 = torch.aten.mm %13814, %13815 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13817 = torch.aten.mul.Scalar %13816, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13818 = torch.aten.mul.Scalar %13813, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13819 = torch.aten.add.Tensor %13817, %13818, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13820 = torch.prims.convert_element_type %13819, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13821 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13822 = torch.aten.view %13820, %13821 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13823 = torch.aten.div.Scalar %13822, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13824 = torch.aten.add.Tensor %13823, %13753, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13825 = torch.prims.convert_element_type %13824, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13826 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_417, %result1_418 = torch.aten.var_mean.correction %13825, %13826, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13827 = torch.aten.add.Scalar %result0_417, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13828 = torch.aten.rsqrt %13827 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13829 = torch.aten.sub.Tensor %13824, %result1_418, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13830 = torch.aten.mul.Tensor %13829, %13828 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %13831 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13832 = torch.aten.mul.Tensor %13830, %13831 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %13833 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13834 = torch.aten.add.Tensor %13832, %13833, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13835 = torch.prims.convert_element_type %13834, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13836 = torch.prims.convert_element_type %result1_418, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13837 = torch.prims.convert_element_type %13828, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13838 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13839 = torch.aten.view %13835, %13838 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13840 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13841 = torch.aten.transpose.int %13840, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %13842 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13843 = torch.prims.convert_element_type %13842, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13844 = torch.prims.convert_element_type %13839, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13845 = torch.prims.convert_element_type %13841, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13846 = torch.aten.mm %13844, %13845 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13847 = torch.aten.mul.Scalar %13846, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13848 = torch.aten.mul.Scalar %13843, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13849 = torch.aten.add.Tensor %13847, %13848, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13850 = torch.prims.convert_element_type %13849, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13851 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13852 = torch.aten.view %13850, %13851 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13853 = torch.aten.slice.Tensor %13852, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13854 = torch.aten.slice.Tensor %13852, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13855 = torch.aten.gelu %13854, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13856 = torch.aten.mul.Tensor %13853, %13855 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13857 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13858 = torch.aten.view %13856, %13857 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %13859 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13860 = torch.aten.transpose.int %13859, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %13861 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13862 = torch.prims.convert_element_type %13861, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13863 = torch.prims.convert_element_type %13858, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13864 = torch.prims.convert_element_type %13860, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13865 = torch.aten.mm %13863, %13864 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13866 = torch.aten.mul.Scalar %13865, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13867 = torch.aten.mul.Scalar %13862, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13868 = torch.aten.add.Tensor %13866, %13867, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13869 = torch.prims.convert_element_type %13868, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13870 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13871 = torch.aten.view %13869, %13870 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13872 = torch.aten.add.Tensor %13871, %13824, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13873 = torch.prims.convert_element_type %13872, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13874 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_419, %result1_420 = torch.aten.var_mean.correction %13873, %13874, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13875 = torch.aten.add.Scalar %result0_419, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13876 = torch.aten.rsqrt %13875 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13877 = torch.aten.sub.Tensor %13872, %result1_420, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13878 = torch.aten.mul.Tensor %13877, %13876 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %13879 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13880 = torch.aten.mul.Tensor %13878, %13879 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %13881 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13882 = torch.aten.add.Tensor %13880, %13881, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13883 = torch.prims.convert_element_type %13882, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13884 = torch.prims.convert_element_type %result1_420, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13885 = torch.prims.convert_element_type %13876, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %13886 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13887 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13888 = torch.aten.view %13883, %13887 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13889 = torch_c.to_builtin_tensor %13888 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13890 = torch_c.to_builtin_tensor %13886 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13891 = tensor.empty() : tensor<2048x1280xf32>
    %13892 = linalg.fill ins(%cst : f32) outs(%13891 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13893 = tensor.empty() : tensor<2048x1280xf32>
    %13894 = linalg.fill ins(%cst : f32) outs(%13893 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13895:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13892, %13894, %13889, %13890, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13892, %13894)
    %13896 = arith.truncf %13895#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13897 = torch_c.from_builtin_tensor %13896 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13898 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13899 = torch.aten.view %13897, %13898 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %13900 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13901 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13902 = torch.aten.view %13883, %13901 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13903 = torch_c.to_builtin_tensor %13902 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13904 = torch_c.to_builtin_tensor %13900 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13905 = tensor.empty() : tensor<2048x1280xf32>
    %13906 = linalg.fill ins(%cst : f32) outs(%13905 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13907 = tensor.empty() : tensor<2048x1280xf32>
    %13908 = linalg.fill ins(%cst : f32) outs(%13907 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13909:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13906, %13908, %13903, %13904, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13906, %13908)
    %13910 = arith.truncf %13909#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13911 = torch_c.from_builtin_tensor %13910 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13912 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13913 = torch.aten.view %13911, %13912 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %13914 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13915 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13916 = torch.aten.view %13883, %13915 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13917 = torch_c.to_builtin_tensor %13916 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13918 = torch_c.to_builtin_tensor %13914 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13919 = tensor.empty() : tensor<2048x1280xf32>
    %13920 = linalg.fill ins(%cst : f32) outs(%13919 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13921 = tensor.empty() : tensor<2048x1280xf32>
    %13922 = linalg.fill ins(%cst : f32) outs(%13921 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13923:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13920, %13922, %13917, %13918, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13920, %13922)
    %13924 = arith.truncf %13923#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13925 = torch_c.from_builtin_tensor %13924 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13926 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13927 = torch.aten.view %13925, %13926 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13928 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13929 = torch.aten.view %13899, %13928 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13930 = torch.aten.transpose.int %13929, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13931 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13932 = torch.aten.view %13913, %13931 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13933 = torch.aten.transpose.int %13932, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13934 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13935 = torch.aten.view %13927, %13934 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13936 = torch.aten.transpose.int %13935, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13937:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13930, %13933, %13936, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13938 = torch.aten.transpose.int %13937#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13939 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13940 = torch.aten.view %13938, %13939 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13941 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13942 = torch.aten.view %13940, %13941 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13943 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13944 = torch.aten.transpose.int %13943, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %13945 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13946 = torch.prims.convert_element_type %13945, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13947 = torch.prims.convert_element_type %13942, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13948 = torch.prims.convert_element_type %13944, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13949 = torch.aten.mm %13947, %13948 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13950 = torch.aten.mul.Scalar %13949, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13951 = torch.aten.mul.Scalar %13946, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13952 = torch.aten.add.Tensor %13950, %13951, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13953 = torch.prims.convert_element_type %13952, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13954 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13955 = torch.aten.view %13953, %13954 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13956 = torch.aten.div.Scalar %13955, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13957 = torch.aten.add.Tensor %13956, %13872, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13958 = torch.prims.convert_element_type %13957, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13959 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_421, %result1_422 = torch.aten.var_mean.correction %13958, %13959, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13960 = torch.aten.add.Scalar %result0_421, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13961 = torch.aten.rsqrt %13960 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13962 = torch.aten.sub.Tensor %13957, %result1_422, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13963 = torch.aten.mul.Tensor %13962, %13961 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %13964 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13965 = torch.aten.mul.Tensor %13963, %13964 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %13966 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13967 = torch.aten.add.Tensor %13965, %13966, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13968 = torch.prims.convert_element_type %13967, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13969 = torch.prims.convert_element_type %result1_422, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13970 = torch.prims.convert_element_type %13961, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %13971 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13972 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13973 = torch.aten.view %13968, %13972 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13974 = torch_c.to_builtin_tensor %13973 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13975 = torch_c.to_builtin_tensor %13971 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13976 = tensor.empty() : tensor<2048x1280xf32>
    %13977 = linalg.fill ins(%cst : f32) outs(%13976 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13978 = tensor.empty() : tensor<2048x1280xf32>
    %13979 = linalg.fill ins(%cst : f32) outs(%13978 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13980:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13977, %13979, %13974, %13975, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13977, %13979)
    %13981 = arith.truncf %13980#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13982 = torch_c.from_builtin_tensor %13981 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13983 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13984 = torch.aten.view %13982, %13983 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %13985 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13986 = torch.aten.transpose.int %13985, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13987 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13988 = torch.aten.view %4, %13987 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13989 = torch.aten.mm %13988, %13986 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13990 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13991 = torch.aten.view %13989, %13990 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %13992 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13993 = torch.aten.transpose.int %13992, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13994 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13995 = torch.aten.view %4, %13994 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13996 = torch.aten.mm %13995, %13993 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %13997 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13998 = torch.aten.view %13996, %13997 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13999 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14000 = torch.aten.view %13984, %13999 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14001 = torch.aten.transpose.int %14000, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14002 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14003 = torch.aten.view %13991, %14002 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14004 = torch.aten.transpose.int %14003, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14005 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14006 = torch.aten.view %13998, %14005 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14007 = torch.aten.transpose.int %14006, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14008:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14001, %14004, %14007, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14009 = torch.aten.transpose.int %14008#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14010 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14011 = torch.aten.view %14009, %14010 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14012 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14013 = torch.aten.view %14011, %14012 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14014 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14015 = torch.aten.transpose.int %14014, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %14016 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14017 = torch.prims.convert_element_type %14016, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14018 = torch.prims.convert_element_type %14013, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14019 = torch.prims.convert_element_type %14015, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14020 = torch.aten.mm %14018, %14019 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14021 = torch.aten.mul.Scalar %14020, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14022 = torch.aten.mul.Scalar %14017, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14023 = torch.aten.add.Tensor %14021, %14022, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14024 = torch.prims.convert_element_type %14023, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14025 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14026 = torch.aten.view %14024, %14025 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14027 = torch.aten.div.Scalar %14026, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14028 = torch.aten.add.Tensor %14027, %13957, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14029 = torch.prims.convert_element_type %14028, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14030 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_423, %result1_424 = torch.aten.var_mean.correction %14029, %14030, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14031 = torch.aten.add.Scalar %result0_423, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14032 = torch.aten.rsqrt %14031 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14033 = torch.aten.sub.Tensor %14028, %result1_424, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14034 = torch.aten.mul.Tensor %14033, %14032 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %14035 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14036 = torch.aten.mul.Tensor %14034, %14035 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %14037 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14038 = torch.aten.add.Tensor %14036, %14037, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14039 = torch.prims.convert_element_type %14038, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14040 = torch.prims.convert_element_type %result1_424, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14041 = torch.prims.convert_element_type %14032, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14042 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14043 = torch.aten.view %14039, %14042 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14044 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14045 = torch.aten.transpose.int %14044, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %14046 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14047 = torch.prims.convert_element_type %14046, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14048 = torch.prims.convert_element_type %14043, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14049 = torch.prims.convert_element_type %14045, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14050 = torch.aten.mm %14048, %14049 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14051 = torch.aten.mul.Scalar %14050, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14052 = torch.aten.mul.Scalar %14047, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14053 = torch.aten.add.Tensor %14051, %14052, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14054 = torch.prims.convert_element_type %14053, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14055 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14056 = torch.aten.view %14054, %14055 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14057 = torch.aten.slice.Tensor %14056, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14058 = torch.aten.slice.Tensor %14056, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14059 = torch.aten.gelu %14058, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14060 = torch.aten.mul.Tensor %14057, %14059 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14061 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14062 = torch.aten.view %14060, %14061 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %14063 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14064 = torch.aten.transpose.int %14063, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %14065 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14066 = torch.prims.convert_element_type %14065, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14067 = torch.prims.convert_element_type %14062, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14068 = torch.prims.convert_element_type %14064, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14069 = torch.aten.mm %14067, %14068 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14070 = torch.aten.mul.Scalar %14069, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14071 = torch.aten.mul.Scalar %14066, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14072 = torch.aten.add.Tensor %14070, %14071, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14073 = torch.prims.convert_element_type %14072, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14074 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14075 = torch.aten.view %14073, %14074 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14076 = torch.aten.add.Tensor %14075, %14028, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14077 = torch.prims.convert_element_type %14076, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14078 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_425, %result1_426 = torch.aten.var_mean.correction %14077, %14078, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14079 = torch.aten.add.Scalar %result0_425, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14080 = torch.aten.rsqrt %14079 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14081 = torch.aten.sub.Tensor %14076, %result1_426, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14082 = torch.aten.mul.Tensor %14081, %14080 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %14083 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14084 = torch.aten.mul.Tensor %14082, %14083 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %14085 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14086 = torch.aten.add.Tensor %14084, %14085, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14087 = torch.prims.convert_element_type %14086, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14088 = torch.prims.convert_element_type %result1_426, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14089 = torch.prims.convert_element_type %14080, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %14090 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14091 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14092 = torch.aten.view %14087, %14091 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14093 = torch_c.to_builtin_tensor %14092 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14094 = torch_c.to_builtin_tensor %14090 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14095 = tensor.empty() : tensor<2048x1280xf32>
    %14096 = linalg.fill ins(%cst : f32) outs(%14095 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14097 = tensor.empty() : tensor<2048x1280xf32>
    %14098 = linalg.fill ins(%cst : f32) outs(%14097 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14099:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14096, %14098, %14093, %14094, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14096, %14098)
    %14100 = arith.truncf %14099#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14101 = torch_c.from_builtin_tensor %14100 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14102 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14103 = torch.aten.view %14101, %14102 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %14104 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14105 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14106 = torch.aten.view %14087, %14105 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14107 = torch_c.to_builtin_tensor %14106 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14108 = torch_c.to_builtin_tensor %14104 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14109 = tensor.empty() : tensor<2048x1280xf32>
    %14110 = linalg.fill ins(%cst : f32) outs(%14109 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14111 = tensor.empty() : tensor<2048x1280xf32>
    %14112 = linalg.fill ins(%cst : f32) outs(%14111 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14113:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14110, %14112, %14107, %14108, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14110, %14112)
    %14114 = arith.truncf %14113#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14115 = torch_c.from_builtin_tensor %14114 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14116 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14117 = torch.aten.view %14115, %14116 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %14118 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14119 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14120 = torch.aten.view %14087, %14119 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14121 = torch_c.to_builtin_tensor %14120 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14122 = torch_c.to_builtin_tensor %14118 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14123 = tensor.empty() : tensor<2048x1280xf32>
    %14124 = linalg.fill ins(%cst : f32) outs(%14123 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14125 = tensor.empty() : tensor<2048x1280xf32>
    %14126 = linalg.fill ins(%cst : f32) outs(%14125 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14127:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14124, %14126, %14121, %14122, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14124, %14126)
    %14128 = arith.truncf %14127#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14129 = torch_c.from_builtin_tensor %14128 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14130 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14131 = torch.aten.view %14129, %14130 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14132 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14133 = torch.aten.view %14103, %14132 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14134 = torch.aten.transpose.int %14133, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14135 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14136 = torch.aten.view %14117, %14135 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14137 = torch.aten.transpose.int %14136, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14138 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14139 = torch.aten.view %14131, %14138 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14140 = torch.aten.transpose.int %14139, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14141:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14134, %14137, %14140, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14142 = torch.aten.transpose.int %14141#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14143 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14144 = torch.aten.view %14142, %14143 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14145 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14146 = torch.aten.view %14144, %14145 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14147 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14148 = torch.aten.transpose.int %14147, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %14149 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14150 = torch.prims.convert_element_type %14149, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14151 = torch.prims.convert_element_type %14146, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14152 = torch.prims.convert_element_type %14148, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14153 = torch.aten.mm %14151, %14152 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14154 = torch.aten.mul.Scalar %14153, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14155 = torch.aten.mul.Scalar %14150, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14156 = torch.aten.add.Tensor %14154, %14155, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14157 = torch.prims.convert_element_type %14156, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14158 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14159 = torch.aten.view %14157, %14158 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14160 = torch.aten.div.Scalar %14159, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14161 = torch.aten.add.Tensor %14160, %14076, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14162 = torch.prims.convert_element_type %14161, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14163 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_427, %result1_428 = torch.aten.var_mean.correction %14162, %14163, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14164 = torch.aten.add.Scalar %result0_427, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14165 = torch.aten.rsqrt %14164 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14166 = torch.aten.sub.Tensor %14161, %result1_428, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14167 = torch.aten.mul.Tensor %14166, %14165 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %14168 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14169 = torch.aten.mul.Tensor %14167, %14168 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %14170 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14171 = torch.aten.add.Tensor %14169, %14170, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14172 = torch.prims.convert_element_type %14171, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14173 = torch.prims.convert_element_type %result1_428, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14174 = torch.prims.convert_element_type %14165, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %14175 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14176 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14177 = torch.aten.view %14172, %14176 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14178 = torch_c.to_builtin_tensor %14177 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14179 = torch_c.to_builtin_tensor %14175 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14180 = tensor.empty() : tensor<2048x1280xf32>
    %14181 = linalg.fill ins(%cst : f32) outs(%14180 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14182 = tensor.empty() : tensor<2048x1280xf32>
    %14183 = linalg.fill ins(%cst : f32) outs(%14182 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14184:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14181, %14183, %14178, %14179, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14181, %14183)
    %14185 = arith.truncf %14184#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14186 = torch_c.from_builtin_tensor %14185 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14187 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14188 = torch.aten.view %14186, %14187 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %14189 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14190 = torch.aten.transpose.int %14189, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14191 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14192 = torch.aten.view %4, %14191 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14193 = torch.aten.mm %14192, %14190 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %14194 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14195 = torch.aten.view %14193, %14194 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %14196 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14197 = torch.aten.transpose.int %14196, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14198 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14199 = torch.aten.view %4, %14198 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14200 = torch.aten.mm %14199, %14197 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %14201 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14202 = torch.aten.view %14200, %14201 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %14203 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14204 = torch.aten.view %14188, %14203 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14205 = torch.aten.transpose.int %14204, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14206 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14207 = torch.aten.view %14195, %14206 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14208 = torch.aten.transpose.int %14207, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14209 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14210 = torch.aten.view %14202, %14209 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14211 = torch.aten.transpose.int %14210, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14212:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14205, %14208, %14211, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14213 = torch.aten.transpose.int %14212#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14214 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14215 = torch.aten.view %14213, %14214 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14216 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14217 = torch.aten.view %14215, %14216 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14218 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14219 = torch.aten.transpose.int %14218, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %14220 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14221 = torch.prims.convert_element_type %14220, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14222 = torch.prims.convert_element_type %14217, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14223 = torch.prims.convert_element_type %14219, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14224 = torch.aten.mm %14222, %14223 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14225 = torch.aten.mul.Scalar %14224, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14226 = torch.aten.mul.Scalar %14221, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14227 = torch.aten.add.Tensor %14225, %14226, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14228 = torch.prims.convert_element_type %14227, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14229 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14230 = torch.aten.view %14228, %14229 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14231 = torch.aten.div.Scalar %14230, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14232 = torch.aten.add.Tensor %14231, %14161, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14233 = torch.prims.convert_element_type %14232, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14234 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_429, %result1_430 = torch.aten.var_mean.correction %14233, %14234, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14235 = torch.aten.add.Scalar %result0_429, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14236 = torch.aten.rsqrt %14235 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14237 = torch.aten.sub.Tensor %14232, %result1_430, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14238 = torch.aten.mul.Tensor %14237, %14236 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %14239 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14240 = torch.aten.mul.Tensor %14238, %14239 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %14241 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14242 = torch.aten.add.Tensor %14240, %14241, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14243 = torch.prims.convert_element_type %14242, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14244 = torch.prims.convert_element_type %result1_430, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14245 = torch.prims.convert_element_type %14236, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14246 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14247 = torch.aten.view %14243, %14246 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14248 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14249 = torch.aten.transpose.int %14248, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %14250 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14251 = torch.prims.convert_element_type %14250, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14252 = torch.prims.convert_element_type %14247, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14253 = torch.prims.convert_element_type %14249, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14254 = torch.aten.mm %14252, %14253 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14255 = torch.aten.mul.Scalar %14254, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14256 = torch.aten.mul.Scalar %14251, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14257 = torch.aten.add.Tensor %14255, %14256, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14258 = torch.prims.convert_element_type %14257, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14259 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14260 = torch.aten.view %14258, %14259 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14261 = torch.aten.slice.Tensor %14260, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14262 = torch.aten.slice.Tensor %14260, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14263 = torch.aten.gelu %14262, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14264 = torch.aten.mul.Tensor %14261, %14263 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14265 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14266 = torch.aten.view %14264, %14265 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %14267 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14268 = torch.aten.transpose.int %14267, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %14269 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14270 = torch.prims.convert_element_type %14269, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14271 = torch.prims.convert_element_type %14266, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14272 = torch.prims.convert_element_type %14268, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14273 = torch.aten.mm %14271, %14272 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14274 = torch.aten.mul.Scalar %14273, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14275 = torch.aten.mul.Scalar %14270, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14276 = torch.aten.add.Tensor %14274, %14275, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14277 = torch.prims.convert_element_type %14276, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14278 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14279 = torch.aten.view %14277, %14278 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14280 = torch.aten.add.Tensor %14279, %14232, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14281 = torch.prims.convert_element_type %14280, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14282 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_431, %result1_432 = torch.aten.var_mean.correction %14281, %14282, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14283 = torch.aten.add.Scalar %result0_431, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14284 = torch.aten.rsqrt %14283 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14285 = torch.aten.sub.Tensor %14280, %result1_432, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14286 = torch.aten.mul.Tensor %14285, %14284 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %14287 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14288 = torch.aten.mul.Tensor %14286, %14287 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %14289 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14290 = torch.aten.add.Tensor %14288, %14289, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14291 = torch.prims.convert_element_type %14290, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14292 = torch.prims.convert_element_type %result1_432, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14293 = torch.prims.convert_element_type %14284, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %14294 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14295 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14296 = torch.aten.view %14291, %14295 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14297 = torch_c.to_builtin_tensor %14296 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14298 = torch_c.to_builtin_tensor %14294 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14299 = tensor.empty() : tensor<2048x1280xf32>
    %14300 = linalg.fill ins(%cst : f32) outs(%14299 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14301 = tensor.empty() : tensor<2048x1280xf32>
    %14302 = linalg.fill ins(%cst : f32) outs(%14301 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14303:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14300, %14302, %14297, %14298, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14300, %14302)
    %14304 = arith.truncf %14303#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14305 = torch_c.from_builtin_tensor %14304 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14306 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14307 = torch.aten.view %14305, %14306 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %14308 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14309 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14310 = torch.aten.view %14291, %14309 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14311 = torch_c.to_builtin_tensor %14310 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14312 = torch_c.to_builtin_tensor %14308 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14313 = tensor.empty() : tensor<2048x1280xf32>
    %14314 = linalg.fill ins(%cst : f32) outs(%14313 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14315 = tensor.empty() : tensor<2048x1280xf32>
    %14316 = linalg.fill ins(%cst : f32) outs(%14315 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14317:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14314, %14316, %14311, %14312, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14314, %14316)
    %14318 = arith.truncf %14317#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14319 = torch_c.from_builtin_tensor %14318 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14320 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14321 = torch.aten.view %14319, %14320 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %14322 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14323 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14324 = torch.aten.view %14291, %14323 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14325 = torch_c.to_builtin_tensor %14324 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14326 = torch_c.to_builtin_tensor %14322 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14327 = tensor.empty() : tensor<2048x1280xf32>
    %14328 = linalg.fill ins(%cst : f32) outs(%14327 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14329 = tensor.empty() : tensor<2048x1280xf32>
    %14330 = linalg.fill ins(%cst : f32) outs(%14329 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14331:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14328, %14330, %14325, %14326, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14328, %14330)
    %14332 = arith.truncf %14331#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14333 = torch_c.from_builtin_tensor %14332 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14334 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14335 = torch.aten.view %14333, %14334 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14336 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14337 = torch.aten.view %14307, %14336 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14338 = torch.aten.transpose.int %14337, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14339 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14340 = torch.aten.view %14321, %14339 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14341 = torch.aten.transpose.int %14340, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14342 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14343 = torch.aten.view %14335, %14342 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14344 = torch.aten.transpose.int %14343, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14345:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14338, %14341, %14344, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14346 = torch.aten.transpose.int %14345#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14347 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14348 = torch.aten.view %14346, %14347 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14349 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14350 = torch.aten.view %14348, %14349 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14351 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14352 = torch.aten.transpose.int %14351, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %14353 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14354 = torch.prims.convert_element_type %14353, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14355 = torch.prims.convert_element_type %14350, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14356 = torch.prims.convert_element_type %14352, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14357 = torch.aten.mm %14355, %14356 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14358 = torch.aten.mul.Scalar %14357, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14359 = torch.aten.mul.Scalar %14354, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14360 = torch.aten.add.Tensor %14358, %14359, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14361 = torch.prims.convert_element_type %14360, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14362 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14363 = torch.aten.view %14361, %14362 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14364 = torch.aten.div.Scalar %14363, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14365 = torch.aten.add.Tensor %14364, %14280, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14366 = torch.prims.convert_element_type %14365, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14367 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_433, %result1_434 = torch.aten.var_mean.correction %14366, %14367, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14368 = torch.aten.add.Scalar %result0_433, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14369 = torch.aten.rsqrt %14368 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14370 = torch.aten.sub.Tensor %14365, %result1_434, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14371 = torch.aten.mul.Tensor %14370, %14369 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %14372 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14373 = torch.aten.mul.Tensor %14371, %14372 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %14374 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14375 = torch.aten.add.Tensor %14373, %14374, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14376 = torch.prims.convert_element_type %14375, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14377 = torch.prims.convert_element_type %result1_434, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14378 = torch.prims.convert_element_type %14369, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %14379 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14380 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14381 = torch.aten.view %14376, %14380 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14382 = torch_c.to_builtin_tensor %14381 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14383 = torch_c.to_builtin_tensor %14379 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14384 = tensor.empty() : tensor<2048x1280xf32>
    %14385 = linalg.fill ins(%cst : f32) outs(%14384 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14386 = tensor.empty() : tensor<2048x1280xf32>
    %14387 = linalg.fill ins(%cst : f32) outs(%14386 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14388:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14385, %14387, %14382, %14383, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14385, %14387)
    %14389 = arith.truncf %14388#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14390 = torch_c.from_builtin_tensor %14389 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14391 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14392 = torch.aten.view %14390, %14391 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %14393 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14394 = torch.aten.transpose.int %14393, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14395 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14396 = torch.aten.view %4, %14395 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14397 = torch.aten.mm %14396, %14394 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %14398 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14399 = torch.aten.view %14397, %14398 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %14400 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14401 = torch.aten.transpose.int %14400, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14402 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14403 = torch.aten.view %4, %14402 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14404 = torch.aten.mm %14403, %14401 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %14405 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14406 = torch.aten.view %14404, %14405 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %14407 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14408 = torch.aten.view %14392, %14407 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14409 = torch.aten.transpose.int %14408, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14410 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14411 = torch.aten.view %14399, %14410 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14412 = torch.aten.transpose.int %14411, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14413 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14414 = torch.aten.view %14406, %14413 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14415 = torch.aten.transpose.int %14414, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14416:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14409, %14412, %14415, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14417 = torch.aten.transpose.int %14416#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14418 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14419 = torch.aten.view %14417, %14418 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14420 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14421 = torch.aten.view %14419, %14420 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14422 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14423 = torch.aten.transpose.int %14422, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %14424 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14425 = torch.prims.convert_element_type %14424, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14426 = torch.prims.convert_element_type %14421, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14427 = torch.prims.convert_element_type %14423, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14428 = torch.aten.mm %14426, %14427 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14429 = torch.aten.mul.Scalar %14428, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14430 = torch.aten.mul.Scalar %14425, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14431 = torch.aten.add.Tensor %14429, %14430, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14432 = torch.prims.convert_element_type %14431, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14433 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14434 = torch.aten.view %14432, %14433 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14435 = torch.aten.div.Scalar %14434, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14436 = torch.aten.add.Tensor %14435, %14365, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14437 = torch.prims.convert_element_type %14436, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14438 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_435, %result1_436 = torch.aten.var_mean.correction %14437, %14438, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14439 = torch.aten.add.Scalar %result0_435, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14440 = torch.aten.rsqrt %14439 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14441 = torch.aten.sub.Tensor %14436, %result1_436, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14442 = torch.aten.mul.Tensor %14441, %14440 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %14443 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14444 = torch.aten.mul.Tensor %14442, %14443 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %14445 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14446 = torch.aten.add.Tensor %14444, %14445, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14447 = torch.prims.convert_element_type %14446, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14448 = torch.prims.convert_element_type %result1_436, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14449 = torch.prims.convert_element_type %14440, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14450 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14451 = torch.aten.view %14447, %14450 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14452 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14453 = torch.aten.transpose.int %14452, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %14454 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14455 = torch.prims.convert_element_type %14454, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14456 = torch.prims.convert_element_type %14451, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14457 = torch.prims.convert_element_type %14453, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14458 = torch.aten.mm %14456, %14457 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14459 = torch.aten.mul.Scalar %14458, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14460 = torch.aten.mul.Scalar %14455, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14461 = torch.aten.add.Tensor %14459, %14460, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14462 = torch.prims.convert_element_type %14461, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14463 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14464 = torch.aten.view %14462, %14463 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14465 = torch.aten.slice.Tensor %14464, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14466 = torch.aten.slice.Tensor %14464, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14467 = torch.aten.gelu %14466, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14468 = torch.aten.mul.Tensor %14465, %14467 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14469 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14470 = torch.aten.view %14468, %14469 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %14471 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14472 = torch.aten.transpose.int %14471, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %14473 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14474 = torch.prims.convert_element_type %14473, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14475 = torch.prims.convert_element_type %14470, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14476 = torch.prims.convert_element_type %14472, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14477 = torch.aten.mm %14475, %14476 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14478 = torch.aten.mul.Scalar %14477, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14479 = torch.aten.mul.Scalar %14474, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14480 = torch.aten.add.Tensor %14478, %14479, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14481 = torch.prims.convert_element_type %14480, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14482 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14483 = torch.aten.view %14481, %14482 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14484 = torch.aten.add.Tensor %14483, %14436, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14485 = torch.prims.convert_element_type %14484, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14486 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_437, %result1_438 = torch.aten.var_mean.correction %14485, %14486, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14487 = torch.aten.add.Scalar %result0_437, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14488 = torch.aten.rsqrt %14487 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14489 = torch.aten.sub.Tensor %14484, %result1_438, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14490 = torch.aten.mul.Tensor %14489, %14488 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %14491 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14492 = torch.aten.mul.Tensor %14490, %14491 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %14493 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14494 = torch.aten.add.Tensor %14492, %14493, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14495 = torch.prims.convert_element_type %14494, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14496 = torch.prims.convert_element_type %result1_438, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14497 = torch.prims.convert_element_type %14488, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %14498 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14499 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14500 = torch.aten.view %14495, %14499 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14501 = torch_c.to_builtin_tensor %14500 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14502 = torch_c.to_builtin_tensor %14498 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14503 = tensor.empty() : tensor<2048x1280xf32>
    %14504 = linalg.fill ins(%cst : f32) outs(%14503 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14505 = tensor.empty() : tensor<2048x1280xf32>
    %14506 = linalg.fill ins(%cst : f32) outs(%14505 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14507:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14504, %14506, %14501, %14502, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14504, %14506)
    %14508 = arith.truncf %14507#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14509 = torch_c.from_builtin_tensor %14508 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14510 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14511 = torch.aten.view %14509, %14510 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %14512 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14513 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14514 = torch.aten.view %14495, %14513 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14515 = torch_c.to_builtin_tensor %14514 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14516 = torch_c.to_builtin_tensor %14512 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14517 = tensor.empty() : tensor<2048x1280xf32>
    %14518 = linalg.fill ins(%cst : f32) outs(%14517 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14519 = tensor.empty() : tensor<2048x1280xf32>
    %14520 = linalg.fill ins(%cst : f32) outs(%14519 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14521:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14518, %14520, %14515, %14516, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14518, %14520)
    %14522 = arith.truncf %14521#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14523 = torch_c.from_builtin_tensor %14522 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14524 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14525 = torch.aten.view %14523, %14524 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %14526 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14527 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14528 = torch.aten.view %14495, %14527 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14529 = torch_c.to_builtin_tensor %14528 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14530 = torch_c.to_builtin_tensor %14526 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14531 = tensor.empty() : tensor<2048x1280xf32>
    %14532 = linalg.fill ins(%cst : f32) outs(%14531 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14533 = tensor.empty() : tensor<2048x1280xf32>
    %14534 = linalg.fill ins(%cst : f32) outs(%14533 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14535:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14532, %14534, %14529, %14530, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14532, %14534)
    %14536 = arith.truncf %14535#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14537 = torch_c.from_builtin_tensor %14536 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14538 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14539 = torch.aten.view %14537, %14538 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14540 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14541 = torch.aten.view %14511, %14540 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14542 = torch.aten.transpose.int %14541, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14543 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14544 = torch.aten.view %14525, %14543 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14545 = torch.aten.transpose.int %14544, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14546 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14547 = torch.aten.view %14539, %14546 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14548 = torch.aten.transpose.int %14547, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14549:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14542, %14545, %14548, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14550 = torch.aten.transpose.int %14549#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14551 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14552 = torch.aten.view %14550, %14551 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14553 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14554 = torch.aten.view %14552, %14553 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14555 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14556 = torch.aten.transpose.int %14555, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %14557 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14558 = torch.prims.convert_element_type %14557, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14559 = torch.prims.convert_element_type %14554, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14560 = torch.prims.convert_element_type %14556, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14561 = torch.aten.mm %14559, %14560 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14562 = torch.aten.mul.Scalar %14561, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14563 = torch.aten.mul.Scalar %14558, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14564 = torch.aten.add.Tensor %14562, %14563, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14565 = torch.prims.convert_element_type %14564, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14566 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14567 = torch.aten.view %14565, %14566 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14568 = torch.aten.div.Scalar %14567, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14569 = torch.aten.add.Tensor %14568, %14484, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14570 = torch.prims.convert_element_type %14569, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14571 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_439, %result1_440 = torch.aten.var_mean.correction %14570, %14571, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14572 = torch.aten.add.Scalar %result0_439, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14573 = torch.aten.rsqrt %14572 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14574 = torch.aten.sub.Tensor %14569, %result1_440, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14575 = torch.aten.mul.Tensor %14574, %14573 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %14576 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14577 = torch.aten.mul.Tensor %14575, %14576 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %14578 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14579 = torch.aten.add.Tensor %14577, %14578, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14580 = torch.prims.convert_element_type %14579, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14581 = torch.prims.convert_element_type %result1_440, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14582 = torch.prims.convert_element_type %14573, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %14583 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14584 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14585 = torch.aten.view %14580, %14584 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14586 = torch_c.to_builtin_tensor %14585 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14587 = torch_c.to_builtin_tensor %14583 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14588 = tensor.empty() : tensor<2048x1280xf32>
    %14589 = linalg.fill ins(%cst : f32) outs(%14588 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14590 = tensor.empty() : tensor<2048x1280xf32>
    %14591 = linalg.fill ins(%cst : f32) outs(%14590 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14592:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14589, %14591, %14586, %14587, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14589, %14591)
    %14593 = arith.truncf %14592#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14594 = torch_c.from_builtin_tensor %14593 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14595 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14596 = torch.aten.view %14594, %14595 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %14597 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14598 = torch.aten.transpose.int %14597, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14599 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14600 = torch.aten.view %4, %14599 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14601 = torch.aten.mm %14600, %14598 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %14602 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14603 = torch.aten.view %14601, %14602 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %14604 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14605 = torch.aten.transpose.int %14604, %int0, %int1 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14606 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14607 = torch.aten.view %4, %14606 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14608 = torch.aten.mm %14607, %14605 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[128,1280],f16>
    %14609 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14610 = torch.aten.view %14608, %14609 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %14611 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14612 = torch.aten.view %14596, %14611 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14613 = torch.aten.transpose.int %14612, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14614 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14615 = torch.aten.view %14603, %14614 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14616 = torch.aten.transpose.int %14615, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14617 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14618 = torch.aten.view %14610, %14617 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14619 = torch.aten.transpose.int %14618, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14620:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14613, %14616, %14619, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14621 = torch.aten.transpose.int %14620#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14622 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14623 = torch.aten.view %14621, %14622 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14624 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14625 = torch.aten.view %14623, %14624 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14626 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14627 = torch.aten.transpose.int %14626, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %14628 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14629 = torch.prims.convert_element_type %14628, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14630 = torch.prims.convert_element_type %14625, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14631 = torch.prims.convert_element_type %14627, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14632 = torch.aten.mm %14630, %14631 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14633 = torch.aten.mul.Scalar %14632, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14634 = torch.aten.mul.Scalar %14629, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14635 = torch.aten.add.Tensor %14633, %14634, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14636 = torch.prims.convert_element_type %14635, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14637 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14638 = torch.aten.view %14636, %14637 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14639 = torch.aten.div.Scalar %14638, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14640 = torch.aten.add.Tensor %14639, %14569, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14641 = torch.prims.convert_element_type %14640, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14642 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_441, %result1_442 = torch.aten.var_mean.correction %14641, %14642, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14643 = torch.aten.add.Scalar %result0_441, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14644 = torch.aten.rsqrt %14643 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14645 = torch.aten.sub.Tensor %14640, %result1_442, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14646 = torch.aten.mul.Tensor %14645, %14644 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %14647 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14648 = torch.aten.mul.Tensor %14646, %14647 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %14649 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14650 = torch.aten.add.Tensor %14648, %14649, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14651 = torch.prims.convert_element_type %14650, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14652 = torch.prims.convert_element_type %result1_442, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14653 = torch.prims.convert_element_type %14644, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14654 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14655 = torch.aten.view %14651, %14654 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14656 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14657 = torch.aten.transpose.int %14656, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %14658 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14659 = torch.prims.convert_element_type %14658, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14660 = torch.prims.convert_element_type %14655, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14661 = torch.prims.convert_element_type %14657, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14662 = torch.aten.mm %14660, %14661 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14663 = torch.aten.mul.Scalar %14662, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14664 = torch.aten.mul.Scalar %14659, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14665 = torch.aten.add.Tensor %14663, %14664, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14666 = torch.prims.convert_element_type %14665, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14667 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14668 = torch.aten.view %14666, %14667 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14669 = torch.aten.slice.Tensor %14668, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14670 = torch.aten.slice.Tensor %14668, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14671 = torch.aten.gelu %14670, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14672 = torch.aten.mul.Tensor %14669, %14671 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14673 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14674 = torch.aten.view %14672, %14673 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %14675 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14676 = torch.aten.transpose.int %14675, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %14677 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14678 = torch.prims.convert_element_type %14677, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14679 = torch.prims.convert_element_type %14674, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14680 = torch.prims.convert_element_type %14676, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14681 = torch.aten.mm %14679, %14680 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14682 = torch.aten.mul.Scalar %14681, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14683 = torch.aten.mul.Scalar %14678, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14684 = torch.aten.add.Tensor %14682, %14683, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14685 = torch.prims.convert_element_type %14684, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14686 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14687 = torch.aten.view %14685, %14686 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14688 = torch.aten.add.Tensor %14687, %14640, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14689 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14690 = torch.aten.view %14688, %14689 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_out.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_out.weight : tensor<1280x1280xf16>
    %14691 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14692 = torch.aten.transpose.int %14691, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_out.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_out.bias : tensor<1280xf16>
    %14693 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14694 = torch.prims.convert_element_type %14693, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14695 = torch.prims.convert_element_type %14690, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14696 = torch.prims.convert_element_type %14692, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14697 = torch.aten.mm %14695, %14696 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14698 = torch.aten.mul.Scalar %14697, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14699 = torch.aten.mul.Scalar %14694, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14700 = torch.aten.add.Tensor %14698, %14699, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14701 = torch.prims.convert_element_type %14700, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14702 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14703 = torch.aten.view %14701, %14702 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14704 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14705 = torch.aten.view %14703, %14704 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %14706 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14707 = torch.aten.permute %14705, %14706 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %14708 = torch.aten.add.Tensor %14707, %12597, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %14709 = torch.prims.convert_element_type %14708, %int6 : !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %cpu_443 = torch.constant.device "cpu"
    %14710 = torch.aten.arange %int64, %int6, %none, %cpu_443, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[64],f32>
    %14711 = torch.aten.add.Scalar %14710, %float0.000000e00, %int1 : !torch.vtensor<[64],f32>, !torch.float, !torch.int -> !torch.vtensor<[64],f32>
    %14712 = torch.aten.mul.Scalar %14711, %float5.000000e-01 : !torch.vtensor<[64],f32>, !torch.float -> !torch.vtensor<[64],f32>
    %14713 = torch.prims.convert_element_type %14712, %int4 : !torch.vtensor<[64],f32>, !torch.int -> !torch.vtensor<[64],si64>
    %14714 = torch.aten.unsqueeze %14713, %int-1 : !torch.vtensor<[64],si64>, !torch.int -> !torch.vtensor<[64,1],si64>
    %cpu_444 = torch.constant.device "cpu"
    %14715 = torch.aten.arange %int64, %int6, %none, %cpu_444, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[64],f32>
    %14716 = torch.aten.add.Scalar %14715, %float0.000000e00, %int1 : !torch.vtensor<[64],f32>, !torch.float, !torch.int -> !torch.vtensor<[64],f32>
    %14717 = torch.aten.mul.Scalar %14716, %float5.000000e-01 : !torch.vtensor<[64],f32>, !torch.float -> !torch.vtensor<[64],f32>
    %14718 = torch.prims.convert_element_type %14717, %int4 : !torch.vtensor<[64],f32>, !torch.int -> !torch.vtensor<[64],si64>
    %14719 = torch.prim.ListConstruct %none, %none, %14714, %14718 : (!torch.none, !torch.none, !torch.vtensor<[64,1],si64>, !torch.vtensor<[64],si64>) -> !torch.list<optional<vtensor>>
    %14720 = torch.aten.index.Tensor %14709, %14719 : !torch.vtensor<[2,1280,32,32],f32>, !torch.list<optional<vtensor>> -> !torch.vtensor<[2,1280,64,64],f32>
    %14721 = torch.prims.convert_element_type %14720, %int5 : !torch.vtensor<[2,1280,64,64],f32>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %_params.unet.up_blocks.0.upsamplers.0.conv.weight = util.global.load @_params.unet.up_blocks.0.upsamplers.0.conv.weight : tensor<1280x1280x3x3xf16>
    %14722 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.upsamplers.0.conv.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.upsamplers.0.conv.bias = util.global.load @_params.unet.up_blocks.0.upsamplers.0.conv.bias : tensor<1280xf16>
    %14723 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.upsamplers.0.conv.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14724 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14725 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14726 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14727 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %14728 = torch.aten.convolution %14721, %14722, %14723, %14724, %14725, %14726, %false, %14727, %int1 : !torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %14729 = torch.prim.ListConstruct %14728, %1336 : (!torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>) -> !torch.list<vtensor>
    %14730 = torch.aten.cat %14729, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,1920,64,64],f16>
    %14731 = torch.prim.ListConstruct %int2, %int32, %int60, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14732 = torch.aten.view %14730, %14731 : !torch.vtensor<[2,1920,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,60,4096],f16>
    %14733 = torch.prims.convert_element_type %14732, %int6 : !torch.vtensor<[2,32,60,4096],f16>, !torch.int -> !torch.vtensor<[2,32,60,4096],f32>
    %14734 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_445, %result1_446 = torch.aten.var_mean.correction %14733, %14734, %int0, %true : !torch.vtensor<[2,32,60,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %14735 = torch.aten.add.Scalar %result0_445, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %14736 = torch.aten.rsqrt %14735 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %14737 = torch.aten.sub.Tensor %14732, %result1_446, %int1 : !torch.vtensor<[2,32,60,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,60,4096],f32>
    %14738 = torch.aten.mul.Tensor %14737, %14736 : !torch.vtensor<[2,32,60,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,60,4096],f32>
    %14739 = torch.prim.ListConstruct %int2, %int1920, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14740 = torch.aten.view %14738, %14739 : !torch.vtensor<[2,32,60,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,1920,64,64],f32>
    %_params.unet.up_blocks.1.resnets.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.norm1.bias : tensor<1920xf16>
    %14741 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm1.bias : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %14742 = torch.aten.unsqueeze %14741, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %14743 = torch.aten.unsqueeze %14742, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %14744 = torch.aten.unsqueeze %14743, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %_params.unet.up_blocks.1.resnets.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.norm1.weight : tensor<1920xf16>
    %14745 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm1.weight : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %14746 = torch.aten.unsqueeze %14745, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %14747 = torch.aten.unsqueeze %14746, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %14748 = torch.aten.unsqueeze %14747, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %14749 = torch.aten.mul.Tensor %14740, %14748 : !torch.vtensor<[2,1920,64,64],f32>, !torch.vtensor<[1,1920,1,1],f16> -> !torch.vtensor<[2,1920,64,64],f32>
    %14750 = torch.aten.add.Tensor %14749, %14744, %int1 : !torch.vtensor<[2,1920,64,64],f32>, !torch.vtensor<[1,1920,1,1],f16>, !torch.int -> !torch.vtensor<[2,1920,64,64],f32>
    %14751 = torch.prims.convert_element_type %14750, %int5 : !torch.vtensor<[2,1920,64,64],f32>, !torch.int -> !torch.vtensor<[2,1920,64,64],f16>
    %14752 = torch.prims.convert_element_type %result1_446, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %14753 = torch.prims.convert_element_type %14736, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %14754 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %14755 = torch.prims.squeeze %14752, %14754 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %14756 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14757 = torch.prims.squeeze %14755, %14756 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %14758 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %14759 = torch.prims.squeeze %14753, %14758 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %14760 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14761 = torch.prims.squeeze %14759, %14760 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %14762 = torch.aten.silu %14751 : !torch.vtensor<[2,1920,64,64],f16> -> !torch.vtensor<[2,1920,64,64],f16>
    %_params.unet.up_blocks.1.resnets.0.conv1.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.conv1.weight : tensor<640x1920x3x3xf16>
    %14763 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv1.weight : tensor<640x1920x3x3xf16> -> !torch.vtensor<[640,1920,3,3],f16>
    %_params.unet.up_blocks.1.resnets.0.conv1.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.conv1.bias : tensor<640xf16>
    %14764 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14765 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14766 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14767 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14768 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %14769 = torch.aten.convolution %14762, %14763, %14764, %14765, %14766, %14767, %false, %14768, %int1 : !torch.vtensor<[2,1920,64,64],f16>, !torch.vtensor<[640,1920,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %14770 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16>
    %14771 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %14772 = torch.aten.transpose.int %14771, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16>
    %14773 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14774 = torch.prims.convert_element_type %14773, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %14775 = torch.prims.convert_element_type %14770, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %14776 = torch.prims.convert_element_type %14772, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %14777 = torch.aten.mm %14775, %14776 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %14778 = torch.aten.mul.Scalar %14777, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %14779 = torch.aten.mul.Scalar %14774, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %14780 = torch.aten.add.Tensor %14778, %14779, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %14781 = torch.prims.convert_element_type %14780, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %14782 = torch.aten.unsqueeze %14781, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %14783 = torch.aten.unsqueeze %14782, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %14784 = torch.aten.add.Tensor %14769, %14783, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %14785 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14786 = torch.aten.view %14784, %14785 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %14787 = torch.prims.convert_element_type %14786, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %14788 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_447, %result1_448 = torch.aten.var_mean.correction %14787, %14788, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %14789 = torch.aten.add.Scalar %result0_447, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %14790 = torch.aten.rsqrt %14789 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %14791 = torch.aten.sub.Tensor %14786, %result1_448, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %14792 = torch.aten.mul.Tensor %14791, %14790 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %14793 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14794 = torch.aten.view %14792, %14793 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.resnets.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.norm2.bias : tensor<640xf16>
    %14795 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14796 = torch.aten.unsqueeze %14795, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %14797 = torch.aten.unsqueeze %14796, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %14798 = torch.aten.unsqueeze %14797, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.resnets.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.norm2.weight : tensor<640xf16>
    %14799 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14800 = torch.aten.unsqueeze %14799, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %14801 = torch.aten.unsqueeze %14800, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %14802 = torch.aten.unsqueeze %14801, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %14803 = torch.aten.mul.Tensor %14794, %14802 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %14804 = torch.aten.add.Tensor %14803, %14798, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %14805 = torch.prims.convert_element_type %14804, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %14806 = torch.prims.convert_element_type %result1_448, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %14807 = torch.prims.convert_element_type %14790, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %14808 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %14809 = torch.prims.squeeze %14806, %14808 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %14810 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14811 = torch.prims.squeeze %14809, %14810 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %14812 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %14813 = torch.prims.squeeze %14807, %14812 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %14814 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14815 = torch.prims.squeeze %14813, %14814 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %14816 = torch.aten.silu %14805 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.0.conv2.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16>
    %14817 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.resnets.0.conv2.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.conv2.bias : tensor<640xf16>
    %14818 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14819 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14820 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14821 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14822 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %14823 = torch.aten.convolution %14816, %14817, %14818, %14819, %14820, %14821, %false, %14822, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x1920x1x1xf16>
    %14824 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x1920x1x1xf16> -> !torch.vtensor<[640,1920,1,1],f16>
    %_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16>
    %14825 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14826 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14827 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %14828 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %14829 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %14830 = torch.aten.convolution %14730, %14824, %14825, %14826, %14827, %14828, %false, %14829, %int1 : !torch.vtensor<[2,1920,64,64],f16>, !torch.vtensor<[640,1920,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %14831 = torch.aten.add.Tensor %14830, %14823, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %14832 = torch.aten.div.Scalar %14831, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %14833 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14834 = torch.aten.view %14832, %14833 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %14835 = torch.prims.convert_element_type %14834, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %14836 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_449, %result1_450 = torch.aten.var_mean.correction %14835, %14836, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %14837 = torch.aten.add.Scalar %result0_449, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %14838 = torch.aten.rsqrt %14837 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %14839 = torch.aten.sub.Tensor %14834, %result1_450, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %14840 = torch.aten.mul.Tensor %14839, %14838 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %14841 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14842 = torch.aten.view %14840, %14841 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.attentions.0.norm.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.norm.bias : tensor<640xf16>
    %14843 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14844 = torch.aten.unsqueeze %14843, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %14845 = torch.aten.unsqueeze %14844, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %14846 = torch.aten.unsqueeze %14845, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.attentions.0.norm.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.norm.weight : tensor<640xf16>
    %14847 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14848 = torch.aten.unsqueeze %14847, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %14849 = torch.aten.unsqueeze %14848, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %14850 = torch.aten.unsqueeze %14849, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %14851 = torch.aten.mul.Tensor %14842, %14850 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %14852 = torch.aten.add.Tensor %14851, %14846, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %14853 = torch.prims.convert_element_type %14852, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %14854 = torch.prims.convert_element_type %result1_450, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %14855 = torch.prims.convert_element_type %14838, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %14856 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %14857 = torch.prims.squeeze %14854, %14856 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %14858 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14859 = torch.prims.squeeze %14857, %14858 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %14860 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %14861 = torch.prims.squeeze %14855, %14860 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %14862 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14863 = torch.prims.squeeze %14861, %14862 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %14864 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14865 = torch.aten.permute %14853, %14864 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %14866 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14867 = torch.aten.view %14865, %14866 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_in.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16>
    %14868 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %14869 = torch.aten.transpose.int %14868, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %14870 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %14871 = torch.aten._unsafe_view %14867, %14870 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %14872 = torch.aten.mm %14871, %14869 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %14873 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14874 = torch.aten.view %14872, %14873 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_in.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_in.bias : tensor<640xf16>
    %14875 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14876 = torch.aten.add.Tensor %14874, %14875, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %14877 = torch.prims.convert_element_type %14876, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %14878 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_451, %result1_452 = torch.aten.var_mean.correction %14877, %14878, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %14879 = torch.aten.add.Scalar %result0_451, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %14880 = torch.aten.rsqrt %14879 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %14881 = torch.aten.sub.Tensor %14876, %result1_452, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %14882 = torch.aten.mul.Tensor %14881, %14880 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %14883 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14884 = torch.aten.mul.Tensor %14882, %14883 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %14885 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14886 = torch.aten.add.Tensor %14884, %14885, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %14887 = torch.prims.convert_element_type %14886, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %14888 = torch.prims.convert_element_type %result1_452, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %14889 = torch.prims.convert_element_type %14880, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %14890 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %14891 = torch.aten.transpose.int %14890, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %14892 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %14893 = torch.aten.view %14887, %14892 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %14894 = torch.aten.mm %14893, %14891 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %14895 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14896 = torch.aten.view %14894, %14895 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %14897 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %14898 = torch.aten.transpose.int %14897, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %14899 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %14900 = torch.aten.view %14887, %14899 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %14901 = torch.aten.mm %14900, %14898 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %14902 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14903 = torch.aten.view %14901, %14902 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %14904 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %14905 = torch.aten.transpose.int %14904, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %14906 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %14907 = torch.aten.view %14887, %14906 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %14908 = torch.aten.mm %14907, %14905 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %14909 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14910 = torch.aten.view %14908, %14909 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %14911 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14912 = torch.aten.view %14896, %14911 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %14913 = torch.aten.transpose.int %14912, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %14914 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14915 = torch.aten.view %14903, %14914 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %14916 = torch.aten.transpose.int %14915, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %14917 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14918 = torch.aten.view %14910, %14917 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %14919 = torch.aten.transpose.int %14918, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %14920:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14913, %14916, %14919, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %14921 = torch.aten.transpose.int %14920#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %14922 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14923 = torch.aten.view %14921, %14922 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %14924 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %14925 = torch.aten.view %14923, %14924 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %14926 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %14927 = torch.aten.transpose.int %14926, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %14928 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14929 = torch.prims.convert_element_type %14928, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %14930 = torch.prims.convert_element_type %14925, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %14931 = torch.prims.convert_element_type %14927, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %14932 = torch.aten.mm %14930, %14931 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %14933 = torch.aten.mul.Scalar %14932, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %14934 = torch.aten.mul.Scalar %14929, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %14935 = torch.aten.add.Tensor %14933, %14934, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %14936 = torch.prims.convert_element_type %14935, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %14937 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14938 = torch.aten.view %14936, %14937 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %14939 = torch.aten.div.Scalar %14938, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %14940 = torch.aten.add.Tensor %14939, %14876, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %14941 = torch.prims.convert_element_type %14940, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %14942 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_453, %result1_454 = torch.aten.var_mean.correction %14941, %14942, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %14943 = torch.aten.add.Scalar %result0_453, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %14944 = torch.aten.rsqrt %14943 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %14945 = torch.aten.sub.Tensor %14940, %result1_454, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %14946 = torch.aten.mul.Tensor %14945, %14944 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %14947 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14948 = torch.aten.mul.Tensor %14946, %14947 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %14949 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14950 = torch.aten.add.Tensor %14948, %14949, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %14951 = torch.prims.convert_element_type %14950, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %14952 = torch.prims.convert_element_type %result1_454, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %14953 = torch.prims.convert_element_type %14944, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %14954 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %14955 = torch.aten.transpose.int %14954, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %14956 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %14957 = torch.aten.view %14951, %14956 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %14958 = torch.aten.mm %14957, %14955 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %14959 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14960 = torch.aten.view %14958, %14959 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %14961 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %14962 = torch.aten.transpose.int %14961, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %14963 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14964 = torch.aten.view %4, %14963 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14965 = torch.aten.mm %14964, %14962 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %14966 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14967 = torch.aten.view %14965, %14966 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %14968 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %14969 = torch.aten.transpose.int %14968, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %14970 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14971 = torch.aten.view %4, %14970 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14972 = torch.aten.mm %14971, %14969 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %14973 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14974 = torch.aten.view %14972, %14973 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %14975 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14976 = torch.aten.view %14960, %14975 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %14977 = torch.aten.transpose.int %14976, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %14978 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14979 = torch.aten.view %14967, %14978 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %14980 = torch.aten.transpose.int %14979, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %14981 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14982 = torch.aten.view %14974, %14981 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %14983 = torch.aten.transpose.int %14982, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %14984:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14977, %14980, %14983, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %14985 = torch.aten.transpose.int %14984#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %14986 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14987 = torch.aten.view %14985, %14986 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %14988 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %14989 = torch.aten.view %14987, %14988 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %14990 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %14991 = torch.aten.transpose.int %14990, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %14992 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %14993 = torch.prims.convert_element_type %14992, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %14994 = torch.prims.convert_element_type %14989, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %14995 = torch.prims.convert_element_type %14991, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %14996 = torch.aten.mm %14994, %14995 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %14997 = torch.aten.mul.Scalar %14996, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %14998 = torch.aten.mul.Scalar %14993, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %14999 = torch.aten.add.Tensor %14997, %14998, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15000 = torch.prims.convert_element_type %14999, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15001 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15002 = torch.aten.view %15000, %15001 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15003 = torch.aten.div.Scalar %15002, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15004 = torch.aten.add.Tensor %15003, %14940, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15005 = torch.prims.convert_element_type %15004, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15006 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_455, %result1_456 = torch.aten.var_mean.correction %15005, %15006, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15007 = torch.aten.add.Scalar %result0_455, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15008 = torch.aten.rsqrt %15007 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15009 = torch.aten.sub.Tensor %15004, %result1_456, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15010 = torch.aten.mul.Tensor %15009, %15008 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %15011 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15012 = torch.aten.mul.Tensor %15010, %15011 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %15013 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15014 = torch.aten.add.Tensor %15012, %15013, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15015 = torch.prims.convert_element_type %15014, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15016 = torch.prims.convert_element_type %result1_456, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15017 = torch.prims.convert_element_type %15008, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15018 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15019 = torch.aten.view %15015, %15018 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %15020 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %15021 = torch.aten.transpose.int %15020, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %15022 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %15023 = torch.prims.convert_element_type %15022, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %15024 = torch.prims.convert_element_type %15019, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15025 = torch.prims.convert_element_type %15021, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %15026 = torch.aten.mm %15024, %15025 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %15027 = torch.aten.mul.Scalar %15026, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15028 = torch.aten.mul.Scalar %15023, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %15029 = torch.aten.add.Tensor %15027, %15028, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15030 = torch.prims.convert_element_type %15029, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %15031 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15032 = torch.aten.view %15030, %15031 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %15033 = torch.aten.slice.Tensor %15032, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15034 = torch.aten.slice.Tensor %15032, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15035 = torch.aten.gelu %15034, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %15036 = torch.aten.mul.Tensor %15033, %15035 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %15037 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %15038 = torch.aten.view %15036, %15037 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %15039 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %15040 = torch.aten.transpose.int %15039, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %15041 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15042 = torch.prims.convert_element_type %15041, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15043 = torch.prims.convert_element_type %15038, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %15044 = torch.prims.convert_element_type %15040, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %15045 = torch.aten.mm %15043, %15044 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15046 = torch.aten.mul.Scalar %15045, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15047 = torch.aten.mul.Scalar %15042, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15048 = torch.aten.add.Tensor %15046, %15047, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15049 = torch.prims.convert_element_type %15048, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15050 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15051 = torch.aten.view %15049, %15050 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15052 = torch.aten.add.Tensor %15051, %15004, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15053 = torch.prims.convert_element_type %15052, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15054 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_457, %result1_458 = torch.aten.var_mean.correction %15053, %15054, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15055 = torch.aten.add.Scalar %result0_457, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15056 = torch.aten.rsqrt %15055 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15057 = torch.aten.sub.Tensor %15052, %result1_458, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15058 = torch.aten.mul.Tensor %15057, %15056 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %15059 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15060 = torch.aten.mul.Tensor %15058, %15059 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %15061 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15062 = torch.aten.add.Tensor %15060, %15061, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15063 = torch.prims.convert_element_type %15062, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15064 = torch.prims.convert_element_type %result1_458, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15065 = torch.prims.convert_element_type %15056, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %15066 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15067 = torch.aten.transpose.int %15066, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15068 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15069 = torch.aten.view %15063, %15068 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15070 = torch.aten.mm %15069, %15067 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15071 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15072 = torch.aten.view %15070, %15071 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %15073 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15074 = torch.aten.transpose.int %15073, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15075 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15076 = torch.aten.view %15063, %15075 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15077 = torch.aten.mm %15076, %15074 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15078 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15079 = torch.aten.view %15077, %15078 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %15080 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15081 = torch.aten.transpose.int %15080, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15082 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15083 = torch.aten.view %15063, %15082 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15084 = torch.aten.mm %15083, %15081 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15085 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15086 = torch.aten.view %15084, %15085 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15087 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15088 = torch.aten.view %15072, %15087 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15089 = torch.aten.transpose.int %15088, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15090 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15091 = torch.aten.view %15079, %15090 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15092 = torch.aten.transpose.int %15091, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15093 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15094 = torch.aten.view %15086, %15093 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15095 = torch.aten.transpose.int %15094, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15096:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15089, %15092, %15095, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15097 = torch.aten.transpose.int %15096#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15098 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15099 = torch.aten.view %15097, %15098 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15100 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15101 = torch.aten.view %15099, %15100 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %15102 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15103 = torch.aten.transpose.int %15102, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %15104 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15105 = torch.prims.convert_element_type %15104, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15106 = torch.prims.convert_element_type %15101, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15107 = torch.prims.convert_element_type %15103, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15108 = torch.aten.mm %15106, %15107 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15109 = torch.aten.mul.Scalar %15108, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15110 = torch.aten.mul.Scalar %15105, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15111 = torch.aten.add.Tensor %15109, %15110, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15112 = torch.prims.convert_element_type %15111, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15113 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15114 = torch.aten.view %15112, %15113 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15115 = torch.aten.div.Scalar %15114, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15116 = torch.aten.add.Tensor %15115, %15052, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15117 = torch.prims.convert_element_type %15116, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15118 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_459, %result1_460 = torch.aten.var_mean.correction %15117, %15118, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15119 = torch.aten.add.Scalar %result0_459, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15120 = torch.aten.rsqrt %15119 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15121 = torch.aten.sub.Tensor %15116, %result1_460, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15122 = torch.aten.mul.Tensor %15121, %15120 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %15123 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15124 = torch.aten.mul.Tensor %15122, %15123 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %15125 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15126 = torch.aten.add.Tensor %15124, %15125, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15127 = torch.prims.convert_element_type %15126, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15128 = torch.prims.convert_element_type %result1_460, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15129 = torch.prims.convert_element_type %15120, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %15130 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15131 = torch.aten.transpose.int %15130, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15132 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15133 = torch.aten.view %15127, %15132 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15134 = torch.aten.mm %15133, %15131 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15135 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15136 = torch.aten.view %15134, %15135 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %15137 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15138 = torch.aten.transpose.int %15137, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15139 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15140 = torch.aten.view %4, %15139 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15141 = torch.aten.mm %15140, %15138 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15142 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15143 = torch.aten.view %15141, %15142 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %15144 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15145 = torch.aten.transpose.int %15144, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15146 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15147 = torch.aten.view %4, %15146 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15148 = torch.aten.mm %15147, %15145 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15149 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15150 = torch.aten.view %15148, %15149 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %15151 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15152 = torch.aten.view %15136, %15151 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15153 = torch.aten.transpose.int %15152, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15154 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15155 = torch.aten.view %15143, %15154 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15156 = torch.aten.transpose.int %15155, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15157 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15158 = torch.aten.view %15150, %15157 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15159 = torch.aten.transpose.int %15158, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15160:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15153, %15156, %15159, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15161 = torch.aten.transpose.int %15160#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15162 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15163 = torch.aten.view %15161, %15162 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15164 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15165 = torch.aten.view %15163, %15164 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %15166 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15167 = torch.aten.transpose.int %15166, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %15168 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15169 = torch.prims.convert_element_type %15168, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15170 = torch.prims.convert_element_type %15165, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15171 = torch.prims.convert_element_type %15167, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15172 = torch.aten.mm %15170, %15171 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15173 = torch.aten.mul.Scalar %15172, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15174 = torch.aten.mul.Scalar %15169, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15175 = torch.aten.add.Tensor %15173, %15174, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15176 = torch.prims.convert_element_type %15175, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15177 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15178 = torch.aten.view %15176, %15177 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15179 = torch.aten.div.Scalar %15178, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15180 = torch.aten.add.Tensor %15179, %15116, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15181 = torch.prims.convert_element_type %15180, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15182 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_461, %result1_462 = torch.aten.var_mean.correction %15181, %15182, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15183 = torch.aten.add.Scalar %result0_461, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15184 = torch.aten.rsqrt %15183 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15185 = torch.aten.sub.Tensor %15180, %result1_462, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15186 = torch.aten.mul.Tensor %15185, %15184 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %15187 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15188 = torch.aten.mul.Tensor %15186, %15187 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %15189 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15190 = torch.aten.add.Tensor %15188, %15189, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15191 = torch.prims.convert_element_type %15190, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15192 = torch.prims.convert_element_type %result1_462, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15193 = torch.prims.convert_element_type %15184, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15194 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15195 = torch.aten.view %15191, %15194 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %15196 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %15197 = torch.aten.transpose.int %15196, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %15198 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %15199 = torch.prims.convert_element_type %15198, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %15200 = torch.prims.convert_element_type %15195, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15201 = torch.prims.convert_element_type %15197, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %15202 = torch.aten.mm %15200, %15201 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %15203 = torch.aten.mul.Scalar %15202, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15204 = torch.aten.mul.Scalar %15199, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %15205 = torch.aten.add.Tensor %15203, %15204, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15206 = torch.prims.convert_element_type %15205, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %15207 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15208 = torch.aten.view %15206, %15207 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %15209 = torch.aten.slice.Tensor %15208, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15210 = torch.aten.slice.Tensor %15208, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15211 = torch.aten.gelu %15210, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %15212 = torch.aten.mul.Tensor %15209, %15211 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %15213 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %15214 = torch.aten.view %15212, %15213 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %15215 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %15216 = torch.aten.transpose.int %15215, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %15217 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15218 = torch.prims.convert_element_type %15217, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15219 = torch.prims.convert_element_type %15214, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %15220 = torch.prims.convert_element_type %15216, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %15221 = torch.aten.mm %15219, %15220 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15222 = torch.aten.mul.Scalar %15221, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15223 = torch.aten.mul.Scalar %15218, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15224 = torch.aten.add.Tensor %15222, %15223, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15225 = torch.prims.convert_element_type %15224, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15226 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15227 = torch.aten.view %15225, %15226 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15228 = torch.aten.add.Tensor %15227, %15180, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15229 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15230 = torch.aten.view %15228, %15229 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_out.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16>
    %15231 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15232 = torch.aten.transpose.int %15231, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_out.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_out.bias : tensor<640xf16>
    %15233 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15234 = torch.prims.convert_element_type %15233, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15235 = torch.prims.convert_element_type %15230, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15236 = torch.prims.convert_element_type %15232, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15237 = torch.aten.mm %15235, %15236 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15238 = torch.aten.mul.Scalar %15237, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15239 = torch.aten.mul.Scalar %15234, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15240 = torch.aten.add.Tensor %15238, %15239, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15241 = torch.prims.convert_element_type %15240, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15242 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15243 = torch.aten.view %15241, %15242 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15244 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15245 = torch.aten.view %15243, %15244 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %15246 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15247 = torch.aten.permute %15245, %15246 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %15248 = torch.aten.add.Tensor %15247, %14832, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15249 = torch.prim.ListConstruct %15248, %825 : (!torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>) -> !torch.list<vtensor>
    %15250 = torch.aten.cat %15249, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %15251 = torch.prim.ListConstruct %int2, %int32, %int40, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15252 = torch.aten.view %15250, %15251 : !torch.vtensor<[2,1280,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,4096],f16>
    %15253 = torch.prims.convert_element_type %15252, %int6 : !torch.vtensor<[2,32,40,4096],f16>, !torch.int -> !torch.vtensor<[2,32,40,4096],f32>
    %15254 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_463, %result1_464 = torch.aten.var_mean.correction %15253, %15254, %int0, %true : !torch.vtensor<[2,32,40,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15255 = torch.aten.add.Scalar %result0_463, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15256 = torch.aten.rsqrt %15255 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15257 = torch.aten.sub.Tensor %15252, %result1_464, %int1 : !torch.vtensor<[2,32,40,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,4096],f32>
    %15258 = torch.aten.mul.Tensor %15257, %15256 : !torch.vtensor<[2,32,40,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,4096],f32>
    %15259 = torch.prim.ListConstruct %int2, %int1280, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15260 = torch.aten.view %15258, %15259 : !torch.vtensor<[2,32,40,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,64,64],f32>
    %_params.unet.up_blocks.1.resnets.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.norm1.bias : tensor<1280xf16>
    %15261 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15262 = torch.aten.unsqueeze %15261, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %15263 = torch.aten.unsqueeze %15262, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %15264 = torch.aten.unsqueeze %15263, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.1.resnets.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.norm1.weight : tensor<1280xf16>
    %15265 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15266 = torch.aten.unsqueeze %15265, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %15267 = torch.aten.unsqueeze %15266, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %15268 = torch.aten.unsqueeze %15267, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %15269 = torch.aten.mul.Tensor %15260, %15268 : !torch.vtensor<[2,1280,64,64],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,64,64],f32>
    %15270 = torch.aten.add.Tensor %15269, %15264, %int1 : !torch.vtensor<[2,1280,64,64],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,64,64],f32>
    %15271 = torch.prims.convert_element_type %15270, %int5 : !torch.vtensor<[2,1280,64,64],f32>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %15272 = torch.prims.convert_element_type %result1_464, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15273 = torch.prims.convert_element_type %15256, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15274 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15275 = torch.prims.squeeze %15272, %15274 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15276 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15277 = torch.prims.squeeze %15275, %15276 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15278 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15279 = torch.prims.squeeze %15273, %15278 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15280 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15281 = torch.prims.squeeze %15279, %15280 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15282 = torch.aten.silu %15271 : !torch.vtensor<[2,1280,64,64],f16> -> !torch.vtensor<[2,1280,64,64],f16>
    %_params.unet.up_blocks.1.resnets.1.conv1.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.conv1.weight : tensor<640x1280x3x3xf16>
    %15283 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv1.weight : tensor<640x1280x3x3xf16> -> !torch.vtensor<[640,1280,3,3],f16>
    %_params.unet.up_blocks.1.resnets.1.conv1.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.conv1.bias : tensor<640xf16>
    %15284 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15285 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15286 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15287 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15288 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15289 = torch.aten.convolution %15282, %15283, %15284, %15285, %15286, %15287, %false, %15288, %int1 : !torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[640,1280,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15290 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16>
    %15291 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %15292 = torch.aten.transpose.int %15291, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16>
    %15293 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15294 = torch.prims.convert_element_type %15293, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15295 = torch.prims.convert_element_type %15290, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %15296 = torch.prims.convert_element_type %15292, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %15297 = torch.aten.mm %15295, %15296 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %15298 = torch.aten.mul.Scalar %15297, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %15299 = torch.aten.mul.Scalar %15294, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15300 = torch.aten.add.Tensor %15298, %15299, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %15301 = torch.prims.convert_element_type %15300, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %15302 = torch.aten.unsqueeze %15301, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %15303 = torch.aten.unsqueeze %15302, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %15304 = torch.aten.add.Tensor %15289, %15303, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15305 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15306 = torch.aten.view %15304, %15305 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %15307 = torch.prims.convert_element_type %15306, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15308 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_465, %result1_466 = torch.aten.var_mean.correction %15307, %15308, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15309 = torch.aten.add.Scalar %result0_465, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15310 = torch.aten.rsqrt %15309 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15311 = torch.aten.sub.Tensor %15306, %result1_466, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15312 = torch.aten.mul.Tensor %15311, %15310 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %15313 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15314 = torch.aten.view %15312, %15313 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.resnets.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.norm2.bias : tensor<640xf16>
    %15315 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15316 = torch.aten.unsqueeze %15315, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15317 = torch.aten.unsqueeze %15316, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15318 = torch.aten.unsqueeze %15317, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.resnets.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.norm2.weight : tensor<640xf16>
    %15319 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15320 = torch.aten.unsqueeze %15319, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15321 = torch.aten.unsqueeze %15320, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15322 = torch.aten.unsqueeze %15321, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %15323 = torch.aten.mul.Tensor %15314, %15322 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %15324 = torch.aten.add.Tensor %15323, %15318, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %15325 = torch.prims.convert_element_type %15324, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15326 = torch.prims.convert_element_type %result1_466, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15327 = torch.prims.convert_element_type %15310, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15328 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15329 = torch.prims.squeeze %15326, %15328 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15330 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15331 = torch.prims.squeeze %15329, %15330 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15332 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15333 = torch.prims.squeeze %15327, %15332 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15334 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15335 = torch.prims.squeeze %15333, %15334 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15336 = torch.aten.silu %15325 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.1.conv2.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16>
    %15337 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.resnets.1.conv2.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.conv2.bias : tensor<640xf16>
    %15338 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15339 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15340 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15341 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15342 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15343 = torch.aten.convolution %15336, %15337, %15338, %15339, %15340, %15341, %false, %15342, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight : tensor<640x1280x1x1xf16>
    %15344 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight : tensor<640x1280x1x1xf16> -> !torch.vtensor<[640,1280,1,1],f16>
    %_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias : tensor<640xf16>
    %15345 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15346 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15347 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15348 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15349 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15350 = torch.aten.convolution %15250, %15344, %15345, %15346, %15347, %15348, %false, %15349, %int1 : !torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[640,1280,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15351 = torch.aten.add.Tensor %15350, %15343, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15352 = torch.aten.div.Scalar %15351, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %15353 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15354 = torch.aten.view %15352, %15353 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %15355 = torch.prims.convert_element_type %15354, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15356 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_467, %result1_468 = torch.aten.var_mean.correction %15355, %15356, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15357 = torch.aten.add.Scalar %result0_467, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15358 = torch.aten.rsqrt %15357 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15359 = torch.aten.sub.Tensor %15354, %result1_468, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15360 = torch.aten.mul.Tensor %15359, %15358 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %15361 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15362 = torch.aten.view %15360, %15361 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.attentions.1.norm.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.norm.bias : tensor<640xf16>
    %15363 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15364 = torch.aten.unsqueeze %15363, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15365 = torch.aten.unsqueeze %15364, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15366 = torch.aten.unsqueeze %15365, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.attentions.1.norm.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.norm.weight : tensor<640xf16>
    %15367 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15368 = torch.aten.unsqueeze %15367, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15369 = torch.aten.unsqueeze %15368, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15370 = torch.aten.unsqueeze %15369, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %15371 = torch.aten.mul.Tensor %15362, %15370 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %15372 = torch.aten.add.Tensor %15371, %15366, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %15373 = torch.prims.convert_element_type %15372, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15374 = torch.prims.convert_element_type %result1_468, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15375 = torch.prims.convert_element_type %15358, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15376 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15377 = torch.prims.squeeze %15374, %15376 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15378 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15379 = torch.prims.squeeze %15377, %15378 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15380 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15381 = torch.prims.squeeze %15375, %15380 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15382 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15383 = torch.prims.squeeze %15381, %15382 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15384 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15385 = torch.aten.permute %15373, %15384 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %15386 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15387 = torch.aten.view %15385, %15386 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_in.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16>
    %15388 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15389 = torch.aten.transpose.int %15388, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15390 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15391 = torch.aten._unsafe_view %15387, %15390 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15392 = torch.aten.mm %15391, %15389 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15393 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15394 = torch.aten.view %15392, %15393 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_in.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_in.bias : tensor<640xf16>
    %15395 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15396 = torch.aten.add.Tensor %15394, %15395, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15397 = torch.prims.convert_element_type %15396, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15398 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_469, %result1_470 = torch.aten.var_mean.correction %15397, %15398, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15399 = torch.aten.add.Scalar %result0_469, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15400 = torch.aten.rsqrt %15399 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15401 = torch.aten.sub.Tensor %15396, %result1_470, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15402 = torch.aten.mul.Tensor %15401, %15400 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %15403 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15404 = torch.aten.mul.Tensor %15402, %15403 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %15405 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15406 = torch.aten.add.Tensor %15404, %15405, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15407 = torch.prims.convert_element_type %15406, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15408 = torch.prims.convert_element_type %result1_470, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15409 = torch.prims.convert_element_type %15400, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %15410 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15411 = torch.aten.transpose.int %15410, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15412 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15413 = torch.aten.view %15407, %15412 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15414 = torch.aten.mm %15413, %15411 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15415 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15416 = torch.aten.view %15414, %15415 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %15417 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15418 = torch.aten.transpose.int %15417, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15419 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15420 = torch.aten.view %15407, %15419 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15421 = torch.aten.mm %15420, %15418 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15422 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15423 = torch.aten.view %15421, %15422 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %15424 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15425 = torch.aten.transpose.int %15424, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15426 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15427 = torch.aten.view %15407, %15426 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15428 = torch.aten.mm %15427, %15425 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15429 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15430 = torch.aten.view %15428, %15429 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15431 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15432 = torch.aten.view %15416, %15431 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15433 = torch.aten.transpose.int %15432, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15434 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15435 = torch.aten.view %15423, %15434 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15436 = torch.aten.transpose.int %15435, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15437 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15438 = torch.aten.view %15430, %15437 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15439 = torch.aten.transpose.int %15438, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15440:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15433, %15436, %15439, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15441 = torch.aten.transpose.int %15440#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15442 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15443 = torch.aten.view %15441, %15442 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15444 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15445 = torch.aten.view %15443, %15444 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %15446 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15447 = torch.aten.transpose.int %15446, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %15448 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15449 = torch.prims.convert_element_type %15448, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15450 = torch.prims.convert_element_type %15445, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15451 = torch.prims.convert_element_type %15447, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15452 = torch.aten.mm %15450, %15451 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15453 = torch.aten.mul.Scalar %15452, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15454 = torch.aten.mul.Scalar %15449, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15455 = torch.aten.add.Tensor %15453, %15454, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15456 = torch.prims.convert_element_type %15455, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15457 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15458 = torch.aten.view %15456, %15457 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15459 = torch.aten.div.Scalar %15458, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15460 = torch.aten.add.Tensor %15459, %15396, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15461 = torch.prims.convert_element_type %15460, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15462 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_471, %result1_472 = torch.aten.var_mean.correction %15461, %15462, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15463 = torch.aten.add.Scalar %result0_471, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15464 = torch.aten.rsqrt %15463 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15465 = torch.aten.sub.Tensor %15460, %result1_472, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15466 = torch.aten.mul.Tensor %15465, %15464 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %15467 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15468 = torch.aten.mul.Tensor %15466, %15467 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %15469 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15470 = torch.aten.add.Tensor %15468, %15469, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15471 = torch.prims.convert_element_type %15470, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15472 = torch.prims.convert_element_type %result1_472, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15473 = torch.prims.convert_element_type %15464, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %15474 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15475 = torch.aten.transpose.int %15474, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15476 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15477 = torch.aten.view %15471, %15476 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15478 = torch.aten.mm %15477, %15475 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15479 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15480 = torch.aten.view %15478, %15479 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %15481 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15482 = torch.aten.transpose.int %15481, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15483 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15484 = torch.aten.view %4, %15483 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15485 = torch.aten.mm %15484, %15482 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15486 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15487 = torch.aten.view %15485, %15486 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %15488 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15489 = torch.aten.transpose.int %15488, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15490 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15491 = torch.aten.view %4, %15490 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15492 = torch.aten.mm %15491, %15489 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15493 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15494 = torch.aten.view %15492, %15493 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %15495 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15496 = torch.aten.view %15480, %15495 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15497 = torch.aten.transpose.int %15496, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15498 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15499 = torch.aten.view %15487, %15498 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15500 = torch.aten.transpose.int %15499, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15501 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15502 = torch.aten.view %15494, %15501 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15503 = torch.aten.transpose.int %15502, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15504:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15497, %15500, %15503, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15505 = torch.aten.transpose.int %15504#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15506 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15507 = torch.aten.view %15505, %15506 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15508 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15509 = torch.aten.view %15507, %15508 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %15510 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15511 = torch.aten.transpose.int %15510, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %15512 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15513 = torch.prims.convert_element_type %15512, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15514 = torch.prims.convert_element_type %15509, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15515 = torch.prims.convert_element_type %15511, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15516 = torch.aten.mm %15514, %15515 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15517 = torch.aten.mul.Scalar %15516, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15518 = torch.aten.mul.Scalar %15513, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15519 = torch.aten.add.Tensor %15517, %15518, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15520 = torch.prims.convert_element_type %15519, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15521 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15522 = torch.aten.view %15520, %15521 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15523 = torch.aten.div.Scalar %15522, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15524 = torch.aten.add.Tensor %15523, %15460, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15525 = torch.prims.convert_element_type %15524, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15526 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_473, %result1_474 = torch.aten.var_mean.correction %15525, %15526, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15527 = torch.aten.add.Scalar %result0_473, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15528 = torch.aten.rsqrt %15527 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15529 = torch.aten.sub.Tensor %15524, %result1_474, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15530 = torch.aten.mul.Tensor %15529, %15528 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %15531 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15532 = torch.aten.mul.Tensor %15530, %15531 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %15533 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15534 = torch.aten.add.Tensor %15532, %15533, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15535 = torch.prims.convert_element_type %15534, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15536 = torch.prims.convert_element_type %result1_474, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15537 = torch.prims.convert_element_type %15528, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15538 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15539 = torch.aten.view %15535, %15538 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %15540 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %15541 = torch.aten.transpose.int %15540, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %15542 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %15543 = torch.prims.convert_element_type %15542, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %15544 = torch.prims.convert_element_type %15539, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15545 = torch.prims.convert_element_type %15541, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %15546 = torch.aten.mm %15544, %15545 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %15547 = torch.aten.mul.Scalar %15546, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15548 = torch.aten.mul.Scalar %15543, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %15549 = torch.aten.add.Tensor %15547, %15548, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15550 = torch.prims.convert_element_type %15549, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %15551 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15552 = torch.aten.view %15550, %15551 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %15553 = torch.aten.slice.Tensor %15552, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15554 = torch.aten.slice.Tensor %15552, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15555 = torch.aten.gelu %15554, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %15556 = torch.aten.mul.Tensor %15553, %15555 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %15557 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %15558 = torch.aten.view %15556, %15557 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %15559 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %15560 = torch.aten.transpose.int %15559, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %15561 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15562 = torch.prims.convert_element_type %15561, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15563 = torch.prims.convert_element_type %15558, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %15564 = torch.prims.convert_element_type %15560, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %15565 = torch.aten.mm %15563, %15564 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15566 = torch.aten.mul.Scalar %15565, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15567 = torch.aten.mul.Scalar %15562, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15568 = torch.aten.add.Tensor %15566, %15567, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15569 = torch.prims.convert_element_type %15568, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15570 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15571 = torch.aten.view %15569, %15570 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15572 = torch.aten.add.Tensor %15571, %15524, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15573 = torch.prims.convert_element_type %15572, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15574 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_475, %result1_476 = torch.aten.var_mean.correction %15573, %15574, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15575 = torch.aten.add.Scalar %result0_475, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15576 = torch.aten.rsqrt %15575 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15577 = torch.aten.sub.Tensor %15572, %result1_476, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15578 = torch.aten.mul.Tensor %15577, %15576 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %15579 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15580 = torch.aten.mul.Tensor %15578, %15579 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %15581 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15582 = torch.aten.add.Tensor %15580, %15581, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15583 = torch.prims.convert_element_type %15582, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15584 = torch.prims.convert_element_type %result1_476, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15585 = torch.prims.convert_element_type %15576, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %15586 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15587 = torch.aten.transpose.int %15586, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15588 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15589 = torch.aten.view %15583, %15588 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15590 = torch.aten.mm %15589, %15587 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15591 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15592 = torch.aten.view %15590, %15591 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %15593 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15594 = torch.aten.transpose.int %15593, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15595 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15596 = torch.aten.view %15583, %15595 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15597 = torch.aten.mm %15596, %15594 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15598 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15599 = torch.aten.view %15597, %15598 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %15600 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15601 = torch.aten.transpose.int %15600, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15602 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15603 = torch.aten.view %15583, %15602 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15604 = torch.aten.mm %15603, %15601 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15605 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15606 = torch.aten.view %15604, %15605 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15607 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15608 = torch.aten.view %15592, %15607 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15609 = torch.aten.transpose.int %15608, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15610 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15611 = torch.aten.view %15599, %15610 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15612 = torch.aten.transpose.int %15611, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15613 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15614 = torch.aten.view %15606, %15613 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15615 = torch.aten.transpose.int %15614, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15616:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15609, %15612, %15615, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15617 = torch.aten.transpose.int %15616#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15618 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15619 = torch.aten.view %15617, %15618 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15620 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15621 = torch.aten.view %15619, %15620 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %15622 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15623 = torch.aten.transpose.int %15622, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %15624 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15625 = torch.prims.convert_element_type %15624, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15626 = torch.prims.convert_element_type %15621, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15627 = torch.prims.convert_element_type %15623, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15628 = torch.aten.mm %15626, %15627 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15629 = torch.aten.mul.Scalar %15628, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15630 = torch.aten.mul.Scalar %15625, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15631 = torch.aten.add.Tensor %15629, %15630, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15632 = torch.prims.convert_element_type %15631, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15633 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15634 = torch.aten.view %15632, %15633 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15635 = torch.aten.div.Scalar %15634, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15636 = torch.aten.add.Tensor %15635, %15572, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15637 = torch.prims.convert_element_type %15636, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15638 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_477, %result1_478 = torch.aten.var_mean.correction %15637, %15638, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15639 = torch.aten.add.Scalar %result0_477, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15640 = torch.aten.rsqrt %15639 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15641 = torch.aten.sub.Tensor %15636, %result1_478, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15642 = torch.aten.mul.Tensor %15641, %15640 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %15643 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15644 = torch.aten.mul.Tensor %15642, %15643 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %15645 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15646 = torch.aten.add.Tensor %15644, %15645, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15647 = torch.prims.convert_element_type %15646, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15648 = torch.prims.convert_element_type %result1_478, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15649 = torch.prims.convert_element_type %15640, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %15650 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15651 = torch.aten.transpose.int %15650, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15652 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15653 = torch.aten.view %15647, %15652 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15654 = torch.aten.mm %15653, %15651 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15655 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15656 = torch.aten.view %15654, %15655 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %15657 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15658 = torch.aten.transpose.int %15657, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15659 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15660 = torch.aten.view %4, %15659 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15661 = torch.aten.mm %15660, %15658 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15662 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15663 = torch.aten.view %15661, %15662 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %15664 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15665 = torch.aten.transpose.int %15664, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15666 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15667 = torch.aten.view %4, %15666 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15668 = torch.aten.mm %15667, %15665 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15669 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15670 = torch.aten.view %15668, %15669 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %15671 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15672 = torch.aten.view %15656, %15671 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15673 = torch.aten.transpose.int %15672, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15674 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15675 = torch.aten.view %15663, %15674 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15676 = torch.aten.transpose.int %15675, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15677 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15678 = torch.aten.view %15670, %15677 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15679 = torch.aten.transpose.int %15678, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15680:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15673, %15676, %15679, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15681 = torch.aten.transpose.int %15680#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15682 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15683 = torch.aten.view %15681, %15682 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15684 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15685 = torch.aten.view %15683, %15684 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %15686 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15687 = torch.aten.transpose.int %15686, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %15688 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15689 = torch.prims.convert_element_type %15688, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15690 = torch.prims.convert_element_type %15685, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15691 = torch.prims.convert_element_type %15687, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15692 = torch.aten.mm %15690, %15691 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15693 = torch.aten.mul.Scalar %15692, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15694 = torch.aten.mul.Scalar %15689, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15695 = torch.aten.add.Tensor %15693, %15694, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15696 = torch.prims.convert_element_type %15695, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15697 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15698 = torch.aten.view %15696, %15697 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15699 = torch.aten.div.Scalar %15698, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15700 = torch.aten.add.Tensor %15699, %15636, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15701 = torch.prims.convert_element_type %15700, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15702 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_479, %result1_480 = torch.aten.var_mean.correction %15701, %15702, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15703 = torch.aten.add.Scalar %result0_479, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15704 = torch.aten.rsqrt %15703 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15705 = torch.aten.sub.Tensor %15700, %result1_480, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15706 = torch.aten.mul.Tensor %15705, %15704 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %15707 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15708 = torch.aten.mul.Tensor %15706, %15707 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %15709 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15710 = torch.aten.add.Tensor %15708, %15709, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15711 = torch.prims.convert_element_type %15710, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15712 = torch.prims.convert_element_type %result1_480, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15713 = torch.prims.convert_element_type %15704, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15714 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15715 = torch.aten.view %15711, %15714 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %15716 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %15717 = torch.aten.transpose.int %15716, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %15718 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %15719 = torch.prims.convert_element_type %15718, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %15720 = torch.prims.convert_element_type %15715, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15721 = torch.prims.convert_element_type %15717, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %15722 = torch.aten.mm %15720, %15721 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %15723 = torch.aten.mul.Scalar %15722, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15724 = torch.aten.mul.Scalar %15719, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %15725 = torch.aten.add.Tensor %15723, %15724, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15726 = torch.prims.convert_element_type %15725, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %15727 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15728 = torch.aten.view %15726, %15727 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %15729 = torch.aten.slice.Tensor %15728, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15730 = torch.aten.slice.Tensor %15728, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15731 = torch.aten.gelu %15730, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %15732 = torch.aten.mul.Tensor %15729, %15731 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %15733 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %15734 = torch.aten.view %15732, %15733 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %15735 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %15736 = torch.aten.transpose.int %15735, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %15737 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15738 = torch.prims.convert_element_type %15737, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15739 = torch.prims.convert_element_type %15734, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %15740 = torch.prims.convert_element_type %15736, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %15741 = torch.aten.mm %15739, %15740 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15742 = torch.aten.mul.Scalar %15741, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15743 = torch.aten.mul.Scalar %15738, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15744 = torch.aten.add.Tensor %15742, %15743, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15745 = torch.prims.convert_element_type %15744, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15746 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15747 = torch.aten.view %15745, %15746 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15748 = torch.aten.add.Tensor %15747, %15700, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15749 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15750 = torch.aten.view %15748, %15749 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_out.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16>
    %15751 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15752 = torch.aten.transpose.int %15751, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_out.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_out.bias : tensor<640xf16>
    %15753 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15754 = torch.prims.convert_element_type %15753, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15755 = torch.prims.convert_element_type %15750, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15756 = torch.prims.convert_element_type %15752, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15757 = torch.aten.mm %15755, %15756 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15758 = torch.aten.mul.Scalar %15757, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15759 = torch.aten.mul.Scalar %15754, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15760 = torch.aten.add.Tensor %15758, %15759, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15761 = torch.prims.convert_element_type %15760, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15762 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15763 = torch.aten.view %15761, %15762 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15764 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15765 = torch.aten.view %15763, %15764 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %15766 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15767 = torch.aten.permute %15765, %15766 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %15768 = torch.aten.add.Tensor %15767, %15352, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15769 = torch.prim.ListConstruct %15768, %307 : (!torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,320,64,64],f16>) -> !torch.list<vtensor>
    %15770 = torch.aten.cat %15769, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,960,64,64],f16>
    %15771 = torch.prim.ListConstruct %int2, %int32, %int30, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15772 = torch.aten.view %15770, %15771 : !torch.vtensor<[2,960,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,30,4096],f16>
    %15773 = torch.prims.convert_element_type %15772, %int6 : !torch.vtensor<[2,32,30,4096],f16>, !torch.int -> !torch.vtensor<[2,32,30,4096],f32>
    %15774 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_481, %result1_482 = torch.aten.var_mean.correction %15773, %15774, %int0, %true : !torch.vtensor<[2,32,30,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15775 = torch.aten.add.Scalar %result0_481, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15776 = torch.aten.rsqrt %15775 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15777 = torch.aten.sub.Tensor %15772, %result1_482, %int1 : !torch.vtensor<[2,32,30,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,30,4096],f32>
    %15778 = torch.aten.mul.Tensor %15777, %15776 : !torch.vtensor<[2,32,30,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,30,4096],f32>
    %15779 = torch.prim.ListConstruct %int2, %int960, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15780 = torch.aten.view %15778, %15779 : !torch.vtensor<[2,32,30,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,960,64,64],f32>
    %_params.unet.up_blocks.1.resnets.2.norm1.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.norm1.bias : tensor<960xf16>
    %15781 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm1.bias : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %15782 = torch.aten.unsqueeze %15781, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %15783 = torch.aten.unsqueeze %15782, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %15784 = torch.aten.unsqueeze %15783, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %_params.unet.up_blocks.1.resnets.2.norm1.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.norm1.weight : tensor<960xf16>
    %15785 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm1.weight : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %15786 = torch.aten.unsqueeze %15785, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %15787 = torch.aten.unsqueeze %15786, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %15788 = torch.aten.unsqueeze %15787, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %15789 = torch.aten.mul.Tensor %15780, %15788 : !torch.vtensor<[2,960,64,64],f32>, !torch.vtensor<[1,960,1,1],f16> -> !torch.vtensor<[2,960,64,64],f32>
    %15790 = torch.aten.add.Tensor %15789, %15784, %int1 : !torch.vtensor<[2,960,64,64],f32>, !torch.vtensor<[1,960,1,1],f16>, !torch.int -> !torch.vtensor<[2,960,64,64],f32>
    %15791 = torch.prims.convert_element_type %15790, %int5 : !torch.vtensor<[2,960,64,64],f32>, !torch.int -> !torch.vtensor<[2,960,64,64],f16>
    %15792 = torch.prims.convert_element_type %result1_482, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15793 = torch.prims.convert_element_type %15776, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15794 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15795 = torch.prims.squeeze %15792, %15794 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15796 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15797 = torch.prims.squeeze %15795, %15796 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15798 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15799 = torch.prims.squeeze %15793, %15798 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15800 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15801 = torch.prims.squeeze %15799, %15800 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15802 = torch.aten.silu %15791 : !torch.vtensor<[2,960,64,64],f16> -> !torch.vtensor<[2,960,64,64],f16>
    %_params.unet.up_blocks.1.resnets.2.conv1.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.conv1.weight : tensor<640x960x3x3xf16>
    %15803 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv1.weight : tensor<640x960x3x3xf16> -> !torch.vtensor<[640,960,3,3],f16>
    %_params.unet.up_blocks.1.resnets.2.conv1.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.conv1.bias : tensor<640xf16>
    %15804 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15805 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15806 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15807 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15808 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15809 = torch.aten.convolution %15802, %15803, %15804, %15805, %15806, %15807, %false, %15808, %int1 : !torch.vtensor<[2,960,64,64],f16>, !torch.vtensor<[640,960,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15810 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight : tensor<640x1280xf16>
    %15811 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %15812 = torch.aten.transpose.int %15811, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias : tensor<640xf16>
    %15813 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15814 = torch.prims.convert_element_type %15813, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15815 = torch.prims.convert_element_type %15810, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %15816 = torch.prims.convert_element_type %15812, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %15817 = torch.aten.mm %15815, %15816 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %15818 = torch.aten.mul.Scalar %15817, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %15819 = torch.aten.mul.Scalar %15814, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15820 = torch.aten.add.Tensor %15818, %15819, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %15821 = torch.prims.convert_element_type %15820, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %15822 = torch.aten.unsqueeze %15821, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %15823 = torch.aten.unsqueeze %15822, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %15824 = torch.aten.add.Tensor %15809, %15823, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15825 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15826 = torch.aten.view %15824, %15825 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %15827 = torch.prims.convert_element_type %15826, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15828 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_483, %result1_484 = torch.aten.var_mean.correction %15827, %15828, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15829 = torch.aten.add.Scalar %result0_483, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15830 = torch.aten.rsqrt %15829 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15831 = torch.aten.sub.Tensor %15826, %result1_484, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15832 = torch.aten.mul.Tensor %15831, %15830 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %15833 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15834 = torch.aten.view %15832, %15833 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.resnets.2.norm2.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.norm2.bias : tensor<640xf16>
    %15835 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15836 = torch.aten.unsqueeze %15835, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15837 = torch.aten.unsqueeze %15836, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15838 = torch.aten.unsqueeze %15837, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.resnets.2.norm2.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.norm2.weight : tensor<640xf16>
    %15839 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15840 = torch.aten.unsqueeze %15839, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15841 = torch.aten.unsqueeze %15840, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15842 = torch.aten.unsqueeze %15841, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %15843 = torch.aten.mul.Tensor %15834, %15842 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %15844 = torch.aten.add.Tensor %15843, %15838, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %15845 = torch.prims.convert_element_type %15844, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15846 = torch.prims.convert_element_type %result1_484, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15847 = torch.prims.convert_element_type %15830, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15848 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15849 = torch.prims.squeeze %15846, %15848 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15850 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15851 = torch.prims.squeeze %15849, %15850 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15852 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15853 = torch.prims.squeeze %15847, %15852 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15854 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15855 = torch.prims.squeeze %15853, %15854 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15856 = torch.aten.silu %15845 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.2.conv2.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.conv2.weight : tensor<640x640x3x3xf16>
    %15857 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.resnets.2.conv2.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.conv2.bias : tensor<640xf16>
    %15858 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15859 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15860 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15861 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15862 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15863 = torch.aten.convolution %15856, %15857, %15858, %15859, %15860, %15861, %false, %15862, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight : tensor<640x960x1x1xf16>
    %15864 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight : tensor<640x960x1x1xf16> -> !torch.vtensor<[640,960,1,1],f16>
    %_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias : tensor<640xf16>
    %15865 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15866 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15867 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15868 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15869 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15870 = torch.aten.convolution %15770, %15864, %15865, %15866, %15867, %15868, %false, %15869, %int1 : !torch.vtensor<[2,960,64,64],f16>, !torch.vtensor<[640,960,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15871 = torch.aten.add.Tensor %15870, %15863, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15872 = torch.aten.div.Scalar %15871, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %15873 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15874 = torch.aten.view %15872, %15873 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %15875 = torch.prims.convert_element_type %15874, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15876 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_485, %result1_486 = torch.aten.var_mean.correction %15875, %15876, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15877 = torch.aten.add.Scalar %result0_485, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15878 = torch.aten.rsqrt %15877 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15879 = torch.aten.sub.Tensor %15874, %result1_486, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15880 = torch.aten.mul.Tensor %15879, %15878 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %15881 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15882 = torch.aten.view %15880, %15881 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.attentions.2.norm.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.norm.bias : tensor<640xf16>
    %15883 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15884 = torch.aten.unsqueeze %15883, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15885 = torch.aten.unsqueeze %15884, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15886 = torch.aten.unsqueeze %15885, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.attentions.2.norm.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.norm.weight : tensor<640xf16>
    %15887 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15888 = torch.aten.unsqueeze %15887, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15889 = torch.aten.unsqueeze %15888, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15890 = torch.aten.unsqueeze %15889, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %15891 = torch.aten.mul.Tensor %15882, %15890 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %15892 = torch.aten.add.Tensor %15891, %15886, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %15893 = torch.prims.convert_element_type %15892, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15894 = torch.prims.convert_element_type %result1_486, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15895 = torch.prims.convert_element_type %15878, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15896 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15897 = torch.prims.squeeze %15894, %15896 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15898 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15899 = torch.prims.squeeze %15897, %15898 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15900 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15901 = torch.prims.squeeze %15895, %15900 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15902 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15903 = torch.prims.squeeze %15901, %15902 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15904 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15905 = torch.aten.permute %15893, %15904 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %15906 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15907 = torch.aten.view %15905, %15906 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_in.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_in.weight : tensor<640x640xf16>
    %15908 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15909 = torch.aten.transpose.int %15908, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15910 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15911 = torch.aten._unsafe_view %15907, %15910 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15912 = torch.aten.mm %15911, %15909 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15913 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15914 = torch.aten.view %15912, %15913 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_in.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_in.bias : tensor<640xf16>
    %15915 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15916 = torch.aten.add.Tensor %15914, %15915, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15917 = torch.prims.convert_element_type %15916, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15918 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_487, %result1_488 = torch.aten.var_mean.correction %15917, %15918, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15919 = torch.aten.add.Scalar %result0_487, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15920 = torch.aten.rsqrt %15919 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15921 = torch.aten.sub.Tensor %15916, %result1_488, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15922 = torch.aten.mul.Tensor %15921, %15920 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %15923 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15924 = torch.aten.mul.Tensor %15922, %15923 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %15925 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15926 = torch.aten.add.Tensor %15924, %15925, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15927 = torch.prims.convert_element_type %15926, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15928 = torch.prims.convert_element_type %result1_488, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15929 = torch.prims.convert_element_type %15920, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %15930 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15931 = torch.aten.transpose.int %15930, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15932 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15933 = torch.aten.view %15927, %15932 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15934 = torch.aten.mm %15933, %15931 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15935 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15936 = torch.aten.view %15934, %15935 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %15937 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15938 = torch.aten.transpose.int %15937, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15939 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15940 = torch.aten.view %15927, %15939 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15941 = torch.aten.mm %15940, %15938 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15942 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15943 = torch.aten.view %15941, %15942 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %15944 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15945 = torch.aten.transpose.int %15944, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15946 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15947 = torch.aten.view %15927, %15946 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15948 = torch.aten.mm %15947, %15945 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15949 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15950 = torch.aten.view %15948, %15949 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15951 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15952 = torch.aten.view %15936, %15951 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15953 = torch.aten.transpose.int %15952, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15954 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15955 = torch.aten.view %15943, %15954 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15956 = torch.aten.transpose.int %15955, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15957 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15958 = torch.aten.view %15950, %15957 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15959 = torch.aten.transpose.int %15958, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15960:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15953, %15956, %15959, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15961 = torch.aten.transpose.int %15960#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15962 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15963 = torch.aten.view %15961, %15962 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15964 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15965 = torch.aten.view %15963, %15964 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %15966 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15967 = torch.aten.transpose.int %15966, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %15968 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15969 = torch.prims.convert_element_type %15968, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15970 = torch.prims.convert_element_type %15965, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15971 = torch.prims.convert_element_type %15967, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15972 = torch.aten.mm %15970, %15971 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15973 = torch.aten.mul.Scalar %15972, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15974 = torch.aten.mul.Scalar %15969, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15975 = torch.aten.add.Tensor %15973, %15974, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15976 = torch.prims.convert_element_type %15975, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15977 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15978 = torch.aten.view %15976, %15977 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15979 = torch.aten.div.Scalar %15978, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15980 = torch.aten.add.Tensor %15979, %15916, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15981 = torch.prims.convert_element_type %15980, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15982 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_489, %result1_490 = torch.aten.var_mean.correction %15981, %15982, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15983 = torch.aten.add.Scalar %result0_489, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15984 = torch.aten.rsqrt %15983 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15985 = torch.aten.sub.Tensor %15980, %result1_490, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15986 = torch.aten.mul.Tensor %15985, %15984 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %15987 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15988 = torch.aten.mul.Tensor %15986, %15987 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %15989 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15990 = torch.aten.add.Tensor %15988, %15989, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15991 = torch.prims.convert_element_type %15990, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15992 = torch.prims.convert_element_type %result1_490, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15993 = torch.prims.convert_element_type %15984, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %15994 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15995 = torch.aten.transpose.int %15994, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15996 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15997 = torch.aten.view %15991, %15996 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15998 = torch.aten.mm %15997, %15995 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15999 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16000 = torch.aten.view %15998, %15999 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %16001 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16002 = torch.aten.transpose.int %16001, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16003 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16004 = torch.aten.view %4, %16003 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16005 = torch.aten.mm %16004, %16002 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16006 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16007 = torch.aten.view %16005, %16006 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %16008 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16009 = torch.aten.transpose.int %16008, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16010 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16011 = torch.aten.view %4, %16010 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16012 = torch.aten.mm %16011, %16009 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16013 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16014 = torch.aten.view %16012, %16013 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %16015 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16016 = torch.aten.view %16000, %16015 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16017 = torch.aten.transpose.int %16016, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16018 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16019 = torch.aten.view %16007, %16018 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16020 = torch.aten.transpose.int %16019, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16021 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16022 = torch.aten.view %16014, %16021 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16023 = torch.aten.transpose.int %16022, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16024:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16017, %16020, %16023, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16025 = torch.aten.transpose.int %16024#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16026 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16027 = torch.aten.view %16025, %16026 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16028 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16029 = torch.aten.view %16027, %16028 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %16030 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16031 = torch.aten.transpose.int %16030, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %16032 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16033 = torch.prims.convert_element_type %16032, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16034 = torch.prims.convert_element_type %16029, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16035 = torch.prims.convert_element_type %16031, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16036 = torch.aten.mm %16034, %16035 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16037 = torch.aten.mul.Scalar %16036, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16038 = torch.aten.mul.Scalar %16033, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16039 = torch.aten.add.Tensor %16037, %16038, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16040 = torch.prims.convert_element_type %16039, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16041 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16042 = torch.aten.view %16040, %16041 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16043 = torch.aten.div.Scalar %16042, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16044 = torch.aten.add.Tensor %16043, %15980, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16045 = torch.prims.convert_element_type %16044, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16046 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_491, %result1_492 = torch.aten.var_mean.correction %16045, %16046, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16047 = torch.aten.add.Scalar %result0_491, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16048 = torch.aten.rsqrt %16047 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16049 = torch.aten.sub.Tensor %16044, %result1_492, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16050 = torch.aten.mul.Tensor %16049, %16048 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %16051 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16052 = torch.aten.mul.Tensor %16050, %16051 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %16053 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16054 = torch.aten.add.Tensor %16052, %16053, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16055 = torch.prims.convert_element_type %16054, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16056 = torch.prims.convert_element_type %result1_492, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16057 = torch.prims.convert_element_type %16048, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16058 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16059 = torch.aten.view %16055, %16058 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %16060 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %16061 = torch.aten.transpose.int %16060, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %16062 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %16063 = torch.prims.convert_element_type %16062, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %16064 = torch.prims.convert_element_type %16059, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16065 = torch.prims.convert_element_type %16061, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %16066 = torch.aten.mm %16064, %16065 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %16067 = torch.aten.mul.Scalar %16066, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16068 = torch.aten.mul.Scalar %16063, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %16069 = torch.aten.add.Tensor %16067, %16068, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16070 = torch.prims.convert_element_type %16069, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %16071 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16072 = torch.aten.view %16070, %16071 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %16073 = torch.aten.slice.Tensor %16072, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16074 = torch.aten.slice.Tensor %16072, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16075 = torch.aten.gelu %16074, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %16076 = torch.aten.mul.Tensor %16073, %16075 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %16077 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %16078 = torch.aten.view %16076, %16077 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %16079 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %16080 = torch.aten.transpose.int %16079, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %16081 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16082 = torch.prims.convert_element_type %16081, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16083 = torch.prims.convert_element_type %16078, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %16084 = torch.prims.convert_element_type %16080, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %16085 = torch.aten.mm %16083, %16084 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16086 = torch.aten.mul.Scalar %16085, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16087 = torch.aten.mul.Scalar %16082, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16088 = torch.aten.add.Tensor %16086, %16087, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16089 = torch.prims.convert_element_type %16088, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16090 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16091 = torch.aten.view %16089, %16090 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16092 = torch.aten.add.Tensor %16091, %16044, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16093 = torch.prims.convert_element_type %16092, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16094 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_493, %result1_494 = torch.aten.var_mean.correction %16093, %16094, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16095 = torch.aten.add.Scalar %result0_493, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16096 = torch.aten.rsqrt %16095 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16097 = torch.aten.sub.Tensor %16092, %result1_494, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16098 = torch.aten.mul.Tensor %16097, %16096 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %16099 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16100 = torch.aten.mul.Tensor %16098, %16099 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %16101 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16102 = torch.aten.add.Tensor %16100, %16101, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16103 = torch.prims.convert_element_type %16102, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16104 = torch.prims.convert_element_type %result1_494, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16105 = torch.prims.convert_element_type %16096, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %16106 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16107 = torch.aten.transpose.int %16106, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16108 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16109 = torch.aten.view %16103, %16108 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16110 = torch.aten.mm %16109, %16107 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16111 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16112 = torch.aten.view %16110, %16111 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %16113 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16114 = torch.aten.transpose.int %16113, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16115 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16116 = torch.aten.view %16103, %16115 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16117 = torch.aten.mm %16116, %16114 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16118 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16119 = torch.aten.view %16117, %16118 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %16120 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16121 = torch.aten.transpose.int %16120, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16122 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16123 = torch.aten.view %16103, %16122 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16124 = torch.aten.mm %16123, %16121 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16125 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16126 = torch.aten.view %16124, %16125 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16127 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16128 = torch.aten.view %16112, %16127 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16129 = torch.aten.transpose.int %16128, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16130 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16131 = torch.aten.view %16119, %16130 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16132 = torch.aten.transpose.int %16131, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16133 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16134 = torch.aten.view %16126, %16133 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16135 = torch.aten.transpose.int %16134, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16136:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16129, %16132, %16135, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16137 = torch.aten.transpose.int %16136#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16138 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16139 = torch.aten.view %16137, %16138 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16140 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16141 = torch.aten.view %16139, %16140 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %16142 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16143 = torch.aten.transpose.int %16142, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %16144 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16145 = torch.prims.convert_element_type %16144, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16146 = torch.prims.convert_element_type %16141, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16147 = torch.prims.convert_element_type %16143, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16148 = torch.aten.mm %16146, %16147 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16149 = torch.aten.mul.Scalar %16148, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16150 = torch.aten.mul.Scalar %16145, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16151 = torch.aten.add.Tensor %16149, %16150, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16152 = torch.prims.convert_element_type %16151, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16153 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16154 = torch.aten.view %16152, %16153 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16155 = torch.aten.div.Scalar %16154, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16156 = torch.aten.add.Tensor %16155, %16092, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16157 = torch.prims.convert_element_type %16156, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16158 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_495, %result1_496 = torch.aten.var_mean.correction %16157, %16158, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16159 = torch.aten.add.Scalar %result0_495, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16160 = torch.aten.rsqrt %16159 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16161 = torch.aten.sub.Tensor %16156, %result1_496, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16162 = torch.aten.mul.Tensor %16161, %16160 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %16163 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16164 = torch.aten.mul.Tensor %16162, %16163 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %16165 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16166 = torch.aten.add.Tensor %16164, %16165, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16167 = torch.prims.convert_element_type %16166, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16168 = torch.prims.convert_element_type %result1_496, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16169 = torch.prims.convert_element_type %16160, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %16170 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16171 = torch.aten.transpose.int %16170, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16172 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16173 = torch.aten.view %16167, %16172 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16174 = torch.aten.mm %16173, %16171 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16175 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16176 = torch.aten.view %16174, %16175 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %16177 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16178 = torch.aten.transpose.int %16177, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16179 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16180 = torch.aten.view %4, %16179 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16181 = torch.aten.mm %16180, %16178 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16182 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16183 = torch.aten.view %16181, %16182 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %16184 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16185 = torch.aten.transpose.int %16184, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16186 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16187 = torch.aten.view %4, %16186 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16188 = torch.aten.mm %16187, %16185 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16189 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16190 = torch.aten.view %16188, %16189 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %16191 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16192 = torch.aten.view %16176, %16191 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16193 = torch.aten.transpose.int %16192, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16194 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16195 = torch.aten.view %16183, %16194 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16196 = torch.aten.transpose.int %16195, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16197 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16198 = torch.aten.view %16190, %16197 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16199 = torch.aten.transpose.int %16198, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16200:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16193, %16196, %16199, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16201 = torch.aten.transpose.int %16200#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16202 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16203 = torch.aten.view %16201, %16202 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16204 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16205 = torch.aten.view %16203, %16204 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %16206 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16207 = torch.aten.transpose.int %16206, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %16208 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16209 = torch.prims.convert_element_type %16208, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16210 = torch.prims.convert_element_type %16205, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16211 = torch.prims.convert_element_type %16207, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16212 = torch.aten.mm %16210, %16211 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16213 = torch.aten.mul.Scalar %16212, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16214 = torch.aten.mul.Scalar %16209, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16215 = torch.aten.add.Tensor %16213, %16214, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16216 = torch.prims.convert_element_type %16215, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16217 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16218 = torch.aten.view %16216, %16217 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16219 = torch.aten.div.Scalar %16218, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16220 = torch.aten.add.Tensor %16219, %16156, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16221 = torch.prims.convert_element_type %16220, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16222 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_497, %result1_498 = torch.aten.var_mean.correction %16221, %16222, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16223 = torch.aten.add.Scalar %result0_497, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16224 = torch.aten.rsqrt %16223 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16225 = torch.aten.sub.Tensor %16220, %result1_498, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16226 = torch.aten.mul.Tensor %16225, %16224 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %16227 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16228 = torch.aten.mul.Tensor %16226, %16227 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %16229 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16230 = torch.aten.add.Tensor %16228, %16229, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16231 = torch.prims.convert_element_type %16230, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16232 = torch.prims.convert_element_type %result1_498, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16233 = torch.prims.convert_element_type %16224, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16234 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16235 = torch.aten.view %16231, %16234 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %16236 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %16237 = torch.aten.transpose.int %16236, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %16238 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %16239 = torch.prims.convert_element_type %16238, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %16240 = torch.prims.convert_element_type %16235, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16241 = torch.prims.convert_element_type %16237, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %16242 = torch.aten.mm %16240, %16241 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %16243 = torch.aten.mul.Scalar %16242, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16244 = torch.aten.mul.Scalar %16239, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %16245 = torch.aten.add.Tensor %16243, %16244, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16246 = torch.prims.convert_element_type %16245, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %16247 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16248 = torch.aten.view %16246, %16247 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %16249 = torch.aten.slice.Tensor %16248, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16250 = torch.aten.slice.Tensor %16248, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16251 = torch.aten.gelu %16250, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %16252 = torch.aten.mul.Tensor %16249, %16251 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %16253 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %16254 = torch.aten.view %16252, %16253 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %16255 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %16256 = torch.aten.transpose.int %16255, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %16257 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16258 = torch.prims.convert_element_type %16257, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16259 = torch.prims.convert_element_type %16254, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %16260 = torch.prims.convert_element_type %16256, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %16261 = torch.aten.mm %16259, %16260 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16262 = torch.aten.mul.Scalar %16261, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16263 = torch.aten.mul.Scalar %16258, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16264 = torch.aten.add.Tensor %16262, %16263, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16265 = torch.prims.convert_element_type %16264, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16266 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16267 = torch.aten.view %16265, %16266 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16268 = torch.aten.add.Tensor %16267, %16220, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16269 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16270 = torch.aten.view %16268, %16269 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_out.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_out.weight : tensor<640x640xf16>
    %16271 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16272 = torch.aten.transpose.int %16271, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_out.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_out.bias : tensor<640xf16>
    %16273 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16274 = torch.prims.convert_element_type %16273, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16275 = torch.prims.convert_element_type %16270, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16276 = torch.prims.convert_element_type %16272, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16277 = torch.aten.mm %16275, %16276 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16278 = torch.aten.mul.Scalar %16277, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16279 = torch.aten.mul.Scalar %16274, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16280 = torch.aten.add.Tensor %16278, %16279, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16281 = torch.prims.convert_element_type %16280, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16282 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16283 = torch.aten.view %16281, %16282 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16284 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16285 = torch.aten.view %16283, %16284 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %16286 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16287 = torch.aten.permute %16285, %16286 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %16288 = torch.aten.add.Tensor %16287, %15872, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16289 = torch.prims.convert_element_type %16288, %int6 : !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %cpu_499 = torch.constant.device "cpu"
    %16290 = torch.aten.arange %int128, %int6, %none, %cpu_499, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],f32>
    %16291 = torch.aten.add.Scalar %16290, %float0.000000e00, %int1 : !torch.vtensor<[128],f32>, !torch.float, !torch.int -> !torch.vtensor<[128],f32>
    %16292 = torch.aten.mul.Scalar %16291, %float5.000000e-01 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %16293 = torch.prims.convert_element_type %16292, %int4 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],si64>
    %16294 = torch.aten.unsqueeze %16293, %int-1 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128,1],si64>
    %cpu_500 = torch.constant.device "cpu"
    %16295 = torch.aten.arange %int128, %int6, %none, %cpu_500, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],f32>
    %16296 = torch.aten.add.Scalar %16295, %float0.000000e00, %int1 : !torch.vtensor<[128],f32>, !torch.float, !torch.int -> !torch.vtensor<[128],f32>
    %16297 = torch.aten.mul.Scalar %16296, %float5.000000e-01 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %16298 = torch.prims.convert_element_type %16297, %int4 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],si64>
    %16299 = torch.prim.ListConstruct %none, %none, %16294, %16298 : (!torch.none, !torch.none, !torch.vtensor<[128,1],si64>, !torch.vtensor<[128],si64>) -> !torch.list<optional<vtensor>>
    %16300 = torch.aten.index.Tensor %16289, %16299 : !torch.vtensor<[2,640,64,64],f32>, !torch.list<optional<vtensor>> -> !torch.vtensor<[2,640,128,128],f32>
    %16301 = torch.prims.convert_element_type %16300, %int5 : !torch.vtensor<[2,640,128,128],f32>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %_params.unet.up_blocks.1.upsamplers.0.conv.weight = util.global.load @_params.unet.up_blocks.1.upsamplers.0.conv.weight : tensor<640x640x3x3xf16>
    %16302 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.upsamplers.0.conv.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.upsamplers.0.conv.bias = util.global.load @_params.unet.up_blocks.1.upsamplers.0.conv.bias : tensor<640xf16>
    %16303 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.upsamplers.0.conv.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16304 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16305 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16306 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16307 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16308 = torch.aten.convolution %16301, %16302, %16303, %16304, %16305, %16306, %false, %16307, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %16309 = torch.prim.ListConstruct %16308, %300 : (!torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>) -> !torch.list<vtensor>
    %16310 = torch.aten.cat %16309, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,960,128,128],f16>
    %16311 = torch.prim.ListConstruct %int2, %int32, %int30, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16312 = torch.aten.view %16310, %16311 : !torch.vtensor<[2,960,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,30,16384],f16>
    %16313 = torch.prims.convert_element_type %16312, %int6 : !torch.vtensor<[2,32,30,16384],f16>, !torch.int -> !torch.vtensor<[2,32,30,16384],f32>
    %16314 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_501, %result1_502 = torch.aten.var_mean.correction %16313, %16314, %int0, %true : !torch.vtensor<[2,32,30,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16315 = torch.aten.add.Scalar %result0_501, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16316 = torch.aten.rsqrt %16315 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16317 = torch.aten.sub.Tensor %16312, %result1_502, %int1 : !torch.vtensor<[2,32,30,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,30,16384],f32>
    %16318 = torch.aten.mul.Tensor %16317, %16316 : !torch.vtensor<[2,32,30,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,30,16384],f32>
    %16319 = torch.prim.ListConstruct %int2, %int960, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16320 = torch.aten.view %16318, %16319 : !torch.vtensor<[2,32,30,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,960,128,128],f32>
    %_params.unet.up_blocks.2.resnets.0.norm1.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.norm1.bias : tensor<960xf16>
    %16321 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm1.bias : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %16322 = torch.aten.unsqueeze %16321, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %16323 = torch.aten.unsqueeze %16322, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %16324 = torch.aten.unsqueeze %16323, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %_params.unet.up_blocks.2.resnets.0.norm1.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.norm1.weight : tensor<960xf16>
    %16325 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm1.weight : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %16326 = torch.aten.unsqueeze %16325, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %16327 = torch.aten.unsqueeze %16326, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %16328 = torch.aten.unsqueeze %16327, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %16329 = torch.aten.mul.Tensor %16320, %16328 : !torch.vtensor<[2,960,128,128],f32>, !torch.vtensor<[1,960,1,1],f16> -> !torch.vtensor<[2,960,128,128],f32>
    %16330 = torch.aten.add.Tensor %16329, %16324, %int1 : !torch.vtensor<[2,960,128,128],f32>, !torch.vtensor<[1,960,1,1],f16>, !torch.int -> !torch.vtensor<[2,960,128,128],f32>
    %16331 = torch.prims.convert_element_type %16330, %int5 : !torch.vtensor<[2,960,128,128],f32>, !torch.int -> !torch.vtensor<[2,960,128,128],f16>
    %16332 = torch.prims.convert_element_type %result1_502, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16333 = torch.prims.convert_element_type %16316, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16334 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16335 = torch.prims.squeeze %16332, %16334 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16336 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16337 = torch.prims.squeeze %16335, %16336 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16338 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16339 = torch.prims.squeeze %16333, %16338 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16340 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16341 = torch.prims.squeeze %16339, %16340 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16342 = torch.aten.silu %16331 : !torch.vtensor<[2,960,128,128],f16> -> !torch.vtensor<[2,960,128,128],f16>
    %_params.unet.up_blocks.2.resnets.0.conv1.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.conv1.weight : tensor<320x960x3x3xf16>
    %16343 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv1.weight : tensor<320x960x3x3xf16> -> !torch.vtensor<[320,960,3,3],f16>
    %_params.unet.up_blocks.2.resnets.0.conv1.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.conv1.bias : tensor<320xf16>
    %16344 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16345 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16346 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16347 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16348 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16349 = torch.aten.convolution %16342, %16343, %16344, %16345, %16346, %16347, %false, %16348, %int1 : !torch.vtensor<[2,960,128,128],f16>, !torch.vtensor<[320,960,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16350 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight : tensor<320x1280xf16>
    %16351 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %16352 = torch.aten.transpose.int %16351, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias : tensor<320xf16>
    %16353 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16354 = torch.prims.convert_element_type %16353, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %16355 = torch.prims.convert_element_type %16350, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %16356 = torch.prims.convert_element_type %16352, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %16357 = torch.aten.mm %16355, %16356 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %16358 = torch.aten.mul.Scalar %16357, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %16359 = torch.aten.mul.Scalar %16354, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %16360 = torch.aten.add.Tensor %16358, %16359, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %16361 = torch.prims.convert_element_type %16360, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %16362 = torch.aten.unsqueeze %16361, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %16363 = torch.aten.unsqueeze %16362, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %16364 = torch.aten.add.Tensor %16349, %16363, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16365 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16366 = torch.aten.view %16364, %16365 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %16367 = torch.prims.convert_element_type %16366, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16368 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_503, %result1_504 = torch.aten.var_mean.correction %16367, %16368, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16369 = torch.aten.add.Scalar %result0_503, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16370 = torch.aten.rsqrt %16369 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16371 = torch.aten.sub.Tensor %16366, %result1_504, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16372 = torch.aten.mul.Tensor %16371, %16370 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %16373 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16374 = torch.aten.view %16372, %16373 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.up_blocks.2.resnets.0.norm2.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.norm2.bias : tensor<320xf16>
    %16375 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16376 = torch.aten.unsqueeze %16375, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16377 = torch.aten.unsqueeze %16376, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16378 = torch.aten.unsqueeze %16377, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.up_blocks.2.resnets.0.norm2.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.norm2.weight : tensor<320xf16>
    %16379 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16380 = torch.aten.unsqueeze %16379, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16381 = torch.aten.unsqueeze %16380, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16382 = torch.aten.unsqueeze %16381, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %16383 = torch.aten.mul.Tensor %16374, %16382 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %16384 = torch.aten.add.Tensor %16383, %16378, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %16385 = torch.prims.convert_element_type %16384, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16386 = torch.prims.convert_element_type %result1_504, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16387 = torch.prims.convert_element_type %16370, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16388 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16389 = torch.prims.squeeze %16386, %16388 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16390 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16391 = torch.prims.squeeze %16389, %16390 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16392 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16393 = torch.prims.squeeze %16387, %16392 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16394 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16395 = torch.prims.squeeze %16393, %16394 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16396 = torch.aten.silu %16385 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.0.conv2.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.conv2.weight : tensor<320x320x3x3xf16>
    %16397 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.up_blocks.2.resnets.0.conv2.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.conv2.bias : tensor<320xf16>
    %16398 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16399 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16400 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16401 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16402 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16403 = torch.aten.convolution %16396, %16397, %16398, %16399, %16400, %16401, %false, %16402, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight : tensor<320x960x1x1xf16>
    %16404 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight : tensor<320x960x1x1xf16> -> !torch.vtensor<[320,960,1,1],f16>
    %_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias : tensor<320xf16>
    %16405 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16406 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16407 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16408 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16409 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16410 = torch.aten.convolution %16310, %16404, %16405, %16406, %16407, %16408, %false, %16409, %int1 : !torch.vtensor<[2,960,128,128],f16>, !torch.vtensor<[320,960,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16411 = torch.aten.add.Tensor %16410, %16403, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16412 = torch.aten.div.Scalar %16411, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %16413 = torch.prim.ListConstruct %16412, %205 : (!torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>) -> !torch.list<vtensor>
    %16414 = torch.aten.cat %16413, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %16415 = torch.prim.ListConstruct %int2, %int32, %int20, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16416 = torch.aten.view %16414, %16415 : !torch.vtensor<[2,640,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,16384],f16>
    %16417 = torch.prims.convert_element_type %16416, %int6 : !torch.vtensor<[2,32,20,16384],f16>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %16418 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_505, %result1_506 = torch.aten.var_mean.correction %16417, %16418, %int0, %true : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16419 = torch.aten.add.Scalar %result0_505, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16420 = torch.aten.rsqrt %16419 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16421 = torch.aten.sub.Tensor %16416, %result1_506, %int1 : !torch.vtensor<[2,32,20,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %16422 = torch.aten.mul.Tensor %16421, %16420 : !torch.vtensor<[2,32,20,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,16384],f32>
    %16423 = torch.prim.ListConstruct %int2, %int640, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16424 = torch.aten.view %16422, %16423 : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,640,128,128],f32>
    %_params.unet.up_blocks.2.resnets.1.norm1.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.norm1.bias : tensor<640xf16>
    %16425 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16426 = torch.aten.unsqueeze %16425, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16427 = torch.aten.unsqueeze %16426, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16428 = torch.aten.unsqueeze %16427, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.1.norm1.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.norm1.weight : tensor<640xf16>
    %16429 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16430 = torch.aten.unsqueeze %16429, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16431 = torch.aten.unsqueeze %16430, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16432 = torch.aten.unsqueeze %16431, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %16433 = torch.aten.mul.Tensor %16424, %16432 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,128,128],f32>
    %16434 = torch.aten.add.Tensor %16433, %16428, %int1 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,128,128],f32>
    %16435 = torch.prims.convert_element_type %16434, %int5 : !torch.vtensor<[2,640,128,128],f32>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %16436 = torch.prims.convert_element_type %result1_506, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16437 = torch.prims.convert_element_type %16420, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16438 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16439 = torch.prims.squeeze %16436, %16438 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16440 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16441 = torch.prims.squeeze %16439, %16440 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16442 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16443 = torch.prims.squeeze %16437, %16442 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16444 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16445 = torch.prims.squeeze %16443, %16444 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16446 = torch.aten.silu %16435 : !torch.vtensor<[2,640,128,128],f16> -> !torch.vtensor<[2,640,128,128],f16>
    %_params.unet.up_blocks.2.resnets.1.conv1.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.conv1.weight : tensor<320x640x3x3xf16>
    %16447 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv1.weight : tensor<320x640x3x3xf16> -> !torch.vtensor<[320,640,3,3],f16>
    %_params.unet.up_blocks.2.resnets.1.conv1.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.conv1.bias : tensor<320xf16>
    %16448 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16449 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16450 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16451 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16452 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16453 = torch.aten.convolution %16446, %16447, %16448, %16449, %16450, %16451, %false, %16452, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16454 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight : tensor<320x1280xf16>
    %16455 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %16456 = torch.aten.transpose.int %16455, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias : tensor<320xf16>
    %16457 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16458 = torch.prims.convert_element_type %16457, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %16459 = torch.prims.convert_element_type %16454, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %16460 = torch.prims.convert_element_type %16456, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %16461 = torch.aten.mm %16459, %16460 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %16462 = torch.aten.mul.Scalar %16461, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %16463 = torch.aten.mul.Scalar %16458, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %16464 = torch.aten.add.Tensor %16462, %16463, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %16465 = torch.prims.convert_element_type %16464, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %16466 = torch.aten.unsqueeze %16465, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %16467 = torch.aten.unsqueeze %16466, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %16468 = torch.aten.add.Tensor %16453, %16467, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16469 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16470 = torch.aten.view %16468, %16469 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %16471 = torch.prims.convert_element_type %16470, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16472 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_507, %result1_508 = torch.aten.var_mean.correction %16471, %16472, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16473 = torch.aten.add.Scalar %result0_507, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16474 = torch.aten.rsqrt %16473 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16475 = torch.aten.sub.Tensor %16470, %result1_508, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16476 = torch.aten.mul.Tensor %16475, %16474 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %16477 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16478 = torch.aten.view %16476, %16477 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.up_blocks.2.resnets.1.norm2.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.norm2.bias : tensor<320xf16>
    %16479 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16480 = torch.aten.unsqueeze %16479, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16481 = torch.aten.unsqueeze %16480, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16482 = torch.aten.unsqueeze %16481, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.up_blocks.2.resnets.1.norm2.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.norm2.weight : tensor<320xf16>
    %16483 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16484 = torch.aten.unsqueeze %16483, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16485 = torch.aten.unsqueeze %16484, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16486 = torch.aten.unsqueeze %16485, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %16487 = torch.aten.mul.Tensor %16478, %16486 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %16488 = torch.aten.add.Tensor %16487, %16482, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %16489 = torch.prims.convert_element_type %16488, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16490 = torch.prims.convert_element_type %result1_508, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16491 = torch.prims.convert_element_type %16474, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16492 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16493 = torch.prims.squeeze %16490, %16492 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16494 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16495 = torch.prims.squeeze %16493, %16494 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16496 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16497 = torch.prims.squeeze %16491, %16496 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16498 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16499 = torch.prims.squeeze %16497, %16498 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16500 = torch.aten.silu %16489 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.1.conv2.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.conv2.weight : tensor<320x320x3x3xf16>
    %16501 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.up_blocks.2.resnets.1.conv2.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.conv2.bias : tensor<320xf16>
    %16502 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16503 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16504 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16505 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16506 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16507 = torch.aten.convolution %16500, %16501, %16502, %16503, %16504, %16505, %false, %16506, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight : tensor<320x640x1x1xf16>
    %16508 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight : tensor<320x640x1x1xf16> -> !torch.vtensor<[320,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias : tensor<320xf16>
    %16509 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16510 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16511 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16512 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16513 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16514 = torch.aten.convolution %16414, %16508, %16509, %16510, %16511, %16512, %false, %16513, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16515 = torch.aten.add.Tensor %16514, %16507, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16516 = torch.aten.div.Scalar %16515, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %16517 = torch.prim.ListConstruct %16516, %110 : (!torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>) -> !torch.list<vtensor>
    %16518 = torch.aten.cat %16517, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %16519 = torch.prim.ListConstruct %int2, %int32, %int20, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16520 = torch.aten.view %16518, %16519 : !torch.vtensor<[2,640,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,16384],f16>
    %16521 = torch.prims.convert_element_type %16520, %int6 : !torch.vtensor<[2,32,20,16384],f16>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %16522 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_509, %result1_510 = torch.aten.var_mean.correction %16521, %16522, %int0, %true : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16523 = torch.aten.add.Scalar %result0_509, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16524 = torch.aten.rsqrt %16523 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16525 = torch.aten.sub.Tensor %16520, %result1_510, %int1 : !torch.vtensor<[2,32,20,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %16526 = torch.aten.mul.Tensor %16525, %16524 : !torch.vtensor<[2,32,20,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,16384],f32>
    %16527 = torch.prim.ListConstruct %int2, %int640, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16528 = torch.aten.view %16526, %16527 : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,640,128,128],f32>
    %_params.unet.up_blocks.2.resnets.2.norm1.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.norm1.bias : tensor<640xf16>
    %16529 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16530 = torch.aten.unsqueeze %16529, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16531 = torch.aten.unsqueeze %16530, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16532 = torch.aten.unsqueeze %16531, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.2.norm1.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.norm1.weight : tensor<640xf16>
    %16533 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16534 = torch.aten.unsqueeze %16533, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16535 = torch.aten.unsqueeze %16534, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16536 = torch.aten.unsqueeze %16535, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %16537 = torch.aten.mul.Tensor %16528, %16536 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,128,128],f32>
    %16538 = torch.aten.add.Tensor %16537, %16532, %int1 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,128,128],f32>
    %16539 = torch.prims.convert_element_type %16538, %int5 : !torch.vtensor<[2,640,128,128],f32>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %16540 = torch.prims.convert_element_type %result1_510, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16541 = torch.prims.convert_element_type %16524, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16542 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16543 = torch.prims.squeeze %16540, %16542 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16544 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16545 = torch.prims.squeeze %16543, %16544 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16546 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16547 = torch.prims.squeeze %16541, %16546 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16548 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16549 = torch.prims.squeeze %16547, %16548 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16550 = torch.aten.silu %16539 : !torch.vtensor<[2,640,128,128],f16> -> !torch.vtensor<[2,640,128,128],f16>
    %_params.unet.up_blocks.2.resnets.2.conv1.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.conv1.weight : tensor<320x640x3x3xf16>
    %16551 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv1.weight : tensor<320x640x3x3xf16> -> !torch.vtensor<[320,640,3,3],f16>
    %_params.unet.up_blocks.2.resnets.2.conv1.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.conv1.bias : tensor<320xf16>
    %16552 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16553 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16554 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16555 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16556 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16557 = torch.aten.convolution %16550, %16551, %16552, %16553, %16554, %16555, %false, %16556, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16558 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight : tensor<320x1280xf16>
    %16559 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %16560 = torch.aten.transpose.int %16559, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias : tensor<320xf16>
    %16561 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16562 = torch.prims.convert_element_type %16561, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %16563 = torch.prims.convert_element_type %16558, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %16564 = torch.prims.convert_element_type %16560, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %16565 = torch.aten.mm %16563, %16564 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %16566 = torch.aten.mul.Scalar %16565, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %16567 = torch.aten.mul.Scalar %16562, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %16568 = torch.aten.add.Tensor %16566, %16567, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %16569 = torch.prims.convert_element_type %16568, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %16570 = torch.aten.unsqueeze %16569, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %16571 = torch.aten.unsqueeze %16570, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %16572 = torch.aten.add.Tensor %16557, %16571, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16573 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16574 = torch.aten.view %16572, %16573 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %16575 = torch.prims.convert_element_type %16574, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16576 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_511, %result1_512 = torch.aten.var_mean.correction %16575, %16576, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16577 = torch.aten.add.Scalar %result0_511, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16578 = torch.aten.rsqrt %16577 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16579 = torch.aten.sub.Tensor %16574, %result1_512, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16580 = torch.aten.mul.Tensor %16579, %16578 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %16581 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16582 = torch.aten.view %16580, %16581 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.up_blocks.2.resnets.2.norm2.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.norm2.bias : tensor<320xf16>
    %16583 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16584 = torch.aten.unsqueeze %16583, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16585 = torch.aten.unsqueeze %16584, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16586 = torch.aten.unsqueeze %16585, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.up_blocks.2.resnets.2.norm2.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.norm2.weight : tensor<320xf16>
    %16587 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16588 = torch.aten.unsqueeze %16587, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16589 = torch.aten.unsqueeze %16588, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16590 = torch.aten.unsqueeze %16589, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %16591 = torch.aten.mul.Tensor %16582, %16590 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %16592 = torch.aten.add.Tensor %16591, %16586, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %16593 = torch.prims.convert_element_type %16592, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16594 = torch.prims.convert_element_type %result1_512, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16595 = torch.prims.convert_element_type %16578, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16596 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16597 = torch.prims.squeeze %16594, %16596 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16598 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16599 = torch.prims.squeeze %16597, %16598 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16600 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16601 = torch.prims.squeeze %16595, %16600 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16602 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16603 = torch.prims.squeeze %16601, %16602 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16604 = torch.aten.silu %16593 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.2.conv2.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.conv2.weight : tensor<320x320x3x3xf16>
    %16605 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.up_blocks.2.resnets.2.conv2.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.conv2.bias : tensor<320xf16>
    %16606 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16607 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16608 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16609 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16610 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16611 = torch.aten.convolution %16604, %16605, %16606, %16607, %16608, %16609, %false, %16610, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight : tensor<320x640x1x1xf16>
    %16612 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight : tensor<320x640x1x1xf16> -> !torch.vtensor<[320,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias : tensor<320xf16>
    %16613 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16614 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16615 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16616 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16617 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16618 = torch.aten.convolution %16518, %16612, %16613, %16614, %16615, %16616, %false, %16617, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16619 = torch.aten.add.Tensor %16618, %16611, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16620 = torch.aten.div.Scalar %16619, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %16621 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16622 = torch.aten.view %16620, %16621 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %16623 = torch.prims.convert_element_type %16622, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16624 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_513, %result1_514 = torch.aten.var_mean.correction %16623, %16624, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16625 = torch.aten.add.Scalar %result0_513, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16626 = torch.aten.rsqrt %16625 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16627 = torch.aten.sub.Tensor %16622, %result1_514, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %16628 = torch.aten.mul.Tensor %16627, %16626 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %16629 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16630 = torch.aten.view %16628, %16629 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.conv_norm_out.bias = util.global.load @_params.unet.conv_norm_out.bias : tensor<320xf16>
    %16631 = torch_c.from_builtin_tensor %_params.unet.conv_norm_out.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16632 = torch.aten.unsqueeze %16631, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16633 = torch.aten.unsqueeze %16632, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16634 = torch.aten.unsqueeze %16633, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.conv_norm_out.weight = util.global.load @_params.unet.conv_norm_out.weight : tensor<320xf16>
    %16635 = torch_c.from_builtin_tensor %_params.unet.conv_norm_out.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %16636 = torch.aten.unsqueeze %16635, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %16637 = torch.aten.unsqueeze %16636, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %16638 = torch.aten.unsqueeze %16637, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %16639 = torch.aten.mul.Tensor %16630, %16638 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %16640 = torch.aten.add.Tensor %16639, %16634, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %16641 = torch.prims.convert_element_type %16640, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %16642 = torch.prims.convert_element_type %result1_514, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16643 = torch.prims.convert_element_type %16626, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16644 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16645 = torch.prims.squeeze %16642, %16644 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16646 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16647 = torch.prims.squeeze %16645, %16646 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16648 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16649 = torch.prims.squeeze %16643, %16648 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16650 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16651 = torch.prims.squeeze %16649, %16650 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16652 = torch.aten.silu %16641 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.conv_out.weight = util.global.load @_params.unet.conv_out.weight : tensor<4x320x3x3xf16>
    %16653 = torch_c.from_builtin_tensor %_params.unet.conv_out.weight : tensor<4x320x3x3xf16> -> !torch.vtensor<[4,320,3,3],f16>
    %_params.unet.conv_out.bias = util.global.load @_params.unet.conv_out.bias : tensor<4xf16>
    %16654 = torch_c.from_builtin_tensor %_params.unet.conv_out.bias : tensor<4xf16> -> !torch.vtensor<[4],f16>
    %16655 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16656 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16657 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16658 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16659 = torch.aten.convolution %16652, %16653, %16654, %16655, %16656, %16657, %false, %16658, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[4,320,3,3],f16>, !torch.vtensor<[4],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,4,128,128],f16>
    %16660 = torch.aten.slice.Tensor %16659, %int0, %int0, %int1, %int1 : !torch.vtensor<[2,4,128,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %16661 = torch.aten.slice.Tensor %16659, %int0, %int1, %int2, %int1 : !torch.vtensor<[2,4,128,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %16662 = torch.aten.sub.Tensor %16661, %16660, %int1 : !torch.vtensor<[1,4,128,128],f16>, !torch.vtensor<[1,4,128,128],f16>, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %16663 = torch.aten.mul.Tensor %7, %16662 : !torch.vtensor<[1],f16>, !torch.vtensor<[1,4,128,128],f16> -> !torch.vtensor<[1,4,128,128],f16>
    %16664 = torch.aten.add.Tensor %16660, %16663, %int1 : !torch.vtensor<[1,4,128,128],f16>, !torch.vtensor<[1,4,128,128],f16>, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %16665 = torch.aten.sub.Scalar %10, %int34, %int1 : !torch.vtensor<[1],si64>, !torch.int, !torch.int -> !torch.vtensor<[1],si64>
    %16666 = torch.aten.index_select %1, %int0, %10 : !torch.vtensor<[1000],f32>, !torch.int, !torch.vtensor<[1],si64> -> !torch.vtensor<[1],f32>
    %16667 = torch.aten.ge.Scalar %16665, %int0 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1],i1>
    %16668 = torch.aten.add.Scalar %16665, %int1000, %int1 : !torch.vtensor<[1],si64>, !torch.int, !torch.int -> !torch.vtensor<[1],si64>
    %16669 = torch.aten.where.self %16667, %16665, %16668 : !torch.vtensor<[1],i1>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64> -> !torch.vtensor<[1],si64>
    %16670 = torch.aten.ge.Scalar %16665, %int0 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1],i1>
    %16671 = torch.aten.index_select %1, %int0, %16669 : !torch.vtensor<[1000],f32>, !torch.int, !torch.vtensor<[1],si64> -> !torch.vtensor<[1],f32>
    %16672 = torch.aten.where.self %16670, %16671, %2 : !torch.vtensor<[1],i1>, !torch.vtensor<[1],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1],f32>
    %16673 = torch.aten.rsub.Scalar %16666, %int1, %int1 : !torch.vtensor<[1],f32>, !torch.int, !torch.int -> !torch.vtensor<[1],f32>
    %16674 = torch.aten.rsub.Scalar %16672, %int1, %int1 : !torch.vtensor<[1],f32>, !torch.int, !torch.int -> !torch.vtensor<[1],f32>
    %16675 = torch.aten.div.Tensor %16672, %16666 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %16676 = torch.aten.pow.Tensor_Scalar %16675, %float5.000000e-01 : !torch.vtensor<[1],f32>, !torch.float -> !torch.vtensor<[1],f32>
    %16677 = torch.aten.pow.Tensor_Scalar %16674, %float5.000000e-01 : !torch.vtensor<[1],f32>, !torch.float -> !torch.vtensor<[1],f32>
    %16678 = torch.aten.mul.Tensor %16666, %16677 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %16679 = torch.aten.mul.Tensor %16666, %16673 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %16680 = torch.aten.mul.Tensor %16679, %16672 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %16681 = torch.aten.pow.Tensor_Scalar %16680, %float5.000000e-01 : !torch.vtensor<[1],f32>, !torch.float -> !torch.vtensor<[1],f32>
    %16682 = torch.aten.add.Tensor %16678, %16681, %int1 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32>, !torch.int -> !torch.vtensor<[1],f32>
    %16683 = torch.aten.mul.Tensor %16676, %3 : !torch.vtensor<[1],f32>, !torch.vtensor<[1,4,128,128],f16> -> !torch.vtensor<[1,4,128,128],f32>
    %16684 = torch.aten.sub.Tensor %16672, %16666, %int1 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32>, !torch.int -> !torch.vtensor<[1],f32>
    %16685 = torch.aten.mul.Tensor %16684, %16664 : !torch.vtensor<[1],f32>, !torch.vtensor<[1,4,128,128],f16> -> !torch.vtensor<[1,4,128,128],f32>
    %16686 = torch.aten.div.Tensor %16685, %16682 : !torch.vtensor<[1,4,128,128],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1,4,128,128],f32>
    %16687 = torch.aten.sub.Tensor %16683, %16686, %int1 : !torch.vtensor<[1,4,128,128],f32>, !torch.vtensor<[1,4,128,128],f32>, !torch.int -> !torch.vtensor<[1,4,128,128],f32>
    %16688 = torch.prims.convert_element_type %16687, %int5 : !torch.vtensor<[1,4,128,128],f32>, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %16689 = torch_c.to_builtin_tensor %16688 : !torch.vtensor<[1,4,128,128],f16> -> tensor<1x4x128x128xf16>
    return %16689 : tensor<1x4x128x128xf16>
  }
  hal.executable.source private @hip_matmul_exe_2048x1280_1280x1280 attributes {objects = #hal.executable.objects<{#executable_target_rocm_hsaco_fb = [#hal.executable.object<{path = "../tensile/output_2048x1280x1280/Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128LGssD3D5uUpJpwMOz9bV1KUetBSFPuVbIJbtPQNALVk=.co"}>]}>} {
    hal.executable.export public @Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1 ordinal(0) layout(#pipeline_layout) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>, #hal.interface.binding<0, 3>], rocm.parameter_mapping = "c4:0:0,c4:4:4,c4:8:8,c4:12:12,c4:16:16,c4:20:20,b8:0:0:24,b8:0:1:32,b8:0:2:40,b8:0:3:48,c4:24:56,c4:28:60,c4:32:64,c4:36:68,c4:40:72,c4:44:76,c4:48:80,c4:52:84,c4:56:88,c4:60:92,c4:64:96,c4:68:100,c4:72:104,c4:76:108,c4:80:112,c4:84:116,c4:88:120,c4:92:124,c4:96:128", workgroup_size = [256 : index, 1 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device, %arg1: index):
      %c16 = arith.constant 16 : index
      %c1 = arith.constant 1 : index
      hal.return %c16, %c16, %c1 : index, index, index
    }
  }
}

{-#
  dialect_resources: {
    builtin: {
      torch_tensor_1_6_torch.int64: "0x08000000000400000000000000040000000000000000000000000000000000000000000000040000000000000004000000000000",
      torch_tensor_30_torch.int64: "0x08000000B903000000000000970300000000000097030000000000007503000000000000530300000000000031030000000000000F03000000000000ED02000000000000CB02000000000000A9020000000000008702000000000000650200000000000043020000000000002102000000000000FF01000000000000DD01000000000000BB0100000000000099010000000000007701000000000000550100000000000033010000000000001101000000000000EF00000000000000CD00000000000000AB0000000000000089000000000000006700000000000000450000000000000023000000000000000100000000000000",
      torch_tensor_1000_torch.float32: "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
    }
  }
#-}
