#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {target_arch = "gfx942"}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 1, sets = [<0, bindings = [<0, storage_buffer>, <1, storage_buffer>, <2, storage_buffer, ReadOnly>, <3, storage_buffer, ReadOnly>]>]>
module @compiled_scheduled_unet {
  util.global private @_params.unet.conv_in.weight {noinline} = #stream.parameter.named<"model"::"unet.conv_in.weight"> : tensor<320x4x3x3xf16>
  util.global private @_params.unet.conv_in.bias {noinline} = #stream.parameter.named<"model"::"unet.conv_in.bias"> : tensor<320xf16>
  util.global private @_params.unet.time_embedding.linear_1.weight {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_1.weight"> : tensor<1280x320xf16>
  util.global private @_params.unet.time_embedding.linear_1.bias {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.time_embedding.linear_2.weight {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_2.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.time_embedding.linear_2.bias {noinline} = #stream.parameter.named<"model"::"unet.time_embedding.linear_2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.add_embedding.linear_1.weight {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_1.weight"> : tensor<1280x2816xf16>
  util.global private @_params.unet.add_embedding.linear_1.bias {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.add_embedding.linear_2.weight {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_2.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.add_embedding.linear_2.bias {noinline} = #stream.parameter.named<"model"::"unet.add_embedding.linear_2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm1.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv1.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.0.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm1.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv1.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.resnets.1.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.0.downsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.downsamplers.0.conv.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.down_blocks.0.downsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.0.downsamplers.0.conv.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.0.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.down_blocks.1.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.attentions.1.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm1.weight"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm1.bias"> : tensor<320xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv1.weight"> : tensor<640x320x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv_shortcut.weight"> : tensor<640x320x1x1xf16>
  util.global private @_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.0.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv1.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.resnets.1.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.1.downsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.downsamplers.0.conv.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.down_blocks.1.downsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.1.downsamplers.0.conv.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.attentions.1.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv1.weight"> : tensor<1280x640x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv_shortcut.weight"> : tensor<1280x640x1x1xf16>
  util.global private @_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.0.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.down_blocks.2.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.down_blocks.2.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.1.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.attentions.2.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.attentions.2.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm1.weight"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm1.bias"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv1.weight"> : tensor<1280x2560x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv_shortcut.weight"> : tensor<1280x2560x1x1xf16>
  util.global private @_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.0.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm1.weight"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm1.bias"> : tensor<2560xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv1.weight"> : tensor<1280x2560x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv_shortcut.weight"> : tensor<1280x2560x1x1xf16>
  util.global private @_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.1.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm1.weight"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm1.bias"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv1.weight"> : tensor<1280x1920x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv_shortcut.weight"> : tensor<1280x1920x1x1xf16>
  util.global private @_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.resnets.2.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.0.upsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.upsamplers.0.conv.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.0.upsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.0.upsamplers.0.conv.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.0.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.1.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.1.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.norm.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.norm.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_in.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_in.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_out.weight"> : tensor<640x640xf16>
  util.global private @_params.unet.up_blocks.1.attentions.2.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.attentions.2.proj_out.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm1.weight"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm1.bias"> : tensor<1920xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv1.weight"> : tensor<640x1920x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv_shortcut.weight"> : tensor<640x1920x1x1xf16>
  util.global private @_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.0.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv1.weight"> : tensor<640x1280x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv_shortcut.weight"> : tensor<640x1280x1x1xf16>
  util.global private @_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.1.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm1.weight"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm1.bias"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv1.weight"> : tensor<640x960x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm2.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.norm2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv2.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv_shortcut.weight"> : tensor<640x960x1x1xf16>
  util.global private @_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.resnets.2.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.1.upsamplers.0.conv.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.upsamplers.0.conv.weight"> : tensor<640x640x3x3xf16>
  util.global private @_params.unet.up_blocks.1.upsamplers.0.conv.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.1.upsamplers.0.conv.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm1.weight"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm1.bias"> : tensor<960xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv1.weight"> : tensor<320x960x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv_shortcut.weight"> : tensor<320x960x1x1xf16>
  util.global private @_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.0.conv_shortcut.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv1.weight"> : tensor<320x640x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv_shortcut.weight"> : tensor<320x640x1x1xf16>
  util.global private @_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.1.conv_shortcut.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm1.weight"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm1.bias"> : tensor<640xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv1.weight"> : tensor<320x640x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv1.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm2.weight"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.norm2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv2.bias"> : tensor<320xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv_shortcut.weight"> : tensor<320x640x1x1xf16>
  util.global private @_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias {noinline} = #stream.parameter.named<"model"::"unet.up_blocks.2.resnets.2.conv_shortcut.bias"> : tensor<320xf16>
  util.global private @_params.unet.mid_block.attentions.0.norm.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.norm.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_in.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_in.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_out.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.attentions.0.proj_out.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.0.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv1.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv1.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.time_emb_proj.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.time_emb_proj.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.norm2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv2.weight {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @_params.unet.mid_block.resnets.1.conv2.bias {noinline} = #stream.parameter.named<"model"::"unet.mid_block.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @_params.unet.conv_norm_out.weight {noinline} = #stream.parameter.named<"model"::"unet.conv_norm_out.weight"> : tensor<320xf16>
  util.global private @_params.unet.conv_norm_out.bias {noinline} = #stream.parameter.named<"model"::"unet.conv_norm_out.bias"> : tensor<320xf16>
  util.global private @_params.unet.conv_out.weight {noinline} = #stream.parameter.named<"model"::"unet.conv_out.weight"> : tensor<4x320x3x3xf16>
  util.global private @_params.unet.conv_out.bias {noinline} = #stream.parameter.named<"model"::"unet.conv_out.bias"> : tensor<4xf16>
  func.func @run_initialize(%arg0: tensor<1x4x128x128xf16>) -> (tensor<1x4x128x128xf16>, tensor<2x6xf16>, tensor<i64>) attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.return_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]}]"} {
    %0 = torch.vtensor.literal(dense_resource<torch_tensor_1_6_torch.int64> : tensor<1x6xsi64>) : !torch.vtensor<[1,6],si64>
    %int0 = torch.constant.int 0
    %int1 = torch.constant.int 1
    %int5 = torch.constant.int 5
    %1 = torch.vtensor.literal(dense<30> : tensor<si64>) : !torch.vtensor<[],si64>
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %2 = torch_c.from_builtin_tensor %arg0 : tensor<1x4x128x128xf16> -> !torch.vtensor<[1,4,128,128],f16>
    %3 = torch.prim.ListConstruct %0, %0 : (!torch.vtensor<[1,6],si64>, !torch.vtensor<[1,6],si64>) -> !torch.list<vtensor>
    %4 = torch.aten.cat %3, %int0 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,6],si64>
    %5 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6 = torch.aten.repeat %4, %5 : !torch.vtensor<[2,6],si64>, !torch.list<int> -> !torch.vtensor<[2,6],si64>
    %7 = torch.prims.convert_element_type %6, %int5 : !torch.vtensor<[2,6],si64>, !torch.int -> !torch.vtensor<[2,6],f16>
    %8 = torch.aten.mul.Scalar %2, %float1.000000e00 : !torch.vtensor<[1,4,128,128],f16>, !torch.float -> !torch.vtensor<[1,4,128,128],f16>
    %9 = torch_c.to_builtin_tensor %8 : !torch.vtensor<[1,4,128,128],f16> -> tensor<1x4x128x128xf16>
    %10 = torch_c.to_builtin_tensor %7 : !torch.vtensor<[2,6],f16> -> tensor<2x6xf16>
    %11 = torch_c.to_builtin_tensor %1 : !torch.vtensor<[],si64> -> tensor<i64>
    return %9, %10, %11 : tensor<1x4x128x128xf16>, tensor<2x6xf16>, tensor<i64>
  }
  func.func @run_forward(%arg0: tensor<1x4x128x128xf16>, %arg1: tensor<2x64x2048xf16>, %arg2: tensor<2x1280xf16>, %arg3: tensor<2x6xf16>, %arg4: tensor<1xf16>, %arg5: tensor<1xi64>) -> tensor<1x4x128x128xf16> attributes {torch.args_schema = "[1, {\22type\22: \22builtins.tuple\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: \22builtins.list\22, \22context\22: \22null\22, \22children_spec\22: [{\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]}, {\22type\22: \22builtins.dict\22, \22context\22: \22[]\22, \22children_spec\22: []}]}]", torch.return_schema = "[1, {\22type\22: null, \22context\22: null, \22children_spec\22: []}]"} {
    %0 = torch.vtensor.literal(dense_resource<torch_tensor_30_torch.int64> : tensor<30xsi64>) : !torch.vtensor<[30],si64>
    %int0 = torch.constant.int 0
    %int2 = torch.constant.int 2
    %false = torch.constant.bool false
    %int160 = torch.constant.int 160
    %int6 = torch.constant.int 6
    %none = torch.constant.none
    %float-9.210340e00 = torch.constant.float -9.2103403719761836
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1 = torch.constant.int 1
    %int-1 = torch.constant.int -1
    %int5 = torch.constant.int 5
    %int12 = torch.constant.int 12
    %int128 = torch.constant.int 128
    %int32 = torch.constant.int 32
    %int10 = torch.constant.int 10
    %int16384 = torch.constant.int 16384
    %int3 = torch.constant.int 3
    %true = torch.constant.bool true
    %float1.000000e-05 = torch.constant.float 1.000000e-05
    %int320 = torch.constant.int 320
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %int4096 = torch.constant.int 4096
    %int64 = torch.constant.int 64
    %int20 = torch.constant.int 20
    %int640 = torch.constant.int 640
    %float9.999990e-07 = torch.constant.float 9.9999999999999995E-7
    %int8192 = torch.constant.int 8192
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %int2048 = torch.constant.int 2048
    %int5120 = torch.constant.int 5120
    %int2560 = torch.constant.int 2560
    %str = torch.constant.str "none"
    %int1024 = torch.constant.int 1024
    %int40 = torch.constant.int 40
    %int1280 = torch.constant.int 1280
    %int10240 = torch.constant.int 10240
    %int80 = torch.constant.int 80
    %int60 = torch.constant.int 60
    %int1920 = torch.constant.int 1920
    %float5.000000e-01 = torch.constant.float 5.000000e-01
    %int4 = torch.constant.int 4
    %int30 = torch.constant.int 30
    %int960 = torch.constant.int 960
    %int34 = torch.constant.int 34
    %1 = torch.vtensor.literal(dense_resource<torch_tensor_1000_torch.float32> : tensor<1000xf32>) : !torch.vtensor<[1000],f32>
    %int1000 = torch.constant.int 1000
    %2 = torch.vtensor.literal(dense<0.999149978> : tensor<f32>) : !torch.vtensor<[],f32>
    %c1 = arith.constant 1 : index
    %c16 = arith.constant 16 : index
    %c4096 = arith.constant 4096 : index
    %c0_i32 = arith.constant 0 : i32
    %c1065353216_i32 = arith.constant 1065353216 : i32
    %c1638400_i32 = arith.constant 1638400 : i32
    %c2621440_i32 = arith.constant 2621440 : i32
    %c1280_i32 = arith.constant 1280 : i32
    %c2048_i32 = arith.constant 2048 : i32
    %c17301761_i32 = arith.constant 17301761 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant 0.000000e+00 : f32
    %c40 = arith.constant 40 : index
    %c1024 = arith.constant 1024 : index
    %c262144_i32 = arith.constant 262144 : i32
    %c163840_i32 = arith.constant 163840 : i32
    %c128_i32 = arith.constant 128 : i32
    %c524545_i32 = arith.constant 524545 : i32
    %3 = torch_c.from_builtin_tensor %arg0 : tensor<1x4x128x128xf16> -> !torch.vtensor<[1,4,128,128],f16>
    %4 = torch_c.from_builtin_tensor %arg1 : tensor<2x64x2048xf16> -> !torch.vtensor<[2,64,2048],f16>
    %5 = torch_c.from_builtin_tensor %arg2 : tensor<2x1280xf16> -> !torch.vtensor<[2,1280],f16>
    %6 = torch_c.from_builtin_tensor %arg3 : tensor<2x6xf16> -> !torch.vtensor<[2,6],f16>
    %7 = torch_c.from_builtin_tensor %arg4 : tensor<1xf16> -> !torch.vtensor<[1],f16>
    %8 = torch_c.from_builtin_tensor %arg5 : tensor<1xi64> -> !torch.vtensor<[1],si64>
    %9 = torch.prim.ListConstruct %8 : (!torch.vtensor<[1],si64>) -> !torch.list<optional<vtensor>>
    %10 = torch.aten.index.Tensor %0, %9 : !torch.vtensor<[30],si64>, !torch.list<optional<vtensor>> -> !torch.vtensor<[1],si64>
    %11 = torch.prim.ListConstruct %3, %3 : (!torch.vtensor<[1,4,128,128],f16>, !torch.vtensor<[1,4,128,128],f16>) -> !torch.list<vtensor>
    %12 = torch.aten.cat %11, %int0 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,4,128,128],f16>
    %13 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %14 = torch.aten.expand %10, %13, %false : !torch.vtensor<[1],si64>, !torch.list<int>, !torch.bool -> !torch.vtensor<[2],si64>
    %cpu = torch.constant.device "cpu"
    %15 = torch.aten.arange.start %int0, %int160, %int6, %none, %cpu, %false : !torch.int, !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[160],f32>
    %16 = torch.aten.mul.Scalar %15, %float-9.210340e00 : !torch.vtensor<[160],f32>, !torch.float -> !torch.vtensor<[160],f32>
    %17 = torch.aten.div.Scalar %16, %int160 : !torch.vtensor<[160],f32>, !torch.int -> !torch.vtensor<[160],f32>
    %18 = torch.aten.exp %17 : !torch.vtensor<[160],f32> -> !torch.vtensor<[160],f32>
    %19 = torch.aten.unsqueeze %14, %int1 : !torch.vtensor<[2],si64>, !torch.int -> !torch.vtensor<[2,1],si64>
    %20 = torch.prims.convert_element_type %19, %int6 : !torch.vtensor<[2,1],si64>, !torch.int -> !torch.vtensor<[2,1],f32>
    %21 = torch.aten.unsqueeze %18, %int0 : !torch.vtensor<[160],f32>, !torch.int -> !torch.vtensor<[1,160],f32>
    %22 = torch.aten.mul.Tensor %20, %21 : !torch.vtensor<[2,1],f32>, !torch.vtensor<[1,160],f32> -> !torch.vtensor<[2,160],f32>
    %23 = torch.aten.mul.Scalar %22, %int1 : !torch.vtensor<[2,160],f32>, !torch.int -> !torch.vtensor<[2,160],f32>
    %24 = torch.aten.sin %23 : !torch.vtensor<[2,160],f32> -> !torch.vtensor<[2,160],f32>
    %25 = torch.aten.cos %23 : !torch.vtensor<[2,160],f32> -> !torch.vtensor<[2,160],f32>
    %26 = torch.prim.ListConstruct %24, %25 : (!torch.vtensor<[2,160],f32>, !torch.vtensor<[2,160],f32>) -> !torch.list<vtensor>
    %27 = torch.aten.cat %26, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,320],f32>
    %28 = torch.aten.slice.Tensor %27, %int1, %int160, %int9223372036854775807, %int1 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,160],f32>
    %29 = torch.aten.slice.Tensor %27, %int1, %int0, %int160, %int1 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,160],f32>
    %30 = torch.prim.ListConstruct %28, %29 : (!torch.vtensor<[2,160],f32>, !torch.vtensor<[2,160],f32>) -> !torch.list<vtensor>
    %31 = torch.aten.cat %30, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,320],f32>
    %32 = torch.prims.convert_element_type %31, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %_params.unet.time_embedding.linear_1.weight = util.global.load @_params.unet.time_embedding.linear_1.weight : tensor<1280x320xf16>
    %33 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_1.weight : tensor<1280x320xf16> -> !torch.vtensor<[1280,320],f16>
    %34 = torch.aten.transpose.int %33, %int0, %int1 : !torch.vtensor<[1280,320],f16>, !torch.int, !torch.int -> !torch.vtensor<[320,1280],f16>
    %_params.unet.time_embedding.linear_1.bias = util.global.load @_params.unet.time_embedding.linear_1.bias : tensor<1280xf16>
    %35 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %36 = torch.prims.convert_element_type %35, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %37 = torch.prims.convert_element_type %32, %int6 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320],f32>
    %38 = torch.prims.convert_element_type %34, %int6 : !torch.vtensor<[320,1280],f16>, !torch.int -> !torch.vtensor<[320,1280],f32>
    %39 = torch.aten.mm %37, %38 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %40 = torch.aten.mul.Scalar %39, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %41 = torch.aten.mul.Scalar %36, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %42 = torch.aten.add.Tensor %40, %41, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %43 = torch.prims.convert_element_type %42, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %44 = torch.aten.silu %43 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.time_embedding.linear_2.weight = util.global.load @_params.unet.time_embedding.linear_2.weight : tensor<1280x1280xf16>
    %45 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_2.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %46 = torch.aten.transpose.int %45, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.time_embedding.linear_2.bias = util.global.load @_params.unet.time_embedding.linear_2.bias : tensor<1280xf16>
    %47 = torch_c.from_builtin_tensor %_params.unet.time_embedding.linear_2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %48 = torch.prims.convert_element_type %47, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %49 = torch.prims.convert_element_type %44, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %50 = torch.prims.convert_element_type %46, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %51 = torch.aten.mm %49, %50 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %52 = torch.aten.mul.Scalar %51, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %53 = torch.aten.mul.Scalar %48, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %54 = torch.aten.add.Tensor %52, %53, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %55 = torch.prims.convert_element_type %54, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %56 = torch.prim.ListConstruct %int12 : (!torch.int) -> !torch.list<int>
    %57 = torch.aten.view %6, %56 : !torch.vtensor<[2,6],f16>, !torch.list<int> -> !torch.vtensor<[12],f16>
    %cpu_0 = torch.constant.device "cpu"
    %58 = torch.aten.arange.start %int0, %int128, %int6, %none, %cpu_0, %false : !torch.int, !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],f32>
    %59 = torch.aten.mul.Scalar %58, %float-9.210340e00 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %60 = torch.aten.div.Scalar %59, %int128 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %61 = torch.aten.exp %60 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %62 = torch.aten.unsqueeze %57, %int1 : !torch.vtensor<[12],f16>, !torch.int -> !torch.vtensor<[12,1],f16>
    %63 = torch.prims.convert_element_type %62, %int6 : !torch.vtensor<[12,1],f16>, !torch.int -> !torch.vtensor<[12,1],f32>
    %64 = torch.aten.unsqueeze %61, %int0 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %65 = torch.aten.mul.Tensor %63, %64 : !torch.vtensor<[12,1],f32>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[12,128],f32>
    %66 = torch.aten.mul.Scalar %65, %int1 : !torch.vtensor<[12,128],f32>, !torch.int -> !torch.vtensor<[12,128],f32>
    %67 = torch.aten.sin %66 : !torch.vtensor<[12,128],f32> -> !torch.vtensor<[12,128],f32>
    %68 = torch.aten.cos %66 : !torch.vtensor<[12,128],f32> -> !torch.vtensor<[12,128],f32>
    %69 = torch.prim.ListConstruct %67, %68 : (!torch.vtensor<[12,128],f32>, !torch.vtensor<[12,128],f32>) -> !torch.list<vtensor>
    %70 = torch.aten.cat %69, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[12,256],f32>
    %71 = torch.aten.slice.Tensor %70, %int1, %int128, %int9223372036854775807, %int1 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,128],f32>
    %72 = torch.aten.slice.Tensor %70, %int1, %int0, %int128, %int1 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,128],f32>
    %73 = torch.prim.ListConstruct %71, %72 : (!torch.vtensor<[12,128],f32>, !torch.vtensor<[12,128],f32>) -> !torch.list<vtensor>
    %74 = torch.aten.cat %73, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[12,256],f32>
    %75 = torch.prim.ListConstruct %int2, %int-1 : (!torch.int, !torch.int) -> !torch.list<int>
    %76 = torch.aten.view %74, %75 : !torch.vtensor<[12,256],f32>, !torch.list<int> -> !torch.vtensor<[2,1536],f32>
    %77 = torch.prim.ListConstruct %5, %76 : (!torch.vtensor<[2,1280],f16>, !torch.vtensor<[2,1536],f32>) -> !torch.list<vtensor>
    %78 = torch.aten.cat %77, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,2816],f32>
    %79 = torch.prims.convert_element_type %78, %int5 : !torch.vtensor<[2,2816],f32>, !torch.int -> !torch.vtensor<[2,2816],f16>
    %_params.unet.add_embedding.linear_1.weight = util.global.load @_params.unet.add_embedding.linear_1.weight : tensor<1280x2816xf16>
    %80 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_1.weight : tensor<1280x2816xf16> -> !torch.vtensor<[1280,2816],f16>
    %81 = torch.aten.transpose.int %80, %int0, %int1 : !torch.vtensor<[1280,2816],f16>, !torch.int, !torch.int -> !torch.vtensor<[2816,1280],f16>
    %_params.unet.add_embedding.linear_1.bias = util.global.load @_params.unet.add_embedding.linear_1.bias : tensor<1280xf16>
    %82 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %83 = torch.prims.convert_element_type %82, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %84 = torch.prims.convert_element_type %79, %int6 : !torch.vtensor<[2,2816],f16>, !torch.int -> !torch.vtensor<[2,2816],f32>
    %85 = torch.prims.convert_element_type %81, %int6 : !torch.vtensor<[2816,1280],f16>, !torch.int -> !torch.vtensor<[2816,1280],f32>
    %86 = torch.aten.mm %84, %85 : !torch.vtensor<[2,2816],f32>, !torch.vtensor<[2816,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %87 = torch.aten.mul.Scalar %86, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %88 = torch.aten.mul.Scalar %83, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %89 = torch.aten.add.Tensor %87, %88, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %90 = torch.prims.convert_element_type %89, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %91 = torch.aten.silu %90 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.add_embedding.linear_2.weight = util.global.load @_params.unet.add_embedding.linear_2.weight : tensor<1280x1280xf16>
    %92 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_2.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %93 = torch.aten.transpose.int %92, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.add_embedding.linear_2.bias = util.global.load @_params.unet.add_embedding.linear_2.bias : tensor<1280xf16>
    %94 = torch_c.from_builtin_tensor %_params.unet.add_embedding.linear_2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %95 = torch.prims.convert_element_type %94, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %96 = torch.prims.convert_element_type %91, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %97 = torch.prims.convert_element_type %93, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %98 = torch.aten.mm %96, %97 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %99 = torch.aten.mul.Scalar %98, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %100 = torch.aten.mul.Scalar %95, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %101 = torch.aten.add.Tensor %99, %100, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %102 = torch.prims.convert_element_type %101, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %103 = torch.aten.add.Tensor %55, %102, %int1 : !torch.vtensor<[2,1280],f16>, !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %_params.unet.conv_in.weight = util.global.load @_params.unet.conv_in.weight : tensor<320x4x3x3xf16>
    %104 = torch_c.from_builtin_tensor %_params.unet.conv_in.weight : tensor<320x4x3x3xf16> -> !torch.vtensor<[320,4,3,3],f16>
    %_params.unet.conv_in.bias = util.global.load @_params.unet.conv_in.bias : tensor<320xf16>
    %105 = torch_c.from_builtin_tensor %_params.unet.conv_in.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %106 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %107 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %108 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %109 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %110 = torch.aten.convolution %12, %104, %105, %106, %107, %108, %false, %109, %int1 : !torch.vtensor<[2,4,128,128],f16>, !torch.vtensor<[320,4,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %111 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %112 = torch.aten.view %110, %111 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %113 = torch.prims.convert_element_type %112, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %114 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0, %result1 = torch.aten.var_mean.correction %113, %114, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %115 = torch.aten.add.Scalar %result0, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %116 = torch.aten.rsqrt %115 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %117 = torch.aten.sub.Tensor %112, %result1, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %118 = torch.aten.mul.Tensor %117, %116 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %119 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %120 = torch.aten.view %118, %119 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.0.norm1.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.norm1.bias : tensor<320xf16>
    %121 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %122 = torch.aten.unsqueeze %121, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %123 = torch.aten.unsqueeze %122, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %124 = torch.aten.unsqueeze %123, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.0.norm1.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.norm1.weight : tensor<320xf16>
    %125 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %126 = torch.aten.unsqueeze %125, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %127 = torch.aten.unsqueeze %126, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %128 = torch.aten.unsqueeze %127, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %129 = torch.aten.mul.Tensor %120, %128 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %130 = torch.aten.add.Tensor %129, %124, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %131 = torch.prims.convert_element_type %130, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %132 = torch.prims.convert_element_type %result1, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %133 = torch.prims.convert_element_type %116, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %134 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %135 = torch.prims.squeeze %132, %134 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %136 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %137 = torch.prims.squeeze %135, %136 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %138 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %139 = torch.prims.squeeze %133, %138 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %140 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %141 = torch.prims.squeeze %139, %140 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %142 = torch.aten.silu %131 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.0.conv1.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.conv1.weight : tensor<320x320x3x3xf16>
    %143 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv1.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.0.conv1.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.conv1.bias : tensor<320xf16>
    %144 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %145 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %146 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %147 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %148 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %149 = torch.aten.convolution %142, %143, %144, %145, %146, %147, %false, %148, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %150 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight : tensor<320x1280xf16>
    %151 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %152 = torch.aten.transpose.int %151, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias : tensor<320xf16>
    %153 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %154 = torch.prims.convert_element_type %153, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %155 = torch.prims.convert_element_type %150, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %156 = torch.prims.convert_element_type %152, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %157 = torch.aten.mm %155, %156 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %158 = torch.aten.mul.Scalar %157, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %159 = torch.aten.mul.Scalar %154, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %160 = torch.aten.add.Tensor %158, %159, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %161 = torch.prims.convert_element_type %160, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %162 = torch.aten.unsqueeze %161, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %163 = torch.aten.unsqueeze %162, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %164 = torch.aten.add.Tensor %149, %163, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %165 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %166 = torch.aten.view %164, %165 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %167 = torch.prims.convert_element_type %166, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %168 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_1, %result1_2 = torch.aten.var_mean.correction %167, %168, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %169 = torch.aten.add.Scalar %result0_1, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %170 = torch.aten.rsqrt %169 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %171 = torch.aten.sub.Tensor %166, %result1_2, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %172 = torch.aten.mul.Tensor %171, %170 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %173 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %174 = torch.aten.view %172, %173 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.0.norm2.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.norm2.bias : tensor<320xf16>
    %175 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %176 = torch.aten.unsqueeze %175, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %177 = torch.aten.unsqueeze %176, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %178 = torch.aten.unsqueeze %177, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.0.norm2.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.norm2.weight : tensor<320xf16>
    %179 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %180 = torch.aten.unsqueeze %179, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %181 = torch.aten.unsqueeze %180, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %182 = torch.aten.unsqueeze %181, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %183 = torch.aten.mul.Tensor %174, %182 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %184 = torch.aten.add.Tensor %183, %178, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %185 = torch.prims.convert_element_type %184, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %186 = torch.prims.convert_element_type %result1_2, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %187 = torch.prims.convert_element_type %170, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %188 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %189 = torch.prims.squeeze %186, %188 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %190 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %191 = torch.prims.squeeze %189, %190 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %192 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %193 = torch.prims.squeeze %187, %192 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %194 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %195 = torch.prims.squeeze %193, %194 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %196 = torch.aten.silu %185 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.0.conv2.weight = util.global.load @_params.unet.down_blocks.0.resnets.0.conv2.weight : tensor<320x320x3x3xf16>
    %197 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.0.conv2.bias = util.global.load @_params.unet.down_blocks.0.resnets.0.conv2.bias : tensor<320xf16>
    %198 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.0.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %199 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %200 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %201 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %202 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %203 = torch.aten.convolution %196, %197, %198, %199, %200, %201, %false, %202, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %204 = torch.aten.add.Tensor %110, %203, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %205 = torch.aten.div.Scalar %204, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %206 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %207 = torch.aten.view %205, %206 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %208 = torch.prims.convert_element_type %207, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %209 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_3, %result1_4 = torch.aten.var_mean.correction %208, %209, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %210 = torch.aten.add.Scalar %result0_3, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %211 = torch.aten.rsqrt %210 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %212 = torch.aten.sub.Tensor %207, %result1_4, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %213 = torch.aten.mul.Tensor %212, %211 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %214 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %215 = torch.aten.view %213, %214 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.1.norm1.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.norm1.bias : tensor<320xf16>
    %216 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %217 = torch.aten.unsqueeze %216, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %218 = torch.aten.unsqueeze %217, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %219 = torch.aten.unsqueeze %218, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.1.norm1.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.norm1.weight : tensor<320xf16>
    %220 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %221 = torch.aten.unsqueeze %220, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %222 = torch.aten.unsqueeze %221, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %223 = torch.aten.unsqueeze %222, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %224 = torch.aten.mul.Tensor %215, %223 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %225 = torch.aten.add.Tensor %224, %219, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %226 = torch.prims.convert_element_type %225, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %227 = torch.prims.convert_element_type %result1_4, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %228 = torch.prims.convert_element_type %211, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %229 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %230 = torch.prims.squeeze %227, %229 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %231 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %232 = torch.prims.squeeze %230, %231 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %233 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %234 = torch.prims.squeeze %228, %233 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %235 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %236 = torch.prims.squeeze %234, %235 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %237 = torch.aten.silu %226 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.1.conv1.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.conv1.weight : tensor<320x320x3x3xf16>
    %238 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv1.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.1.conv1.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.conv1.bias : tensor<320xf16>
    %239 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %240 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %241 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %242 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %243 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %244 = torch.aten.convolution %237, %238, %239, %240, %241, %242, %false, %243, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %245 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight : tensor<320x1280xf16>
    %246 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %247 = torch.aten.transpose.int %246, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias : tensor<320xf16>
    %248 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %249 = torch.prims.convert_element_type %248, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %250 = torch.prims.convert_element_type %245, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %251 = torch.prims.convert_element_type %247, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %252 = torch.aten.mm %250, %251 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %253 = torch.aten.mul.Scalar %252, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %254 = torch.aten.mul.Scalar %249, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %255 = torch.aten.add.Tensor %253, %254, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %256 = torch.prims.convert_element_type %255, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %257 = torch.aten.unsqueeze %256, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %258 = torch.aten.unsqueeze %257, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %259 = torch.aten.add.Tensor %244, %258, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %260 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %261 = torch.aten.view %259, %260 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %262 = torch.prims.convert_element_type %261, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %263 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_5, %result1_6 = torch.aten.var_mean.correction %262, %263, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %264 = torch.aten.add.Scalar %result0_5, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %265 = torch.aten.rsqrt %264 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %266 = torch.aten.sub.Tensor %261, %result1_6, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %267 = torch.aten.mul.Tensor %266, %265 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %268 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %269 = torch.aten.view %267, %268 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.down_blocks.0.resnets.1.norm2.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.norm2.bias : tensor<320xf16>
    %270 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %271 = torch.aten.unsqueeze %270, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %272 = torch.aten.unsqueeze %271, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %273 = torch.aten.unsqueeze %272, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.0.resnets.1.norm2.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.norm2.weight : tensor<320xf16>
    %274 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %275 = torch.aten.unsqueeze %274, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %276 = torch.aten.unsqueeze %275, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %277 = torch.aten.unsqueeze %276, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %278 = torch.aten.mul.Tensor %269, %277 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %279 = torch.aten.add.Tensor %278, %273, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %280 = torch.prims.convert_element_type %279, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %281 = torch.prims.convert_element_type %result1_6, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %282 = torch.prims.convert_element_type %265, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %283 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %284 = torch.prims.squeeze %281, %283 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %285 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %286 = torch.prims.squeeze %284, %285 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %287 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %288 = torch.prims.squeeze %282, %287 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %289 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %290 = torch.prims.squeeze %288, %289 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %291 = torch.aten.silu %280 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.resnets.1.conv2.weight = util.global.load @_params.unet.down_blocks.0.resnets.1.conv2.weight : tensor<320x320x3x3xf16>
    %292 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.resnets.1.conv2.bias = util.global.load @_params.unet.down_blocks.0.resnets.1.conv2.bias : tensor<320xf16>
    %293 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.resnets.1.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %294 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %295 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %296 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %297 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %298 = torch.aten.convolution %291, %292, %293, %294, %295, %296, %false, %297, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %299 = torch.aten.add.Tensor %205, %298, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %300 = torch.aten.div.Scalar %299, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.down_blocks.0.downsamplers.0.conv.weight = util.global.load @_params.unet.down_blocks.0.downsamplers.0.conv.weight : tensor<320x320x3x3xf16>
    %301 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.downsamplers.0.conv.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.down_blocks.0.downsamplers.0.conv.bias = util.global.load @_params.unet.down_blocks.0.downsamplers.0.conv.bias : tensor<320xf16>
    %302 = torch_c.from_builtin_tensor %_params.unet.down_blocks.0.downsamplers.0.conv.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %303 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %304 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %305 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %306 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %307 = torch.aten.convolution %300, %301, %302, %303, %304, %305, %false, %306, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,64,64],f16>
    %308 = torch.prim.ListConstruct %int2, %int32, %int10, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %309 = torch.aten.view %307, %308 : !torch.vtensor<[2,320,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,4096],f16>
    %310 = torch.prims.convert_element_type %309, %int6 : !torch.vtensor<[2,32,10,4096],f16>, !torch.int -> !torch.vtensor<[2,32,10,4096],f32>
    %311 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_7, %result1_8 = torch.aten.var_mean.correction %310, %311, %int0, %true : !torch.vtensor<[2,32,10,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %312 = torch.aten.add.Scalar %result0_7, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %313 = torch.aten.rsqrt %312 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %314 = torch.aten.sub.Tensor %309, %result1_8, %int1 : !torch.vtensor<[2,32,10,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,4096],f32>
    %315 = torch.aten.mul.Tensor %314, %313 : !torch.vtensor<[2,32,10,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,4096],f32>
    %316 = torch.prim.ListConstruct %int2, %int320, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %317 = torch.aten.view %315, %316 : !torch.vtensor<[2,32,10,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,320,64,64],f32>
    %_params.unet.down_blocks.1.resnets.0.norm1.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.norm1.bias : tensor<320xf16>
    %318 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %319 = torch.aten.unsqueeze %318, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %320 = torch.aten.unsqueeze %319, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %321 = torch.aten.unsqueeze %320, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.down_blocks.1.resnets.0.norm1.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.norm1.weight : tensor<320xf16>
    %322 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %323 = torch.aten.unsqueeze %322, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %324 = torch.aten.unsqueeze %323, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %325 = torch.aten.unsqueeze %324, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %326 = torch.aten.mul.Tensor %317, %325 : !torch.vtensor<[2,320,64,64],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,64,64],f32>
    %327 = torch.aten.add.Tensor %326, %321, %int1 : !torch.vtensor<[2,320,64,64],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,64,64],f32>
    %328 = torch.prims.convert_element_type %327, %int5 : !torch.vtensor<[2,320,64,64],f32>, !torch.int -> !torch.vtensor<[2,320,64,64],f16>
    %329 = torch.prims.convert_element_type %result1_8, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %330 = torch.prims.convert_element_type %313, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %331 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %332 = torch.prims.squeeze %329, %331 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %333 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %334 = torch.prims.squeeze %332, %333 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %335 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %336 = torch.prims.squeeze %330, %335 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %337 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %338 = torch.prims.squeeze %336, %337 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %339 = torch.aten.silu %328 : !torch.vtensor<[2,320,64,64],f16> -> !torch.vtensor<[2,320,64,64],f16>
    %_params.unet.down_blocks.1.resnets.0.conv1.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.conv1.weight : tensor<640x320x3x3xf16>
    %340 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv1.weight : tensor<640x320x3x3xf16> -> !torch.vtensor<[640,320,3,3],f16>
    %_params.unet.down_blocks.1.resnets.0.conv1.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.conv1.bias : tensor<640xf16>
    %341 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %342 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %343 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %344 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %345 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %346 = torch.aten.convolution %339, %340, %341, %342, %343, %344, %false, %345, %int1 : !torch.vtensor<[2,320,64,64],f16>, !torch.vtensor<[640,320,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %347 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16>
    %348 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %349 = torch.aten.transpose.int %348, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16>
    %350 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %351 = torch.prims.convert_element_type %350, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %352 = torch.prims.convert_element_type %347, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %353 = torch.prims.convert_element_type %349, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %354 = torch.aten.mm %352, %353 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %355 = torch.aten.mul.Scalar %354, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %356 = torch.aten.mul.Scalar %351, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %357 = torch.aten.add.Tensor %355, %356, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %358 = torch.prims.convert_element_type %357, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %359 = torch.aten.unsqueeze %358, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %360 = torch.aten.unsqueeze %359, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %361 = torch.aten.add.Tensor %346, %360, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %362 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %363 = torch.aten.view %361, %362 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %364 = torch.prims.convert_element_type %363, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %365 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_9, %result1_10 = torch.aten.var_mean.correction %364, %365, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %366 = torch.aten.add.Scalar %result0_9, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %367 = torch.aten.rsqrt %366 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %368 = torch.aten.sub.Tensor %363, %result1_10, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %369 = torch.aten.mul.Tensor %368, %367 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %370 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %371 = torch.aten.view %369, %370 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.resnets.0.norm2.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.norm2.bias : tensor<640xf16>
    %372 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %373 = torch.aten.unsqueeze %372, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %374 = torch.aten.unsqueeze %373, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %375 = torch.aten.unsqueeze %374, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.resnets.0.norm2.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.norm2.weight : tensor<640xf16>
    %376 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %377 = torch.aten.unsqueeze %376, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %378 = torch.aten.unsqueeze %377, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %379 = torch.aten.unsqueeze %378, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %380 = torch.aten.mul.Tensor %371, %379 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %381 = torch.aten.add.Tensor %380, %375, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %382 = torch.prims.convert_element_type %381, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %383 = torch.prims.convert_element_type %result1_10, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %384 = torch.prims.convert_element_type %367, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %385 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %386 = torch.prims.squeeze %383, %385 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %387 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %388 = torch.prims.squeeze %386, %387 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %389 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %390 = torch.prims.squeeze %384, %389 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %391 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %392 = torch.prims.squeeze %390, %391 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %393 = torch.aten.silu %382 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.0.conv2.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16>
    %394 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.resnets.0.conv2.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.conv2.bias : tensor<640xf16>
    %395 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %396 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %397 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %398 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %399 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %400 = torch.aten.convolution %393, %394, %395, %396, %397, %398, %false, %399, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x320x1x1xf16>
    %401 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x320x1x1xf16> -> !torch.vtensor<[640,320,1,1],f16>
    %_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16>
    %402 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %403 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %404 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %405 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %406 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %407 = torch.aten.convolution %307, %401, %402, %403, %404, %405, %false, %406, %int1 : !torch.vtensor<[2,320,64,64],f16>, !torch.vtensor<[640,320,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %408 = torch.aten.add.Tensor %407, %400, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %409 = torch.aten.div.Scalar %408, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %410 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %411 = torch.aten.view %409, %410 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %412 = torch.prims.convert_element_type %411, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %413 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_11, %result1_12 = torch.aten.var_mean.correction %412, %413, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %414 = torch.aten.add.Scalar %result0_11, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %415 = torch.aten.rsqrt %414 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %416 = torch.aten.sub.Tensor %411, %result1_12, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %417 = torch.aten.mul.Tensor %416, %415 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %418 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %419 = torch.aten.view %417, %418 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.attentions.0.norm.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.norm.bias : tensor<640xf16>
    %420 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %421 = torch.aten.unsqueeze %420, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %422 = torch.aten.unsqueeze %421, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %423 = torch.aten.unsqueeze %422, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.attentions.0.norm.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.norm.weight : tensor<640xf16>
    %424 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %425 = torch.aten.unsqueeze %424, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %426 = torch.aten.unsqueeze %425, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %427 = torch.aten.unsqueeze %426, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %428 = torch.aten.mul.Tensor %419, %427 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %429 = torch.aten.add.Tensor %428, %423, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %430 = torch.prims.convert_element_type %429, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %431 = torch.prims.convert_element_type %result1_12, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %432 = torch.prims.convert_element_type %415, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %433 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %434 = torch.prims.squeeze %431, %433 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %435 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %436 = torch.prims.squeeze %434, %435 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %437 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %438 = torch.prims.squeeze %432, %437 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %439 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %440 = torch.prims.squeeze %438, %439 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %441 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %442 = torch.aten.permute %430, %441 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %443 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %444 = torch.aten.view %442, %443 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_in.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16>
    %445 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %446 = torch.aten.transpose.int %445, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %447 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %448 = torch.aten._unsafe_view %444, %447 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %449 = torch.aten.mm %448, %446 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %450 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %451 = torch.aten.view %449, %450 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_in.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_in.bias : tensor<640xf16>
    %452 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %453 = torch.aten.add.Tensor %451, %452, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %454 = torch.prims.convert_element_type %453, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %455 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_13, %result1_14 = torch.aten.var_mean.correction %454, %455, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %456 = torch.aten.add.Scalar %result0_13, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %457 = torch.aten.rsqrt %456 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %458 = torch.aten.sub.Tensor %453, %result1_14, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %459 = torch.aten.mul.Tensor %458, %457 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %460 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %461 = torch.aten.mul.Tensor %459, %460 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %462 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %463 = torch.aten.add.Tensor %461, %462, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %464 = torch.prims.convert_element_type %463, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %465 = torch.prims.convert_element_type %result1_14, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %466 = torch.prims.convert_element_type %457, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %467 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %468 = torch.aten.transpose.int %467, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %469 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %470 = torch.aten.view %464, %469 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %471 = torch.aten.mm %470, %468 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %472 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %473 = torch.aten.view %471, %472 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %474 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %475 = torch.aten.transpose.int %474, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %476 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %477 = torch.aten.view %464, %476 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %478 = torch.aten.mm %477, %475 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %479 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %480 = torch.aten.view %478, %479 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %481 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %482 = torch.aten.transpose.int %481, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %483 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %484 = torch.aten.view %464, %483 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %485 = torch.aten.mm %484, %482 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %486 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %487 = torch.aten.view %485, %486 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %488 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %489 = torch.aten.view %473, %488 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %490 = torch.aten.transpose.int %489, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %491 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %492 = torch.aten.view %480, %491 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %493 = torch.aten.transpose.int %492, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %494 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %495 = torch.aten.view %487, %494 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %496 = torch.aten.transpose.int %495, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %497:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%490, %493, %496, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %498 = torch.aten.transpose.int %497#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %499 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %500 = torch.aten.view %498, %499 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %501 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %502 = torch.aten.view %500, %501 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %503 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %504 = torch.aten.transpose.int %503, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %505 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %506 = torch.prims.convert_element_type %505, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %507 = torch.prims.convert_element_type %502, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %508 = torch.prims.convert_element_type %504, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %509 = torch.aten.mm %507, %508 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %510 = torch.aten.mul.Scalar %509, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %511 = torch.aten.mul.Scalar %506, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %512 = torch.aten.add.Tensor %510, %511, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %513 = torch.prims.convert_element_type %512, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %514 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %515 = torch.aten.view %513, %514 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %516 = torch.aten.div.Scalar %515, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %517 = torch.aten.add.Tensor %516, %453, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %518 = torch.prims.convert_element_type %517, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %519 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_15, %result1_16 = torch.aten.var_mean.correction %518, %519, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %520 = torch.aten.add.Scalar %result0_15, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %521 = torch.aten.rsqrt %520 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %522 = torch.aten.sub.Tensor %517, %result1_16, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %523 = torch.aten.mul.Tensor %522, %521 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %524 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %525 = torch.aten.mul.Tensor %523, %524 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %526 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %527 = torch.aten.add.Tensor %525, %526, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %528 = torch.prims.convert_element_type %527, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %529 = torch.prims.convert_element_type %result1_16, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %530 = torch.prims.convert_element_type %521, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %531 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %532 = torch.aten.transpose.int %531, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %533 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %534 = torch.aten.view %528, %533 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %535 = torch.aten.mm %534, %532 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %536 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %537 = torch.aten.view %535, %536 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %538 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %539 = torch.aten.transpose.int %538, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %540 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %541 = torch.aten.view %4, %540 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %542 = torch.aten.mm %541, %539 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %543 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %544 = torch.aten.view %542, %543 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %545 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %546 = torch.aten.transpose.int %545, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %547 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %548 = torch.aten.view %4, %547 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %549 = torch.aten.mm %548, %546 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %550 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %551 = torch.aten.view %549, %550 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %552 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %553 = torch.aten.view %537, %552 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %554 = torch.aten.transpose.int %553, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %555 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %556 = torch.aten.view %544, %555 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %557 = torch.aten.transpose.int %556, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %558 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %559 = torch.aten.view %551, %558 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %560 = torch.aten.transpose.int %559, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %561:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%554, %557, %560, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %562 = torch.aten.transpose.int %561#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %563 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %564 = torch.aten.view %562, %563 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %565 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %566 = torch.aten.view %564, %565 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %567 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %568 = torch.aten.transpose.int %567, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %569 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %570 = torch.prims.convert_element_type %569, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %571 = torch.prims.convert_element_type %566, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %572 = torch.prims.convert_element_type %568, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %573 = torch.aten.mm %571, %572 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %574 = torch.aten.mul.Scalar %573, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %575 = torch.aten.mul.Scalar %570, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %576 = torch.aten.add.Tensor %574, %575, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %577 = torch.prims.convert_element_type %576, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %578 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %579 = torch.aten.view %577, %578 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %580 = torch.aten.div.Scalar %579, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %581 = torch.aten.add.Tensor %580, %517, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %582 = torch.prims.convert_element_type %581, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %583 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_17, %result1_18 = torch.aten.var_mean.correction %582, %583, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %584 = torch.aten.add.Scalar %result0_17, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %585 = torch.aten.rsqrt %584 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %586 = torch.aten.sub.Tensor %581, %result1_18, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %587 = torch.aten.mul.Tensor %586, %585 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %588 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %589 = torch.aten.mul.Tensor %587, %588 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %590 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %591 = torch.aten.add.Tensor %589, %590, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %592 = torch.prims.convert_element_type %591, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %593 = torch.prims.convert_element_type %result1_18, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %594 = torch.prims.convert_element_type %585, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %595 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %596 = torch.aten.view %592, %595 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %597 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %598 = torch.aten.transpose.int %597, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %599 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %600 = torch.prims.convert_element_type %599, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %601 = torch.prims.convert_element_type %596, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %602 = torch.prims.convert_element_type %598, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %603 = torch.aten.mm %601, %602 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %604 = torch.aten.mul.Scalar %603, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %605 = torch.aten.mul.Scalar %600, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %606 = torch.aten.add.Tensor %604, %605, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %607 = torch.prims.convert_element_type %606, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %608 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %609 = torch.aten.view %607, %608 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %610 = torch.aten.slice.Tensor %609, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %611 = torch.aten.slice.Tensor %609, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %612 = torch.aten.gelu %611, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %613 = torch.aten.mul.Tensor %610, %612 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %614 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %615 = torch.aten.view %613, %614 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %616 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %617 = torch.aten.transpose.int %616, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %618 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %619 = torch.prims.convert_element_type %618, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %620 = torch.prims.convert_element_type %615, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %621 = torch.prims.convert_element_type %617, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %622 = torch.aten.mm %620, %621 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %623 = torch.aten.mul.Scalar %622, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %624 = torch.aten.mul.Scalar %619, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %625 = torch.aten.add.Tensor %623, %624, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %626 = torch.prims.convert_element_type %625, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %627 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %628 = torch.aten.view %626, %627 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %629 = torch.aten.add.Tensor %628, %581, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %630 = torch.prims.convert_element_type %629, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %631 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_19, %result1_20 = torch.aten.var_mean.correction %630, %631, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %632 = torch.aten.add.Scalar %result0_19, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %633 = torch.aten.rsqrt %632 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %634 = torch.aten.sub.Tensor %629, %result1_20, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %635 = torch.aten.mul.Tensor %634, %633 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %636 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %637 = torch.aten.mul.Tensor %635, %636 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %638 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %639 = torch.aten.add.Tensor %637, %638, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %640 = torch.prims.convert_element_type %639, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %641 = torch.prims.convert_element_type %result1_20, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %642 = torch.prims.convert_element_type %633, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %643 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %644 = torch.aten.transpose.int %643, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %645 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %646 = torch.aten.view %640, %645 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %647 = torch.aten.mm %646, %644 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %648 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %649 = torch.aten.view %647, %648 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %650 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %651 = torch.aten.transpose.int %650, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %652 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %653 = torch.aten.view %640, %652 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %654 = torch.aten.mm %653, %651 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %655 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %656 = torch.aten.view %654, %655 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %657 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %658 = torch.aten.transpose.int %657, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %659 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %660 = torch.aten.view %640, %659 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %661 = torch.aten.mm %660, %658 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %662 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %663 = torch.aten.view %661, %662 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %664 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %665 = torch.aten.view %649, %664 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %666 = torch.aten.transpose.int %665, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %667 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %668 = torch.aten.view %656, %667 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %669 = torch.aten.transpose.int %668, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %670 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %671 = torch.aten.view %663, %670 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %672 = torch.aten.transpose.int %671, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %673:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%666, %669, %672, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %674 = torch.aten.transpose.int %673#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %675 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %676 = torch.aten.view %674, %675 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %677 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %678 = torch.aten.view %676, %677 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %679 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %680 = torch.aten.transpose.int %679, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %681 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %682 = torch.prims.convert_element_type %681, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %683 = torch.prims.convert_element_type %678, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %684 = torch.prims.convert_element_type %680, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %685 = torch.aten.mm %683, %684 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %686 = torch.aten.mul.Scalar %685, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %687 = torch.aten.mul.Scalar %682, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %688 = torch.aten.add.Tensor %686, %687, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %689 = torch.prims.convert_element_type %688, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %690 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %691 = torch.aten.view %689, %690 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %692 = torch.aten.div.Scalar %691, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %693 = torch.aten.add.Tensor %692, %629, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %694 = torch.prims.convert_element_type %693, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %695 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_21, %result1_22 = torch.aten.var_mean.correction %694, %695, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %696 = torch.aten.add.Scalar %result0_21, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %697 = torch.aten.rsqrt %696 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %698 = torch.aten.sub.Tensor %693, %result1_22, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %699 = torch.aten.mul.Tensor %698, %697 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %700 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %701 = torch.aten.mul.Tensor %699, %700 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %702 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %703 = torch.aten.add.Tensor %701, %702, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %704 = torch.prims.convert_element_type %703, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %705 = torch.prims.convert_element_type %result1_22, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %706 = torch.prims.convert_element_type %697, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %707 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %708 = torch.aten.transpose.int %707, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %709 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %710 = torch.aten.view %704, %709 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %711 = torch.aten.mm %710, %708 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %712 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %713 = torch.aten.view %711, %712 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %714 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %715 = torch.aten.transpose.int %714, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %716 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %717 = torch.aten.view %4, %716 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %718 = torch.aten.mm %717, %715 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %719 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %720 = torch.aten.view %718, %719 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %721 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %722 = torch.aten.transpose.int %721, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %723 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %724 = torch.aten.view %4, %723 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %725 = torch.aten.mm %724, %722 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %726 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %727 = torch.aten.view %725, %726 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %728 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %729 = torch.aten.view %713, %728 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %730 = torch.aten.transpose.int %729, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %731 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %732 = torch.aten.view %720, %731 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %733 = torch.aten.transpose.int %732, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %734 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %735 = torch.aten.view %727, %734 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %736 = torch.aten.transpose.int %735, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %737:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%730, %733, %736, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %738 = torch.aten.transpose.int %737#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %739 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %740 = torch.aten.view %738, %739 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %741 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %742 = torch.aten.view %740, %741 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %743 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %744 = torch.aten.transpose.int %743, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %745 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %746 = torch.prims.convert_element_type %745, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %747 = torch.prims.convert_element_type %742, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %748 = torch.prims.convert_element_type %744, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %749 = torch.aten.mm %747, %748 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %750 = torch.aten.mul.Scalar %749, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %751 = torch.aten.mul.Scalar %746, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %752 = torch.aten.add.Tensor %750, %751, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %753 = torch.prims.convert_element_type %752, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %754 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %755 = torch.aten.view %753, %754 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %756 = torch.aten.div.Scalar %755, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %757 = torch.aten.add.Tensor %756, %693, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %758 = torch.prims.convert_element_type %757, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %759 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_23, %result1_24 = torch.aten.var_mean.correction %758, %759, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %760 = torch.aten.add.Scalar %result0_23, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %761 = torch.aten.rsqrt %760 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %762 = torch.aten.sub.Tensor %757, %result1_24, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %763 = torch.aten.mul.Tensor %762, %761 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %764 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %765 = torch.aten.mul.Tensor %763, %764 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %766 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %767 = torch.aten.add.Tensor %765, %766, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %768 = torch.prims.convert_element_type %767, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %769 = torch.prims.convert_element_type %result1_24, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %770 = torch.prims.convert_element_type %761, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %771 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %772 = torch.aten.view %768, %771 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %773 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %774 = torch.aten.transpose.int %773, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %775 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %776 = torch.prims.convert_element_type %775, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %777 = torch.prims.convert_element_type %772, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %778 = torch.prims.convert_element_type %774, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %779 = torch.aten.mm %777, %778 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %780 = torch.aten.mul.Scalar %779, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %781 = torch.aten.mul.Scalar %776, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %782 = torch.aten.add.Tensor %780, %781, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %783 = torch.prims.convert_element_type %782, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %784 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %785 = torch.aten.view %783, %784 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %786 = torch.aten.slice.Tensor %785, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %787 = torch.aten.slice.Tensor %785, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %788 = torch.aten.gelu %787, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %789 = torch.aten.mul.Tensor %786, %788 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %790 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %791 = torch.aten.view %789, %790 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %792 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %793 = torch.aten.transpose.int %792, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %794 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %795 = torch.prims.convert_element_type %794, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %796 = torch.prims.convert_element_type %791, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %797 = torch.prims.convert_element_type %793, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %798 = torch.aten.mm %796, %797 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %799 = torch.aten.mul.Scalar %798, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %800 = torch.aten.mul.Scalar %795, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %801 = torch.aten.add.Tensor %799, %800, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %802 = torch.prims.convert_element_type %801, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %803 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %804 = torch.aten.view %802, %803 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %805 = torch.aten.add.Tensor %804, %757, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %806 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %807 = torch.aten.view %805, %806 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_out.weight = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16>
    %808 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %809 = torch.aten.transpose.int %808, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.0.proj_out.bias = util.global.load @_params.unet.down_blocks.1.attentions.0.proj_out.bias : tensor<640xf16>
    %810 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.0.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %811 = torch.prims.convert_element_type %810, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %812 = torch.prims.convert_element_type %807, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %813 = torch.prims.convert_element_type %809, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %814 = torch.aten.mm %812, %813 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %815 = torch.aten.mul.Scalar %814, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %816 = torch.aten.mul.Scalar %811, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %817 = torch.aten.add.Tensor %815, %816, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %818 = torch.prims.convert_element_type %817, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %819 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %820 = torch.aten.view %818, %819 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %821 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %822 = torch.aten.view %820, %821 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %823 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %824 = torch.aten.permute %822, %823 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %825 = torch.aten.add.Tensor %824, %409, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %826 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %827 = torch.aten.view %825, %826 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %828 = torch.prims.convert_element_type %827, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %829 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_25, %result1_26 = torch.aten.var_mean.correction %828, %829, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %830 = torch.aten.add.Scalar %result0_25, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %831 = torch.aten.rsqrt %830 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %832 = torch.aten.sub.Tensor %827, %result1_26, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %833 = torch.aten.mul.Tensor %832, %831 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %834 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %835 = torch.aten.view %833, %834 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.resnets.1.norm1.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.norm1.bias : tensor<640xf16>
    %836 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %837 = torch.aten.unsqueeze %836, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %838 = torch.aten.unsqueeze %837, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %839 = torch.aten.unsqueeze %838, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.resnets.1.norm1.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.norm1.weight : tensor<640xf16>
    %840 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %841 = torch.aten.unsqueeze %840, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %842 = torch.aten.unsqueeze %841, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %843 = torch.aten.unsqueeze %842, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %844 = torch.aten.mul.Tensor %835, %843 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %845 = torch.aten.add.Tensor %844, %839, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %846 = torch.prims.convert_element_type %845, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %847 = torch.prims.convert_element_type %result1_26, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %848 = torch.prims.convert_element_type %831, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %849 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %850 = torch.prims.squeeze %847, %849 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %851 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %852 = torch.prims.squeeze %850, %851 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %853 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %854 = torch.prims.squeeze %848, %853 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %855 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %856 = torch.prims.squeeze %854, %855 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %857 = torch.aten.silu %846 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.1.conv1.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.conv1.weight : tensor<640x640x3x3xf16>
    %858 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv1.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.resnets.1.conv1.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.conv1.bias : tensor<640xf16>
    %859 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %860 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %861 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %862 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %863 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %864 = torch.aten.convolution %857, %858, %859, %860, %861, %862, %false, %863, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %865 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16>
    %866 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %867 = torch.aten.transpose.int %866, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16>
    %868 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %869 = torch.prims.convert_element_type %868, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %870 = torch.prims.convert_element_type %865, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %871 = torch.prims.convert_element_type %867, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %872 = torch.aten.mm %870, %871 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %873 = torch.aten.mul.Scalar %872, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %874 = torch.aten.mul.Scalar %869, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %875 = torch.aten.add.Tensor %873, %874, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %876 = torch.prims.convert_element_type %875, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %877 = torch.aten.unsqueeze %876, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %878 = torch.aten.unsqueeze %877, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %879 = torch.aten.add.Tensor %864, %878, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %880 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %881 = torch.aten.view %879, %880 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %882 = torch.prims.convert_element_type %881, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %883 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_27, %result1_28 = torch.aten.var_mean.correction %882, %883, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %884 = torch.aten.add.Scalar %result0_27, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %885 = torch.aten.rsqrt %884 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %886 = torch.aten.sub.Tensor %881, %result1_28, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %887 = torch.aten.mul.Tensor %886, %885 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %888 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %889 = torch.aten.view %887, %888 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.resnets.1.norm2.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.norm2.bias : tensor<640xf16>
    %890 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %891 = torch.aten.unsqueeze %890, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %892 = torch.aten.unsqueeze %891, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %893 = torch.aten.unsqueeze %892, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.resnets.1.norm2.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.norm2.weight : tensor<640xf16>
    %894 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %895 = torch.aten.unsqueeze %894, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %896 = torch.aten.unsqueeze %895, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %897 = torch.aten.unsqueeze %896, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %898 = torch.aten.mul.Tensor %889, %897 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %899 = torch.aten.add.Tensor %898, %893, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %900 = torch.prims.convert_element_type %899, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %901 = torch.prims.convert_element_type %result1_28, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %902 = torch.prims.convert_element_type %885, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %903 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %904 = torch.prims.squeeze %901, %903 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %905 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %906 = torch.prims.squeeze %904, %905 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %907 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %908 = torch.prims.squeeze %902, %907 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %909 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %910 = torch.prims.squeeze %908, %909 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %911 = torch.aten.silu %900 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.resnets.1.conv2.weight = util.global.load @_params.unet.down_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16>
    %912 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.resnets.1.conv2.bias = util.global.load @_params.unet.down_blocks.1.resnets.1.conv2.bias : tensor<640xf16>
    %913 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.resnets.1.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %914 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %915 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %916 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %917 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %918 = torch.aten.convolution %911, %912, %913, %914, %915, %916, %false, %917, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %919 = torch.aten.add.Tensor %825, %918, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %920 = torch.aten.div.Scalar %919, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %921 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %922 = torch.aten.view %920, %921 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %923 = torch.prims.convert_element_type %922, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %924 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_29, %result1_30 = torch.aten.var_mean.correction %923, %924, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %925 = torch.aten.add.Scalar %result0_29, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %926 = torch.aten.rsqrt %925 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %927 = torch.aten.sub.Tensor %922, %result1_30, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %928 = torch.aten.mul.Tensor %927, %926 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %929 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %930 = torch.aten.view %928, %929 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.down_blocks.1.attentions.1.norm.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.norm.bias : tensor<640xf16>
    %931 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %932 = torch.aten.unsqueeze %931, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %933 = torch.aten.unsqueeze %932, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %934 = torch.aten.unsqueeze %933, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.1.attentions.1.norm.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.norm.weight : tensor<640xf16>
    %935 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %936 = torch.aten.unsqueeze %935, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %937 = torch.aten.unsqueeze %936, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %938 = torch.aten.unsqueeze %937, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %939 = torch.aten.mul.Tensor %930, %938 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %940 = torch.aten.add.Tensor %939, %934, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %941 = torch.prims.convert_element_type %940, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %942 = torch.prims.convert_element_type %result1_30, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %943 = torch.prims.convert_element_type %926, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %944 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %945 = torch.prims.squeeze %942, %944 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %946 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %947 = torch.prims.squeeze %945, %946 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %948 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %949 = torch.prims.squeeze %943, %948 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %950 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %951 = torch.prims.squeeze %949, %950 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %952 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %953 = torch.aten.permute %941, %952 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %954 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %955 = torch.aten.view %953, %954 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_in.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16>
    %956 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %957 = torch.aten.transpose.int %956, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %958 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %959 = torch.aten._unsafe_view %955, %958 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %960 = torch.aten.mm %959, %957 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %961 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %962 = torch.aten.view %960, %961 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_in.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_in.bias : tensor<640xf16>
    %963 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %964 = torch.aten.add.Tensor %962, %963, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %965 = torch.prims.convert_element_type %964, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %966 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_31, %result1_32 = torch.aten.var_mean.correction %965, %966, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %967 = torch.aten.add.Scalar %result0_31, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %968 = torch.aten.rsqrt %967 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %969 = torch.aten.sub.Tensor %964, %result1_32, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %970 = torch.aten.mul.Tensor %969, %968 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %971 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %972 = torch.aten.mul.Tensor %970, %971 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %973 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %974 = torch.aten.add.Tensor %972, %973, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %975 = torch.prims.convert_element_type %974, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %976 = torch.prims.convert_element_type %result1_32, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %977 = torch.prims.convert_element_type %968, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %978 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %979 = torch.aten.transpose.int %978, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %980 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %981 = torch.aten.view %975, %980 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %982 = torch.aten.mm %981, %979 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %983 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %984 = torch.aten.view %982, %983 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %985 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %986 = torch.aten.transpose.int %985, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %987 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %988 = torch.aten.view %975, %987 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %989 = torch.aten.mm %988, %986 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %990 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %991 = torch.aten.view %989, %990 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %992 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %993 = torch.aten.transpose.int %992, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %994 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %995 = torch.aten.view %975, %994 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %996 = torch.aten.mm %995, %993 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %997 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %998 = torch.aten.view %996, %997 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %999 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1000 = torch.aten.view %984, %999 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1001 = torch.aten.transpose.int %1000, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1002 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1003 = torch.aten.view %991, %1002 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1004 = torch.aten.transpose.int %1003, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1005 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1006 = torch.aten.view %998, %1005 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1007 = torch.aten.transpose.int %1006, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1008:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1001, %1004, %1007, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1009 = torch.aten.transpose.int %1008#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1010 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1011 = torch.aten.view %1009, %1010 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1012 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1013 = torch.aten.view %1011, %1012 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %1014 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1015 = torch.aten.transpose.int %1014, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %1016 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1017 = torch.prims.convert_element_type %1016, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1018 = torch.prims.convert_element_type %1013, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1019 = torch.prims.convert_element_type %1015, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1020 = torch.aten.mm %1018, %1019 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1021 = torch.aten.mul.Scalar %1020, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1022 = torch.aten.mul.Scalar %1017, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1023 = torch.aten.add.Tensor %1021, %1022, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1024 = torch.prims.convert_element_type %1023, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1025 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1026 = torch.aten.view %1024, %1025 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1027 = torch.aten.div.Scalar %1026, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1028 = torch.aten.add.Tensor %1027, %964, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1029 = torch.prims.convert_element_type %1028, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1030 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_33, %result1_34 = torch.aten.var_mean.correction %1029, %1030, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1031 = torch.aten.add.Scalar %result0_33, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1032 = torch.aten.rsqrt %1031 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1033 = torch.aten.sub.Tensor %1028, %result1_34, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1034 = torch.aten.mul.Tensor %1033, %1032 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %1035 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1036 = torch.aten.mul.Tensor %1034, %1035 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %1037 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1038 = torch.aten.add.Tensor %1036, %1037, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1039 = torch.prims.convert_element_type %1038, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1040 = torch.prims.convert_element_type %result1_34, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1041 = torch.prims.convert_element_type %1032, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %1042 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1043 = torch.aten.transpose.int %1042, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1044 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1045 = torch.aten.view %1039, %1044 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1046 = torch.aten.mm %1045, %1043 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1047 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1048 = torch.aten.view %1046, %1047 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %1049 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1050 = torch.aten.transpose.int %1049, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1051 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1052 = torch.aten.view %4, %1051 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1053 = torch.aten.mm %1052, %1050 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1054 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1055 = torch.aten.view %1053, %1054 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %1056 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1057 = torch.aten.transpose.int %1056, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1058 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1059 = torch.aten.view %4, %1058 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1060 = torch.aten.mm %1059, %1057 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1061 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1062 = torch.aten.view %1060, %1061 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %1063 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1064 = torch.aten.view %1048, %1063 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1065 = torch.aten.transpose.int %1064, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1066 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1067 = torch.aten.view %1055, %1066 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1068 = torch.aten.transpose.int %1067, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1069 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1070 = torch.aten.view %1062, %1069 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1071 = torch.aten.transpose.int %1070, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1072:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1065, %1068, %1071, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1073 = torch.aten.transpose.int %1072#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1074 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1075 = torch.aten.view %1073, %1074 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1076 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1077 = torch.aten.view %1075, %1076 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %1078 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1079 = torch.aten.transpose.int %1078, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %1080 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1081 = torch.prims.convert_element_type %1080, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1082 = torch.prims.convert_element_type %1077, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1083 = torch.prims.convert_element_type %1079, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1084 = torch.aten.mm %1082, %1083 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1085 = torch.aten.mul.Scalar %1084, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1086 = torch.aten.mul.Scalar %1081, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1087 = torch.aten.add.Tensor %1085, %1086, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1088 = torch.prims.convert_element_type %1087, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1089 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1090 = torch.aten.view %1088, %1089 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1091 = torch.aten.div.Scalar %1090, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1092 = torch.aten.add.Tensor %1091, %1028, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1093 = torch.prims.convert_element_type %1092, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1094 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_35, %result1_36 = torch.aten.var_mean.correction %1093, %1094, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1095 = torch.aten.add.Scalar %result0_35, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1096 = torch.aten.rsqrt %1095 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1097 = torch.aten.sub.Tensor %1092, %result1_36, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1098 = torch.aten.mul.Tensor %1097, %1096 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %1099 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1100 = torch.aten.mul.Tensor %1098, %1099 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %1101 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1102 = torch.aten.add.Tensor %1100, %1101, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1103 = torch.prims.convert_element_type %1102, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1104 = torch.prims.convert_element_type %result1_36, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1105 = torch.prims.convert_element_type %1096, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1106 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1107 = torch.aten.view %1103, %1106 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %1108 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %1109 = torch.aten.transpose.int %1108, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %1110 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %1111 = torch.prims.convert_element_type %1110, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %1112 = torch.prims.convert_element_type %1107, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1113 = torch.prims.convert_element_type %1109, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %1114 = torch.aten.mm %1112, %1113 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %1115 = torch.aten.mul.Scalar %1114, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1116 = torch.aten.mul.Scalar %1111, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %1117 = torch.aten.add.Tensor %1115, %1116, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1118 = torch.prims.convert_element_type %1117, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %1119 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1120 = torch.aten.view %1118, %1119 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %1121 = torch.aten.slice.Tensor %1120, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1122 = torch.aten.slice.Tensor %1120, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1123 = torch.aten.gelu %1122, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %1124 = torch.aten.mul.Tensor %1121, %1123 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %1125 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %1126 = torch.aten.view %1124, %1125 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %1127 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %1128 = torch.aten.transpose.int %1127, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %1129 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1130 = torch.prims.convert_element_type %1129, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1131 = torch.prims.convert_element_type %1126, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %1132 = torch.prims.convert_element_type %1128, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %1133 = torch.aten.mm %1131, %1132 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1134 = torch.aten.mul.Scalar %1133, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1135 = torch.aten.mul.Scalar %1130, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1136 = torch.aten.add.Tensor %1134, %1135, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1137 = torch.prims.convert_element_type %1136, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1138 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1139 = torch.aten.view %1137, %1138 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1140 = torch.aten.add.Tensor %1139, %1092, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1141 = torch.prims.convert_element_type %1140, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1142 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_37, %result1_38 = torch.aten.var_mean.correction %1141, %1142, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1143 = torch.aten.add.Scalar %result0_37, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1144 = torch.aten.rsqrt %1143 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1145 = torch.aten.sub.Tensor %1140, %result1_38, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1146 = torch.aten.mul.Tensor %1145, %1144 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %1147 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1148 = torch.aten.mul.Tensor %1146, %1147 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %1149 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1150 = torch.aten.add.Tensor %1148, %1149, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1151 = torch.prims.convert_element_type %1150, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1152 = torch.prims.convert_element_type %result1_38, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1153 = torch.prims.convert_element_type %1144, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %1154 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1155 = torch.aten.transpose.int %1154, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1156 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1157 = torch.aten.view %1151, %1156 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1158 = torch.aten.mm %1157, %1155 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1159 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1160 = torch.aten.view %1158, %1159 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %1161 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1162 = torch.aten.transpose.int %1161, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1163 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1164 = torch.aten.view %1151, %1163 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1165 = torch.aten.mm %1164, %1162 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1166 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1167 = torch.aten.view %1165, %1166 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %1168 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1169 = torch.aten.transpose.int %1168, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1170 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1171 = torch.aten.view %1151, %1170 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1172 = torch.aten.mm %1171, %1169 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1173 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1174 = torch.aten.view %1172, %1173 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1175 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1176 = torch.aten.view %1160, %1175 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1177 = torch.aten.transpose.int %1176, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1178 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1179 = torch.aten.view %1167, %1178 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1180 = torch.aten.transpose.int %1179, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1181 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1182 = torch.aten.view %1174, %1181 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1183 = torch.aten.transpose.int %1182, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1184:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1177, %1180, %1183, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1185 = torch.aten.transpose.int %1184#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1186 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1187 = torch.aten.view %1185, %1186 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1188 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1189 = torch.aten.view %1187, %1188 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %1190 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1191 = torch.aten.transpose.int %1190, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %1192 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1193 = torch.prims.convert_element_type %1192, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1194 = torch.prims.convert_element_type %1189, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1195 = torch.prims.convert_element_type %1191, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1196 = torch.aten.mm %1194, %1195 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1197 = torch.aten.mul.Scalar %1196, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1198 = torch.aten.mul.Scalar %1193, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1199 = torch.aten.add.Tensor %1197, %1198, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1200 = torch.prims.convert_element_type %1199, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1201 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1202 = torch.aten.view %1200, %1201 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1203 = torch.aten.div.Scalar %1202, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1204 = torch.aten.add.Tensor %1203, %1140, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1205 = torch.prims.convert_element_type %1204, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1206 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_39, %result1_40 = torch.aten.var_mean.correction %1205, %1206, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1207 = torch.aten.add.Scalar %result0_39, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1208 = torch.aten.rsqrt %1207 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1209 = torch.aten.sub.Tensor %1204, %result1_40, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1210 = torch.aten.mul.Tensor %1209, %1208 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %1211 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1212 = torch.aten.mul.Tensor %1210, %1211 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %1213 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1214 = torch.aten.add.Tensor %1212, %1213, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1215 = torch.prims.convert_element_type %1214, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1216 = torch.prims.convert_element_type %result1_40, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1217 = torch.prims.convert_element_type %1208, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %1218 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1219 = torch.aten.transpose.int %1218, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %1220 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1221 = torch.aten.view %1215, %1220 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %1222 = torch.aten.mm %1221, %1219 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %1223 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1224 = torch.aten.view %1222, %1223 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %1225 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1226 = torch.aten.transpose.int %1225, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1227 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1228 = torch.aten.view %4, %1227 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1229 = torch.aten.mm %1228, %1226 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1230 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1231 = torch.aten.view %1229, %1230 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %1232 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %1233 = torch.aten.transpose.int %1232, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %1234 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1235 = torch.aten.view %4, %1234 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1236 = torch.aten.mm %1235, %1233 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %1237 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1238 = torch.aten.view %1236, %1237 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %1239 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1240 = torch.aten.view %1224, %1239 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %1241 = torch.aten.transpose.int %1240, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %1242 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1243 = torch.aten.view %1231, %1242 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1244 = torch.aten.transpose.int %1243, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1245 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1246 = torch.aten.view %1238, %1245 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %1247 = torch.aten.transpose.int %1246, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %1248:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1241, %1244, %1247, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %1249 = torch.aten.transpose.int %1248#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %1250 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1251 = torch.aten.view %1249, %1250 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1252 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1253 = torch.aten.view %1251, %1252 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %1254 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1255 = torch.aten.transpose.int %1254, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %1256 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1257 = torch.prims.convert_element_type %1256, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1258 = torch.prims.convert_element_type %1253, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1259 = torch.prims.convert_element_type %1255, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1260 = torch.aten.mm %1258, %1259 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1261 = torch.aten.mul.Scalar %1260, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1262 = torch.aten.mul.Scalar %1257, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1263 = torch.aten.add.Tensor %1261, %1262, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1264 = torch.prims.convert_element_type %1263, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1265 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1266 = torch.aten.view %1264, %1265 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1267 = torch.aten.div.Scalar %1266, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %1268 = torch.aten.add.Tensor %1267, %1204, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1269 = torch.prims.convert_element_type %1268, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1270 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_41, %result1_42 = torch.aten.var_mean.correction %1269, %1270, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %1271 = torch.aten.add.Scalar %result0_41, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %1272 = torch.aten.rsqrt %1271 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %1273 = torch.aten.sub.Tensor %1268, %result1_42, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1274 = torch.aten.mul.Tensor %1273, %1272 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %1275 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1276 = torch.aten.mul.Tensor %1274, %1275 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %1277 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1278 = torch.aten.add.Tensor %1276, %1277, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %1279 = torch.prims.convert_element_type %1278, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1280 = torch.prims.convert_element_type %result1_42, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1281 = torch.prims.convert_element_type %1272, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %1282 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1283 = torch.aten.view %1279, %1282 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %1284 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %1285 = torch.aten.transpose.int %1284, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %1286 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %1287 = torch.prims.convert_element_type %1286, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %1288 = torch.prims.convert_element_type %1283, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1289 = torch.prims.convert_element_type %1285, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %1290 = torch.aten.mm %1288, %1289 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %1291 = torch.aten.mul.Scalar %1290, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1292 = torch.aten.mul.Scalar %1287, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %1293 = torch.aten.add.Tensor %1291, %1292, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %1294 = torch.prims.convert_element_type %1293, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %1295 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1296 = torch.aten.view %1294, %1295 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %1297 = torch.aten.slice.Tensor %1296, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1298 = torch.aten.slice.Tensor %1296, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %1299 = torch.aten.gelu %1298, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %1300 = torch.aten.mul.Tensor %1297, %1299 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %1301 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %1302 = torch.aten.view %1300, %1301 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %1303 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %1304 = torch.aten.transpose.int %1303, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %1305 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1306 = torch.prims.convert_element_type %1305, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1307 = torch.prims.convert_element_type %1302, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %1308 = torch.prims.convert_element_type %1304, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %1309 = torch.aten.mm %1307, %1308 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1310 = torch.aten.mul.Scalar %1309, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1311 = torch.aten.mul.Scalar %1306, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1312 = torch.aten.add.Tensor %1310, %1311, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1313 = torch.prims.convert_element_type %1312, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1314 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1315 = torch.aten.view %1313, %1314 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1316 = torch.aten.add.Tensor %1315, %1268, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %1317 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %1318 = torch.aten.view %1316, %1317 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_out.weight = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16>
    %1319 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %1320 = torch.aten.transpose.int %1319, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.down_blocks.1.attentions.1.proj_out.bias = util.global.load @_params.unet.down_blocks.1.attentions.1.proj_out.bias : tensor<640xf16>
    %1321 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.attentions.1.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1322 = torch.prims.convert_element_type %1321, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %1323 = torch.prims.convert_element_type %1318, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1324 = torch.prims.convert_element_type %1320, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1325 = torch.aten.mm %1323, %1324 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %1326 = torch.aten.mul.Scalar %1325, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1327 = torch.aten.mul.Scalar %1322, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %1328 = torch.aten.add.Tensor %1326, %1327, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %1329 = torch.prims.convert_element_type %1328, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %1330 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1331 = torch.aten.view %1329, %1330 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %1332 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1333 = torch.aten.view %1331, %1332 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %1334 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1335 = torch.aten.permute %1333, %1334 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %1336 = torch.aten.add.Tensor %1335, %920, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.down_blocks.1.downsamplers.0.conv.weight = util.global.load @_params.unet.down_blocks.1.downsamplers.0.conv.weight : tensor<640x640x3x3xf16>
    %1337 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.downsamplers.0.conv.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.down_blocks.1.downsamplers.0.conv.bias = util.global.load @_params.unet.down_blocks.1.downsamplers.0.conv.bias : tensor<640xf16>
    %1338 = torch_c.from_builtin_tensor %_params.unet.down_blocks.1.downsamplers.0.conv.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1339 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %1340 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1341 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1342 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1343 = torch.aten.convolution %1336, %1337, %1338, %1339, %1340, %1341, %false, %1342, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,32,32],f16>
    %1344 = torch.prim.ListConstruct %int2, %int32, %int20, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1345 = torch.aten.view %1343, %1344 : !torch.vtensor<[2,640,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,1024],f16>
    %1346 = torch.prims.convert_element_type %1345, %int6 : !torch.vtensor<[2,32,20,1024],f16>, !torch.int -> !torch.vtensor<[2,32,20,1024],f32>
    %1347 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_43, %result1_44 = torch.aten.var_mean.correction %1346, %1347, %int0, %true : !torch.vtensor<[2,32,20,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %1348 = torch.aten.add.Scalar %result0_43, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1349 = torch.aten.rsqrt %1348 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %1350 = torch.aten.sub.Tensor %1345, %result1_44, %int1 : !torch.vtensor<[2,32,20,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,1024],f32>
    %1351 = torch.aten.mul.Tensor %1350, %1349 : !torch.vtensor<[2,32,20,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,1024],f32>
    %1352 = torch.prim.ListConstruct %int2, %int640, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1353 = torch.aten.view %1351, %1352 : !torch.vtensor<[2,32,20,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,640,32,32],f32>
    %_params.unet.down_blocks.2.resnets.0.norm1.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.norm1.bias : tensor<640xf16>
    %1354 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1355 = torch.aten.unsqueeze %1354, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %1356 = torch.aten.unsqueeze %1355, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %1357 = torch.aten.unsqueeze %1356, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.down_blocks.2.resnets.0.norm1.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.norm1.weight : tensor<640xf16>
    %1358 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1359 = torch.aten.unsqueeze %1358, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %1360 = torch.aten.unsqueeze %1359, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %1361 = torch.aten.unsqueeze %1360, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %1362 = torch.aten.mul.Tensor %1353, %1361 : !torch.vtensor<[2,640,32,32],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,32,32],f32>
    %1363 = torch.aten.add.Tensor %1362, %1357, %int1 : !torch.vtensor<[2,640,32,32],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,32,32],f32>
    %1364 = torch.prims.convert_element_type %1363, %int5 : !torch.vtensor<[2,640,32,32],f32>, !torch.int -> !torch.vtensor<[2,640,32,32],f16>
    %1365 = torch.prims.convert_element_type %result1_44, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1366 = torch.prims.convert_element_type %1349, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1367 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1368 = torch.prims.squeeze %1365, %1367 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1369 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1370 = torch.prims.squeeze %1368, %1369 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1371 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1372 = torch.prims.squeeze %1366, %1371 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1373 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1374 = torch.prims.squeeze %1372, %1373 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1375 = torch.aten.silu %1364 : !torch.vtensor<[2,640,32,32],f16> -> !torch.vtensor<[2,640,32,32],f16>
    %_params.unet.down_blocks.2.resnets.0.conv1.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.conv1.weight : tensor<1280x640x3x3xf16>
    %1376 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv1.weight : tensor<1280x640x3x3xf16> -> !torch.vtensor<[1280,640,3,3],f16>
    %_params.unet.down_blocks.2.resnets.0.conv1.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.conv1.bias : tensor<1280xf16>
    %1377 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1378 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1379 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1380 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1381 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1382 = torch.aten.convolution %1375, %1376, %1377, %1378, %1379, %1380, %false, %1381, %int1 : !torch.vtensor<[2,640,32,32],f16>, !torch.vtensor<[1280,640,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1383 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %1384 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1385 = torch.aten.transpose.int %1384, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %1386 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1387 = torch.prims.convert_element_type %1386, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1388 = torch.prims.convert_element_type %1383, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %1389 = torch.prims.convert_element_type %1385, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1390 = torch.aten.mm %1388, %1389 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %1391 = torch.aten.mul.Scalar %1390, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %1392 = torch.aten.mul.Scalar %1387, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1393 = torch.aten.add.Tensor %1391, %1392, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %1394 = torch.prims.convert_element_type %1393, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %1395 = torch.aten.unsqueeze %1394, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %1396 = torch.aten.unsqueeze %1395, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %1397 = torch.aten.add.Tensor %1382, %1396, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1398 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1399 = torch.aten.view %1397, %1398 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %1400 = torch.prims.convert_element_type %1399, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1401 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_45, %result1_46 = torch.aten.var_mean.correction %1400, %1401, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %1402 = torch.aten.add.Scalar %result0_45, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1403 = torch.aten.rsqrt %1402 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %1404 = torch.aten.sub.Tensor %1399, %result1_46, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1405 = torch.aten.mul.Tensor %1404, %1403 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %1406 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1407 = torch.aten.view %1405, %1406 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.resnets.0.norm2.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.norm2.bias : tensor<1280xf16>
    %1408 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1409 = torch.aten.unsqueeze %1408, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1410 = torch.aten.unsqueeze %1409, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1411 = torch.aten.unsqueeze %1410, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.resnets.0.norm2.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.norm2.weight : tensor<1280xf16>
    %1412 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1413 = torch.aten.unsqueeze %1412, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1414 = torch.aten.unsqueeze %1413, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1415 = torch.aten.unsqueeze %1414, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %1416 = torch.aten.mul.Tensor %1407, %1415 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %1417 = torch.aten.add.Tensor %1416, %1411, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %1418 = torch.prims.convert_element_type %1417, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1419 = torch.prims.convert_element_type %result1_46, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1420 = torch.prims.convert_element_type %1403, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1421 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1422 = torch.prims.squeeze %1419, %1421 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1423 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1424 = torch.prims.squeeze %1422, %1423 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1425 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1426 = torch.prims.squeeze %1420, %1425 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1427 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1428 = torch.prims.squeeze %1426, %1427 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1429 = torch.aten.silu %1418 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.0.conv2.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %1430 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.down_blocks.2.resnets.0.conv2.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.conv2.bias : tensor<1280xf16>
    %1431 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1432 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1433 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1434 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1435 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1436 = torch.aten.convolution %1429, %1430, %1431, %1432, %1433, %1434, %false, %1435, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight : tensor<1280x640x1x1xf16>
    %1437 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv_shortcut.weight : tensor<1280x640x1x1xf16> -> !torch.vtensor<[1280,640,1,1],f16>
    %_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias : tensor<1280xf16>
    %1438 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.0.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1439 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1440 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1441 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %1442 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %1443 = torch.aten.convolution %1343, %1437, %1438, %1439, %1440, %1441, %false, %1442, %int1 : !torch.vtensor<[2,640,32,32],f16>, !torch.vtensor<[1280,640,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1444 = torch.aten.add.Tensor %1443, %1436, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1445 = torch.aten.div.Scalar %1444, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %1446 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1447 = torch.aten.view %1445, %1446 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %1448 = torch.prims.convert_element_type %1447, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1449 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_47, %result1_48 = torch.aten.var_mean.correction %1448, %1449, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %1450 = torch.aten.add.Scalar %result0_47, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1451 = torch.aten.rsqrt %1450 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %1452 = torch.aten.sub.Tensor %1447, %result1_48, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %1453 = torch.aten.mul.Tensor %1452, %1451 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %1454 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1455 = torch.aten.view %1453, %1454 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.attentions.0.norm.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.norm.bias : tensor<1280xf16>
    %1456 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1457 = torch.aten.unsqueeze %1456, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1458 = torch.aten.unsqueeze %1457, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1459 = torch.aten.unsqueeze %1458, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.attentions.0.norm.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.norm.weight : tensor<1280xf16>
    %1460 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1461 = torch.aten.unsqueeze %1460, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %1462 = torch.aten.unsqueeze %1461, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %1463 = torch.aten.unsqueeze %1462, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %1464 = torch.aten.mul.Tensor %1455, %1463 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %1465 = torch.aten.add.Tensor %1464, %1459, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %1466 = torch.prims.convert_element_type %1465, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %1467 = torch.prims.convert_element_type %result1_48, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1468 = torch.prims.convert_element_type %1451, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %1469 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1470 = torch.prims.squeeze %1467, %1469 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1471 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1472 = torch.prims.squeeze %1470, %1471 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1473 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %1474 = torch.prims.squeeze %1468, %1473 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %1475 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %1476 = torch.prims.squeeze %1474, %1475 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %1477 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1478 = torch.aten.permute %1466, %1477 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %1479 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1480 = torch.aten.view %1478, %1479 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_in.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %1481 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1482 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1483 = torch.aten._unsafe_view %1480, %1482 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1484 = torch_c.to_builtin_tensor %1483 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1485 = torch_c.to_builtin_tensor %1481 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1486 = tensor.empty() : tensor<2048x1280xf32>
    %1487 = linalg.fill ins(%cst : f32) outs(%1486 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1488 = tensor.empty() : tensor<2048x1280xf32>
    %1489 = linalg.fill ins(%cst : f32) outs(%1488 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1490:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1487, %1489, %1484, %1485, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1487, %1489)
    %1491 = arith.truncf %1490#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1492 = torch_c.from_builtin_tensor %1491 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1493 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1494 = torch.aten.view %1492, %1493 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_in.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_in.bias : tensor<1280xf16>
    %1495 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1496 = torch.aten.add.Tensor %1494, %1495, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1497 = torch.prims.convert_element_type %1496, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1498 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_49, %result1_50 = torch.aten.var_mean.correction %1497, %1498, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1499 = torch.aten.add.Scalar %result0_49, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1500 = torch.aten.rsqrt %1499 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1501 = torch.aten.sub.Tensor %1496, %result1_50, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1502 = torch.aten.mul.Tensor %1501, %1500 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %1503 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1504 = torch.aten.mul.Tensor %1502, %1503 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %1505 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1506 = torch.aten.add.Tensor %1504, %1505, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1507 = torch.prims.convert_element_type %1506, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1508 = torch.prims.convert_element_type %result1_50, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1509 = torch.prims.convert_element_type %1500, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %1510 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1511 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1512 = torch.aten.view %1507, %1511 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1513 = torch_c.to_builtin_tensor %1512 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1514 = torch_c.to_builtin_tensor %1510 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1515 = tensor.empty() : tensor<2048x1280xf32>
    %1516 = linalg.fill ins(%cst : f32) outs(%1515 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1517 = tensor.empty() : tensor<2048x1280xf32>
    %1518 = linalg.fill ins(%cst : f32) outs(%1517 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1519:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1516, %1518, %1513, %1514, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1516, %1518)
    %1520 = arith.truncf %1519#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1521 = torch_c.from_builtin_tensor %1520 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1522 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1523 = torch.aten.view %1521, %1522 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %1524 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1525 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1526 = torch.aten.view %1507, %1525 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1527 = torch_c.to_builtin_tensor %1526 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1528 = torch_c.to_builtin_tensor %1524 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1529 = tensor.empty() : tensor<2048x1280xf32>
    %1530 = linalg.fill ins(%cst : f32) outs(%1529 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1531 = tensor.empty() : tensor<2048x1280xf32>
    %1532 = linalg.fill ins(%cst : f32) outs(%1531 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1533:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1530, %1532, %1527, %1528, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1530, %1532)
    %1534 = arith.truncf %1533#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1535 = torch_c.from_builtin_tensor %1534 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1536 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1537 = torch.aten.view %1535, %1536 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %1538 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1539 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1540 = torch.aten.view %1507, %1539 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1541 = torch_c.to_builtin_tensor %1540 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1542 = torch_c.to_builtin_tensor %1538 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1543 = tensor.empty() : tensor<2048x1280xf32>
    %1544 = linalg.fill ins(%cst : f32) outs(%1543 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1545 = tensor.empty() : tensor<2048x1280xf32>
    %1546 = linalg.fill ins(%cst : f32) outs(%1545 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1547:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1544, %1546, %1541, %1542, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1544, %1546)
    %1548 = arith.truncf %1547#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1549 = torch_c.from_builtin_tensor %1548 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1550 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1551 = torch.aten.view %1549, %1550 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1552 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1553 = torch.aten.view %1523, %1552 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1554 = torch.aten.transpose.int %1553, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1555 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1556 = torch.aten.view %1537, %1555 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1557 = torch.aten.transpose.int %1556, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1558 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1559 = torch.aten.view %1551, %1558 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1560 = torch.aten.transpose.int %1559, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1561:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1554, %1557, %1560, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1562 = torch.aten.transpose.int %1561#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1563 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1564 = torch.aten.view %1562, %1563 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1565 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1566 = torch.aten.view %1564, %1565 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1567 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1568 = torch.aten.transpose.int %1567, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %1569 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1570 = torch.prims.convert_element_type %1569, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1571 = torch.prims.convert_element_type %1566, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1572 = torch.prims.convert_element_type %1568, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1573 = torch.aten.mm %1571, %1572 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1574 = torch.aten.mul.Scalar %1573, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1575 = torch.aten.mul.Scalar %1570, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1576 = torch.aten.add.Tensor %1574, %1575, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1577 = torch.prims.convert_element_type %1576, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1578 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1579 = torch.aten.view %1577, %1578 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1580 = torch.aten.div.Scalar %1579, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1581 = torch.aten.add.Tensor %1580, %1496, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1582 = torch.prims.convert_element_type %1581, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1583 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_51, %result1_52 = torch.aten.var_mean.correction %1582, %1583, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1584 = torch.aten.add.Scalar %result0_51, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1585 = torch.aten.rsqrt %1584 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1586 = torch.aten.sub.Tensor %1581, %result1_52, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1587 = torch.aten.mul.Tensor %1586, %1585 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %1588 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1589 = torch.aten.mul.Tensor %1587, %1588 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %1590 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1591 = torch.aten.add.Tensor %1589, %1590, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1592 = torch.prims.convert_element_type %1591, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1593 = torch.prims.convert_element_type %result1_52, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1594 = torch.prims.convert_element_type %1585, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %1595 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1596 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1597 = torch.aten.view %1592, %1596 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1598 = torch_c.to_builtin_tensor %1597 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1599 = torch_c.to_builtin_tensor %1595 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1600 = tensor.empty() : tensor<2048x1280xf32>
    %1601 = linalg.fill ins(%cst : f32) outs(%1600 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1602 = tensor.empty() : tensor<2048x1280xf32>
    %1603 = linalg.fill ins(%cst : f32) outs(%1602 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1604:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1601, %1603, %1598, %1599, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1601, %1603)
    %1605 = arith.truncf %1604#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1606 = torch_c.from_builtin_tensor %1605 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1607 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1608 = torch.aten.view %1606, %1607 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %1609 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1610 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1611 = torch.aten.view %4, %1610 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1612 = torch_c.to_builtin_tensor %1611 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %1613 = torch_c.to_builtin_tensor %1609 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %1614 = tensor.empty() : tensor<128x1280xf32>
    %1615 = linalg.fill ins(%cst : f32) outs(%1614 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1616 = tensor.empty() : tensor<128x1280xf32>
    %1617 = linalg.fill ins(%cst : f32) outs(%1616 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1618:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %1615, %1617, %1612, %1613, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1615, %1617)
    %1619 = arith.truncf %1618#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %1620 = torch_c.from_builtin_tensor %1619 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %1621 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1622 = torch.aten.view %1620, %1621 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %1623 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1624 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1625 = torch.aten.view %4, %1624 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1626 = torch_c.to_builtin_tensor %1625 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %1627 = torch_c.to_builtin_tensor %1623 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %1628 = tensor.empty() : tensor<128x1280xf32>
    %1629 = linalg.fill ins(%cst : f32) outs(%1628 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1630 = tensor.empty() : tensor<128x1280xf32>
    %1631 = linalg.fill ins(%cst : f32) outs(%1630 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1632:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %1629, %1631, %1626, %1627, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1629, %1631)
    %1633 = arith.truncf %1632#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %1634 = torch_c.from_builtin_tensor %1633 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %1635 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1636 = torch.aten.view %1634, %1635 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %1637 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1638 = torch.aten.view %1608, %1637 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1639 = torch.aten.transpose.int %1638, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1640 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1641 = torch.aten.view %1622, %1640 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1642 = torch.aten.transpose.int %1641, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1643 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1644 = torch.aten.view %1636, %1643 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1645 = torch.aten.transpose.int %1644, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1646:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1639, %1642, %1645, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1647 = torch.aten.transpose.int %1646#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1648 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1649 = torch.aten.view %1647, %1648 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1650 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1651 = torch.aten.view %1649, %1650 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %1652 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1653 = torch.aten.transpose.int %1652, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %1654 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1655 = torch.prims.convert_element_type %1654, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1656 = torch.prims.convert_element_type %1651, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1657 = torch.prims.convert_element_type %1653, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1658 = torch.aten.mm %1656, %1657 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1659 = torch.aten.mul.Scalar %1658, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1660 = torch.aten.mul.Scalar %1655, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1661 = torch.aten.add.Tensor %1659, %1660, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1662 = torch.prims.convert_element_type %1661, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1663 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1664 = torch.aten.view %1662, %1663 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1665 = torch.aten.div.Scalar %1664, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1666 = torch.aten.add.Tensor %1665, %1581, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1667 = torch.prims.convert_element_type %1666, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1668 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_53, %result1_54 = torch.aten.var_mean.correction %1667, %1668, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1669 = torch.aten.add.Scalar %result0_53, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1670 = torch.aten.rsqrt %1669 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1671 = torch.aten.sub.Tensor %1666, %result1_54, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1672 = torch.aten.mul.Tensor %1671, %1670 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %1673 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1674 = torch.aten.mul.Tensor %1672, %1673 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %1675 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1676 = torch.aten.add.Tensor %1674, %1675, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1677 = torch.prims.convert_element_type %1676, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1678 = torch.prims.convert_element_type %result1_54, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1679 = torch.prims.convert_element_type %1670, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1680 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1681 = torch.aten.view %1677, %1680 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %1682 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %1683 = torch.aten.transpose.int %1682, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %1684 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %1685 = torch.prims.convert_element_type %1684, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %1686 = torch.prims.convert_element_type %1681, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1687 = torch.prims.convert_element_type %1683, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %1688 = torch.aten.mm %1686, %1687 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %1689 = torch.aten.mul.Scalar %1688, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1690 = torch.aten.mul.Scalar %1685, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %1691 = torch.aten.add.Tensor %1689, %1690, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1692 = torch.prims.convert_element_type %1691, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %1693 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1694 = torch.aten.view %1692, %1693 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %1695 = torch.aten.slice.Tensor %1694, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1696 = torch.aten.slice.Tensor %1694, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1697 = torch.aten.gelu %1696, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %1698 = torch.aten.mul.Tensor %1695, %1697 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %1699 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %1700 = torch.aten.view %1698, %1699 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %1701 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %1702 = torch.aten.transpose.int %1701, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %1703 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1704 = torch.prims.convert_element_type %1703, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1705 = torch.prims.convert_element_type %1700, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %1706 = torch.prims.convert_element_type %1702, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1707 = torch.aten.mm %1705, %1706 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1708 = torch.aten.mul.Scalar %1707, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1709 = torch.aten.mul.Scalar %1704, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1710 = torch.aten.add.Tensor %1708, %1709, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1711 = torch.prims.convert_element_type %1710, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1712 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1713 = torch.aten.view %1711, %1712 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1714 = torch.aten.add.Tensor %1713, %1666, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1715 = torch.prims.convert_element_type %1714, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1716 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_55, %result1_56 = torch.aten.var_mean.correction %1715, %1716, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1717 = torch.aten.add.Scalar %result0_55, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1718 = torch.aten.rsqrt %1717 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1719 = torch.aten.sub.Tensor %1714, %result1_56, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1720 = torch.aten.mul.Tensor %1719, %1718 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %1721 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1722 = torch.aten.mul.Tensor %1720, %1721 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %1723 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1724 = torch.aten.add.Tensor %1722, %1723, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1725 = torch.prims.convert_element_type %1724, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1726 = torch.prims.convert_element_type %result1_56, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1727 = torch.prims.convert_element_type %1718, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %1728 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1729 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1730 = torch.aten.view %1725, %1729 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1731 = torch_c.to_builtin_tensor %1730 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1732 = torch_c.to_builtin_tensor %1728 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1733 = tensor.empty() : tensor<2048x1280xf32>
    %1734 = linalg.fill ins(%cst : f32) outs(%1733 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1735 = tensor.empty() : tensor<2048x1280xf32>
    %1736 = linalg.fill ins(%cst : f32) outs(%1735 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1737:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1734, %1736, %1731, %1732, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1734, %1736)
    %1738 = arith.truncf %1737#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1739 = torch_c.from_builtin_tensor %1738 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1740 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1741 = torch.aten.view %1739, %1740 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %1742 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1743 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1744 = torch.aten.view %1725, %1743 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1745 = torch_c.to_builtin_tensor %1744 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1746 = torch_c.to_builtin_tensor %1742 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1747 = tensor.empty() : tensor<2048x1280xf32>
    %1748 = linalg.fill ins(%cst : f32) outs(%1747 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1749 = tensor.empty() : tensor<2048x1280xf32>
    %1750 = linalg.fill ins(%cst : f32) outs(%1749 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1751:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1748, %1750, %1745, %1746, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1748, %1750)
    %1752 = arith.truncf %1751#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1753 = torch_c.from_builtin_tensor %1752 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1754 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1755 = torch.aten.view %1753, %1754 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %1756 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1757 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1758 = torch.aten.view %1725, %1757 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1759 = torch_c.to_builtin_tensor %1758 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1760 = torch_c.to_builtin_tensor %1756 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1761 = tensor.empty() : tensor<2048x1280xf32>
    %1762 = linalg.fill ins(%cst : f32) outs(%1761 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1763 = tensor.empty() : tensor<2048x1280xf32>
    %1764 = linalg.fill ins(%cst : f32) outs(%1763 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1765:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1762, %1764, %1759, %1760, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1762, %1764)
    %1766 = arith.truncf %1765#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1767 = torch_c.from_builtin_tensor %1766 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1768 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1769 = torch.aten.view %1767, %1768 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1770 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1771 = torch.aten.view %1741, %1770 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1772 = torch.aten.transpose.int %1771, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1773 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1774 = torch.aten.view %1755, %1773 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1775 = torch.aten.transpose.int %1774, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1776 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1777 = torch.aten.view %1769, %1776 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1778 = torch.aten.transpose.int %1777, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1779:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1772, %1775, %1778, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1780 = torch.aten.transpose.int %1779#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1781 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1782 = torch.aten.view %1780, %1781 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1783 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1784 = torch.aten.view %1782, %1783 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1785 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1786 = torch.aten.transpose.int %1785, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %1787 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1788 = torch.prims.convert_element_type %1787, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1789 = torch.prims.convert_element_type %1784, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1790 = torch.prims.convert_element_type %1786, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1791 = torch.aten.mm %1789, %1790 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1792 = torch.aten.mul.Scalar %1791, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1793 = torch.aten.mul.Scalar %1788, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1794 = torch.aten.add.Tensor %1792, %1793, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1795 = torch.prims.convert_element_type %1794, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1796 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1797 = torch.aten.view %1795, %1796 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1798 = torch.aten.div.Scalar %1797, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1799 = torch.aten.add.Tensor %1798, %1714, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1800 = torch.prims.convert_element_type %1799, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1801 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_57, %result1_58 = torch.aten.var_mean.correction %1800, %1801, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1802 = torch.aten.add.Scalar %result0_57, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1803 = torch.aten.rsqrt %1802 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1804 = torch.aten.sub.Tensor %1799, %result1_58, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1805 = torch.aten.mul.Tensor %1804, %1803 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %1806 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1807 = torch.aten.mul.Tensor %1805, %1806 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %1808 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1809 = torch.aten.add.Tensor %1807, %1808, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1810 = torch.prims.convert_element_type %1809, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1811 = torch.prims.convert_element_type %result1_58, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1812 = torch.prims.convert_element_type %1803, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %1813 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1814 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1815 = torch.aten.view %1810, %1814 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1816 = torch_c.to_builtin_tensor %1815 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1817 = torch_c.to_builtin_tensor %1813 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1818 = tensor.empty() : tensor<2048x1280xf32>
    %1819 = linalg.fill ins(%cst : f32) outs(%1818 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1820 = tensor.empty() : tensor<2048x1280xf32>
    %1821 = linalg.fill ins(%cst : f32) outs(%1820 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1822:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1819, %1821, %1816, %1817, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1819, %1821)
    %1823 = arith.truncf %1822#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1824 = torch_c.from_builtin_tensor %1823 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1825 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1826 = torch.aten.view %1824, %1825 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %1827 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1828 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1829 = torch.aten.view %4, %1828 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1830 = torch_c.to_builtin_tensor %1829 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %1831 = torch_c.to_builtin_tensor %1827 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %1832 = tensor.empty() : tensor<128x1280xf32>
    %1833 = linalg.fill ins(%cst : f32) outs(%1832 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1834 = tensor.empty() : tensor<128x1280xf32>
    %1835 = linalg.fill ins(%cst : f32) outs(%1834 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1836:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %1833, %1835, %1830, %1831, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1833, %1835)
    %1837 = arith.truncf %1836#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %1838 = torch_c.from_builtin_tensor %1837 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %1839 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1840 = torch.aten.view %1838, %1839 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %1841 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %1842 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %1843 = torch.aten.view %4, %1842 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %1844 = torch_c.to_builtin_tensor %1843 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %1845 = torch_c.to_builtin_tensor %1841 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %1846 = tensor.empty() : tensor<128x1280xf32>
    %1847 = linalg.fill ins(%cst : f32) outs(%1846 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1848 = tensor.empty() : tensor<128x1280xf32>
    %1849 = linalg.fill ins(%cst : f32) outs(%1848 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %1850:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %1847, %1849, %1844, %1845, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1847, %1849)
    %1851 = arith.truncf %1850#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %1852 = torch_c.from_builtin_tensor %1851 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %1853 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1854 = torch.aten.view %1852, %1853 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %1855 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1856 = torch.aten.view %1826, %1855 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1857 = torch.aten.transpose.int %1856, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1858 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1859 = torch.aten.view %1840, %1858 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1860 = torch.aten.transpose.int %1859, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1861 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1862 = torch.aten.view %1854, %1861 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %1863 = torch.aten.transpose.int %1862, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %1864:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1857, %1860, %1863, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1865 = torch.aten.transpose.int %1864#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1866 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1867 = torch.aten.view %1865, %1866 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1868 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1869 = torch.aten.view %1867, %1868 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %1870 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1871 = torch.aten.transpose.int %1870, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %1872 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1873 = torch.prims.convert_element_type %1872, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1874 = torch.prims.convert_element_type %1869, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1875 = torch.prims.convert_element_type %1871, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1876 = torch.aten.mm %1874, %1875 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1877 = torch.aten.mul.Scalar %1876, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1878 = torch.aten.mul.Scalar %1873, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1879 = torch.aten.add.Tensor %1877, %1878, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1880 = torch.prims.convert_element_type %1879, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1881 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1882 = torch.aten.view %1880, %1881 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1883 = torch.aten.div.Scalar %1882, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %1884 = torch.aten.add.Tensor %1883, %1799, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1885 = torch.prims.convert_element_type %1884, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1886 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_59, %result1_60 = torch.aten.var_mean.correction %1885, %1886, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1887 = torch.aten.add.Scalar %result0_59, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1888 = torch.aten.rsqrt %1887 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1889 = torch.aten.sub.Tensor %1884, %result1_60, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1890 = torch.aten.mul.Tensor %1889, %1888 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %1891 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1892 = torch.aten.mul.Tensor %1890, %1891 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %1893 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1894 = torch.aten.add.Tensor %1892, %1893, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1895 = torch.prims.convert_element_type %1894, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1896 = torch.prims.convert_element_type %result1_60, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1897 = torch.prims.convert_element_type %1888, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1898 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1899 = torch.aten.view %1895, %1898 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %1900 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %1901 = torch.aten.transpose.int %1900, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %1902 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %1903 = torch.prims.convert_element_type %1902, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %1904 = torch.prims.convert_element_type %1899, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1905 = torch.prims.convert_element_type %1901, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %1906 = torch.aten.mm %1904, %1905 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %1907 = torch.aten.mul.Scalar %1906, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1908 = torch.aten.mul.Scalar %1903, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %1909 = torch.aten.add.Tensor %1907, %1908, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %1910 = torch.prims.convert_element_type %1909, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %1911 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1912 = torch.aten.view %1910, %1911 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %1913 = torch.aten.slice.Tensor %1912, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1914 = torch.aten.slice.Tensor %1912, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %1915 = torch.aten.gelu %1914, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %1916 = torch.aten.mul.Tensor %1913, %1915 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %1917 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %1918 = torch.aten.view %1916, %1917 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %1919 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %1920 = torch.aten.transpose.int %1919, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %1921 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1922 = torch.prims.convert_element_type %1921, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %1923 = torch.prims.convert_element_type %1918, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %1924 = torch.prims.convert_element_type %1920, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1925 = torch.aten.mm %1923, %1924 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %1926 = torch.aten.mul.Scalar %1925, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1927 = torch.aten.mul.Scalar %1922, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %1928 = torch.aten.add.Tensor %1926, %1927, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %1929 = torch.prims.convert_element_type %1928, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %1930 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1931 = torch.aten.view %1929, %1930 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1932 = torch.aten.add.Tensor %1931, %1884, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1933 = torch.prims.convert_element_type %1932, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1934 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_61, %result1_62 = torch.aten.var_mean.correction %1933, %1934, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %1935 = torch.aten.add.Scalar %result0_61, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %1936 = torch.aten.rsqrt %1935 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %1937 = torch.aten.sub.Tensor %1932, %result1_62, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1938 = torch.aten.mul.Tensor %1937, %1936 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %1939 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1940 = torch.aten.mul.Tensor %1938, %1939 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %1941 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1942 = torch.aten.add.Tensor %1940, %1941, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %1943 = torch.prims.convert_element_type %1942, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %1944 = torch.prims.convert_element_type %result1_62, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %1945 = torch.prims.convert_element_type %1936, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %1946 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1947 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1948 = torch.aten.view %1943, %1947 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1949 = torch_c.to_builtin_tensor %1948 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1950 = torch_c.to_builtin_tensor %1946 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1951 = tensor.empty() : tensor<2048x1280xf32>
    %1952 = linalg.fill ins(%cst : f32) outs(%1951 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1953 = tensor.empty() : tensor<2048x1280xf32>
    %1954 = linalg.fill ins(%cst : f32) outs(%1953 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1955:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1952, %1954, %1949, %1950, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1952, %1954)
    %1956 = arith.truncf %1955#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1957 = torch_c.from_builtin_tensor %1956 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1958 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1959 = torch.aten.view %1957, %1958 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %1960 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1961 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1962 = torch.aten.view %1943, %1961 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1963 = torch_c.to_builtin_tensor %1962 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1964 = torch_c.to_builtin_tensor %1960 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1965 = tensor.empty() : tensor<2048x1280xf32>
    %1966 = linalg.fill ins(%cst : f32) outs(%1965 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1967 = tensor.empty() : tensor<2048x1280xf32>
    %1968 = linalg.fill ins(%cst : f32) outs(%1967 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1969:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1966, %1968, %1963, %1964, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1966, %1968)
    %1970 = arith.truncf %1969#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1971 = torch_c.from_builtin_tensor %1970 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1972 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1973 = torch.aten.view %1971, %1972 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %1974 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %1975 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %1976 = torch.aten.view %1943, %1975 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %1977 = torch_c.to_builtin_tensor %1976 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %1978 = torch_c.to_builtin_tensor %1974 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %1979 = tensor.empty() : tensor<2048x1280xf32>
    %1980 = linalg.fill ins(%cst : f32) outs(%1979 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1981 = tensor.empty() : tensor<2048x1280xf32>
    %1982 = linalg.fill ins(%cst : f32) outs(%1981 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %1983:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %1980, %1982, %1977, %1978, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%1980, %1982)
    %1984 = arith.truncf %1983#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %1985 = torch_c.from_builtin_tensor %1984 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %1986 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1987 = torch.aten.view %1985, %1986 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %1988 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1989 = torch.aten.view %1959, %1988 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1990 = torch.aten.transpose.int %1989, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1991 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1992 = torch.aten.view %1973, %1991 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1993 = torch.aten.transpose.int %1992, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1994 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1995 = torch.aten.view %1987, %1994 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %1996 = torch.aten.transpose.int %1995, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %1997:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1990, %1993, %1996, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %1998 = torch.aten.transpose.int %1997#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %1999 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2000 = torch.aten.view %1998, %1999 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2001 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2002 = torch.aten.view %2000, %2001 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2003 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2004 = torch.aten.transpose.int %2003, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %2005 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2006 = torch.prims.convert_element_type %2005, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2007 = torch.prims.convert_element_type %2002, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2008 = torch.prims.convert_element_type %2004, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2009 = torch.aten.mm %2007, %2008 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2010 = torch.aten.mul.Scalar %2009, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2011 = torch.aten.mul.Scalar %2006, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2012 = torch.aten.add.Tensor %2010, %2011, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2013 = torch.prims.convert_element_type %2012, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2014 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2015 = torch.aten.view %2013, %2014 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2016 = torch.aten.div.Scalar %2015, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2017 = torch.aten.add.Tensor %2016, %1932, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2018 = torch.prims.convert_element_type %2017, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2019 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_63, %result1_64 = torch.aten.var_mean.correction %2018, %2019, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2020 = torch.aten.add.Scalar %result0_63, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2021 = torch.aten.rsqrt %2020 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2022 = torch.aten.sub.Tensor %2017, %result1_64, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2023 = torch.aten.mul.Tensor %2022, %2021 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %2024 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2025 = torch.aten.mul.Tensor %2023, %2024 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %2026 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2027 = torch.aten.add.Tensor %2025, %2026, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2028 = torch.prims.convert_element_type %2027, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2029 = torch.prims.convert_element_type %result1_64, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2030 = torch.prims.convert_element_type %2021, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %2031 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2032 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2033 = torch.aten.view %2028, %2032 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2034 = torch_c.to_builtin_tensor %2033 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2035 = torch_c.to_builtin_tensor %2031 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2036 = tensor.empty() : tensor<2048x1280xf32>
    %2037 = linalg.fill ins(%cst : f32) outs(%2036 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2038 = tensor.empty() : tensor<2048x1280xf32>
    %2039 = linalg.fill ins(%cst : f32) outs(%2038 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2040:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2037, %2039, %2034, %2035, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2037, %2039)
    %2041 = arith.truncf %2040#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2042 = torch_c.from_builtin_tensor %2041 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2043 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2044 = torch.aten.view %2042, %2043 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %2045 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2046 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2047 = torch.aten.view %4, %2046 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2048 = torch_c.to_builtin_tensor %2047 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2049 = torch_c.to_builtin_tensor %2045 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2050 = tensor.empty() : tensor<128x1280xf32>
    %2051 = linalg.fill ins(%cst : f32) outs(%2050 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2052 = tensor.empty() : tensor<128x1280xf32>
    %2053 = linalg.fill ins(%cst : f32) outs(%2052 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2054:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2051, %2053, %2048, %2049, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2051, %2053)
    %2055 = arith.truncf %2054#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2056 = torch_c.from_builtin_tensor %2055 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2057 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2058 = torch.aten.view %2056, %2057 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %2059 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2060 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2061 = torch.aten.view %4, %2060 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2062 = torch_c.to_builtin_tensor %2061 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2063 = torch_c.to_builtin_tensor %2059 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2064 = tensor.empty() : tensor<128x1280xf32>
    %2065 = linalg.fill ins(%cst : f32) outs(%2064 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2066 = tensor.empty() : tensor<128x1280xf32>
    %2067 = linalg.fill ins(%cst : f32) outs(%2066 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2068:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2065, %2067, %2062, %2063, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2065, %2067)
    %2069 = arith.truncf %2068#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2070 = torch_c.from_builtin_tensor %2069 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2071 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2072 = torch.aten.view %2070, %2071 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2073 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2074 = torch.aten.view %2044, %2073 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2075 = torch.aten.transpose.int %2074, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2076 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2077 = torch.aten.view %2058, %2076 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2078 = torch.aten.transpose.int %2077, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2079 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2080 = torch.aten.view %2072, %2079 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2081 = torch.aten.transpose.int %2080, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2082:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2075, %2078, %2081, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2083 = torch.aten.transpose.int %2082#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2084 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2085 = torch.aten.view %2083, %2084 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2086 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2087 = torch.aten.view %2085, %2086 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2088 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2089 = torch.aten.transpose.int %2088, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %2090 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2091 = torch.prims.convert_element_type %2090, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2092 = torch.prims.convert_element_type %2087, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2093 = torch.prims.convert_element_type %2089, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2094 = torch.aten.mm %2092, %2093 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2095 = torch.aten.mul.Scalar %2094, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2096 = torch.aten.mul.Scalar %2091, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2097 = torch.aten.add.Tensor %2095, %2096, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2098 = torch.prims.convert_element_type %2097, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2099 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2100 = torch.aten.view %2098, %2099 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2101 = torch.aten.div.Scalar %2100, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2102 = torch.aten.add.Tensor %2101, %2017, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2103 = torch.prims.convert_element_type %2102, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2104 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_65, %result1_66 = torch.aten.var_mean.correction %2103, %2104, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2105 = torch.aten.add.Scalar %result0_65, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2106 = torch.aten.rsqrt %2105 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2107 = torch.aten.sub.Tensor %2102, %result1_66, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2108 = torch.aten.mul.Tensor %2107, %2106 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %2109 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2110 = torch.aten.mul.Tensor %2108, %2109 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %2111 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2112 = torch.aten.add.Tensor %2110, %2111, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2113 = torch.prims.convert_element_type %2112, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2114 = torch.prims.convert_element_type %result1_66, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2115 = torch.prims.convert_element_type %2106, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2116 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2117 = torch.aten.view %2113, %2116 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2118 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2119 = torch.aten.transpose.int %2118, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %2120 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2121 = torch.prims.convert_element_type %2120, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2122 = torch.prims.convert_element_type %2117, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2123 = torch.prims.convert_element_type %2119, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2124 = torch.aten.mm %2122, %2123 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2125 = torch.aten.mul.Scalar %2124, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2126 = torch.aten.mul.Scalar %2121, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2127 = torch.aten.add.Tensor %2125, %2126, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2128 = torch.prims.convert_element_type %2127, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2129 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2130 = torch.aten.view %2128, %2129 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2131 = torch.aten.slice.Tensor %2130, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2132 = torch.aten.slice.Tensor %2130, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2133 = torch.aten.gelu %2132, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2134 = torch.aten.mul.Tensor %2131, %2133 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2135 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2136 = torch.aten.view %2134, %2135 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %2137 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2138 = torch.aten.transpose.int %2137, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %2139 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2140 = torch.prims.convert_element_type %2139, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2141 = torch.prims.convert_element_type %2136, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2142 = torch.prims.convert_element_type %2138, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2143 = torch.aten.mm %2141, %2142 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2144 = torch.aten.mul.Scalar %2143, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2145 = torch.aten.mul.Scalar %2140, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2146 = torch.aten.add.Tensor %2144, %2145, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2147 = torch.prims.convert_element_type %2146, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2148 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2149 = torch.aten.view %2147, %2148 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2150 = torch.aten.add.Tensor %2149, %2102, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2151 = torch.prims.convert_element_type %2150, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2152 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_67, %result1_68 = torch.aten.var_mean.correction %2151, %2152, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2153 = torch.aten.add.Scalar %result0_67, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2154 = torch.aten.rsqrt %2153 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2155 = torch.aten.sub.Tensor %2150, %result1_68, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2156 = torch.aten.mul.Tensor %2155, %2154 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %2157 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2158 = torch.aten.mul.Tensor %2156, %2157 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %2159 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2160 = torch.aten.add.Tensor %2158, %2159, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2161 = torch.prims.convert_element_type %2160, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2162 = torch.prims.convert_element_type %result1_68, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2163 = torch.prims.convert_element_type %2154, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %2164 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2165 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2166 = torch.aten.view %2161, %2165 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2167 = torch_c.to_builtin_tensor %2166 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2168 = torch_c.to_builtin_tensor %2164 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2169 = tensor.empty() : tensor<2048x1280xf32>
    %2170 = linalg.fill ins(%cst : f32) outs(%2169 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2171 = tensor.empty() : tensor<2048x1280xf32>
    %2172 = linalg.fill ins(%cst : f32) outs(%2171 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2173:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2170, %2172, %2167, %2168, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2170, %2172)
    %2174 = arith.truncf %2173#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2175 = torch_c.from_builtin_tensor %2174 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2176 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2177 = torch.aten.view %2175, %2176 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %2178 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2179 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2180 = torch.aten.view %2161, %2179 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2181 = torch_c.to_builtin_tensor %2180 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2182 = torch_c.to_builtin_tensor %2178 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2183 = tensor.empty() : tensor<2048x1280xf32>
    %2184 = linalg.fill ins(%cst : f32) outs(%2183 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2185 = tensor.empty() : tensor<2048x1280xf32>
    %2186 = linalg.fill ins(%cst : f32) outs(%2185 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2187:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2184, %2186, %2181, %2182, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2184, %2186)
    %2188 = arith.truncf %2187#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2189 = torch_c.from_builtin_tensor %2188 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2190 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2191 = torch.aten.view %2189, %2190 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %2192 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2193 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2194 = torch.aten.view %2161, %2193 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2195 = torch_c.to_builtin_tensor %2194 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2196 = torch_c.to_builtin_tensor %2192 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2197 = tensor.empty() : tensor<2048x1280xf32>
    %2198 = linalg.fill ins(%cst : f32) outs(%2197 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2199 = tensor.empty() : tensor<2048x1280xf32>
    %2200 = linalg.fill ins(%cst : f32) outs(%2199 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2201:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2198, %2200, %2195, %2196, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2198, %2200)
    %2202 = arith.truncf %2201#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2203 = torch_c.from_builtin_tensor %2202 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2204 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2205 = torch.aten.view %2203, %2204 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2206 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2207 = torch.aten.view %2177, %2206 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2208 = torch.aten.transpose.int %2207, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2209 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2210 = torch.aten.view %2191, %2209 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2211 = torch.aten.transpose.int %2210, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2212 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2213 = torch.aten.view %2205, %2212 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2214 = torch.aten.transpose.int %2213, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2215:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2208, %2211, %2214, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2216 = torch.aten.transpose.int %2215#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2217 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2218 = torch.aten.view %2216, %2217 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2219 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2220 = torch.aten.view %2218, %2219 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2221 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2222 = torch.aten.transpose.int %2221, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %2223 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2224 = torch.prims.convert_element_type %2223, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2225 = torch.prims.convert_element_type %2220, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2226 = torch.prims.convert_element_type %2222, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2227 = torch.aten.mm %2225, %2226 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2228 = torch.aten.mul.Scalar %2227, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2229 = torch.aten.mul.Scalar %2224, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2230 = torch.aten.add.Tensor %2228, %2229, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2231 = torch.prims.convert_element_type %2230, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2232 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2233 = torch.aten.view %2231, %2232 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2234 = torch.aten.div.Scalar %2233, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2235 = torch.aten.add.Tensor %2234, %2150, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2236 = torch.prims.convert_element_type %2235, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2237 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_69, %result1_70 = torch.aten.var_mean.correction %2236, %2237, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2238 = torch.aten.add.Scalar %result0_69, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2239 = torch.aten.rsqrt %2238 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2240 = torch.aten.sub.Tensor %2235, %result1_70, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2241 = torch.aten.mul.Tensor %2240, %2239 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %2242 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2243 = torch.aten.mul.Tensor %2241, %2242 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %2244 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2245 = torch.aten.add.Tensor %2243, %2244, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2246 = torch.prims.convert_element_type %2245, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2247 = torch.prims.convert_element_type %result1_70, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2248 = torch.prims.convert_element_type %2239, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %2249 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2250 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2251 = torch.aten.view %2246, %2250 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2252 = torch_c.to_builtin_tensor %2251 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2253 = torch_c.to_builtin_tensor %2249 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2254 = tensor.empty() : tensor<2048x1280xf32>
    %2255 = linalg.fill ins(%cst : f32) outs(%2254 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2256 = tensor.empty() : tensor<2048x1280xf32>
    %2257 = linalg.fill ins(%cst : f32) outs(%2256 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2258:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2255, %2257, %2252, %2253, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2255, %2257)
    %2259 = arith.truncf %2258#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2260 = torch_c.from_builtin_tensor %2259 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2261 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2262 = torch.aten.view %2260, %2261 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %2263 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2264 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2265 = torch.aten.view %4, %2264 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2266 = torch_c.to_builtin_tensor %2265 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2267 = torch_c.to_builtin_tensor %2263 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2268 = tensor.empty() : tensor<128x1280xf32>
    %2269 = linalg.fill ins(%cst : f32) outs(%2268 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2270 = tensor.empty() : tensor<128x1280xf32>
    %2271 = linalg.fill ins(%cst : f32) outs(%2270 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2272:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2269, %2271, %2266, %2267, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2269, %2271)
    %2273 = arith.truncf %2272#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2274 = torch_c.from_builtin_tensor %2273 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2275 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2276 = torch.aten.view %2274, %2275 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %2277 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2278 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2279 = torch.aten.view %4, %2278 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2280 = torch_c.to_builtin_tensor %2279 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2281 = torch_c.to_builtin_tensor %2277 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2282 = tensor.empty() : tensor<128x1280xf32>
    %2283 = linalg.fill ins(%cst : f32) outs(%2282 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2284 = tensor.empty() : tensor<128x1280xf32>
    %2285 = linalg.fill ins(%cst : f32) outs(%2284 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2286:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2283, %2285, %2280, %2281, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2283, %2285)
    %2287 = arith.truncf %2286#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2288 = torch_c.from_builtin_tensor %2287 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2289 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2290 = torch.aten.view %2288, %2289 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2291 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2292 = torch.aten.view %2262, %2291 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2293 = torch.aten.transpose.int %2292, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2294 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2295 = torch.aten.view %2276, %2294 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2296 = torch.aten.transpose.int %2295, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2297 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2298 = torch.aten.view %2290, %2297 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2299 = torch.aten.transpose.int %2298, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2300:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2293, %2296, %2299, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2301 = torch.aten.transpose.int %2300#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2302 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2303 = torch.aten.view %2301, %2302 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2304 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2305 = torch.aten.view %2303, %2304 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2306 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2307 = torch.aten.transpose.int %2306, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %2308 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2309 = torch.prims.convert_element_type %2308, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2310 = torch.prims.convert_element_type %2305, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2311 = torch.prims.convert_element_type %2307, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2312 = torch.aten.mm %2310, %2311 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2313 = torch.aten.mul.Scalar %2312, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2314 = torch.aten.mul.Scalar %2309, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2315 = torch.aten.add.Tensor %2313, %2314, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2316 = torch.prims.convert_element_type %2315, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2317 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2318 = torch.aten.view %2316, %2317 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2319 = torch.aten.div.Scalar %2318, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2320 = torch.aten.add.Tensor %2319, %2235, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2321 = torch.prims.convert_element_type %2320, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2322 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_71, %result1_72 = torch.aten.var_mean.correction %2321, %2322, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2323 = torch.aten.add.Scalar %result0_71, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2324 = torch.aten.rsqrt %2323 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2325 = torch.aten.sub.Tensor %2320, %result1_72, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2326 = torch.aten.mul.Tensor %2325, %2324 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %2327 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2328 = torch.aten.mul.Tensor %2326, %2327 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %2329 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2330 = torch.aten.add.Tensor %2328, %2329, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2331 = torch.prims.convert_element_type %2330, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2332 = torch.prims.convert_element_type %result1_72, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2333 = torch.prims.convert_element_type %2324, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2334 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2335 = torch.aten.view %2331, %2334 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2336 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2337 = torch.aten.transpose.int %2336, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %2338 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2339 = torch.prims.convert_element_type %2338, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2340 = torch.prims.convert_element_type %2335, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2341 = torch.prims.convert_element_type %2337, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2342 = torch.aten.mm %2340, %2341 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2343 = torch.aten.mul.Scalar %2342, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2344 = torch.aten.mul.Scalar %2339, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2345 = torch.aten.add.Tensor %2343, %2344, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2346 = torch.prims.convert_element_type %2345, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2347 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2348 = torch.aten.view %2346, %2347 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2349 = torch.aten.slice.Tensor %2348, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2350 = torch.aten.slice.Tensor %2348, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2351 = torch.aten.gelu %2350, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2352 = torch.aten.mul.Tensor %2349, %2351 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2353 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2354 = torch.aten.view %2352, %2353 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %2355 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2356 = torch.aten.transpose.int %2355, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %2357 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2358 = torch.prims.convert_element_type %2357, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2359 = torch.prims.convert_element_type %2354, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2360 = torch.prims.convert_element_type %2356, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2361 = torch.aten.mm %2359, %2360 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2362 = torch.aten.mul.Scalar %2361, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2363 = torch.aten.mul.Scalar %2358, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2364 = torch.aten.add.Tensor %2362, %2363, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2365 = torch.prims.convert_element_type %2364, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2366 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2367 = torch.aten.view %2365, %2366 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2368 = torch.aten.add.Tensor %2367, %2320, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2369 = torch.prims.convert_element_type %2368, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2370 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_73, %result1_74 = torch.aten.var_mean.correction %2369, %2370, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2371 = torch.aten.add.Scalar %result0_73, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2372 = torch.aten.rsqrt %2371 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2373 = torch.aten.sub.Tensor %2368, %result1_74, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2374 = torch.aten.mul.Tensor %2373, %2372 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %2375 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2376 = torch.aten.mul.Tensor %2374, %2375 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %2377 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2378 = torch.aten.add.Tensor %2376, %2377, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2379 = torch.prims.convert_element_type %2378, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2380 = torch.prims.convert_element_type %result1_74, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2381 = torch.prims.convert_element_type %2372, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %2382 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2383 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2384 = torch.aten.view %2379, %2383 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2385 = torch_c.to_builtin_tensor %2384 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2386 = torch_c.to_builtin_tensor %2382 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2387 = tensor.empty() : tensor<2048x1280xf32>
    %2388 = linalg.fill ins(%cst : f32) outs(%2387 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2389 = tensor.empty() : tensor<2048x1280xf32>
    %2390 = linalg.fill ins(%cst : f32) outs(%2389 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2391:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2388, %2390, %2385, %2386, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2388, %2390)
    %2392 = arith.truncf %2391#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2393 = torch_c.from_builtin_tensor %2392 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2394 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2395 = torch.aten.view %2393, %2394 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %2396 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2397 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2398 = torch.aten.view %2379, %2397 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2399 = torch_c.to_builtin_tensor %2398 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2400 = torch_c.to_builtin_tensor %2396 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2401 = tensor.empty() : tensor<2048x1280xf32>
    %2402 = linalg.fill ins(%cst : f32) outs(%2401 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2403 = tensor.empty() : tensor<2048x1280xf32>
    %2404 = linalg.fill ins(%cst : f32) outs(%2403 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2405:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2402, %2404, %2399, %2400, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2402, %2404)
    %2406 = arith.truncf %2405#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2407 = torch_c.from_builtin_tensor %2406 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2408 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2409 = torch.aten.view %2407, %2408 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %2410 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2411 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2412 = torch.aten.view %2379, %2411 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2413 = torch_c.to_builtin_tensor %2412 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2414 = torch_c.to_builtin_tensor %2410 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2415 = tensor.empty() : tensor<2048x1280xf32>
    %2416 = linalg.fill ins(%cst : f32) outs(%2415 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2417 = tensor.empty() : tensor<2048x1280xf32>
    %2418 = linalg.fill ins(%cst : f32) outs(%2417 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2419:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2416, %2418, %2413, %2414, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2416, %2418)
    %2420 = arith.truncf %2419#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2421 = torch_c.from_builtin_tensor %2420 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2422 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2423 = torch.aten.view %2421, %2422 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2424 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2425 = torch.aten.view %2395, %2424 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2426 = torch.aten.transpose.int %2425, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2427 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2428 = torch.aten.view %2409, %2427 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2429 = torch.aten.transpose.int %2428, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2430 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2431 = torch.aten.view %2423, %2430 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2432 = torch.aten.transpose.int %2431, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2433:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2426, %2429, %2432, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2434 = torch.aten.transpose.int %2433#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2435 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2436 = torch.aten.view %2434, %2435 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2437 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2438 = torch.aten.view %2436, %2437 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2439 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2440 = torch.aten.transpose.int %2439, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %2441 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2442 = torch.prims.convert_element_type %2441, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2443 = torch.prims.convert_element_type %2438, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2444 = torch.prims.convert_element_type %2440, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2445 = torch.aten.mm %2443, %2444 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2446 = torch.aten.mul.Scalar %2445, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2447 = torch.aten.mul.Scalar %2442, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2448 = torch.aten.add.Tensor %2446, %2447, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2449 = torch.prims.convert_element_type %2448, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2450 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2451 = torch.aten.view %2449, %2450 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2452 = torch.aten.div.Scalar %2451, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2453 = torch.aten.add.Tensor %2452, %2368, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2454 = torch.prims.convert_element_type %2453, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2455 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_75, %result1_76 = torch.aten.var_mean.correction %2454, %2455, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2456 = torch.aten.add.Scalar %result0_75, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2457 = torch.aten.rsqrt %2456 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2458 = torch.aten.sub.Tensor %2453, %result1_76, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2459 = torch.aten.mul.Tensor %2458, %2457 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %2460 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2461 = torch.aten.mul.Tensor %2459, %2460 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %2462 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2463 = torch.aten.add.Tensor %2461, %2462, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2464 = torch.prims.convert_element_type %2463, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2465 = torch.prims.convert_element_type %result1_76, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2466 = torch.prims.convert_element_type %2457, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %2467 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2468 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2469 = torch.aten.view %2464, %2468 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2470 = torch_c.to_builtin_tensor %2469 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2471 = torch_c.to_builtin_tensor %2467 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2472 = tensor.empty() : tensor<2048x1280xf32>
    %2473 = linalg.fill ins(%cst : f32) outs(%2472 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2474 = tensor.empty() : tensor<2048x1280xf32>
    %2475 = linalg.fill ins(%cst : f32) outs(%2474 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2476:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2473, %2475, %2470, %2471, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2473, %2475)
    %2477 = arith.truncf %2476#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2478 = torch_c.from_builtin_tensor %2477 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2479 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2480 = torch.aten.view %2478, %2479 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %2481 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2482 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2483 = torch.aten.view %4, %2482 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2484 = torch_c.to_builtin_tensor %2483 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2485 = torch_c.to_builtin_tensor %2481 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2486 = tensor.empty() : tensor<128x1280xf32>
    %2487 = linalg.fill ins(%cst : f32) outs(%2486 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2488 = tensor.empty() : tensor<128x1280xf32>
    %2489 = linalg.fill ins(%cst : f32) outs(%2488 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2490:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2487, %2489, %2484, %2485, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2487, %2489)
    %2491 = arith.truncf %2490#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2492 = torch_c.from_builtin_tensor %2491 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2493 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2494 = torch.aten.view %2492, %2493 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %2495 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2496 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2497 = torch.aten.view %4, %2496 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2498 = torch_c.to_builtin_tensor %2497 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2499 = torch_c.to_builtin_tensor %2495 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2500 = tensor.empty() : tensor<128x1280xf32>
    %2501 = linalg.fill ins(%cst : f32) outs(%2500 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2502 = tensor.empty() : tensor<128x1280xf32>
    %2503 = linalg.fill ins(%cst : f32) outs(%2502 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2504:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2501, %2503, %2498, %2499, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2501, %2503)
    %2505 = arith.truncf %2504#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2506 = torch_c.from_builtin_tensor %2505 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2507 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2508 = torch.aten.view %2506, %2507 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2509 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2510 = torch.aten.view %2480, %2509 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2511 = torch.aten.transpose.int %2510, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2512 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2513 = torch.aten.view %2494, %2512 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2514 = torch.aten.transpose.int %2513, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2515 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2516 = torch.aten.view %2508, %2515 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2517 = torch.aten.transpose.int %2516, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2518:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2511, %2514, %2517, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2519 = torch.aten.transpose.int %2518#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2520 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2521 = torch.aten.view %2519, %2520 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2522 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2523 = torch.aten.view %2521, %2522 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2524 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2525 = torch.aten.transpose.int %2524, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %2526 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2527 = torch.prims.convert_element_type %2526, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2528 = torch.prims.convert_element_type %2523, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2529 = torch.prims.convert_element_type %2525, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2530 = torch.aten.mm %2528, %2529 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2531 = torch.aten.mul.Scalar %2530, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2532 = torch.aten.mul.Scalar %2527, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2533 = torch.aten.add.Tensor %2531, %2532, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2534 = torch.prims.convert_element_type %2533, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2535 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2536 = torch.aten.view %2534, %2535 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2537 = torch.aten.div.Scalar %2536, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2538 = torch.aten.add.Tensor %2537, %2453, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2539 = torch.prims.convert_element_type %2538, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2540 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_77, %result1_78 = torch.aten.var_mean.correction %2539, %2540, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2541 = torch.aten.add.Scalar %result0_77, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2542 = torch.aten.rsqrt %2541 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2543 = torch.aten.sub.Tensor %2538, %result1_78, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2544 = torch.aten.mul.Tensor %2543, %2542 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %2545 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2546 = torch.aten.mul.Tensor %2544, %2545 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %2547 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2548 = torch.aten.add.Tensor %2546, %2547, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2549 = torch.prims.convert_element_type %2548, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2550 = torch.prims.convert_element_type %result1_78, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2551 = torch.prims.convert_element_type %2542, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2552 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2553 = torch.aten.view %2549, %2552 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2554 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2555 = torch.aten.transpose.int %2554, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %2556 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2557 = torch.prims.convert_element_type %2556, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2558 = torch.prims.convert_element_type %2553, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2559 = torch.prims.convert_element_type %2555, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2560 = torch.aten.mm %2558, %2559 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2561 = torch.aten.mul.Scalar %2560, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2562 = torch.aten.mul.Scalar %2557, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2563 = torch.aten.add.Tensor %2561, %2562, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2564 = torch.prims.convert_element_type %2563, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2565 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2566 = torch.aten.view %2564, %2565 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2567 = torch.aten.slice.Tensor %2566, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2568 = torch.aten.slice.Tensor %2566, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2569 = torch.aten.gelu %2568, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2570 = torch.aten.mul.Tensor %2567, %2569 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2571 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2572 = torch.aten.view %2570, %2571 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %2573 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2574 = torch.aten.transpose.int %2573, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %2575 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2576 = torch.prims.convert_element_type %2575, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2577 = torch.prims.convert_element_type %2572, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2578 = torch.prims.convert_element_type %2574, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2579 = torch.aten.mm %2577, %2578 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2580 = torch.aten.mul.Scalar %2579, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2581 = torch.aten.mul.Scalar %2576, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2582 = torch.aten.add.Tensor %2580, %2581, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2583 = torch.prims.convert_element_type %2582, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2584 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2585 = torch.aten.view %2583, %2584 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2586 = torch.aten.add.Tensor %2585, %2538, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2587 = torch.prims.convert_element_type %2586, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2588 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_79, %result1_80 = torch.aten.var_mean.correction %2587, %2588, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2589 = torch.aten.add.Scalar %result0_79, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2590 = torch.aten.rsqrt %2589 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2591 = torch.aten.sub.Tensor %2586, %result1_80, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2592 = torch.aten.mul.Tensor %2591, %2590 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %2593 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2594 = torch.aten.mul.Tensor %2592, %2593 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %2595 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2596 = torch.aten.add.Tensor %2594, %2595, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2597 = torch.prims.convert_element_type %2596, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2598 = torch.prims.convert_element_type %result1_80, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2599 = torch.prims.convert_element_type %2590, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %2600 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2601 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2602 = torch.aten.view %2597, %2601 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2603 = torch_c.to_builtin_tensor %2602 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2604 = torch_c.to_builtin_tensor %2600 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2605 = tensor.empty() : tensor<2048x1280xf32>
    %2606 = linalg.fill ins(%cst : f32) outs(%2605 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2607 = tensor.empty() : tensor<2048x1280xf32>
    %2608 = linalg.fill ins(%cst : f32) outs(%2607 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2609:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2606, %2608, %2603, %2604, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2606, %2608)
    %2610 = arith.truncf %2609#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2611 = torch_c.from_builtin_tensor %2610 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2612 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2613 = torch.aten.view %2611, %2612 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %2614 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2615 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2616 = torch.aten.view %2597, %2615 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2617 = torch_c.to_builtin_tensor %2616 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2618 = torch_c.to_builtin_tensor %2614 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2619 = tensor.empty() : tensor<2048x1280xf32>
    %2620 = linalg.fill ins(%cst : f32) outs(%2619 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2621 = tensor.empty() : tensor<2048x1280xf32>
    %2622 = linalg.fill ins(%cst : f32) outs(%2621 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2623:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2620, %2622, %2617, %2618, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2620, %2622)
    %2624 = arith.truncf %2623#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2625 = torch_c.from_builtin_tensor %2624 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2626 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2627 = torch.aten.view %2625, %2626 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %2628 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2629 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2630 = torch.aten.view %2597, %2629 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2631 = torch_c.to_builtin_tensor %2630 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2632 = torch_c.to_builtin_tensor %2628 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2633 = tensor.empty() : tensor<2048x1280xf32>
    %2634 = linalg.fill ins(%cst : f32) outs(%2633 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2635 = tensor.empty() : tensor<2048x1280xf32>
    %2636 = linalg.fill ins(%cst : f32) outs(%2635 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2637:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2634, %2636, %2631, %2632, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2634, %2636)
    %2638 = arith.truncf %2637#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2639 = torch_c.from_builtin_tensor %2638 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2640 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2641 = torch.aten.view %2639, %2640 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2642 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2643 = torch.aten.view %2613, %2642 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2644 = torch.aten.transpose.int %2643, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2645 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2646 = torch.aten.view %2627, %2645 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2647 = torch.aten.transpose.int %2646, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2648 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2649 = torch.aten.view %2641, %2648 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2650 = torch.aten.transpose.int %2649, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2651:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2644, %2647, %2650, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2652 = torch.aten.transpose.int %2651#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2653 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2654 = torch.aten.view %2652, %2653 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2655 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2656 = torch.aten.view %2654, %2655 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2657 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2658 = torch.aten.transpose.int %2657, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %2659 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2660 = torch.prims.convert_element_type %2659, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2661 = torch.prims.convert_element_type %2656, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2662 = torch.prims.convert_element_type %2658, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2663 = torch.aten.mm %2661, %2662 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2664 = torch.aten.mul.Scalar %2663, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2665 = torch.aten.mul.Scalar %2660, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2666 = torch.aten.add.Tensor %2664, %2665, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2667 = torch.prims.convert_element_type %2666, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2668 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2669 = torch.aten.view %2667, %2668 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2670 = torch.aten.div.Scalar %2669, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2671 = torch.aten.add.Tensor %2670, %2586, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2672 = torch.prims.convert_element_type %2671, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2673 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_81, %result1_82 = torch.aten.var_mean.correction %2672, %2673, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2674 = torch.aten.add.Scalar %result0_81, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2675 = torch.aten.rsqrt %2674 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2676 = torch.aten.sub.Tensor %2671, %result1_82, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2677 = torch.aten.mul.Tensor %2676, %2675 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %2678 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2679 = torch.aten.mul.Tensor %2677, %2678 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %2680 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2681 = torch.aten.add.Tensor %2679, %2680, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2682 = torch.prims.convert_element_type %2681, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2683 = torch.prims.convert_element_type %result1_82, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2684 = torch.prims.convert_element_type %2675, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %2685 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2686 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2687 = torch.aten.view %2682, %2686 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2688 = torch_c.to_builtin_tensor %2687 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2689 = torch_c.to_builtin_tensor %2685 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2690 = tensor.empty() : tensor<2048x1280xf32>
    %2691 = linalg.fill ins(%cst : f32) outs(%2690 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2692 = tensor.empty() : tensor<2048x1280xf32>
    %2693 = linalg.fill ins(%cst : f32) outs(%2692 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2694:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2691, %2693, %2688, %2689, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2691, %2693)
    %2695 = arith.truncf %2694#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2696 = torch_c.from_builtin_tensor %2695 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2697 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2698 = torch.aten.view %2696, %2697 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %2699 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2700 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2701 = torch.aten.view %4, %2700 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2702 = torch_c.to_builtin_tensor %2701 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2703 = torch_c.to_builtin_tensor %2699 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2704 = tensor.empty() : tensor<128x1280xf32>
    %2705 = linalg.fill ins(%cst : f32) outs(%2704 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2706 = tensor.empty() : tensor<128x1280xf32>
    %2707 = linalg.fill ins(%cst : f32) outs(%2706 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2708:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2705, %2707, %2702, %2703, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2705, %2707)
    %2709 = arith.truncf %2708#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2710 = torch_c.from_builtin_tensor %2709 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2711 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2712 = torch.aten.view %2710, %2711 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %2713 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2714 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2715 = torch.aten.view %4, %2714 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2716 = torch_c.to_builtin_tensor %2715 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2717 = torch_c.to_builtin_tensor %2713 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2718 = tensor.empty() : tensor<128x1280xf32>
    %2719 = linalg.fill ins(%cst : f32) outs(%2718 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2720 = tensor.empty() : tensor<128x1280xf32>
    %2721 = linalg.fill ins(%cst : f32) outs(%2720 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2722:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2719, %2721, %2716, %2717, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2719, %2721)
    %2723 = arith.truncf %2722#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2724 = torch_c.from_builtin_tensor %2723 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2725 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2726 = torch.aten.view %2724, %2725 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2727 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2728 = torch.aten.view %2698, %2727 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2729 = torch.aten.transpose.int %2728, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2730 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2731 = torch.aten.view %2712, %2730 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2732 = torch.aten.transpose.int %2731, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2733 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2734 = torch.aten.view %2726, %2733 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2735 = torch.aten.transpose.int %2734, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2736:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2729, %2732, %2735, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2737 = torch.aten.transpose.int %2736#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2738 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2739 = torch.aten.view %2737, %2738 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2740 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2741 = torch.aten.view %2739, %2740 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2742 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2743 = torch.aten.transpose.int %2742, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %2744 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2745 = torch.prims.convert_element_type %2744, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2746 = torch.prims.convert_element_type %2741, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2747 = torch.prims.convert_element_type %2743, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2748 = torch.aten.mm %2746, %2747 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2749 = torch.aten.mul.Scalar %2748, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2750 = torch.aten.mul.Scalar %2745, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2751 = torch.aten.add.Tensor %2749, %2750, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2752 = torch.prims.convert_element_type %2751, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2753 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2754 = torch.aten.view %2752, %2753 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2755 = torch.aten.div.Scalar %2754, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2756 = torch.aten.add.Tensor %2755, %2671, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2757 = torch.prims.convert_element_type %2756, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2758 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_83, %result1_84 = torch.aten.var_mean.correction %2757, %2758, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2759 = torch.aten.add.Scalar %result0_83, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2760 = torch.aten.rsqrt %2759 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2761 = torch.aten.sub.Tensor %2756, %result1_84, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2762 = torch.aten.mul.Tensor %2761, %2760 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %2763 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2764 = torch.aten.mul.Tensor %2762, %2763 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %2765 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2766 = torch.aten.add.Tensor %2764, %2765, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2767 = torch.prims.convert_element_type %2766, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2768 = torch.prims.convert_element_type %result1_84, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2769 = torch.prims.convert_element_type %2760, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2770 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2771 = torch.aten.view %2767, %2770 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2772 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2773 = torch.aten.transpose.int %2772, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %2774 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2775 = torch.prims.convert_element_type %2774, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2776 = torch.prims.convert_element_type %2771, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2777 = torch.prims.convert_element_type %2773, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2778 = torch.aten.mm %2776, %2777 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2779 = torch.aten.mul.Scalar %2778, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2780 = torch.aten.mul.Scalar %2775, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2781 = torch.aten.add.Tensor %2779, %2780, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2782 = torch.prims.convert_element_type %2781, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %2783 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2784 = torch.aten.view %2782, %2783 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %2785 = torch.aten.slice.Tensor %2784, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2786 = torch.aten.slice.Tensor %2784, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %2787 = torch.aten.gelu %2786, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %2788 = torch.aten.mul.Tensor %2785, %2787 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %2789 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %2790 = torch.aten.view %2788, %2789 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %2791 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %2792 = torch.aten.transpose.int %2791, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %2793 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2794 = torch.prims.convert_element_type %2793, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2795 = torch.prims.convert_element_type %2790, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %2796 = torch.prims.convert_element_type %2792, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2797 = torch.aten.mm %2795, %2796 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2798 = torch.aten.mul.Scalar %2797, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2799 = torch.aten.mul.Scalar %2794, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2800 = torch.aten.add.Tensor %2798, %2799, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2801 = torch.prims.convert_element_type %2800, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2802 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2803 = torch.aten.view %2801, %2802 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2804 = torch.aten.add.Tensor %2803, %2756, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2805 = torch.prims.convert_element_type %2804, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2806 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_85, %result1_86 = torch.aten.var_mean.correction %2805, %2806, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2807 = torch.aten.add.Scalar %result0_85, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2808 = torch.aten.rsqrt %2807 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2809 = torch.aten.sub.Tensor %2804, %result1_86, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2810 = torch.aten.mul.Tensor %2809, %2808 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %2811 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2812 = torch.aten.mul.Tensor %2810, %2811 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %2813 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2814 = torch.aten.add.Tensor %2812, %2813, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2815 = torch.prims.convert_element_type %2814, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2816 = torch.prims.convert_element_type %result1_86, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2817 = torch.prims.convert_element_type %2808, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %2818 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2819 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2820 = torch.aten.view %2815, %2819 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2821 = torch_c.to_builtin_tensor %2820 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2822 = torch_c.to_builtin_tensor %2818 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2823 = tensor.empty() : tensor<2048x1280xf32>
    %2824 = linalg.fill ins(%cst : f32) outs(%2823 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2825 = tensor.empty() : tensor<2048x1280xf32>
    %2826 = linalg.fill ins(%cst : f32) outs(%2825 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2827:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2824, %2826, %2821, %2822, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2824, %2826)
    %2828 = arith.truncf %2827#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2829 = torch_c.from_builtin_tensor %2828 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2830 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2831 = torch.aten.view %2829, %2830 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %2832 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2833 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2834 = torch.aten.view %2815, %2833 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2835 = torch_c.to_builtin_tensor %2834 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2836 = torch_c.to_builtin_tensor %2832 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2837 = tensor.empty() : tensor<2048x1280xf32>
    %2838 = linalg.fill ins(%cst : f32) outs(%2837 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2839 = tensor.empty() : tensor<2048x1280xf32>
    %2840 = linalg.fill ins(%cst : f32) outs(%2839 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2841:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2838, %2840, %2835, %2836, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2838, %2840)
    %2842 = arith.truncf %2841#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2843 = torch_c.from_builtin_tensor %2842 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2844 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2845 = torch.aten.view %2843, %2844 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %2846 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2847 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2848 = torch.aten.view %2815, %2847 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2849 = torch_c.to_builtin_tensor %2848 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2850 = torch_c.to_builtin_tensor %2846 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2851 = tensor.empty() : tensor<2048x1280xf32>
    %2852 = linalg.fill ins(%cst : f32) outs(%2851 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2853 = tensor.empty() : tensor<2048x1280xf32>
    %2854 = linalg.fill ins(%cst : f32) outs(%2853 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2855:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2852, %2854, %2849, %2850, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2852, %2854)
    %2856 = arith.truncf %2855#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2857 = torch_c.from_builtin_tensor %2856 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2858 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2859 = torch.aten.view %2857, %2858 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2860 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2861 = torch.aten.view %2831, %2860 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2862 = torch.aten.transpose.int %2861, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2863 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2864 = torch.aten.view %2845, %2863 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2865 = torch.aten.transpose.int %2864, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2866 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2867 = torch.aten.view %2859, %2866 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2868 = torch.aten.transpose.int %2867, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2869:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2862, %2865, %2868, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2870 = torch.aten.transpose.int %2869#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2871 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2872 = torch.aten.view %2870, %2871 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2873 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2874 = torch.aten.view %2872, %2873 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2875 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2876 = torch.aten.transpose.int %2875, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %2877 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2878 = torch.prims.convert_element_type %2877, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2879 = torch.prims.convert_element_type %2874, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2880 = torch.prims.convert_element_type %2876, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2881 = torch.aten.mm %2879, %2880 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2882 = torch.aten.mul.Scalar %2881, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2883 = torch.aten.mul.Scalar %2878, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2884 = torch.aten.add.Tensor %2882, %2883, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2885 = torch.prims.convert_element_type %2884, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2886 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2887 = torch.aten.view %2885, %2886 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2888 = torch.aten.div.Scalar %2887, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2889 = torch.aten.add.Tensor %2888, %2804, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2890 = torch.prims.convert_element_type %2889, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2891 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_87, %result1_88 = torch.aten.var_mean.correction %2890, %2891, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2892 = torch.aten.add.Scalar %result0_87, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2893 = torch.aten.rsqrt %2892 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2894 = torch.aten.sub.Tensor %2889, %result1_88, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2895 = torch.aten.mul.Tensor %2894, %2893 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %2896 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2897 = torch.aten.mul.Tensor %2895, %2896 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %2898 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2899 = torch.aten.add.Tensor %2897, %2898, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2900 = torch.prims.convert_element_type %2899, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2901 = torch.prims.convert_element_type %result1_88, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2902 = torch.prims.convert_element_type %2893, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %2903 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2904 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2905 = torch.aten.view %2900, %2904 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %2906 = torch_c.to_builtin_tensor %2905 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %2907 = torch_c.to_builtin_tensor %2903 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %2908 = tensor.empty() : tensor<2048x1280xf32>
    %2909 = linalg.fill ins(%cst : f32) outs(%2908 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2910 = tensor.empty() : tensor<2048x1280xf32>
    %2911 = linalg.fill ins(%cst : f32) outs(%2910 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %2912:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %2909, %2911, %2906, %2907, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2909, %2911)
    %2913 = arith.truncf %2912#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %2914 = torch_c.from_builtin_tensor %2913 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %2915 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2916 = torch.aten.view %2914, %2915 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %2917 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2918 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2919 = torch.aten.view %4, %2918 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2920 = torch_c.to_builtin_tensor %2919 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2921 = torch_c.to_builtin_tensor %2917 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2922 = tensor.empty() : tensor<128x1280xf32>
    %2923 = linalg.fill ins(%cst : f32) outs(%2922 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2924 = tensor.empty() : tensor<128x1280xf32>
    %2925 = linalg.fill ins(%cst : f32) outs(%2924 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2926:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2923, %2925, %2920, %2921, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2923, %2925)
    %2927 = arith.truncf %2926#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2928 = torch_c.from_builtin_tensor %2927 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2929 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2930 = torch.aten.view %2928, %2929 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %2931 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %2932 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %2933 = torch.aten.view %4, %2932 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %2934 = torch_c.to_builtin_tensor %2933 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %2935 = torch_c.to_builtin_tensor %2931 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %2936 = tensor.empty() : tensor<128x1280xf32>
    %2937 = linalg.fill ins(%cst : f32) outs(%2936 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2938 = tensor.empty() : tensor<128x1280xf32>
    %2939 = linalg.fill ins(%cst : f32) outs(%2938 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %2940:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %2937, %2939, %2934, %2935, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%2937, %2939)
    %2941 = arith.truncf %2940#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %2942 = torch_c.from_builtin_tensor %2941 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %2943 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2944 = torch.aten.view %2942, %2943 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %2945 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2946 = torch.aten.view %2916, %2945 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %2947 = torch.aten.transpose.int %2946, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %2948 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2949 = torch.aten.view %2930, %2948 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2950 = torch.aten.transpose.int %2949, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2951 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2952 = torch.aten.view %2944, %2951 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %2953 = torch.aten.transpose.int %2952, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %2954:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2947, %2950, %2953, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %2955 = torch.aten.transpose.int %2954#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %2956 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2957 = torch.aten.view %2955, %2956 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2958 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2959 = torch.aten.view %2957, %2958 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2960 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %2961 = torch.aten.transpose.int %2960, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %2962 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2963 = torch.prims.convert_element_type %2962, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %2964 = torch.prims.convert_element_type %2959, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2965 = torch.prims.convert_element_type %2961, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2966 = torch.aten.mm %2964, %2965 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %2967 = torch.aten.mul.Scalar %2966, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2968 = torch.aten.mul.Scalar %2963, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %2969 = torch.aten.add.Tensor %2967, %2968, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2970 = torch.prims.convert_element_type %2969, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %2971 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2972 = torch.aten.view %2970, %2971 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %2973 = torch.aten.div.Scalar %2972, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %2974 = torch.aten.add.Tensor %2973, %2889, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2975 = torch.prims.convert_element_type %2974, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2976 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_89, %result1_90 = torch.aten.var_mean.correction %2975, %2976, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %2977 = torch.aten.add.Scalar %result0_89, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %2978 = torch.aten.rsqrt %2977 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %2979 = torch.aten.sub.Tensor %2974, %result1_90, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2980 = torch.aten.mul.Tensor %2979, %2978 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %2981 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2982 = torch.aten.mul.Tensor %2980, %2981 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %2983 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2984 = torch.aten.add.Tensor %2982, %2983, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %2985 = torch.prims.convert_element_type %2984, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %2986 = torch.prims.convert_element_type %result1_90, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2987 = torch.prims.convert_element_type %2978, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %2988 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %2989 = torch.aten.view %2985, %2988 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2990 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %2991 = torch.aten.transpose.int %2990, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %2992 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %2993 = torch.prims.convert_element_type %2992, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %2994 = torch.prims.convert_element_type %2989, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %2995 = torch.prims.convert_element_type %2991, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2996 = torch.aten.mm %2994, %2995 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %2997 = torch.aten.mul.Scalar %2996, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %2998 = torch.aten.mul.Scalar %2993, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %2999 = torch.aten.add.Tensor %2997, %2998, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3000 = torch.prims.convert_element_type %2999, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3001 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3002 = torch.aten.view %3000, %3001 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3003 = torch.aten.slice.Tensor %3002, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3004 = torch.aten.slice.Tensor %3002, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3005 = torch.aten.gelu %3004, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3006 = torch.aten.mul.Tensor %3003, %3005 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3007 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3008 = torch.aten.view %3006, %3007 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %3009 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3010 = torch.aten.transpose.int %3009, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %3011 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3012 = torch.prims.convert_element_type %3011, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3013 = torch.prims.convert_element_type %3008, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3014 = torch.prims.convert_element_type %3010, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3015 = torch.aten.mm %3013, %3014 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3016 = torch.aten.mul.Scalar %3015, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3017 = torch.aten.mul.Scalar %3012, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3018 = torch.aten.add.Tensor %3016, %3017, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3019 = torch.prims.convert_element_type %3018, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3020 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3021 = torch.aten.view %3019, %3020 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3022 = torch.aten.add.Tensor %3021, %2974, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3023 = torch.prims.convert_element_type %3022, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3024 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_91, %result1_92 = torch.aten.var_mean.correction %3023, %3024, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3025 = torch.aten.add.Scalar %result0_91, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3026 = torch.aten.rsqrt %3025 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3027 = torch.aten.sub.Tensor %3022, %result1_92, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3028 = torch.aten.mul.Tensor %3027, %3026 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %3029 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3030 = torch.aten.mul.Tensor %3028, %3029 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %3031 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3032 = torch.aten.add.Tensor %3030, %3031, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3033 = torch.prims.convert_element_type %3032, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3034 = torch.prims.convert_element_type %result1_92, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3035 = torch.prims.convert_element_type %3026, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %3036 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3037 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3038 = torch.aten.view %3033, %3037 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3039 = torch_c.to_builtin_tensor %3038 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3040 = torch_c.to_builtin_tensor %3036 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3041 = tensor.empty() : tensor<2048x1280xf32>
    %3042 = linalg.fill ins(%cst : f32) outs(%3041 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3043 = tensor.empty() : tensor<2048x1280xf32>
    %3044 = linalg.fill ins(%cst : f32) outs(%3043 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3045:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3042, %3044, %3039, %3040, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3042, %3044)
    %3046 = arith.truncf %3045#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3047 = torch_c.from_builtin_tensor %3046 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3048 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3049 = torch.aten.view %3047, %3048 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %3050 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3051 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3052 = torch.aten.view %3033, %3051 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3053 = torch_c.to_builtin_tensor %3052 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3054 = torch_c.to_builtin_tensor %3050 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3055 = tensor.empty() : tensor<2048x1280xf32>
    %3056 = linalg.fill ins(%cst : f32) outs(%3055 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3057 = tensor.empty() : tensor<2048x1280xf32>
    %3058 = linalg.fill ins(%cst : f32) outs(%3057 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3059:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3056, %3058, %3053, %3054, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3056, %3058)
    %3060 = arith.truncf %3059#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3061 = torch_c.from_builtin_tensor %3060 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3062 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3063 = torch.aten.view %3061, %3062 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %3064 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3065 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3066 = torch.aten.view %3033, %3065 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3067 = torch_c.to_builtin_tensor %3066 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3068 = torch_c.to_builtin_tensor %3064 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3069 = tensor.empty() : tensor<2048x1280xf32>
    %3070 = linalg.fill ins(%cst : f32) outs(%3069 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3071 = tensor.empty() : tensor<2048x1280xf32>
    %3072 = linalg.fill ins(%cst : f32) outs(%3071 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3073:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3070, %3072, %3067, %3068, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3070, %3072)
    %3074 = arith.truncf %3073#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3075 = torch_c.from_builtin_tensor %3074 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3076 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3077 = torch.aten.view %3075, %3076 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3078 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3079 = torch.aten.view %3049, %3078 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3080 = torch.aten.transpose.int %3079, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3081 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3082 = torch.aten.view %3063, %3081 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3083 = torch.aten.transpose.int %3082, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3084 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3085 = torch.aten.view %3077, %3084 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3086 = torch.aten.transpose.int %3085, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3087:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3080, %3083, %3086, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3088 = torch.aten.transpose.int %3087#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3089 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3090 = torch.aten.view %3088, %3089 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3091 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3092 = torch.aten.view %3090, %3091 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3093 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3094 = torch.aten.transpose.int %3093, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %3095 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3096 = torch.prims.convert_element_type %3095, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3097 = torch.prims.convert_element_type %3092, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3098 = torch.prims.convert_element_type %3094, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3099 = torch.aten.mm %3097, %3098 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3100 = torch.aten.mul.Scalar %3099, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3101 = torch.aten.mul.Scalar %3096, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3102 = torch.aten.add.Tensor %3100, %3101, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3103 = torch.prims.convert_element_type %3102, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3104 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3105 = torch.aten.view %3103, %3104 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3106 = torch.aten.div.Scalar %3105, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3107 = torch.aten.add.Tensor %3106, %3022, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3108 = torch.prims.convert_element_type %3107, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3109 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_93, %result1_94 = torch.aten.var_mean.correction %3108, %3109, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3110 = torch.aten.add.Scalar %result0_93, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3111 = torch.aten.rsqrt %3110 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3112 = torch.aten.sub.Tensor %3107, %result1_94, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3113 = torch.aten.mul.Tensor %3112, %3111 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %3114 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3115 = torch.aten.mul.Tensor %3113, %3114 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %3116 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3117 = torch.aten.add.Tensor %3115, %3116, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3118 = torch.prims.convert_element_type %3117, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3119 = torch.prims.convert_element_type %result1_94, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3120 = torch.prims.convert_element_type %3111, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %3121 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3122 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3123 = torch.aten.view %3118, %3122 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3124 = torch_c.to_builtin_tensor %3123 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3125 = torch_c.to_builtin_tensor %3121 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3126 = tensor.empty() : tensor<2048x1280xf32>
    %3127 = linalg.fill ins(%cst : f32) outs(%3126 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3128 = tensor.empty() : tensor<2048x1280xf32>
    %3129 = linalg.fill ins(%cst : f32) outs(%3128 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3130:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3127, %3129, %3124, %3125, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3127, %3129)
    %3131 = arith.truncf %3130#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3132 = torch_c.from_builtin_tensor %3131 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3133 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3134 = torch.aten.view %3132, %3133 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %3135 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3136 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3137 = torch.aten.view %4, %3136 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3138 = torch_c.to_builtin_tensor %3137 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3139 = torch_c.to_builtin_tensor %3135 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3140 = tensor.empty() : tensor<128x1280xf32>
    %3141 = linalg.fill ins(%cst : f32) outs(%3140 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3142 = tensor.empty() : tensor<128x1280xf32>
    %3143 = linalg.fill ins(%cst : f32) outs(%3142 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3144:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3141, %3143, %3138, %3139, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3141, %3143)
    %3145 = arith.truncf %3144#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3146 = torch_c.from_builtin_tensor %3145 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3147 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3148 = torch.aten.view %3146, %3147 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %3149 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3150 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3151 = torch.aten.view %4, %3150 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3152 = torch_c.to_builtin_tensor %3151 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3153 = torch_c.to_builtin_tensor %3149 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3154 = tensor.empty() : tensor<128x1280xf32>
    %3155 = linalg.fill ins(%cst : f32) outs(%3154 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3156 = tensor.empty() : tensor<128x1280xf32>
    %3157 = linalg.fill ins(%cst : f32) outs(%3156 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3158:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3155, %3157, %3152, %3153, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3155, %3157)
    %3159 = arith.truncf %3158#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3160 = torch_c.from_builtin_tensor %3159 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3161 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3162 = torch.aten.view %3160, %3161 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3163 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3164 = torch.aten.view %3134, %3163 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3165 = torch.aten.transpose.int %3164, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3166 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3167 = torch.aten.view %3148, %3166 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3168 = torch.aten.transpose.int %3167, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3169 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3170 = torch.aten.view %3162, %3169 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3171 = torch.aten.transpose.int %3170, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3172:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3165, %3168, %3171, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3173 = torch.aten.transpose.int %3172#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3174 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3175 = torch.aten.view %3173, %3174 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3176 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3177 = torch.aten.view %3175, %3176 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3178 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3179 = torch.aten.transpose.int %3178, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %3180 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3181 = torch.prims.convert_element_type %3180, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3182 = torch.prims.convert_element_type %3177, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3183 = torch.prims.convert_element_type %3179, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3184 = torch.aten.mm %3182, %3183 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3185 = torch.aten.mul.Scalar %3184, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3186 = torch.aten.mul.Scalar %3181, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3187 = torch.aten.add.Tensor %3185, %3186, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3188 = torch.prims.convert_element_type %3187, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3189 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3190 = torch.aten.view %3188, %3189 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3191 = torch.aten.div.Scalar %3190, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3192 = torch.aten.add.Tensor %3191, %3107, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3193 = torch.prims.convert_element_type %3192, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3194 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_95, %result1_96 = torch.aten.var_mean.correction %3193, %3194, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3195 = torch.aten.add.Scalar %result0_95, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3196 = torch.aten.rsqrt %3195 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3197 = torch.aten.sub.Tensor %3192, %result1_96, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3198 = torch.aten.mul.Tensor %3197, %3196 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %3199 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3200 = torch.aten.mul.Tensor %3198, %3199 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %3201 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3202 = torch.aten.add.Tensor %3200, %3201, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3203 = torch.prims.convert_element_type %3202, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3204 = torch.prims.convert_element_type %result1_96, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3205 = torch.prims.convert_element_type %3196, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3206 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3207 = torch.aten.view %3203, %3206 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3208 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %3209 = torch.aten.transpose.int %3208, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %3210 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %3211 = torch.prims.convert_element_type %3210, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %3212 = torch.prims.convert_element_type %3207, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3213 = torch.prims.convert_element_type %3209, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3214 = torch.aten.mm %3212, %3213 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %3215 = torch.aten.mul.Scalar %3214, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3216 = torch.aten.mul.Scalar %3211, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %3217 = torch.aten.add.Tensor %3215, %3216, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3218 = torch.prims.convert_element_type %3217, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3219 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3220 = torch.aten.view %3218, %3219 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3221 = torch.aten.slice.Tensor %3220, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3222 = torch.aten.slice.Tensor %3220, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3223 = torch.aten.gelu %3222, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3224 = torch.aten.mul.Tensor %3221, %3223 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3225 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3226 = torch.aten.view %3224, %3225 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %3227 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3228 = torch.aten.transpose.int %3227, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %3229 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3230 = torch.prims.convert_element_type %3229, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3231 = torch.prims.convert_element_type %3226, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3232 = torch.prims.convert_element_type %3228, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3233 = torch.aten.mm %3231, %3232 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3234 = torch.aten.mul.Scalar %3233, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3235 = torch.aten.mul.Scalar %3230, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3236 = torch.aten.add.Tensor %3234, %3235, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3237 = torch.prims.convert_element_type %3236, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3238 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3239 = torch.aten.view %3237, %3238 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3240 = torch.aten.add.Tensor %3239, %3192, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3241 = torch.prims.convert_element_type %3240, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3242 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_97, %result1_98 = torch.aten.var_mean.correction %3241, %3242, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3243 = torch.aten.add.Scalar %result0_97, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3244 = torch.aten.rsqrt %3243 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3245 = torch.aten.sub.Tensor %3240, %result1_98, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3246 = torch.aten.mul.Tensor %3245, %3244 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %3247 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3248 = torch.aten.mul.Tensor %3246, %3247 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %3249 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3250 = torch.aten.add.Tensor %3248, %3249, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3251 = torch.prims.convert_element_type %3250, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3252 = torch.prims.convert_element_type %result1_98, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3253 = torch.prims.convert_element_type %3244, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %3254 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3255 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3256 = torch.aten.view %3251, %3255 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3257 = torch_c.to_builtin_tensor %3256 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3258 = torch_c.to_builtin_tensor %3254 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3259 = tensor.empty() : tensor<2048x1280xf32>
    %3260 = linalg.fill ins(%cst : f32) outs(%3259 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3261 = tensor.empty() : tensor<2048x1280xf32>
    %3262 = linalg.fill ins(%cst : f32) outs(%3261 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3263:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3260, %3262, %3257, %3258, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3260, %3262)
    %3264 = arith.truncf %3263#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3265 = torch_c.from_builtin_tensor %3264 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3266 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3267 = torch.aten.view %3265, %3266 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %3268 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3269 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3270 = torch.aten.view %3251, %3269 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3271 = torch_c.to_builtin_tensor %3270 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3272 = torch_c.to_builtin_tensor %3268 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3273 = tensor.empty() : tensor<2048x1280xf32>
    %3274 = linalg.fill ins(%cst : f32) outs(%3273 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3275 = tensor.empty() : tensor<2048x1280xf32>
    %3276 = linalg.fill ins(%cst : f32) outs(%3275 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3277:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3274, %3276, %3271, %3272, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3274, %3276)
    %3278 = arith.truncf %3277#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3279 = torch_c.from_builtin_tensor %3278 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3280 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3281 = torch.aten.view %3279, %3280 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %3282 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3283 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3284 = torch.aten.view %3251, %3283 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3285 = torch_c.to_builtin_tensor %3284 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3286 = torch_c.to_builtin_tensor %3282 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3287 = tensor.empty() : tensor<2048x1280xf32>
    %3288 = linalg.fill ins(%cst : f32) outs(%3287 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3289 = tensor.empty() : tensor<2048x1280xf32>
    %3290 = linalg.fill ins(%cst : f32) outs(%3289 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3291:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3288, %3290, %3285, %3286, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3288, %3290)
    %3292 = arith.truncf %3291#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3293 = torch_c.from_builtin_tensor %3292 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3294 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3295 = torch.aten.view %3293, %3294 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3296 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3297 = torch.aten.view %3267, %3296 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3298 = torch.aten.transpose.int %3297, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3299 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3300 = torch.aten.view %3281, %3299 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3301 = torch.aten.transpose.int %3300, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3302 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3303 = torch.aten.view %3295, %3302 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3304 = torch.aten.transpose.int %3303, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3305:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3298, %3301, %3304, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3306 = torch.aten.transpose.int %3305#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3307 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3308 = torch.aten.view %3306, %3307 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3309 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3310 = torch.aten.view %3308, %3309 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3311 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3312 = torch.aten.transpose.int %3311, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %3313 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3314 = torch.prims.convert_element_type %3313, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3315 = torch.prims.convert_element_type %3310, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3316 = torch.prims.convert_element_type %3312, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3317 = torch.aten.mm %3315, %3316 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3318 = torch.aten.mul.Scalar %3317, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3319 = torch.aten.mul.Scalar %3314, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3320 = torch.aten.add.Tensor %3318, %3319, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3321 = torch.prims.convert_element_type %3320, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3322 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3323 = torch.aten.view %3321, %3322 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3324 = torch.aten.div.Scalar %3323, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3325 = torch.aten.add.Tensor %3324, %3240, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3326 = torch.prims.convert_element_type %3325, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3327 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_99, %result1_100 = torch.aten.var_mean.correction %3326, %3327, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3328 = torch.aten.add.Scalar %result0_99, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3329 = torch.aten.rsqrt %3328 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3330 = torch.aten.sub.Tensor %3325, %result1_100, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3331 = torch.aten.mul.Tensor %3330, %3329 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %3332 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3333 = torch.aten.mul.Tensor %3331, %3332 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %3334 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3335 = torch.aten.add.Tensor %3333, %3334, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3336 = torch.prims.convert_element_type %3335, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3337 = torch.prims.convert_element_type %result1_100, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3338 = torch.prims.convert_element_type %3329, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %3339 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3340 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3341 = torch.aten.view %3336, %3340 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3342 = torch_c.to_builtin_tensor %3341 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3343 = torch_c.to_builtin_tensor %3339 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3344 = tensor.empty() : tensor<2048x1280xf32>
    %3345 = linalg.fill ins(%cst : f32) outs(%3344 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3346 = tensor.empty() : tensor<2048x1280xf32>
    %3347 = linalg.fill ins(%cst : f32) outs(%3346 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3348:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3345, %3347, %3342, %3343, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3345, %3347)
    %3349 = arith.truncf %3348#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3350 = torch_c.from_builtin_tensor %3349 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3351 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3352 = torch.aten.view %3350, %3351 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %3353 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3354 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3355 = torch.aten.view %4, %3354 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3356 = torch_c.to_builtin_tensor %3355 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3357 = torch_c.to_builtin_tensor %3353 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3358 = tensor.empty() : tensor<128x1280xf32>
    %3359 = linalg.fill ins(%cst : f32) outs(%3358 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3360 = tensor.empty() : tensor<128x1280xf32>
    %3361 = linalg.fill ins(%cst : f32) outs(%3360 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3362:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3359, %3361, %3356, %3357, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3359, %3361)
    %3363 = arith.truncf %3362#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3364 = torch_c.from_builtin_tensor %3363 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3365 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3366 = torch.aten.view %3364, %3365 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %3367 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3368 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3369 = torch.aten.view %4, %3368 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3370 = torch_c.to_builtin_tensor %3369 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3371 = torch_c.to_builtin_tensor %3367 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3372 = tensor.empty() : tensor<128x1280xf32>
    %3373 = linalg.fill ins(%cst : f32) outs(%3372 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3374 = tensor.empty() : tensor<128x1280xf32>
    %3375 = linalg.fill ins(%cst : f32) outs(%3374 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3376:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3373, %3375, %3370, %3371, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3373, %3375)
    %3377 = arith.truncf %3376#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3378 = torch_c.from_builtin_tensor %3377 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3379 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3380 = torch.aten.view %3378, %3379 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3381 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3382 = torch.aten.view %3352, %3381 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3383 = torch.aten.transpose.int %3382, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3384 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3385 = torch.aten.view %3366, %3384 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3386 = torch.aten.transpose.int %3385, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3387 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3388 = torch.aten.view %3380, %3387 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3389 = torch.aten.transpose.int %3388, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3390:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3383, %3386, %3389, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3391 = torch.aten.transpose.int %3390#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3392 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3393 = torch.aten.view %3391, %3392 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3394 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3395 = torch.aten.view %3393, %3394 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3396 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3397 = torch.aten.transpose.int %3396, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %3398 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3399 = torch.prims.convert_element_type %3398, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3400 = torch.prims.convert_element_type %3395, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3401 = torch.prims.convert_element_type %3397, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3402 = torch.aten.mm %3400, %3401 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3403 = torch.aten.mul.Scalar %3402, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3404 = torch.aten.mul.Scalar %3399, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3405 = torch.aten.add.Tensor %3403, %3404, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3406 = torch.prims.convert_element_type %3405, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3407 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3408 = torch.aten.view %3406, %3407 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3409 = torch.aten.div.Scalar %3408, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3410 = torch.aten.add.Tensor %3409, %3325, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3411 = torch.prims.convert_element_type %3410, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3412 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_101, %result1_102 = torch.aten.var_mean.correction %3411, %3412, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3413 = torch.aten.add.Scalar %result0_101, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3414 = torch.aten.rsqrt %3413 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3415 = torch.aten.sub.Tensor %3410, %result1_102, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3416 = torch.aten.mul.Tensor %3415, %3414 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %3417 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3418 = torch.aten.mul.Tensor %3416, %3417 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %3419 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3420 = torch.aten.add.Tensor %3418, %3419, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3421 = torch.prims.convert_element_type %3420, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3422 = torch.prims.convert_element_type %result1_102, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3423 = torch.prims.convert_element_type %3414, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3424 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3425 = torch.aten.view %3421, %3424 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3426 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %3427 = torch.aten.transpose.int %3426, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %3428 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %3429 = torch.prims.convert_element_type %3428, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %3430 = torch.prims.convert_element_type %3425, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3431 = torch.prims.convert_element_type %3427, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3432 = torch.aten.mm %3430, %3431 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %3433 = torch.aten.mul.Scalar %3432, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3434 = torch.aten.mul.Scalar %3429, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %3435 = torch.aten.add.Tensor %3433, %3434, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3436 = torch.prims.convert_element_type %3435, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3437 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3438 = torch.aten.view %3436, %3437 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3439 = torch.aten.slice.Tensor %3438, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3440 = torch.aten.slice.Tensor %3438, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3441 = torch.aten.gelu %3440, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3442 = torch.aten.mul.Tensor %3439, %3441 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3443 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3444 = torch.aten.view %3442, %3443 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %3445 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3446 = torch.aten.transpose.int %3445, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %3447 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3448 = torch.prims.convert_element_type %3447, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3449 = torch.prims.convert_element_type %3444, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3450 = torch.prims.convert_element_type %3446, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3451 = torch.aten.mm %3449, %3450 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3452 = torch.aten.mul.Scalar %3451, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3453 = torch.aten.mul.Scalar %3448, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3454 = torch.aten.add.Tensor %3452, %3453, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3455 = torch.prims.convert_element_type %3454, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3456 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3457 = torch.aten.view %3455, %3456 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3458 = torch.aten.add.Tensor %3457, %3410, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3459 = torch.prims.convert_element_type %3458, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3460 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_103, %result1_104 = torch.aten.var_mean.correction %3459, %3460, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3461 = torch.aten.add.Scalar %result0_103, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3462 = torch.aten.rsqrt %3461 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3463 = torch.aten.sub.Tensor %3458, %result1_104, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3464 = torch.aten.mul.Tensor %3463, %3462 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %3465 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3466 = torch.aten.mul.Tensor %3464, %3465 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %3467 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3468 = torch.aten.add.Tensor %3466, %3467, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3469 = torch.prims.convert_element_type %3468, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3470 = torch.prims.convert_element_type %result1_104, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3471 = torch.prims.convert_element_type %3462, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %3472 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3473 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3474 = torch.aten.view %3469, %3473 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3475 = torch_c.to_builtin_tensor %3474 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3476 = torch_c.to_builtin_tensor %3472 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3477 = tensor.empty() : tensor<2048x1280xf32>
    %3478 = linalg.fill ins(%cst : f32) outs(%3477 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3479 = tensor.empty() : tensor<2048x1280xf32>
    %3480 = linalg.fill ins(%cst : f32) outs(%3479 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3481:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3478, %3480, %3475, %3476, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3478, %3480)
    %3482 = arith.truncf %3481#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3483 = torch_c.from_builtin_tensor %3482 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3484 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3485 = torch.aten.view %3483, %3484 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %3486 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3487 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3488 = torch.aten.view %3469, %3487 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3489 = torch_c.to_builtin_tensor %3488 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3490 = torch_c.to_builtin_tensor %3486 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3491 = tensor.empty() : tensor<2048x1280xf32>
    %3492 = linalg.fill ins(%cst : f32) outs(%3491 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3493 = tensor.empty() : tensor<2048x1280xf32>
    %3494 = linalg.fill ins(%cst : f32) outs(%3493 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3495:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3492, %3494, %3489, %3490, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3492, %3494)
    %3496 = arith.truncf %3495#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3497 = torch_c.from_builtin_tensor %3496 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3498 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3499 = torch.aten.view %3497, %3498 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %3500 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3501 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3502 = torch.aten.view %3469, %3501 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3503 = torch_c.to_builtin_tensor %3502 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3504 = torch_c.to_builtin_tensor %3500 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3505 = tensor.empty() : tensor<2048x1280xf32>
    %3506 = linalg.fill ins(%cst : f32) outs(%3505 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3507 = tensor.empty() : tensor<2048x1280xf32>
    %3508 = linalg.fill ins(%cst : f32) outs(%3507 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3509:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3506, %3508, %3503, %3504, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3506, %3508)
    %3510 = arith.truncf %3509#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3511 = torch_c.from_builtin_tensor %3510 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3512 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3513 = torch.aten.view %3511, %3512 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3514 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3515 = torch.aten.view %3485, %3514 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3516 = torch.aten.transpose.int %3515, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3517 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3518 = torch.aten.view %3499, %3517 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3519 = torch.aten.transpose.int %3518, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3520 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3521 = torch.aten.view %3513, %3520 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3522 = torch.aten.transpose.int %3521, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3523:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3516, %3519, %3522, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3524 = torch.aten.transpose.int %3523#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3525 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3526 = torch.aten.view %3524, %3525 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3527 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3528 = torch.aten.view %3526, %3527 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3529 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3530 = torch.aten.transpose.int %3529, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %3531 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3532 = torch.prims.convert_element_type %3531, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3533 = torch.prims.convert_element_type %3528, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3534 = torch.prims.convert_element_type %3530, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3535 = torch.aten.mm %3533, %3534 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3536 = torch.aten.mul.Scalar %3535, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3537 = torch.aten.mul.Scalar %3532, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3538 = torch.aten.add.Tensor %3536, %3537, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3539 = torch.prims.convert_element_type %3538, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3540 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3541 = torch.aten.view %3539, %3540 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3542 = torch.aten.div.Scalar %3541, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3543 = torch.aten.add.Tensor %3542, %3458, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3544 = torch.prims.convert_element_type %3543, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3545 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_105, %result1_106 = torch.aten.var_mean.correction %3544, %3545, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3546 = torch.aten.add.Scalar %result0_105, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3547 = torch.aten.rsqrt %3546 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3548 = torch.aten.sub.Tensor %3543, %result1_106, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3549 = torch.aten.mul.Tensor %3548, %3547 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %3550 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3551 = torch.aten.mul.Tensor %3549, %3550 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %3552 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3553 = torch.aten.add.Tensor %3551, %3552, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3554 = torch.prims.convert_element_type %3553, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3555 = torch.prims.convert_element_type %result1_106, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3556 = torch.prims.convert_element_type %3547, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %3557 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3558 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3559 = torch.aten.view %3554, %3558 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3560 = torch_c.to_builtin_tensor %3559 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3561 = torch_c.to_builtin_tensor %3557 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3562 = tensor.empty() : tensor<2048x1280xf32>
    %3563 = linalg.fill ins(%cst : f32) outs(%3562 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3564 = tensor.empty() : tensor<2048x1280xf32>
    %3565 = linalg.fill ins(%cst : f32) outs(%3564 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3566:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3563, %3565, %3560, %3561, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3563, %3565)
    %3567 = arith.truncf %3566#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3568 = torch_c.from_builtin_tensor %3567 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3569 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3570 = torch.aten.view %3568, %3569 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %3571 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3572 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3573 = torch.aten.view %4, %3572 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3574 = torch_c.to_builtin_tensor %3573 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3575 = torch_c.to_builtin_tensor %3571 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3576 = tensor.empty() : tensor<128x1280xf32>
    %3577 = linalg.fill ins(%cst : f32) outs(%3576 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3578 = tensor.empty() : tensor<128x1280xf32>
    %3579 = linalg.fill ins(%cst : f32) outs(%3578 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3580:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3577, %3579, %3574, %3575, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3577, %3579)
    %3581 = arith.truncf %3580#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3582 = torch_c.from_builtin_tensor %3581 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3583 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3584 = torch.aten.view %3582, %3583 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %3585 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3586 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3587 = torch.aten.view %4, %3586 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3588 = torch_c.to_builtin_tensor %3587 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3589 = torch_c.to_builtin_tensor %3585 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3590 = tensor.empty() : tensor<128x1280xf32>
    %3591 = linalg.fill ins(%cst : f32) outs(%3590 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3592 = tensor.empty() : tensor<128x1280xf32>
    %3593 = linalg.fill ins(%cst : f32) outs(%3592 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3594:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3591, %3593, %3588, %3589, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3591, %3593)
    %3595 = arith.truncf %3594#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3596 = torch_c.from_builtin_tensor %3595 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3597 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3598 = torch.aten.view %3596, %3597 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3599 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3600 = torch.aten.view %3570, %3599 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3601 = torch.aten.transpose.int %3600, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3602 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3603 = torch.aten.view %3584, %3602 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3604 = torch.aten.transpose.int %3603, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3605 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3606 = torch.aten.view %3598, %3605 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3607 = torch.aten.transpose.int %3606, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3608:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3601, %3604, %3607, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3609 = torch.aten.transpose.int %3608#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3610 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3611 = torch.aten.view %3609, %3610 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3612 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3613 = torch.aten.view %3611, %3612 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3614 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3615 = torch.aten.transpose.int %3614, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %3616 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3617 = torch.prims.convert_element_type %3616, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3618 = torch.prims.convert_element_type %3613, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3619 = torch.prims.convert_element_type %3615, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3620 = torch.aten.mm %3618, %3619 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3621 = torch.aten.mul.Scalar %3620, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3622 = torch.aten.mul.Scalar %3617, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3623 = torch.aten.add.Tensor %3621, %3622, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3624 = torch.prims.convert_element_type %3623, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3625 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3626 = torch.aten.view %3624, %3625 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3627 = torch.aten.div.Scalar %3626, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3628 = torch.aten.add.Tensor %3627, %3543, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3629 = torch.prims.convert_element_type %3628, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3630 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_107, %result1_108 = torch.aten.var_mean.correction %3629, %3630, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3631 = torch.aten.add.Scalar %result0_107, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3632 = torch.aten.rsqrt %3631 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3633 = torch.aten.sub.Tensor %3628, %result1_108, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3634 = torch.aten.mul.Tensor %3633, %3632 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %3635 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3636 = torch.aten.mul.Tensor %3634, %3635 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %3637 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3638 = torch.aten.add.Tensor %3636, %3637, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3639 = torch.prims.convert_element_type %3638, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3640 = torch.prims.convert_element_type %result1_108, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3641 = torch.prims.convert_element_type %3632, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3642 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3643 = torch.aten.view %3639, %3642 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3644 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %3645 = torch.aten.transpose.int %3644, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %3646 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %3647 = torch.prims.convert_element_type %3646, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %3648 = torch.prims.convert_element_type %3643, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3649 = torch.prims.convert_element_type %3645, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3650 = torch.aten.mm %3648, %3649 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %3651 = torch.aten.mul.Scalar %3650, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3652 = torch.aten.mul.Scalar %3647, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %3653 = torch.aten.add.Tensor %3651, %3652, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %3654 = torch.prims.convert_element_type %3653, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %3655 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3656 = torch.aten.view %3654, %3655 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %3657 = torch.aten.slice.Tensor %3656, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3658 = torch.aten.slice.Tensor %3656, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %3659 = torch.aten.gelu %3658, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %3660 = torch.aten.mul.Tensor %3657, %3659 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %3661 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %3662 = torch.aten.view %3660, %3661 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %3663 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %3664 = torch.aten.transpose.int %3663, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %3665 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3666 = torch.prims.convert_element_type %3665, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3667 = torch.prims.convert_element_type %3662, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %3668 = torch.prims.convert_element_type %3664, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3669 = torch.aten.mm %3667, %3668 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3670 = torch.aten.mul.Scalar %3669, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3671 = torch.aten.mul.Scalar %3666, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3672 = torch.aten.add.Tensor %3670, %3671, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3673 = torch.prims.convert_element_type %3672, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3674 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3675 = torch.aten.view %3673, %3674 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3676 = torch.aten.add.Tensor %3675, %3628, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3677 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3678 = torch.aten.view %3676, %3677 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_out.weight = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %3679 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3680 = torch.aten.transpose.int %3679, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.0.proj_out.bias = util.global.load @_params.unet.down_blocks.2.attentions.0.proj_out.bias : tensor<1280xf16>
    %3681 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3682 = torch.prims.convert_element_type %3681, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3683 = torch.prims.convert_element_type %3678, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3684 = torch.prims.convert_element_type %3680, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3685 = torch.aten.mm %3683, %3684 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3686 = torch.aten.mul.Scalar %3685, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3687 = torch.aten.mul.Scalar %3682, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3688 = torch.aten.add.Tensor %3686, %3687, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3689 = torch.prims.convert_element_type %3688, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3690 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3691 = torch.aten.view %3689, %3690 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3692 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3693 = torch.aten.view %3691, %3692 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %3694 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3695 = torch.aten.permute %3693, %3694 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %3696 = torch.aten.add.Tensor %3695, %1445, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3697 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3698 = torch.aten.view %3696, %3697 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %3699 = torch.prims.convert_element_type %3698, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3700 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_109, %result1_110 = torch.aten.var_mean.correction %3699, %3700, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %3701 = torch.aten.add.Scalar %result0_109, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3702 = torch.aten.rsqrt %3701 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %3703 = torch.aten.sub.Tensor %3698, %result1_110, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3704 = torch.aten.mul.Tensor %3703, %3702 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %3705 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3706 = torch.aten.view %3704, %3705 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.resnets.1.norm1.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.norm1.bias : tensor<1280xf16>
    %3707 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3708 = torch.aten.unsqueeze %3707, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3709 = torch.aten.unsqueeze %3708, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3710 = torch.aten.unsqueeze %3709, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.resnets.1.norm1.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.norm1.weight : tensor<1280xf16>
    %3711 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3712 = torch.aten.unsqueeze %3711, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3713 = torch.aten.unsqueeze %3712, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3714 = torch.aten.unsqueeze %3713, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3715 = torch.aten.mul.Tensor %3706, %3714 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %3716 = torch.aten.add.Tensor %3715, %3710, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %3717 = torch.prims.convert_element_type %3716, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3718 = torch.prims.convert_element_type %result1_110, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3719 = torch.prims.convert_element_type %3702, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3720 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3721 = torch.prims.squeeze %3718, %3720 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3722 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3723 = torch.prims.squeeze %3721, %3722 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3724 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3725 = torch.prims.squeeze %3719, %3724 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3726 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3727 = torch.prims.squeeze %3725, %3726 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3728 = torch.aten.silu %3717 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.1.conv1.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16>
    %3729 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.down_blocks.2.resnets.1.conv1.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.conv1.bias : tensor<1280xf16>
    %3730 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3731 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3732 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3733 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3734 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %3735 = torch.aten.convolution %3728, %3729, %3730, %3731, %3732, %3733, %false, %3734, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3736 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %3737 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3738 = torch.aten.transpose.int %3737, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %3739 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3740 = torch.prims.convert_element_type %3739, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3741 = torch.prims.convert_element_type %3736, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %3742 = torch.prims.convert_element_type %3738, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3743 = torch.aten.mm %3741, %3742 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %3744 = torch.aten.mul.Scalar %3743, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %3745 = torch.aten.mul.Scalar %3740, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3746 = torch.aten.add.Tensor %3744, %3745, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %3747 = torch.prims.convert_element_type %3746, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %3748 = torch.aten.unsqueeze %3747, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %3749 = torch.aten.unsqueeze %3748, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %3750 = torch.aten.add.Tensor %3735, %3749, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3751 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3752 = torch.aten.view %3750, %3751 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %3753 = torch.prims.convert_element_type %3752, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3754 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_111, %result1_112 = torch.aten.var_mean.correction %3753, %3754, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %3755 = torch.aten.add.Scalar %result0_111, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3756 = torch.aten.rsqrt %3755 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %3757 = torch.aten.sub.Tensor %3752, %result1_112, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3758 = torch.aten.mul.Tensor %3757, %3756 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %3759 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3760 = torch.aten.view %3758, %3759 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.resnets.1.norm2.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.norm2.bias : tensor<1280xf16>
    %3761 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3762 = torch.aten.unsqueeze %3761, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3763 = torch.aten.unsqueeze %3762, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3764 = torch.aten.unsqueeze %3763, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.resnets.1.norm2.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.norm2.weight : tensor<1280xf16>
    %3765 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3766 = torch.aten.unsqueeze %3765, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3767 = torch.aten.unsqueeze %3766, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3768 = torch.aten.unsqueeze %3767, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3769 = torch.aten.mul.Tensor %3760, %3768 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %3770 = torch.aten.add.Tensor %3769, %3764, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %3771 = torch.prims.convert_element_type %3770, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3772 = torch.prims.convert_element_type %result1_112, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3773 = torch.prims.convert_element_type %3756, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3774 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3775 = torch.prims.squeeze %3772, %3774 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3776 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3777 = torch.prims.squeeze %3775, %3776 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3778 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3779 = torch.prims.squeeze %3773, %3778 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3780 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3781 = torch.prims.squeeze %3779, %3780 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3782 = torch.aten.silu %3771 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.down_blocks.2.resnets.1.conv2.weight = util.global.load @_params.unet.down_blocks.2.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %3783 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.down_blocks.2.resnets.1.conv2.bias = util.global.load @_params.unet.down_blocks.2.resnets.1.conv2.bias : tensor<1280xf16>
    %3784 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3785 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3786 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3787 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %3788 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %3789 = torch.aten.convolution %3782, %3783, %3784, %3785, %3786, %3787, %false, %3788, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3790 = torch.aten.add.Tensor %3696, %3789, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3791 = torch.aten.div.Scalar %3790, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %3792 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3793 = torch.aten.view %3791, %3792 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %3794 = torch.prims.convert_element_type %3793, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3795 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_113, %result1_114 = torch.aten.var_mean.correction %3794, %3795, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %3796 = torch.aten.add.Scalar %result0_113, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3797 = torch.aten.rsqrt %3796 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %3798 = torch.aten.sub.Tensor %3793, %result1_114, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %3799 = torch.aten.mul.Tensor %3798, %3797 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %3800 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3801 = torch.aten.view %3799, %3800 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.down_blocks.2.attentions.1.norm.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.norm.bias : tensor<1280xf16>
    %3802 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3803 = torch.aten.unsqueeze %3802, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3804 = torch.aten.unsqueeze %3803, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3805 = torch.aten.unsqueeze %3804, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.down_blocks.2.attentions.1.norm.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.norm.weight : tensor<1280xf16>
    %3806 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3807 = torch.aten.unsqueeze %3806, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %3808 = torch.aten.unsqueeze %3807, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %3809 = torch.aten.unsqueeze %3808, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3810 = torch.aten.mul.Tensor %3801, %3809 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %3811 = torch.aten.add.Tensor %3810, %3805, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %3812 = torch.prims.convert_element_type %3811, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %3813 = torch.prims.convert_element_type %result1_114, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3814 = torch.prims.convert_element_type %3797, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %3815 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3816 = torch.prims.squeeze %3813, %3815 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3817 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3818 = torch.prims.squeeze %3816, %3817 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3819 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %3820 = torch.prims.squeeze %3814, %3819 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %3821 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %3822 = torch.prims.squeeze %3820, %3821 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %3823 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3824 = torch.aten.permute %3812, %3823 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %3825 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3826 = torch.aten.view %3824, %3825 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_in.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_in.weight : tensor<1280x1280xf16>
    %3827 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3828 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3829 = torch.aten._unsafe_view %3826, %3828 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3830 = torch_c.to_builtin_tensor %3829 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3831 = torch_c.to_builtin_tensor %3827 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3832 = tensor.empty() : tensor<2048x1280xf32>
    %3833 = linalg.fill ins(%cst : f32) outs(%3832 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3834 = tensor.empty() : tensor<2048x1280xf32>
    %3835 = linalg.fill ins(%cst : f32) outs(%3834 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3836:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3833, %3835, %3830, %3831, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3833, %3835)
    %3837 = arith.truncf %3836#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3838 = torch_c.from_builtin_tensor %3837 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3839 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3840 = torch.aten.view %3838, %3839 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_in.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_in.bias : tensor<1280xf16>
    %3841 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3842 = torch.aten.add.Tensor %3840, %3841, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3843 = torch.prims.convert_element_type %3842, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3844 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_115, %result1_116 = torch.aten.var_mean.correction %3843, %3844, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3845 = torch.aten.add.Scalar %result0_115, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3846 = torch.aten.rsqrt %3845 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3847 = torch.aten.sub.Tensor %3842, %result1_116, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3848 = torch.aten.mul.Tensor %3847, %3846 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %3849 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3850 = torch.aten.mul.Tensor %3848, %3849 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %3851 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3852 = torch.aten.add.Tensor %3850, %3851, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3853 = torch.prims.convert_element_type %3852, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3854 = torch.prims.convert_element_type %result1_116, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3855 = torch.prims.convert_element_type %3846, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %3856 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3857 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3858 = torch.aten.view %3853, %3857 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3859 = torch_c.to_builtin_tensor %3858 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3860 = torch_c.to_builtin_tensor %3856 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3861 = tensor.empty() : tensor<2048x1280xf32>
    %3862 = linalg.fill ins(%cst : f32) outs(%3861 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3863 = tensor.empty() : tensor<2048x1280xf32>
    %3864 = linalg.fill ins(%cst : f32) outs(%3863 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3865:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3862, %3864, %3859, %3860, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3862, %3864)
    %3866 = arith.truncf %3865#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3867 = torch_c.from_builtin_tensor %3866 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3868 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3869 = torch.aten.view %3867, %3868 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %3870 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3871 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3872 = torch.aten.view %3853, %3871 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3873 = torch_c.to_builtin_tensor %3872 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3874 = torch_c.to_builtin_tensor %3870 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3875 = tensor.empty() : tensor<2048x1280xf32>
    %3876 = linalg.fill ins(%cst : f32) outs(%3875 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3877 = tensor.empty() : tensor<2048x1280xf32>
    %3878 = linalg.fill ins(%cst : f32) outs(%3877 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3879:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3876, %3878, %3873, %3874, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3876, %3878)
    %3880 = arith.truncf %3879#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3881 = torch_c.from_builtin_tensor %3880 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3882 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3883 = torch.aten.view %3881, %3882 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %3884 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3885 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3886 = torch.aten.view %3853, %3885 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3887 = torch_c.to_builtin_tensor %3886 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3888 = torch_c.to_builtin_tensor %3884 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3889 = tensor.empty() : tensor<2048x1280xf32>
    %3890 = linalg.fill ins(%cst : f32) outs(%3889 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3891 = tensor.empty() : tensor<2048x1280xf32>
    %3892 = linalg.fill ins(%cst : f32) outs(%3891 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3893:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3890, %3892, %3887, %3888, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3890, %3892)
    %3894 = arith.truncf %3893#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3895 = torch_c.from_builtin_tensor %3894 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3896 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3897 = torch.aten.view %3895, %3896 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3898 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3899 = torch.aten.view %3869, %3898 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3900 = torch.aten.transpose.int %3899, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3901 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3902 = torch.aten.view %3883, %3901 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3903 = torch.aten.transpose.int %3902, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3904 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3905 = torch.aten.view %3897, %3904 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3906 = torch.aten.transpose.int %3905, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3907:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3900, %3903, %3906, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3908 = torch.aten.transpose.int %3907#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3909 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3910 = torch.aten.view %3908, %3909 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3911 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3912 = torch.aten.view %3910, %3911 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3913 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3914 = torch.aten.transpose.int %3913, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %3915 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3916 = torch.prims.convert_element_type %3915, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %3917 = torch.prims.convert_element_type %3912, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3918 = torch.prims.convert_element_type %3914, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3919 = torch.aten.mm %3917, %3918 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %3920 = torch.aten.mul.Scalar %3919, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3921 = torch.aten.mul.Scalar %3916, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %3922 = torch.aten.add.Tensor %3920, %3921, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %3923 = torch.prims.convert_element_type %3922, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %3924 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3925 = torch.aten.view %3923, %3924 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3926 = torch.aten.div.Scalar %3925, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %3927 = torch.aten.add.Tensor %3926, %3842, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3928 = torch.prims.convert_element_type %3927, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3929 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_117, %result1_118 = torch.aten.var_mean.correction %3928, %3929, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %3930 = torch.aten.add.Scalar %result0_117, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %3931 = torch.aten.rsqrt %3930 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %3932 = torch.aten.sub.Tensor %3927, %result1_118, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3933 = torch.aten.mul.Tensor %3932, %3931 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %3934 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3935 = torch.aten.mul.Tensor %3933, %3934 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %3936 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3937 = torch.aten.add.Tensor %3935, %3936, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %3938 = torch.prims.convert_element_type %3937, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %3939 = torch.prims.convert_element_type %result1_118, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %3940 = torch.prims.convert_element_type %3931, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %3941 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3942 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3943 = torch.aten.view %3938, %3942 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %3944 = torch_c.to_builtin_tensor %3943 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %3945 = torch_c.to_builtin_tensor %3941 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %3946 = tensor.empty() : tensor<2048x1280xf32>
    %3947 = linalg.fill ins(%cst : f32) outs(%3946 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3948 = tensor.empty() : tensor<2048x1280xf32>
    %3949 = linalg.fill ins(%cst : f32) outs(%3948 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %3950:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %3947, %3949, %3944, %3945, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3947, %3949)
    %3951 = arith.truncf %3950#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %3952 = torch_c.from_builtin_tensor %3951 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %3953 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3954 = torch.aten.view %3952, %3953 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %3955 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3956 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3957 = torch.aten.view %4, %3956 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3958 = torch_c.to_builtin_tensor %3957 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3959 = torch_c.to_builtin_tensor %3955 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3960 = tensor.empty() : tensor<128x1280xf32>
    %3961 = linalg.fill ins(%cst : f32) outs(%3960 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3962 = tensor.empty() : tensor<128x1280xf32>
    %3963 = linalg.fill ins(%cst : f32) outs(%3962 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3964:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3961, %3963, %3958, %3959, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3961, %3963)
    %3965 = arith.truncf %3964#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3966 = torch_c.from_builtin_tensor %3965 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3967 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3968 = torch.aten.view %3966, %3967 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %3969 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %3970 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %3971 = torch.aten.view %4, %3970 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %3972 = torch_c.to_builtin_tensor %3971 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %3973 = torch_c.to_builtin_tensor %3969 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %3974 = tensor.empty() : tensor<128x1280xf32>
    %3975 = linalg.fill ins(%cst : f32) outs(%3974 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3976 = tensor.empty() : tensor<128x1280xf32>
    %3977 = linalg.fill ins(%cst : f32) outs(%3976 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %3978:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %3975, %3977, %3972, %3973, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%3975, %3977)
    %3979 = arith.truncf %3978#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %3980 = torch_c.from_builtin_tensor %3979 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %3981 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3982 = torch.aten.view %3980, %3981 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %3983 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3984 = torch.aten.view %3954, %3983 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %3985 = torch.aten.transpose.int %3984, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %3986 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3987 = torch.aten.view %3968, %3986 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3988 = torch.aten.transpose.int %3987, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3989 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3990 = torch.aten.view %3982, %3989 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %3991 = torch.aten.transpose.int %3990, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %3992:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3985, %3988, %3991, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %3993 = torch.aten.transpose.int %3992#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %3994 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3995 = torch.aten.view %3993, %3994 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %3996 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %3997 = torch.aten.view %3995, %3996 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3998 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %3999 = torch.aten.transpose.int %3998, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %4000 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4001 = torch.prims.convert_element_type %4000, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4002 = torch.prims.convert_element_type %3997, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4003 = torch.prims.convert_element_type %3999, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4004 = torch.aten.mm %4002, %4003 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4005 = torch.aten.mul.Scalar %4004, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4006 = torch.aten.mul.Scalar %4001, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4007 = torch.aten.add.Tensor %4005, %4006, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4008 = torch.prims.convert_element_type %4007, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4009 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4010 = torch.aten.view %4008, %4009 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4011 = torch.aten.div.Scalar %4010, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4012 = torch.aten.add.Tensor %4011, %3927, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4013 = torch.prims.convert_element_type %4012, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4014 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_119, %result1_120 = torch.aten.var_mean.correction %4013, %4014, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4015 = torch.aten.add.Scalar %result0_119, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4016 = torch.aten.rsqrt %4015 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4017 = torch.aten.sub.Tensor %4012, %result1_120, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4018 = torch.aten.mul.Tensor %4017, %4016 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %4019 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4020 = torch.aten.mul.Tensor %4018, %4019 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %4021 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4022 = torch.aten.add.Tensor %4020, %4021, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4023 = torch.prims.convert_element_type %4022, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4024 = torch.prims.convert_element_type %result1_120, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4025 = torch.prims.convert_element_type %4016, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4026 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4027 = torch.aten.view %4023, %4026 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4028 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4029 = torch.aten.transpose.int %4028, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %4030 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4031 = torch.prims.convert_element_type %4030, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4032 = torch.prims.convert_element_type %4027, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4033 = torch.prims.convert_element_type %4029, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4034 = torch.aten.mm %4032, %4033 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4035 = torch.aten.mul.Scalar %4034, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4036 = torch.aten.mul.Scalar %4031, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4037 = torch.aten.add.Tensor %4035, %4036, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4038 = torch.prims.convert_element_type %4037, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4039 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4040 = torch.aten.view %4038, %4039 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4041 = torch.aten.slice.Tensor %4040, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4042 = torch.aten.slice.Tensor %4040, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4043 = torch.aten.gelu %4042, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4044 = torch.aten.mul.Tensor %4041, %4043 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4045 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4046 = torch.aten.view %4044, %4045 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %4047 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4048 = torch.aten.transpose.int %4047, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %4049 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4050 = torch.prims.convert_element_type %4049, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4051 = torch.prims.convert_element_type %4046, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4052 = torch.prims.convert_element_type %4048, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4053 = torch.aten.mm %4051, %4052 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4054 = torch.aten.mul.Scalar %4053, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4055 = torch.aten.mul.Scalar %4050, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4056 = torch.aten.add.Tensor %4054, %4055, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4057 = torch.prims.convert_element_type %4056, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4058 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4059 = torch.aten.view %4057, %4058 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4060 = torch.aten.add.Tensor %4059, %4012, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4061 = torch.prims.convert_element_type %4060, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4062 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_121, %result1_122 = torch.aten.var_mean.correction %4061, %4062, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4063 = torch.aten.add.Scalar %result0_121, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4064 = torch.aten.rsqrt %4063 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4065 = torch.aten.sub.Tensor %4060, %result1_122, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4066 = torch.aten.mul.Tensor %4065, %4064 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %4067 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4068 = torch.aten.mul.Tensor %4066, %4067 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %4069 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4070 = torch.aten.add.Tensor %4068, %4069, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4071 = torch.prims.convert_element_type %4070, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4072 = torch.prims.convert_element_type %result1_122, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4073 = torch.prims.convert_element_type %4064, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %4074 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4075 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4076 = torch.aten.view %4071, %4075 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4077 = torch_c.to_builtin_tensor %4076 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4078 = torch_c.to_builtin_tensor %4074 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4079 = tensor.empty() : tensor<2048x1280xf32>
    %4080 = linalg.fill ins(%cst : f32) outs(%4079 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4081 = tensor.empty() : tensor<2048x1280xf32>
    %4082 = linalg.fill ins(%cst : f32) outs(%4081 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4083:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4080, %4082, %4077, %4078, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4080, %4082)
    %4084 = arith.truncf %4083#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4085 = torch_c.from_builtin_tensor %4084 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4086 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4087 = torch.aten.view %4085, %4086 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %4088 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4089 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4090 = torch.aten.view %4071, %4089 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4091 = torch_c.to_builtin_tensor %4090 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4092 = torch_c.to_builtin_tensor %4088 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4093 = tensor.empty() : tensor<2048x1280xf32>
    %4094 = linalg.fill ins(%cst : f32) outs(%4093 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4095 = tensor.empty() : tensor<2048x1280xf32>
    %4096 = linalg.fill ins(%cst : f32) outs(%4095 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4097:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4094, %4096, %4091, %4092, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4094, %4096)
    %4098 = arith.truncf %4097#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4099 = torch_c.from_builtin_tensor %4098 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4100 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4101 = torch.aten.view %4099, %4100 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %4102 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4103 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4104 = torch.aten.view %4071, %4103 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4105 = torch_c.to_builtin_tensor %4104 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4106 = torch_c.to_builtin_tensor %4102 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4107 = tensor.empty() : tensor<2048x1280xf32>
    %4108 = linalg.fill ins(%cst : f32) outs(%4107 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4109 = tensor.empty() : tensor<2048x1280xf32>
    %4110 = linalg.fill ins(%cst : f32) outs(%4109 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4111:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4108, %4110, %4105, %4106, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4108, %4110)
    %4112 = arith.truncf %4111#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4113 = torch_c.from_builtin_tensor %4112 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4114 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4115 = torch.aten.view %4113, %4114 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4116 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4117 = torch.aten.view %4087, %4116 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4118 = torch.aten.transpose.int %4117, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4119 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4120 = torch.aten.view %4101, %4119 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4121 = torch.aten.transpose.int %4120, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4122 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4123 = torch.aten.view %4115, %4122 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4124 = torch.aten.transpose.int %4123, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4125:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4118, %4121, %4124, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4126 = torch.aten.transpose.int %4125#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4127 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4128 = torch.aten.view %4126, %4127 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4129 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4130 = torch.aten.view %4128, %4129 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4131 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4132 = torch.aten.transpose.int %4131, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %4133 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4134 = torch.prims.convert_element_type %4133, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4135 = torch.prims.convert_element_type %4130, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4136 = torch.prims.convert_element_type %4132, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4137 = torch.aten.mm %4135, %4136 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4138 = torch.aten.mul.Scalar %4137, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4139 = torch.aten.mul.Scalar %4134, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4140 = torch.aten.add.Tensor %4138, %4139, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4141 = torch.prims.convert_element_type %4140, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4142 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4143 = torch.aten.view %4141, %4142 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4144 = torch.aten.div.Scalar %4143, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4145 = torch.aten.add.Tensor %4144, %4060, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4146 = torch.prims.convert_element_type %4145, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4147 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_123, %result1_124 = torch.aten.var_mean.correction %4146, %4147, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4148 = torch.aten.add.Scalar %result0_123, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4149 = torch.aten.rsqrt %4148 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4150 = torch.aten.sub.Tensor %4145, %result1_124, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4151 = torch.aten.mul.Tensor %4150, %4149 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %4152 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4153 = torch.aten.mul.Tensor %4151, %4152 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %4154 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4155 = torch.aten.add.Tensor %4153, %4154, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4156 = torch.prims.convert_element_type %4155, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4157 = torch.prims.convert_element_type %result1_124, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4158 = torch.prims.convert_element_type %4149, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %4159 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4160 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4161 = torch.aten.view %4156, %4160 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4162 = torch_c.to_builtin_tensor %4161 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4163 = torch_c.to_builtin_tensor %4159 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4164 = tensor.empty() : tensor<2048x1280xf32>
    %4165 = linalg.fill ins(%cst : f32) outs(%4164 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4166 = tensor.empty() : tensor<2048x1280xf32>
    %4167 = linalg.fill ins(%cst : f32) outs(%4166 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4168:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4165, %4167, %4162, %4163, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4165, %4167)
    %4169 = arith.truncf %4168#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4170 = torch_c.from_builtin_tensor %4169 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4171 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4172 = torch.aten.view %4170, %4171 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %4173 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4174 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4175 = torch.aten.view %4, %4174 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4176 = torch_c.to_builtin_tensor %4175 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4177 = torch_c.to_builtin_tensor %4173 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4178 = tensor.empty() : tensor<128x1280xf32>
    %4179 = linalg.fill ins(%cst : f32) outs(%4178 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4180 = tensor.empty() : tensor<128x1280xf32>
    %4181 = linalg.fill ins(%cst : f32) outs(%4180 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4182:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4179, %4181, %4176, %4177, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4179, %4181)
    %4183 = arith.truncf %4182#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4184 = torch_c.from_builtin_tensor %4183 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4185 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4186 = torch.aten.view %4184, %4185 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %4187 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4188 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4189 = torch.aten.view %4, %4188 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4190 = torch_c.to_builtin_tensor %4189 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4191 = torch_c.to_builtin_tensor %4187 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4192 = tensor.empty() : tensor<128x1280xf32>
    %4193 = linalg.fill ins(%cst : f32) outs(%4192 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4194 = tensor.empty() : tensor<128x1280xf32>
    %4195 = linalg.fill ins(%cst : f32) outs(%4194 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4196:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4193, %4195, %4190, %4191, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4193, %4195)
    %4197 = arith.truncf %4196#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4198 = torch_c.from_builtin_tensor %4197 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4199 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4200 = torch.aten.view %4198, %4199 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4201 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4202 = torch.aten.view %4172, %4201 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4203 = torch.aten.transpose.int %4202, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4204 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4205 = torch.aten.view %4186, %4204 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4206 = torch.aten.transpose.int %4205, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4207 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4208 = torch.aten.view %4200, %4207 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4209 = torch.aten.transpose.int %4208, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4210:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4203, %4206, %4209, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4211 = torch.aten.transpose.int %4210#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4212 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4213 = torch.aten.view %4211, %4212 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4214 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4215 = torch.aten.view %4213, %4214 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4216 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4217 = torch.aten.transpose.int %4216, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %4218 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4219 = torch.prims.convert_element_type %4218, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4220 = torch.prims.convert_element_type %4215, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4221 = torch.prims.convert_element_type %4217, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4222 = torch.aten.mm %4220, %4221 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4223 = torch.aten.mul.Scalar %4222, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4224 = torch.aten.mul.Scalar %4219, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4225 = torch.aten.add.Tensor %4223, %4224, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4226 = torch.prims.convert_element_type %4225, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4227 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4228 = torch.aten.view %4226, %4227 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4229 = torch.aten.div.Scalar %4228, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4230 = torch.aten.add.Tensor %4229, %4145, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4231 = torch.prims.convert_element_type %4230, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4232 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_125, %result1_126 = torch.aten.var_mean.correction %4231, %4232, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4233 = torch.aten.add.Scalar %result0_125, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4234 = torch.aten.rsqrt %4233 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4235 = torch.aten.sub.Tensor %4230, %result1_126, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4236 = torch.aten.mul.Tensor %4235, %4234 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %4237 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4238 = torch.aten.mul.Tensor %4236, %4237 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %4239 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4240 = torch.aten.add.Tensor %4238, %4239, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4241 = torch.prims.convert_element_type %4240, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4242 = torch.prims.convert_element_type %result1_126, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4243 = torch.prims.convert_element_type %4234, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4244 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4245 = torch.aten.view %4241, %4244 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4246 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4247 = torch.aten.transpose.int %4246, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %4248 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4249 = torch.prims.convert_element_type %4248, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4250 = torch.prims.convert_element_type %4245, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4251 = torch.prims.convert_element_type %4247, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4252 = torch.aten.mm %4250, %4251 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4253 = torch.aten.mul.Scalar %4252, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4254 = torch.aten.mul.Scalar %4249, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4255 = torch.aten.add.Tensor %4253, %4254, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4256 = torch.prims.convert_element_type %4255, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4257 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4258 = torch.aten.view %4256, %4257 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4259 = torch.aten.slice.Tensor %4258, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4260 = torch.aten.slice.Tensor %4258, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4261 = torch.aten.gelu %4260, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4262 = torch.aten.mul.Tensor %4259, %4261 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4263 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4264 = torch.aten.view %4262, %4263 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %4265 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4266 = torch.aten.transpose.int %4265, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %4267 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4268 = torch.prims.convert_element_type %4267, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4269 = torch.prims.convert_element_type %4264, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4270 = torch.prims.convert_element_type %4266, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4271 = torch.aten.mm %4269, %4270 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4272 = torch.aten.mul.Scalar %4271, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4273 = torch.aten.mul.Scalar %4268, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4274 = torch.aten.add.Tensor %4272, %4273, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4275 = torch.prims.convert_element_type %4274, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4276 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4277 = torch.aten.view %4275, %4276 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4278 = torch.aten.add.Tensor %4277, %4230, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4279 = torch.prims.convert_element_type %4278, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4280 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_127, %result1_128 = torch.aten.var_mean.correction %4279, %4280, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4281 = torch.aten.add.Scalar %result0_127, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4282 = torch.aten.rsqrt %4281 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4283 = torch.aten.sub.Tensor %4278, %result1_128, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4284 = torch.aten.mul.Tensor %4283, %4282 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %4285 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4286 = torch.aten.mul.Tensor %4284, %4285 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %4287 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4288 = torch.aten.add.Tensor %4286, %4287, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4289 = torch.prims.convert_element_type %4288, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4290 = torch.prims.convert_element_type %result1_128, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4291 = torch.prims.convert_element_type %4282, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %4292 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4293 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4294 = torch.aten.view %4289, %4293 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4295 = torch_c.to_builtin_tensor %4294 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4296 = torch_c.to_builtin_tensor %4292 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4297 = tensor.empty() : tensor<2048x1280xf32>
    %4298 = linalg.fill ins(%cst : f32) outs(%4297 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4299 = tensor.empty() : tensor<2048x1280xf32>
    %4300 = linalg.fill ins(%cst : f32) outs(%4299 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4301:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4298, %4300, %4295, %4296, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4298, %4300)
    %4302 = arith.truncf %4301#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4303 = torch_c.from_builtin_tensor %4302 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4304 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4305 = torch.aten.view %4303, %4304 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %4306 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4307 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4308 = torch.aten.view %4289, %4307 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4309 = torch_c.to_builtin_tensor %4308 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4310 = torch_c.to_builtin_tensor %4306 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4311 = tensor.empty() : tensor<2048x1280xf32>
    %4312 = linalg.fill ins(%cst : f32) outs(%4311 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4313 = tensor.empty() : tensor<2048x1280xf32>
    %4314 = linalg.fill ins(%cst : f32) outs(%4313 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4315:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4312, %4314, %4309, %4310, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4312, %4314)
    %4316 = arith.truncf %4315#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4317 = torch_c.from_builtin_tensor %4316 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4318 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4319 = torch.aten.view %4317, %4318 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %4320 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4321 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4322 = torch.aten.view %4289, %4321 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4323 = torch_c.to_builtin_tensor %4322 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4324 = torch_c.to_builtin_tensor %4320 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4325 = tensor.empty() : tensor<2048x1280xf32>
    %4326 = linalg.fill ins(%cst : f32) outs(%4325 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4327 = tensor.empty() : tensor<2048x1280xf32>
    %4328 = linalg.fill ins(%cst : f32) outs(%4327 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4329:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4326, %4328, %4323, %4324, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4326, %4328)
    %4330 = arith.truncf %4329#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4331 = torch_c.from_builtin_tensor %4330 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4332 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4333 = torch.aten.view %4331, %4332 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4334 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4335 = torch.aten.view %4305, %4334 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4336 = torch.aten.transpose.int %4335, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4337 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4338 = torch.aten.view %4319, %4337 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4339 = torch.aten.transpose.int %4338, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4340 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4341 = torch.aten.view %4333, %4340 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4342 = torch.aten.transpose.int %4341, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4343:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4336, %4339, %4342, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4344 = torch.aten.transpose.int %4343#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4345 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4346 = torch.aten.view %4344, %4345 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4347 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4348 = torch.aten.view %4346, %4347 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4349 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4350 = torch.aten.transpose.int %4349, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %4351 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4352 = torch.prims.convert_element_type %4351, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4353 = torch.prims.convert_element_type %4348, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4354 = torch.prims.convert_element_type %4350, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4355 = torch.aten.mm %4353, %4354 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4356 = torch.aten.mul.Scalar %4355, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4357 = torch.aten.mul.Scalar %4352, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4358 = torch.aten.add.Tensor %4356, %4357, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4359 = torch.prims.convert_element_type %4358, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4360 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4361 = torch.aten.view %4359, %4360 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4362 = torch.aten.div.Scalar %4361, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4363 = torch.aten.add.Tensor %4362, %4278, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4364 = torch.prims.convert_element_type %4363, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4365 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_129, %result1_130 = torch.aten.var_mean.correction %4364, %4365, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4366 = torch.aten.add.Scalar %result0_129, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4367 = torch.aten.rsqrt %4366 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4368 = torch.aten.sub.Tensor %4363, %result1_130, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4369 = torch.aten.mul.Tensor %4368, %4367 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %4370 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4371 = torch.aten.mul.Tensor %4369, %4370 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %4372 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4373 = torch.aten.add.Tensor %4371, %4372, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4374 = torch.prims.convert_element_type %4373, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4375 = torch.prims.convert_element_type %result1_130, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4376 = torch.prims.convert_element_type %4367, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %4377 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4378 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4379 = torch.aten.view %4374, %4378 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4380 = torch_c.to_builtin_tensor %4379 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4381 = torch_c.to_builtin_tensor %4377 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4382 = tensor.empty() : tensor<2048x1280xf32>
    %4383 = linalg.fill ins(%cst : f32) outs(%4382 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4384 = tensor.empty() : tensor<2048x1280xf32>
    %4385 = linalg.fill ins(%cst : f32) outs(%4384 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4386:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4383, %4385, %4380, %4381, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4383, %4385)
    %4387 = arith.truncf %4386#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4388 = torch_c.from_builtin_tensor %4387 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4389 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4390 = torch.aten.view %4388, %4389 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %4391 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4392 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4393 = torch.aten.view %4, %4392 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4394 = torch_c.to_builtin_tensor %4393 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4395 = torch_c.to_builtin_tensor %4391 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4396 = tensor.empty() : tensor<128x1280xf32>
    %4397 = linalg.fill ins(%cst : f32) outs(%4396 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4398 = tensor.empty() : tensor<128x1280xf32>
    %4399 = linalg.fill ins(%cst : f32) outs(%4398 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4400:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4397, %4399, %4394, %4395, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4397, %4399)
    %4401 = arith.truncf %4400#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4402 = torch_c.from_builtin_tensor %4401 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4403 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4404 = torch.aten.view %4402, %4403 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %4405 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4406 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4407 = torch.aten.view %4, %4406 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4408 = torch_c.to_builtin_tensor %4407 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4409 = torch_c.to_builtin_tensor %4405 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4410 = tensor.empty() : tensor<128x1280xf32>
    %4411 = linalg.fill ins(%cst : f32) outs(%4410 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4412 = tensor.empty() : tensor<128x1280xf32>
    %4413 = linalg.fill ins(%cst : f32) outs(%4412 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4414:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4411, %4413, %4408, %4409, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4411, %4413)
    %4415 = arith.truncf %4414#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4416 = torch_c.from_builtin_tensor %4415 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4417 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4418 = torch.aten.view %4416, %4417 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4419 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4420 = torch.aten.view %4390, %4419 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4421 = torch.aten.transpose.int %4420, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4422 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4423 = torch.aten.view %4404, %4422 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4424 = torch.aten.transpose.int %4423, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4425 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4426 = torch.aten.view %4418, %4425 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4427 = torch.aten.transpose.int %4426, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4428:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4421, %4424, %4427, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4429 = torch.aten.transpose.int %4428#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4430 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4431 = torch.aten.view %4429, %4430 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4432 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4433 = torch.aten.view %4431, %4432 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4434 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4435 = torch.aten.transpose.int %4434, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %4436 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4437 = torch.prims.convert_element_type %4436, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4438 = torch.prims.convert_element_type %4433, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4439 = torch.prims.convert_element_type %4435, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4440 = torch.aten.mm %4438, %4439 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4441 = torch.aten.mul.Scalar %4440, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4442 = torch.aten.mul.Scalar %4437, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4443 = torch.aten.add.Tensor %4441, %4442, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4444 = torch.prims.convert_element_type %4443, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4445 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4446 = torch.aten.view %4444, %4445 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4447 = torch.aten.div.Scalar %4446, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4448 = torch.aten.add.Tensor %4447, %4363, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4449 = torch.prims.convert_element_type %4448, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4450 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_131, %result1_132 = torch.aten.var_mean.correction %4449, %4450, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4451 = torch.aten.add.Scalar %result0_131, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4452 = torch.aten.rsqrt %4451 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4453 = torch.aten.sub.Tensor %4448, %result1_132, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4454 = torch.aten.mul.Tensor %4453, %4452 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %4455 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4456 = torch.aten.mul.Tensor %4454, %4455 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %4457 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4458 = torch.aten.add.Tensor %4456, %4457, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4459 = torch.prims.convert_element_type %4458, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4460 = torch.prims.convert_element_type %result1_132, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4461 = torch.prims.convert_element_type %4452, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4462 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4463 = torch.aten.view %4459, %4462 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4464 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4465 = torch.aten.transpose.int %4464, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %4466 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4467 = torch.prims.convert_element_type %4466, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4468 = torch.prims.convert_element_type %4463, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4469 = torch.prims.convert_element_type %4465, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4470 = torch.aten.mm %4468, %4469 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4471 = torch.aten.mul.Scalar %4470, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4472 = torch.aten.mul.Scalar %4467, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4473 = torch.aten.add.Tensor %4471, %4472, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4474 = torch.prims.convert_element_type %4473, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4475 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4476 = torch.aten.view %4474, %4475 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4477 = torch.aten.slice.Tensor %4476, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4478 = torch.aten.slice.Tensor %4476, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4479 = torch.aten.gelu %4478, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4480 = torch.aten.mul.Tensor %4477, %4479 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4481 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4482 = torch.aten.view %4480, %4481 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %4483 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4484 = torch.aten.transpose.int %4483, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %4485 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4486 = torch.prims.convert_element_type %4485, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4487 = torch.prims.convert_element_type %4482, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4488 = torch.prims.convert_element_type %4484, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4489 = torch.aten.mm %4487, %4488 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4490 = torch.aten.mul.Scalar %4489, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4491 = torch.aten.mul.Scalar %4486, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4492 = torch.aten.add.Tensor %4490, %4491, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4493 = torch.prims.convert_element_type %4492, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4494 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4495 = torch.aten.view %4493, %4494 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4496 = torch.aten.add.Tensor %4495, %4448, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4497 = torch.prims.convert_element_type %4496, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4498 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_133, %result1_134 = torch.aten.var_mean.correction %4497, %4498, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4499 = torch.aten.add.Scalar %result0_133, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4500 = torch.aten.rsqrt %4499 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4501 = torch.aten.sub.Tensor %4496, %result1_134, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4502 = torch.aten.mul.Tensor %4501, %4500 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %4503 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4504 = torch.aten.mul.Tensor %4502, %4503 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %4505 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4506 = torch.aten.add.Tensor %4504, %4505, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4507 = torch.prims.convert_element_type %4506, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4508 = torch.prims.convert_element_type %result1_134, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4509 = torch.prims.convert_element_type %4500, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %4510 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4511 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4512 = torch.aten.view %4507, %4511 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4513 = torch_c.to_builtin_tensor %4512 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4514 = torch_c.to_builtin_tensor %4510 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4515 = tensor.empty() : tensor<2048x1280xf32>
    %4516 = linalg.fill ins(%cst : f32) outs(%4515 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4517 = tensor.empty() : tensor<2048x1280xf32>
    %4518 = linalg.fill ins(%cst : f32) outs(%4517 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4519:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4516, %4518, %4513, %4514, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4516, %4518)
    %4520 = arith.truncf %4519#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4521 = torch_c.from_builtin_tensor %4520 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4522 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4523 = torch.aten.view %4521, %4522 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %4524 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4525 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4526 = torch.aten.view %4507, %4525 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4527 = torch_c.to_builtin_tensor %4526 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4528 = torch_c.to_builtin_tensor %4524 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4529 = tensor.empty() : tensor<2048x1280xf32>
    %4530 = linalg.fill ins(%cst : f32) outs(%4529 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4531 = tensor.empty() : tensor<2048x1280xf32>
    %4532 = linalg.fill ins(%cst : f32) outs(%4531 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4533:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4530, %4532, %4527, %4528, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4530, %4532)
    %4534 = arith.truncf %4533#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4535 = torch_c.from_builtin_tensor %4534 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4536 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4537 = torch.aten.view %4535, %4536 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %4538 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4539 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4540 = torch.aten.view %4507, %4539 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4541 = torch_c.to_builtin_tensor %4540 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4542 = torch_c.to_builtin_tensor %4538 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4543 = tensor.empty() : tensor<2048x1280xf32>
    %4544 = linalg.fill ins(%cst : f32) outs(%4543 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4545 = tensor.empty() : tensor<2048x1280xf32>
    %4546 = linalg.fill ins(%cst : f32) outs(%4545 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4547:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4544, %4546, %4541, %4542, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4544, %4546)
    %4548 = arith.truncf %4547#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4549 = torch_c.from_builtin_tensor %4548 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4550 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4551 = torch.aten.view %4549, %4550 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4552 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4553 = torch.aten.view %4523, %4552 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4554 = torch.aten.transpose.int %4553, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4555 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4556 = torch.aten.view %4537, %4555 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4557 = torch.aten.transpose.int %4556, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4558 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4559 = torch.aten.view %4551, %4558 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4560 = torch.aten.transpose.int %4559, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4561:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4554, %4557, %4560, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4562 = torch.aten.transpose.int %4561#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4563 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4564 = torch.aten.view %4562, %4563 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4565 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4566 = torch.aten.view %4564, %4565 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4567 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4568 = torch.aten.transpose.int %4567, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %4569 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4570 = torch.prims.convert_element_type %4569, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4571 = torch.prims.convert_element_type %4566, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4572 = torch.prims.convert_element_type %4568, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4573 = torch.aten.mm %4571, %4572 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4574 = torch.aten.mul.Scalar %4573, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4575 = torch.aten.mul.Scalar %4570, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4576 = torch.aten.add.Tensor %4574, %4575, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4577 = torch.prims.convert_element_type %4576, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4578 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4579 = torch.aten.view %4577, %4578 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4580 = torch.aten.div.Scalar %4579, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4581 = torch.aten.add.Tensor %4580, %4496, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4582 = torch.prims.convert_element_type %4581, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4583 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_135, %result1_136 = torch.aten.var_mean.correction %4582, %4583, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4584 = torch.aten.add.Scalar %result0_135, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4585 = torch.aten.rsqrt %4584 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4586 = torch.aten.sub.Tensor %4581, %result1_136, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4587 = torch.aten.mul.Tensor %4586, %4585 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %4588 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4589 = torch.aten.mul.Tensor %4587, %4588 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %4590 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4591 = torch.aten.add.Tensor %4589, %4590, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4592 = torch.prims.convert_element_type %4591, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4593 = torch.prims.convert_element_type %result1_136, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4594 = torch.prims.convert_element_type %4585, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %4595 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4596 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4597 = torch.aten.view %4592, %4596 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4598 = torch_c.to_builtin_tensor %4597 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4599 = torch_c.to_builtin_tensor %4595 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4600 = tensor.empty() : tensor<2048x1280xf32>
    %4601 = linalg.fill ins(%cst : f32) outs(%4600 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4602 = tensor.empty() : tensor<2048x1280xf32>
    %4603 = linalg.fill ins(%cst : f32) outs(%4602 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4604:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4601, %4603, %4598, %4599, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4601, %4603)
    %4605 = arith.truncf %4604#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4606 = torch_c.from_builtin_tensor %4605 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4607 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4608 = torch.aten.view %4606, %4607 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %4609 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4610 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4611 = torch.aten.view %4, %4610 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4612 = torch_c.to_builtin_tensor %4611 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4613 = torch_c.to_builtin_tensor %4609 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4614 = tensor.empty() : tensor<128x1280xf32>
    %4615 = linalg.fill ins(%cst : f32) outs(%4614 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4616 = tensor.empty() : tensor<128x1280xf32>
    %4617 = linalg.fill ins(%cst : f32) outs(%4616 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4618:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4615, %4617, %4612, %4613, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4615, %4617)
    %4619 = arith.truncf %4618#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4620 = torch_c.from_builtin_tensor %4619 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4621 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4622 = torch.aten.view %4620, %4621 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %4623 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4624 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4625 = torch.aten.view %4, %4624 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4626 = torch_c.to_builtin_tensor %4625 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4627 = torch_c.to_builtin_tensor %4623 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4628 = tensor.empty() : tensor<128x1280xf32>
    %4629 = linalg.fill ins(%cst : f32) outs(%4628 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4630 = tensor.empty() : tensor<128x1280xf32>
    %4631 = linalg.fill ins(%cst : f32) outs(%4630 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4632:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4629, %4631, %4626, %4627, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4629, %4631)
    %4633 = arith.truncf %4632#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4634 = torch_c.from_builtin_tensor %4633 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4635 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4636 = torch.aten.view %4634, %4635 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4637 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4638 = torch.aten.view %4608, %4637 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4639 = torch.aten.transpose.int %4638, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4640 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4641 = torch.aten.view %4622, %4640 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4642 = torch.aten.transpose.int %4641, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4643 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4644 = torch.aten.view %4636, %4643 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4645 = torch.aten.transpose.int %4644, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4646:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4639, %4642, %4645, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4647 = torch.aten.transpose.int %4646#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4648 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4649 = torch.aten.view %4647, %4648 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4650 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4651 = torch.aten.view %4649, %4650 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4652 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4653 = torch.aten.transpose.int %4652, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %4654 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4655 = torch.prims.convert_element_type %4654, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4656 = torch.prims.convert_element_type %4651, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4657 = torch.prims.convert_element_type %4653, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4658 = torch.aten.mm %4656, %4657 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4659 = torch.aten.mul.Scalar %4658, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4660 = torch.aten.mul.Scalar %4655, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4661 = torch.aten.add.Tensor %4659, %4660, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4662 = torch.prims.convert_element_type %4661, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4663 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4664 = torch.aten.view %4662, %4663 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4665 = torch.aten.div.Scalar %4664, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4666 = torch.aten.add.Tensor %4665, %4581, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4667 = torch.prims.convert_element_type %4666, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4668 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_137, %result1_138 = torch.aten.var_mean.correction %4667, %4668, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4669 = torch.aten.add.Scalar %result0_137, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4670 = torch.aten.rsqrt %4669 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4671 = torch.aten.sub.Tensor %4666, %result1_138, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4672 = torch.aten.mul.Tensor %4671, %4670 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %4673 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4674 = torch.aten.mul.Tensor %4672, %4673 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %4675 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4676 = torch.aten.add.Tensor %4674, %4675, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4677 = torch.prims.convert_element_type %4676, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4678 = torch.prims.convert_element_type %result1_138, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4679 = torch.prims.convert_element_type %4670, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4680 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4681 = torch.aten.view %4677, %4680 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4682 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4683 = torch.aten.transpose.int %4682, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %4684 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4685 = torch.prims.convert_element_type %4684, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4686 = torch.prims.convert_element_type %4681, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4687 = torch.prims.convert_element_type %4683, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4688 = torch.aten.mm %4686, %4687 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4689 = torch.aten.mul.Scalar %4688, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4690 = torch.aten.mul.Scalar %4685, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4691 = torch.aten.add.Tensor %4689, %4690, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4692 = torch.prims.convert_element_type %4691, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4693 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4694 = torch.aten.view %4692, %4693 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4695 = torch.aten.slice.Tensor %4694, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4696 = torch.aten.slice.Tensor %4694, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4697 = torch.aten.gelu %4696, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4698 = torch.aten.mul.Tensor %4695, %4697 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4699 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4700 = torch.aten.view %4698, %4699 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %4701 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4702 = torch.aten.transpose.int %4701, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %4703 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4704 = torch.prims.convert_element_type %4703, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4705 = torch.prims.convert_element_type %4700, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4706 = torch.prims.convert_element_type %4702, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4707 = torch.aten.mm %4705, %4706 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4708 = torch.aten.mul.Scalar %4707, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4709 = torch.aten.mul.Scalar %4704, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4710 = torch.aten.add.Tensor %4708, %4709, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4711 = torch.prims.convert_element_type %4710, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4712 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4713 = torch.aten.view %4711, %4712 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4714 = torch.aten.add.Tensor %4713, %4666, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4715 = torch.prims.convert_element_type %4714, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4716 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_139, %result1_140 = torch.aten.var_mean.correction %4715, %4716, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4717 = torch.aten.add.Scalar %result0_139, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4718 = torch.aten.rsqrt %4717 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4719 = torch.aten.sub.Tensor %4714, %result1_140, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4720 = torch.aten.mul.Tensor %4719, %4718 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %4721 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4722 = torch.aten.mul.Tensor %4720, %4721 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %4723 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4724 = torch.aten.add.Tensor %4722, %4723, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4725 = torch.prims.convert_element_type %4724, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4726 = torch.prims.convert_element_type %result1_140, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4727 = torch.prims.convert_element_type %4718, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %4728 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4729 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4730 = torch.aten.view %4725, %4729 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4731 = torch_c.to_builtin_tensor %4730 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4732 = torch_c.to_builtin_tensor %4728 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4733 = tensor.empty() : tensor<2048x1280xf32>
    %4734 = linalg.fill ins(%cst : f32) outs(%4733 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4735 = tensor.empty() : tensor<2048x1280xf32>
    %4736 = linalg.fill ins(%cst : f32) outs(%4735 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4737:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4734, %4736, %4731, %4732, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4734, %4736)
    %4738 = arith.truncf %4737#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4739 = torch_c.from_builtin_tensor %4738 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4740 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4741 = torch.aten.view %4739, %4740 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %4742 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4743 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4744 = torch.aten.view %4725, %4743 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4745 = torch_c.to_builtin_tensor %4744 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4746 = torch_c.to_builtin_tensor %4742 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4747 = tensor.empty() : tensor<2048x1280xf32>
    %4748 = linalg.fill ins(%cst : f32) outs(%4747 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4749 = tensor.empty() : tensor<2048x1280xf32>
    %4750 = linalg.fill ins(%cst : f32) outs(%4749 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4751:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4748, %4750, %4745, %4746, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4748, %4750)
    %4752 = arith.truncf %4751#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4753 = torch_c.from_builtin_tensor %4752 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4754 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4755 = torch.aten.view %4753, %4754 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %4756 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4757 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4758 = torch.aten.view %4725, %4757 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4759 = torch_c.to_builtin_tensor %4758 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4760 = torch_c.to_builtin_tensor %4756 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4761 = tensor.empty() : tensor<2048x1280xf32>
    %4762 = linalg.fill ins(%cst : f32) outs(%4761 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4763 = tensor.empty() : tensor<2048x1280xf32>
    %4764 = linalg.fill ins(%cst : f32) outs(%4763 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4765:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4762, %4764, %4759, %4760, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4762, %4764)
    %4766 = arith.truncf %4765#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4767 = torch_c.from_builtin_tensor %4766 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4768 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4769 = torch.aten.view %4767, %4768 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4770 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4771 = torch.aten.view %4741, %4770 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4772 = torch.aten.transpose.int %4771, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4773 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4774 = torch.aten.view %4755, %4773 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4775 = torch.aten.transpose.int %4774, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4776 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4777 = torch.aten.view %4769, %4776 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4778 = torch.aten.transpose.int %4777, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4779:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4772, %4775, %4778, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4780 = torch.aten.transpose.int %4779#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4781 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4782 = torch.aten.view %4780, %4781 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4783 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4784 = torch.aten.view %4782, %4783 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4785 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4786 = torch.aten.transpose.int %4785, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %4787 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4788 = torch.prims.convert_element_type %4787, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4789 = torch.prims.convert_element_type %4784, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4790 = torch.prims.convert_element_type %4786, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4791 = torch.aten.mm %4789, %4790 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4792 = torch.aten.mul.Scalar %4791, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4793 = torch.aten.mul.Scalar %4788, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4794 = torch.aten.add.Tensor %4792, %4793, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4795 = torch.prims.convert_element_type %4794, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4796 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4797 = torch.aten.view %4795, %4796 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4798 = torch.aten.div.Scalar %4797, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4799 = torch.aten.add.Tensor %4798, %4714, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4800 = torch.prims.convert_element_type %4799, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4801 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_141, %result1_142 = torch.aten.var_mean.correction %4800, %4801, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4802 = torch.aten.add.Scalar %result0_141, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4803 = torch.aten.rsqrt %4802 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4804 = torch.aten.sub.Tensor %4799, %result1_142, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4805 = torch.aten.mul.Tensor %4804, %4803 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %4806 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4807 = torch.aten.mul.Tensor %4805, %4806 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %4808 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4809 = torch.aten.add.Tensor %4807, %4808, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4810 = torch.prims.convert_element_type %4809, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4811 = torch.prims.convert_element_type %result1_142, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4812 = torch.prims.convert_element_type %4803, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %4813 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4814 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4815 = torch.aten.view %4810, %4814 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4816 = torch_c.to_builtin_tensor %4815 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4817 = torch_c.to_builtin_tensor %4813 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4818 = tensor.empty() : tensor<2048x1280xf32>
    %4819 = linalg.fill ins(%cst : f32) outs(%4818 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4820 = tensor.empty() : tensor<2048x1280xf32>
    %4821 = linalg.fill ins(%cst : f32) outs(%4820 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4822:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4819, %4821, %4816, %4817, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4819, %4821)
    %4823 = arith.truncf %4822#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4824 = torch_c.from_builtin_tensor %4823 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4825 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4826 = torch.aten.view %4824, %4825 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %4827 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4828 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4829 = torch.aten.view %4, %4828 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4830 = torch_c.to_builtin_tensor %4829 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4831 = torch_c.to_builtin_tensor %4827 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4832 = tensor.empty() : tensor<128x1280xf32>
    %4833 = linalg.fill ins(%cst : f32) outs(%4832 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4834 = tensor.empty() : tensor<128x1280xf32>
    %4835 = linalg.fill ins(%cst : f32) outs(%4834 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4836:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4833, %4835, %4830, %4831, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4833, %4835)
    %4837 = arith.truncf %4836#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4838 = torch_c.from_builtin_tensor %4837 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4839 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4840 = torch.aten.view %4838, %4839 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %4841 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %4842 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %4843 = torch.aten.view %4, %4842 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %4844 = torch_c.to_builtin_tensor %4843 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %4845 = torch_c.to_builtin_tensor %4841 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %4846 = tensor.empty() : tensor<128x1280xf32>
    %4847 = linalg.fill ins(%cst : f32) outs(%4846 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4848 = tensor.empty() : tensor<128x1280xf32>
    %4849 = linalg.fill ins(%cst : f32) outs(%4848 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %4850:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %4847, %4849, %4844, %4845, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4847, %4849)
    %4851 = arith.truncf %4850#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %4852 = torch_c.from_builtin_tensor %4851 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %4853 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4854 = torch.aten.view %4852, %4853 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %4855 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4856 = torch.aten.view %4826, %4855 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4857 = torch.aten.transpose.int %4856, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4858 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4859 = torch.aten.view %4840, %4858 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4860 = torch.aten.transpose.int %4859, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4861 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4862 = torch.aten.view %4854, %4861 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %4863 = torch.aten.transpose.int %4862, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %4864:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4857, %4860, %4863, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4865 = torch.aten.transpose.int %4864#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4866 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4867 = torch.aten.view %4865, %4866 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4868 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4869 = torch.aten.view %4867, %4868 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4870 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4871 = torch.aten.transpose.int %4870, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %4872 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4873 = torch.prims.convert_element_type %4872, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4874 = torch.prims.convert_element_type %4869, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4875 = torch.prims.convert_element_type %4871, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4876 = torch.aten.mm %4874, %4875 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4877 = torch.aten.mul.Scalar %4876, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4878 = torch.aten.mul.Scalar %4873, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4879 = torch.aten.add.Tensor %4877, %4878, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4880 = torch.prims.convert_element_type %4879, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4881 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4882 = torch.aten.view %4880, %4881 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4883 = torch.aten.div.Scalar %4882, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %4884 = torch.aten.add.Tensor %4883, %4799, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4885 = torch.prims.convert_element_type %4884, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4886 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_143, %result1_144 = torch.aten.var_mean.correction %4885, %4886, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4887 = torch.aten.add.Scalar %result0_143, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4888 = torch.aten.rsqrt %4887 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4889 = torch.aten.sub.Tensor %4884, %result1_144, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4890 = torch.aten.mul.Tensor %4889, %4888 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %4891 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4892 = torch.aten.mul.Tensor %4890, %4891 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %4893 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4894 = torch.aten.add.Tensor %4892, %4893, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4895 = torch.prims.convert_element_type %4894, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4896 = torch.prims.convert_element_type %result1_144, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4897 = torch.prims.convert_element_type %4888, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4898 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4899 = torch.aten.view %4895, %4898 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4900 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %4901 = torch.aten.transpose.int %4900, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %4902 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %4903 = torch.prims.convert_element_type %4902, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %4904 = torch.prims.convert_element_type %4899, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4905 = torch.prims.convert_element_type %4901, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4906 = torch.aten.mm %4904, %4905 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %4907 = torch.aten.mul.Scalar %4906, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4908 = torch.aten.mul.Scalar %4903, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %4909 = torch.aten.add.Tensor %4907, %4908, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %4910 = torch.prims.convert_element_type %4909, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %4911 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4912 = torch.aten.view %4910, %4911 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %4913 = torch.aten.slice.Tensor %4912, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4914 = torch.aten.slice.Tensor %4912, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %4915 = torch.aten.gelu %4914, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %4916 = torch.aten.mul.Tensor %4913, %4915 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %4917 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4918 = torch.aten.view %4916, %4917 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %4919 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %4920 = torch.aten.transpose.int %4919, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %4921 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4922 = torch.prims.convert_element_type %4921, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %4923 = torch.prims.convert_element_type %4918, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %4924 = torch.prims.convert_element_type %4920, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4925 = torch.aten.mm %4923, %4924 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %4926 = torch.aten.mul.Scalar %4925, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4927 = torch.aten.mul.Scalar %4922, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %4928 = torch.aten.add.Tensor %4926, %4927, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %4929 = torch.prims.convert_element_type %4928, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %4930 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4931 = torch.aten.view %4929, %4930 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4932 = torch.aten.add.Tensor %4931, %4884, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4933 = torch.prims.convert_element_type %4932, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4934 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_145, %result1_146 = torch.aten.var_mean.correction %4933, %4934, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %4935 = torch.aten.add.Scalar %result0_145, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %4936 = torch.aten.rsqrt %4935 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %4937 = torch.aten.sub.Tensor %4932, %result1_146, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4938 = torch.aten.mul.Tensor %4937, %4936 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %4939 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4940 = torch.aten.mul.Tensor %4938, %4939 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %4941 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4942 = torch.aten.add.Tensor %4940, %4941, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %4943 = torch.prims.convert_element_type %4942, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %4944 = torch.prims.convert_element_type %result1_146, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %4945 = torch.prims.convert_element_type %4936, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %4946 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4947 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4948 = torch.aten.view %4943, %4947 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4949 = torch_c.to_builtin_tensor %4948 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4950 = torch_c.to_builtin_tensor %4946 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4951 = tensor.empty() : tensor<2048x1280xf32>
    %4952 = linalg.fill ins(%cst : f32) outs(%4951 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4953 = tensor.empty() : tensor<2048x1280xf32>
    %4954 = linalg.fill ins(%cst : f32) outs(%4953 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4955:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4952, %4954, %4949, %4950, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4952, %4954)
    %4956 = arith.truncf %4955#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4957 = torch_c.from_builtin_tensor %4956 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4958 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4959 = torch.aten.view %4957, %4958 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %4960 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4961 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4962 = torch.aten.view %4943, %4961 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4963 = torch_c.to_builtin_tensor %4962 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4964 = torch_c.to_builtin_tensor %4960 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4965 = tensor.empty() : tensor<2048x1280xf32>
    %4966 = linalg.fill ins(%cst : f32) outs(%4965 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4967 = tensor.empty() : tensor<2048x1280xf32>
    %4968 = linalg.fill ins(%cst : f32) outs(%4967 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4969:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4966, %4968, %4963, %4964, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4966, %4968)
    %4970 = arith.truncf %4969#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4971 = torch_c.from_builtin_tensor %4970 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4972 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4973 = torch.aten.view %4971, %4972 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %4974 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %4975 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %4976 = torch.aten.view %4943, %4975 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %4977 = torch_c.to_builtin_tensor %4976 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %4978 = torch_c.to_builtin_tensor %4974 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %4979 = tensor.empty() : tensor<2048x1280xf32>
    %4980 = linalg.fill ins(%cst : f32) outs(%4979 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4981 = tensor.empty() : tensor<2048x1280xf32>
    %4982 = linalg.fill ins(%cst : f32) outs(%4981 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %4983:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %4980, %4982, %4977, %4978, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%4980, %4982)
    %4984 = arith.truncf %4983#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %4985 = torch_c.from_builtin_tensor %4984 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %4986 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4987 = torch.aten.view %4985, %4986 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %4988 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4989 = torch.aten.view %4959, %4988 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4990 = torch.aten.transpose.int %4989, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4991 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4992 = torch.aten.view %4973, %4991 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4993 = torch.aten.transpose.int %4992, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4994 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4995 = torch.aten.view %4987, %4994 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %4996 = torch.aten.transpose.int %4995, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %4997:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4990, %4993, %4996, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %4998 = torch.aten.transpose.int %4997#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %4999 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5000 = torch.aten.view %4998, %4999 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5001 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5002 = torch.aten.view %5000, %5001 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5003 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5004 = torch.aten.transpose.int %5003, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %5005 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5006 = torch.prims.convert_element_type %5005, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5007 = torch.prims.convert_element_type %5002, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5008 = torch.prims.convert_element_type %5004, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5009 = torch.aten.mm %5007, %5008 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5010 = torch.aten.mul.Scalar %5009, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5011 = torch.aten.mul.Scalar %5006, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5012 = torch.aten.add.Tensor %5010, %5011, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5013 = torch.prims.convert_element_type %5012, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5014 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5015 = torch.aten.view %5013, %5014 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5016 = torch.aten.div.Scalar %5015, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5017 = torch.aten.add.Tensor %5016, %4932, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5018 = torch.prims.convert_element_type %5017, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5019 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_147, %result1_148 = torch.aten.var_mean.correction %5018, %5019, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5020 = torch.aten.add.Scalar %result0_147, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5021 = torch.aten.rsqrt %5020 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5022 = torch.aten.sub.Tensor %5017, %result1_148, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5023 = torch.aten.mul.Tensor %5022, %5021 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %5024 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5025 = torch.aten.mul.Tensor %5023, %5024 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %5026 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5027 = torch.aten.add.Tensor %5025, %5026, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5028 = torch.prims.convert_element_type %5027, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5029 = torch.prims.convert_element_type %result1_148, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5030 = torch.prims.convert_element_type %5021, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %5031 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5032 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5033 = torch.aten.view %5028, %5032 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5034 = torch_c.to_builtin_tensor %5033 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5035 = torch_c.to_builtin_tensor %5031 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5036 = tensor.empty() : tensor<2048x1280xf32>
    %5037 = linalg.fill ins(%cst : f32) outs(%5036 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5038 = tensor.empty() : tensor<2048x1280xf32>
    %5039 = linalg.fill ins(%cst : f32) outs(%5038 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5040:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5037, %5039, %5034, %5035, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5037, %5039)
    %5041 = arith.truncf %5040#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5042 = torch_c.from_builtin_tensor %5041 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5043 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5044 = torch.aten.view %5042, %5043 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %5045 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5046 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5047 = torch.aten.view %4, %5046 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5048 = torch_c.to_builtin_tensor %5047 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5049 = torch_c.to_builtin_tensor %5045 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5050 = tensor.empty() : tensor<128x1280xf32>
    %5051 = linalg.fill ins(%cst : f32) outs(%5050 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5052 = tensor.empty() : tensor<128x1280xf32>
    %5053 = linalg.fill ins(%cst : f32) outs(%5052 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5054:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5051, %5053, %5048, %5049, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5051, %5053)
    %5055 = arith.truncf %5054#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5056 = torch_c.from_builtin_tensor %5055 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5057 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5058 = torch.aten.view %5056, %5057 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %5059 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5060 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5061 = torch.aten.view %4, %5060 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5062 = torch_c.to_builtin_tensor %5061 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5063 = torch_c.to_builtin_tensor %5059 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5064 = tensor.empty() : tensor<128x1280xf32>
    %5065 = linalg.fill ins(%cst : f32) outs(%5064 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5066 = tensor.empty() : tensor<128x1280xf32>
    %5067 = linalg.fill ins(%cst : f32) outs(%5066 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5068:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5065, %5067, %5062, %5063, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5065, %5067)
    %5069 = arith.truncf %5068#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5070 = torch_c.from_builtin_tensor %5069 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5071 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5072 = torch.aten.view %5070, %5071 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5073 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5074 = torch.aten.view %5044, %5073 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5075 = torch.aten.transpose.int %5074, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5076 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5077 = torch.aten.view %5058, %5076 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5078 = torch.aten.transpose.int %5077, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5079 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5080 = torch.aten.view %5072, %5079 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5081 = torch.aten.transpose.int %5080, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5082:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5075, %5078, %5081, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5083 = torch.aten.transpose.int %5082#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5084 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5085 = torch.aten.view %5083, %5084 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5086 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5087 = torch.aten.view %5085, %5086 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5088 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5089 = torch.aten.transpose.int %5088, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %5090 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5091 = torch.prims.convert_element_type %5090, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5092 = torch.prims.convert_element_type %5087, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5093 = torch.prims.convert_element_type %5089, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5094 = torch.aten.mm %5092, %5093 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5095 = torch.aten.mul.Scalar %5094, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5096 = torch.aten.mul.Scalar %5091, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5097 = torch.aten.add.Tensor %5095, %5096, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5098 = torch.prims.convert_element_type %5097, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5099 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5100 = torch.aten.view %5098, %5099 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5101 = torch.aten.div.Scalar %5100, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5102 = torch.aten.add.Tensor %5101, %5017, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5103 = torch.prims.convert_element_type %5102, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5104 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_149, %result1_150 = torch.aten.var_mean.correction %5103, %5104, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5105 = torch.aten.add.Scalar %result0_149, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5106 = torch.aten.rsqrt %5105 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5107 = torch.aten.sub.Tensor %5102, %result1_150, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5108 = torch.aten.mul.Tensor %5107, %5106 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %5109 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5110 = torch.aten.mul.Tensor %5108, %5109 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %5111 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5112 = torch.aten.add.Tensor %5110, %5111, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5113 = torch.prims.convert_element_type %5112, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5114 = torch.prims.convert_element_type %result1_150, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5115 = torch.prims.convert_element_type %5106, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5116 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5117 = torch.aten.view %5113, %5116 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5118 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5119 = torch.aten.transpose.int %5118, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %5120 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5121 = torch.prims.convert_element_type %5120, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5122 = torch.prims.convert_element_type %5117, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5123 = torch.prims.convert_element_type %5119, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5124 = torch.aten.mm %5122, %5123 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5125 = torch.aten.mul.Scalar %5124, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5126 = torch.aten.mul.Scalar %5121, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5127 = torch.aten.add.Tensor %5125, %5126, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5128 = torch.prims.convert_element_type %5127, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5129 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5130 = torch.aten.view %5128, %5129 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5131 = torch.aten.slice.Tensor %5130, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5132 = torch.aten.slice.Tensor %5130, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5133 = torch.aten.gelu %5132, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5134 = torch.aten.mul.Tensor %5131, %5133 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5135 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5136 = torch.aten.view %5134, %5135 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %5137 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5138 = torch.aten.transpose.int %5137, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %5139 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5140 = torch.prims.convert_element_type %5139, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5141 = torch.prims.convert_element_type %5136, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5142 = torch.prims.convert_element_type %5138, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5143 = torch.aten.mm %5141, %5142 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5144 = torch.aten.mul.Scalar %5143, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5145 = torch.aten.mul.Scalar %5140, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5146 = torch.aten.add.Tensor %5144, %5145, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5147 = torch.prims.convert_element_type %5146, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5148 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5149 = torch.aten.view %5147, %5148 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5150 = torch.aten.add.Tensor %5149, %5102, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5151 = torch.prims.convert_element_type %5150, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5152 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_151, %result1_152 = torch.aten.var_mean.correction %5151, %5152, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5153 = torch.aten.add.Scalar %result0_151, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5154 = torch.aten.rsqrt %5153 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5155 = torch.aten.sub.Tensor %5150, %result1_152, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5156 = torch.aten.mul.Tensor %5155, %5154 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %5157 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5158 = torch.aten.mul.Tensor %5156, %5157 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %5159 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5160 = torch.aten.add.Tensor %5158, %5159, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5161 = torch.prims.convert_element_type %5160, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5162 = torch.prims.convert_element_type %result1_152, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5163 = torch.prims.convert_element_type %5154, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %5164 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5165 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5166 = torch.aten.view %5161, %5165 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5167 = torch_c.to_builtin_tensor %5166 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5168 = torch_c.to_builtin_tensor %5164 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5169 = tensor.empty() : tensor<2048x1280xf32>
    %5170 = linalg.fill ins(%cst : f32) outs(%5169 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5171 = tensor.empty() : tensor<2048x1280xf32>
    %5172 = linalg.fill ins(%cst : f32) outs(%5171 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5173:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5170, %5172, %5167, %5168, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5170, %5172)
    %5174 = arith.truncf %5173#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5175 = torch_c.from_builtin_tensor %5174 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5176 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5177 = torch.aten.view %5175, %5176 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %5178 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5179 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5180 = torch.aten.view %5161, %5179 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5181 = torch_c.to_builtin_tensor %5180 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5182 = torch_c.to_builtin_tensor %5178 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5183 = tensor.empty() : tensor<2048x1280xf32>
    %5184 = linalg.fill ins(%cst : f32) outs(%5183 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5185 = tensor.empty() : tensor<2048x1280xf32>
    %5186 = linalg.fill ins(%cst : f32) outs(%5185 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5187:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5184, %5186, %5181, %5182, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5184, %5186)
    %5188 = arith.truncf %5187#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5189 = torch_c.from_builtin_tensor %5188 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5190 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5191 = torch.aten.view %5189, %5190 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %5192 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5193 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5194 = torch.aten.view %5161, %5193 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5195 = torch_c.to_builtin_tensor %5194 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5196 = torch_c.to_builtin_tensor %5192 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5197 = tensor.empty() : tensor<2048x1280xf32>
    %5198 = linalg.fill ins(%cst : f32) outs(%5197 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5199 = tensor.empty() : tensor<2048x1280xf32>
    %5200 = linalg.fill ins(%cst : f32) outs(%5199 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5201:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5198, %5200, %5195, %5196, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5198, %5200)
    %5202 = arith.truncf %5201#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5203 = torch_c.from_builtin_tensor %5202 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5204 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5205 = torch.aten.view %5203, %5204 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5206 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5207 = torch.aten.view %5177, %5206 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5208 = torch.aten.transpose.int %5207, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5209 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5210 = torch.aten.view %5191, %5209 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5211 = torch.aten.transpose.int %5210, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5212 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5213 = torch.aten.view %5205, %5212 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5214 = torch.aten.transpose.int %5213, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5215:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5208, %5211, %5214, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5216 = torch.aten.transpose.int %5215#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5217 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5218 = torch.aten.view %5216, %5217 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5219 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5220 = torch.aten.view %5218, %5219 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5221 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5222 = torch.aten.transpose.int %5221, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %5223 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5224 = torch.prims.convert_element_type %5223, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5225 = torch.prims.convert_element_type %5220, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5226 = torch.prims.convert_element_type %5222, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5227 = torch.aten.mm %5225, %5226 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5228 = torch.aten.mul.Scalar %5227, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5229 = torch.aten.mul.Scalar %5224, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5230 = torch.aten.add.Tensor %5228, %5229, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5231 = torch.prims.convert_element_type %5230, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5232 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5233 = torch.aten.view %5231, %5232 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5234 = torch.aten.div.Scalar %5233, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5235 = torch.aten.add.Tensor %5234, %5150, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5236 = torch.prims.convert_element_type %5235, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5237 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_153, %result1_154 = torch.aten.var_mean.correction %5236, %5237, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5238 = torch.aten.add.Scalar %result0_153, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5239 = torch.aten.rsqrt %5238 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5240 = torch.aten.sub.Tensor %5235, %result1_154, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5241 = torch.aten.mul.Tensor %5240, %5239 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %5242 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5243 = torch.aten.mul.Tensor %5241, %5242 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %5244 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5245 = torch.aten.add.Tensor %5243, %5244, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5246 = torch.prims.convert_element_type %5245, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5247 = torch.prims.convert_element_type %result1_154, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5248 = torch.prims.convert_element_type %5239, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %5249 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5250 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5251 = torch.aten.view %5246, %5250 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5252 = torch_c.to_builtin_tensor %5251 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5253 = torch_c.to_builtin_tensor %5249 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5254 = tensor.empty() : tensor<2048x1280xf32>
    %5255 = linalg.fill ins(%cst : f32) outs(%5254 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5256 = tensor.empty() : tensor<2048x1280xf32>
    %5257 = linalg.fill ins(%cst : f32) outs(%5256 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5258:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5255, %5257, %5252, %5253, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5255, %5257)
    %5259 = arith.truncf %5258#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5260 = torch_c.from_builtin_tensor %5259 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5261 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5262 = torch.aten.view %5260, %5261 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %5263 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5264 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5265 = torch.aten.view %4, %5264 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5266 = torch_c.to_builtin_tensor %5265 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5267 = torch_c.to_builtin_tensor %5263 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5268 = tensor.empty() : tensor<128x1280xf32>
    %5269 = linalg.fill ins(%cst : f32) outs(%5268 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5270 = tensor.empty() : tensor<128x1280xf32>
    %5271 = linalg.fill ins(%cst : f32) outs(%5270 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5272:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5269, %5271, %5266, %5267, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5269, %5271)
    %5273 = arith.truncf %5272#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5274 = torch_c.from_builtin_tensor %5273 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5275 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5276 = torch.aten.view %5274, %5275 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %5277 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5278 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5279 = torch.aten.view %4, %5278 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5280 = torch_c.to_builtin_tensor %5279 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5281 = torch_c.to_builtin_tensor %5277 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5282 = tensor.empty() : tensor<128x1280xf32>
    %5283 = linalg.fill ins(%cst : f32) outs(%5282 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5284 = tensor.empty() : tensor<128x1280xf32>
    %5285 = linalg.fill ins(%cst : f32) outs(%5284 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5286:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5283, %5285, %5280, %5281, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5283, %5285)
    %5287 = arith.truncf %5286#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5288 = torch_c.from_builtin_tensor %5287 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5289 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5290 = torch.aten.view %5288, %5289 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5291 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5292 = torch.aten.view %5262, %5291 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5293 = torch.aten.transpose.int %5292, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5294 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5295 = torch.aten.view %5276, %5294 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5296 = torch.aten.transpose.int %5295, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5297 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5298 = torch.aten.view %5290, %5297 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5299 = torch.aten.transpose.int %5298, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5300:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5293, %5296, %5299, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5301 = torch.aten.transpose.int %5300#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5302 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5303 = torch.aten.view %5301, %5302 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5304 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5305 = torch.aten.view %5303, %5304 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5306 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5307 = torch.aten.transpose.int %5306, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %5308 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5309 = torch.prims.convert_element_type %5308, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5310 = torch.prims.convert_element_type %5305, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5311 = torch.prims.convert_element_type %5307, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5312 = torch.aten.mm %5310, %5311 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5313 = torch.aten.mul.Scalar %5312, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5314 = torch.aten.mul.Scalar %5309, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5315 = torch.aten.add.Tensor %5313, %5314, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5316 = torch.prims.convert_element_type %5315, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5317 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5318 = torch.aten.view %5316, %5317 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5319 = torch.aten.div.Scalar %5318, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5320 = torch.aten.add.Tensor %5319, %5235, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5321 = torch.prims.convert_element_type %5320, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5322 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_155, %result1_156 = torch.aten.var_mean.correction %5321, %5322, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5323 = torch.aten.add.Scalar %result0_155, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5324 = torch.aten.rsqrt %5323 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5325 = torch.aten.sub.Tensor %5320, %result1_156, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5326 = torch.aten.mul.Tensor %5325, %5324 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %5327 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5328 = torch.aten.mul.Tensor %5326, %5327 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %5329 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5330 = torch.aten.add.Tensor %5328, %5329, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5331 = torch.prims.convert_element_type %5330, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5332 = torch.prims.convert_element_type %result1_156, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5333 = torch.prims.convert_element_type %5324, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5334 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5335 = torch.aten.view %5331, %5334 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5336 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5337 = torch.aten.transpose.int %5336, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %5338 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5339 = torch.prims.convert_element_type %5338, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5340 = torch.prims.convert_element_type %5335, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5341 = torch.prims.convert_element_type %5337, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5342 = torch.aten.mm %5340, %5341 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5343 = torch.aten.mul.Scalar %5342, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5344 = torch.aten.mul.Scalar %5339, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5345 = torch.aten.add.Tensor %5343, %5344, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5346 = torch.prims.convert_element_type %5345, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5347 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5348 = torch.aten.view %5346, %5347 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5349 = torch.aten.slice.Tensor %5348, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5350 = torch.aten.slice.Tensor %5348, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5351 = torch.aten.gelu %5350, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5352 = torch.aten.mul.Tensor %5349, %5351 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5353 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5354 = torch.aten.view %5352, %5353 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %5355 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5356 = torch.aten.transpose.int %5355, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %5357 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5358 = torch.prims.convert_element_type %5357, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5359 = torch.prims.convert_element_type %5354, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5360 = torch.prims.convert_element_type %5356, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5361 = torch.aten.mm %5359, %5360 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5362 = torch.aten.mul.Scalar %5361, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5363 = torch.aten.mul.Scalar %5358, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5364 = torch.aten.add.Tensor %5362, %5363, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5365 = torch.prims.convert_element_type %5364, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5366 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5367 = torch.aten.view %5365, %5366 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5368 = torch.aten.add.Tensor %5367, %5320, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5369 = torch.prims.convert_element_type %5368, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5370 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_157, %result1_158 = torch.aten.var_mean.correction %5369, %5370, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5371 = torch.aten.add.Scalar %result0_157, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5372 = torch.aten.rsqrt %5371 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5373 = torch.aten.sub.Tensor %5368, %result1_158, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5374 = torch.aten.mul.Tensor %5373, %5372 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %5375 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5376 = torch.aten.mul.Tensor %5374, %5375 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %5377 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5378 = torch.aten.add.Tensor %5376, %5377, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5379 = torch.prims.convert_element_type %5378, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5380 = torch.prims.convert_element_type %result1_158, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5381 = torch.prims.convert_element_type %5372, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %5382 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5383 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5384 = torch.aten.view %5379, %5383 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5385 = torch_c.to_builtin_tensor %5384 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5386 = torch_c.to_builtin_tensor %5382 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5387 = tensor.empty() : tensor<2048x1280xf32>
    %5388 = linalg.fill ins(%cst : f32) outs(%5387 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5389 = tensor.empty() : tensor<2048x1280xf32>
    %5390 = linalg.fill ins(%cst : f32) outs(%5389 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5391:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5388, %5390, %5385, %5386, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5388, %5390)
    %5392 = arith.truncf %5391#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5393 = torch_c.from_builtin_tensor %5392 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5394 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5395 = torch.aten.view %5393, %5394 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %5396 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5397 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5398 = torch.aten.view %5379, %5397 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5399 = torch_c.to_builtin_tensor %5398 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5400 = torch_c.to_builtin_tensor %5396 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5401 = tensor.empty() : tensor<2048x1280xf32>
    %5402 = linalg.fill ins(%cst : f32) outs(%5401 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5403 = tensor.empty() : tensor<2048x1280xf32>
    %5404 = linalg.fill ins(%cst : f32) outs(%5403 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5405:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5402, %5404, %5399, %5400, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5402, %5404)
    %5406 = arith.truncf %5405#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5407 = torch_c.from_builtin_tensor %5406 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5408 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5409 = torch.aten.view %5407, %5408 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %5410 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5411 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5412 = torch.aten.view %5379, %5411 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5413 = torch_c.to_builtin_tensor %5412 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5414 = torch_c.to_builtin_tensor %5410 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5415 = tensor.empty() : tensor<2048x1280xf32>
    %5416 = linalg.fill ins(%cst : f32) outs(%5415 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5417 = tensor.empty() : tensor<2048x1280xf32>
    %5418 = linalg.fill ins(%cst : f32) outs(%5417 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5419:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5416, %5418, %5413, %5414, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5416, %5418)
    %5420 = arith.truncf %5419#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5421 = torch_c.from_builtin_tensor %5420 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5422 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5423 = torch.aten.view %5421, %5422 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5424 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5425 = torch.aten.view %5395, %5424 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5426 = torch.aten.transpose.int %5425, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5427 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5428 = torch.aten.view %5409, %5427 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5429 = torch.aten.transpose.int %5428, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5430 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5431 = torch.aten.view %5423, %5430 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5432 = torch.aten.transpose.int %5431, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5433:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5426, %5429, %5432, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5434 = torch.aten.transpose.int %5433#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5435 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5436 = torch.aten.view %5434, %5435 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5437 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5438 = torch.aten.view %5436, %5437 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5439 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5440 = torch.aten.transpose.int %5439, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %5441 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5442 = torch.prims.convert_element_type %5441, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5443 = torch.prims.convert_element_type %5438, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5444 = torch.prims.convert_element_type %5440, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5445 = torch.aten.mm %5443, %5444 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5446 = torch.aten.mul.Scalar %5445, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5447 = torch.aten.mul.Scalar %5442, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5448 = torch.aten.add.Tensor %5446, %5447, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5449 = torch.prims.convert_element_type %5448, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5450 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5451 = torch.aten.view %5449, %5450 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5452 = torch.aten.div.Scalar %5451, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5453 = torch.aten.add.Tensor %5452, %5368, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5454 = torch.prims.convert_element_type %5453, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5455 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_159, %result1_160 = torch.aten.var_mean.correction %5454, %5455, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5456 = torch.aten.add.Scalar %result0_159, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5457 = torch.aten.rsqrt %5456 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5458 = torch.aten.sub.Tensor %5453, %result1_160, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5459 = torch.aten.mul.Tensor %5458, %5457 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %5460 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5461 = torch.aten.mul.Tensor %5459, %5460 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %5462 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5463 = torch.aten.add.Tensor %5461, %5462, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5464 = torch.prims.convert_element_type %5463, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5465 = torch.prims.convert_element_type %result1_160, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5466 = torch.prims.convert_element_type %5457, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %5467 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5468 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5469 = torch.aten.view %5464, %5468 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5470 = torch_c.to_builtin_tensor %5469 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5471 = torch_c.to_builtin_tensor %5467 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5472 = tensor.empty() : tensor<2048x1280xf32>
    %5473 = linalg.fill ins(%cst : f32) outs(%5472 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5474 = tensor.empty() : tensor<2048x1280xf32>
    %5475 = linalg.fill ins(%cst : f32) outs(%5474 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5476:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5473, %5475, %5470, %5471, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5473, %5475)
    %5477 = arith.truncf %5476#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5478 = torch_c.from_builtin_tensor %5477 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5479 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5480 = torch.aten.view %5478, %5479 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %5481 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5482 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5483 = torch.aten.view %4, %5482 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5484 = torch_c.to_builtin_tensor %5483 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5485 = torch_c.to_builtin_tensor %5481 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5486 = tensor.empty() : tensor<128x1280xf32>
    %5487 = linalg.fill ins(%cst : f32) outs(%5486 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5488 = tensor.empty() : tensor<128x1280xf32>
    %5489 = linalg.fill ins(%cst : f32) outs(%5488 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5490:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5487, %5489, %5484, %5485, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5487, %5489)
    %5491 = arith.truncf %5490#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5492 = torch_c.from_builtin_tensor %5491 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5493 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5494 = torch.aten.view %5492, %5493 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %5495 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5496 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5497 = torch.aten.view %4, %5496 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5498 = torch_c.to_builtin_tensor %5497 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5499 = torch_c.to_builtin_tensor %5495 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5500 = tensor.empty() : tensor<128x1280xf32>
    %5501 = linalg.fill ins(%cst : f32) outs(%5500 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5502 = tensor.empty() : tensor<128x1280xf32>
    %5503 = linalg.fill ins(%cst : f32) outs(%5502 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5504:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5501, %5503, %5498, %5499, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5501, %5503)
    %5505 = arith.truncf %5504#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5506 = torch_c.from_builtin_tensor %5505 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5507 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5508 = torch.aten.view %5506, %5507 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5509 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5510 = torch.aten.view %5480, %5509 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5511 = torch.aten.transpose.int %5510, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5512 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5513 = torch.aten.view %5494, %5512 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5514 = torch.aten.transpose.int %5513, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5515 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5516 = torch.aten.view %5508, %5515 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5517 = torch.aten.transpose.int %5516, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5518:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5511, %5514, %5517, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5519 = torch.aten.transpose.int %5518#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5520 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5521 = torch.aten.view %5519, %5520 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5522 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5523 = torch.aten.view %5521, %5522 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5524 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5525 = torch.aten.transpose.int %5524, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %5526 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5527 = torch.prims.convert_element_type %5526, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5528 = torch.prims.convert_element_type %5523, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5529 = torch.prims.convert_element_type %5525, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5530 = torch.aten.mm %5528, %5529 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5531 = torch.aten.mul.Scalar %5530, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5532 = torch.aten.mul.Scalar %5527, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5533 = torch.aten.add.Tensor %5531, %5532, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5534 = torch.prims.convert_element_type %5533, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5535 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5536 = torch.aten.view %5534, %5535 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5537 = torch.aten.div.Scalar %5536, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5538 = torch.aten.add.Tensor %5537, %5453, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5539 = torch.prims.convert_element_type %5538, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5540 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_161, %result1_162 = torch.aten.var_mean.correction %5539, %5540, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5541 = torch.aten.add.Scalar %result0_161, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5542 = torch.aten.rsqrt %5541 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5543 = torch.aten.sub.Tensor %5538, %result1_162, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5544 = torch.aten.mul.Tensor %5543, %5542 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %5545 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5546 = torch.aten.mul.Tensor %5544, %5545 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %5547 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5548 = torch.aten.add.Tensor %5546, %5547, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5549 = torch.prims.convert_element_type %5548, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5550 = torch.prims.convert_element_type %result1_162, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5551 = torch.prims.convert_element_type %5542, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5552 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5553 = torch.aten.view %5549, %5552 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5554 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5555 = torch.aten.transpose.int %5554, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %5556 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5557 = torch.prims.convert_element_type %5556, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5558 = torch.prims.convert_element_type %5553, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5559 = torch.prims.convert_element_type %5555, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5560 = torch.aten.mm %5558, %5559 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5561 = torch.aten.mul.Scalar %5560, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5562 = torch.aten.mul.Scalar %5557, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5563 = torch.aten.add.Tensor %5561, %5562, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5564 = torch.prims.convert_element_type %5563, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5565 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5566 = torch.aten.view %5564, %5565 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5567 = torch.aten.slice.Tensor %5566, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5568 = torch.aten.slice.Tensor %5566, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5569 = torch.aten.gelu %5568, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5570 = torch.aten.mul.Tensor %5567, %5569 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5571 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5572 = torch.aten.view %5570, %5571 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %5573 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5574 = torch.aten.transpose.int %5573, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %5575 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5576 = torch.prims.convert_element_type %5575, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5577 = torch.prims.convert_element_type %5572, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5578 = torch.prims.convert_element_type %5574, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5579 = torch.aten.mm %5577, %5578 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5580 = torch.aten.mul.Scalar %5579, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5581 = torch.aten.mul.Scalar %5576, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5582 = torch.aten.add.Tensor %5580, %5581, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5583 = torch.prims.convert_element_type %5582, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5584 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5585 = torch.aten.view %5583, %5584 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5586 = torch.aten.add.Tensor %5585, %5538, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5587 = torch.prims.convert_element_type %5586, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5588 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_163, %result1_164 = torch.aten.var_mean.correction %5587, %5588, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5589 = torch.aten.add.Scalar %result0_163, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5590 = torch.aten.rsqrt %5589 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5591 = torch.aten.sub.Tensor %5586, %result1_164, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5592 = torch.aten.mul.Tensor %5591, %5590 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %5593 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5594 = torch.aten.mul.Tensor %5592, %5593 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %5595 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5596 = torch.aten.add.Tensor %5594, %5595, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5597 = torch.prims.convert_element_type %5596, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5598 = torch.prims.convert_element_type %result1_164, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5599 = torch.prims.convert_element_type %5590, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %5600 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5601 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5602 = torch.aten.view %5597, %5601 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5603 = torch_c.to_builtin_tensor %5602 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5604 = torch_c.to_builtin_tensor %5600 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5605 = tensor.empty() : tensor<2048x1280xf32>
    %5606 = linalg.fill ins(%cst : f32) outs(%5605 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5607 = tensor.empty() : tensor<2048x1280xf32>
    %5608 = linalg.fill ins(%cst : f32) outs(%5607 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5609:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5606, %5608, %5603, %5604, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5606, %5608)
    %5610 = arith.truncf %5609#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5611 = torch_c.from_builtin_tensor %5610 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5612 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5613 = torch.aten.view %5611, %5612 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %5614 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5615 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5616 = torch.aten.view %5597, %5615 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5617 = torch_c.to_builtin_tensor %5616 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5618 = torch_c.to_builtin_tensor %5614 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5619 = tensor.empty() : tensor<2048x1280xf32>
    %5620 = linalg.fill ins(%cst : f32) outs(%5619 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5621 = tensor.empty() : tensor<2048x1280xf32>
    %5622 = linalg.fill ins(%cst : f32) outs(%5621 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5623:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5620, %5622, %5617, %5618, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5620, %5622)
    %5624 = arith.truncf %5623#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5625 = torch_c.from_builtin_tensor %5624 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5626 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5627 = torch.aten.view %5625, %5626 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %5628 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5629 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5630 = torch.aten.view %5597, %5629 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5631 = torch_c.to_builtin_tensor %5630 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5632 = torch_c.to_builtin_tensor %5628 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5633 = tensor.empty() : tensor<2048x1280xf32>
    %5634 = linalg.fill ins(%cst : f32) outs(%5633 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5635 = tensor.empty() : tensor<2048x1280xf32>
    %5636 = linalg.fill ins(%cst : f32) outs(%5635 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5637:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5634, %5636, %5631, %5632, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5634, %5636)
    %5638 = arith.truncf %5637#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5639 = torch_c.from_builtin_tensor %5638 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5640 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5641 = torch.aten.view %5639, %5640 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5642 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5643 = torch.aten.view %5613, %5642 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5644 = torch.aten.transpose.int %5643, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5645 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5646 = torch.aten.view %5627, %5645 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5647 = torch.aten.transpose.int %5646, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5648 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5649 = torch.aten.view %5641, %5648 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5650 = torch.aten.transpose.int %5649, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5651:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5644, %5647, %5650, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5652 = torch.aten.transpose.int %5651#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5653 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5654 = torch.aten.view %5652, %5653 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5655 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5656 = torch.aten.view %5654, %5655 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5657 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5658 = torch.aten.transpose.int %5657, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %5659 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5660 = torch.prims.convert_element_type %5659, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5661 = torch.prims.convert_element_type %5656, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5662 = torch.prims.convert_element_type %5658, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5663 = torch.aten.mm %5661, %5662 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5664 = torch.aten.mul.Scalar %5663, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5665 = torch.aten.mul.Scalar %5660, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5666 = torch.aten.add.Tensor %5664, %5665, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5667 = torch.prims.convert_element_type %5666, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5668 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5669 = torch.aten.view %5667, %5668 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5670 = torch.aten.div.Scalar %5669, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5671 = torch.aten.add.Tensor %5670, %5586, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5672 = torch.prims.convert_element_type %5671, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5673 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_165, %result1_166 = torch.aten.var_mean.correction %5672, %5673, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5674 = torch.aten.add.Scalar %result0_165, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5675 = torch.aten.rsqrt %5674 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5676 = torch.aten.sub.Tensor %5671, %result1_166, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5677 = torch.aten.mul.Tensor %5676, %5675 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %5678 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5679 = torch.aten.mul.Tensor %5677, %5678 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %5680 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5681 = torch.aten.add.Tensor %5679, %5680, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5682 = torch.prims.convert_element_type %5681, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5683 = torch.prims.convert_element_type %result1_166, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5684 = torch.prims.convert_element_type %5675, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %5685 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5686 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5687 = torch.aten.view %5682, %5686 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5688 = torch_c.to_builtin_tensor %5687 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5689 = torch_c.to_builtin_tensor %5685 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5690 = tensor.empty() : tensor<2048x1280xf32>
    %5691 = linalg.fill ins(%cst : f32) outs(%5690 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5692 = tensor.empty() : tensor<2048x1280xf32>
    %5693 = linalg.fill ins(%cst : f32) outs(%5692 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5694:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5691, %5693, %5688, %5689, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5691, %5693)
    %5695 = arith.truncf %5694#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5696 = torch_c.from_builtin_tensor %5695 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5697 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5698 = torch.aten.view %5696, %5697 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %5699 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5700 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5701 = torch.aten.view %4, %5700 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5702 = torch_c.to_builtin_tensor %5701 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5703 = torch_c.to_builtin_tensor %5699 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5704 = tensor.empty() : tensor<128x1280xf32>
    %5705 = linalg.fill ins(%cst : f32) outs(%5704 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5706 = tensor.empty() : tensor<128x1280xf32>
    %5707 = linalg.fill ins(%cst : f32) outs(%5706 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5708:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5705, %5707, %5702, %5703, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5705, %5707)
    %5709 = arith.truncf %5708#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5710 = torch_c.from_builtin_tensor %5709 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5711 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5712 = torch.aten.view %5710, %5711 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %5713 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5714 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5715 = torch.aten.view %4, %5714 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5716 = torch_c.to_builtin_tensor %5715 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5717 = torch_c.to_builtin_tensor %5713 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5718 = tensor.empty() : tensor<128x1280xf32>
    %5719 = linalg.fill ins(%cst : f32) outs(%5718 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5720 = tensor.empty() : tensor<128x1280xf32>
    %5721 = linalg.fill ins(%cst : f32) outs(%5720 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5722:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5719, %5721, %5716, %5717, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5719, %5721)
    %5723 = arith.truncf %5722#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5724 = torch_c.from_builtin_tensor %5723 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5725 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5726 = torch.aten.view %5724, %5725 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5727 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5728 = torch.aten.view %5698, %5727 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5729 = torch.aten.transpose.int %5728, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5730 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5731 = torch.aten.view %5712, %5730 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5732 = torch.aten.transpose.int %5731, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5733 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5734 = torch.aten.view %5726, %5733 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5735 = torch.aten.transpose.int %5734, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5736:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5729, %5732, %5735, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5737 = torch.aten.transpose.int %5736#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5738 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5739 = torch.aten.view %5737, %5738 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5740 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5741 = torch.aten.view %5739, %5740 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5742 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5743 = torch.aten.transpose.int %5742, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %5744 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5745 = torch.prims.convert_element_type %5744, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5746 = torch.prims.convert_element_type %5741, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5747 = torch.prims.convert_element_type %5743, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5748 = torch.aten.mm %5746, %5747 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5749 = torch.aten.mul.Scalar %5748, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5750 = torch.aten.mul.Scalar %5745, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5751 = torch.aten.add.Tensor %5749, %5750, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5752 = torch.prims.convert_element_type %5751, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5753 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5754 = torch.aten.view %5752, %5753 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5755 = torch.aten.div.Scalar %5754, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5756 = torch.aten.add.Tensor %5755, %5671, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5757 = torch.prims.convert_element_type %5756, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5758 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_167, %result1_168 = torch.aten.var_mean.correction %5757, %5758, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5759 = torch.aten.add.Scalar %result0_167, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5760 = torch.aten.rsqrt %5759 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5761 = torch.aten.sub.Tensor %5756, %result1_168, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5762 = torch.aten.mul.Tensor %5761, %5760 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %5763 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5764 = torch.aten.mul.Tensor %5762, %5763 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %5765 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5766 = torch.aten.add.Tensor %5764, %5765, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5767 = torch.prims.convert_element_type %5766, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5768 = torch.prims.convert_element_type %result1_168, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5769 = torch.prims.convert_element_type %5760, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5770 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5771 = torch.aten.view %5767, %5770 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5772 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5773 = torch.aten.transpose.int %5772, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %5774 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5775 = torch.prims.convert_element_type %5774, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5776 = torch.prims.convert_element_type %5771, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5777 = torch.prims.convert_element_type %5773, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5778 = torch.aten.mm %5776, %5777 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5779 = torch.aten.mul.Scalar %5778, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5780 = torch.aten.mul.Scalar %5775, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5781 = torch.aten.add.Tensor %5779, %5780, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5782 = torch.prims.convert_element_type %5781, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %5783 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5784 = torch.aten.view %5782, %5783 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %5785 = torch.aten.slice.Tensor %5784, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5786 = torch.aten.slice.Tensor %5784, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %5787 = torch.aten.gelu %5786, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %5788 = torch.aten.mul.Tensor %5785, %5787 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %5789 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %5790 = torch.aten.view %5788, %5789 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %5791 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %5792 = torch.aten.transpose.int %5791, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %5793 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5794 = torch.prims.convert_element_type %5793, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5795 = torch.prims.convert_element_type %5790, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %5796 = torch.prims.convert_element_type %5792, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5797 = torch.aten.mm %5795, %5796 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5798 = torch.aten.mul.Scalar %5797, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5799 = torch.aten.mul.Scalar %5794, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5800 = torch.aten.add.Tensor %5798, %5799, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5801 = torch.prims.convert_element_type %5800, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5802 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5803 = torch.aten.view %5801, %5802 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5804 = torch.aten.add.Tensor %5803, %5756, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5805 = torch.prims.convert_element_type %5804, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5806 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_169, %result1_170 = torch.aten.var_mean.correction %5805, %5806, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5807 = torch.aten.add.Scalar %result0_169, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5808 = torch.aten.rsqrt %5807 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5809 = torch.aten.sub.Tensor %5804, %result1_170, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5810 = torch.aten.mul.Tensor %5809, %5808 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %5811 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5812 = torch.aten.mul.Tensor %5810, %5811 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %5813 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5814 = torch.aten.add.Tensor %5812, %5813, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5815 = torch.prims.convert_element_type %5814, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5816 = torch.prims.convert_element_type %result1_170, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5817 = torch.prims.convert_element_type %5808, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %5818 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5819 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5820 = torch.aten.view %5815, %5819 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5821 = torch_c.to_builtin_tensor %5820 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5822 = torch_c.to_builtin_tensor %5818 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5823 = tensor.empty() : tensor<2048x1280xf32>
    %5824 = linalg.fill ins(%cst : f32) outs(%5823 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5825 = tensor.empty() : tensor<2048x1280xf32>
    %5826 = linalg.fill ins(%cst : f32) outs(%5825 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5827:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5824, %5826, %5821, %5822, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5824, %5826)
    %5828 = arith.truncf %5827#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5829 = torch_c.from_builtin_tensor %5828 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5830 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5831 = torch.aten.view %5829, %5830 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %5832 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5833 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5834 = torch.aten.view %5815, %5833 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5835 = torch_c.to_builtin_tensor %5834 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5836 = torch_c.to_builtin_tensor %5832 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5837 = tensor.empty() : tensor<2048x1280xf32>
    %5838 = linalg.fill ins(%cst : f32) outs(%5837 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5839 = tensor.empty() : tensor<2048x1280xf32>
    %5840 = linalg.fill ins(%cst : f32) outs(%5839 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5841:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5838, %5840, %5835, %5836, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5838, %5840)
    %5842 = arith.truncf %5841#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5843 = torch_c.from_builtin_tensor %5842 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5844 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5845 = torch.aten.view %5843, %5844 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %5846 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5847 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5848 = torch.aten.view %5815, %5847 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5849 = torch_c.to_builtin_tensor %5848 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5850 = torch_c.to_builtin_tensor %5846 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5851 = tensor.empty() : tensor<2048x1280xf32>
    %5852 = linalg.fill ins(%cst : f32) outs(%5851 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5853 = tensor.empty() : tensor<2048x1280xf32>
    %5854 = linalg.fill ins(%cst : f32) outs(%5853 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5855:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5852, %5854, %5849, %5850, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5852, %5854)
    %5856 = arith.truncf %5855#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5857 = torch_c.from_builtin_tensor %5856 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5858 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5859 = torch.aten.view %5857, %5858 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5860 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5861 = torch.aten.view %5831, %5860 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5862 = torch.aten.transpose.int %5861, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5863 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5864 = torch.aten.view %5845, %5863 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5865 = torch.aten.transpose.int %5864, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5866 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5867 = torch.aten.view %5859, %5866 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5868 = torch.aten.transpose.int %5867, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5869:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5862, %5865, %5868, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5870 = torch.aten.transpose.int %5869#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5871 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5872 = torch.aten.view %5870, %5871 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5873 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5874 = torch.aten.view %5872, %5873 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5875 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5876 = torch.aten.transpose.int %5875, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %5877 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5878 = torch.prims.convert_element_type %5877, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5879 = torch.prims.convert_element_type %5874, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5880 = torch.prims.convert_element_type %5876, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5881 = torch.aten.mm %5879, %5880 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5882 = torch.aten.mul.Scalar %5881, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5883 = torch.aten.mul.Scalar %5878, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5884 = torch.aten.add.Tensor %5882, %5883, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5885 = torch.prims.convert_element_type %5884, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5886 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5887 = torch.aten.view %5885, %5886 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5888 = torch.aten.div.Scalar %5887, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5889 = torch.aten.add.Tensor %5888, %5804, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5890 = torch.prims.convert_element_type %5889, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5891 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_171, %result1_172 = torch.aten.var_mean.correction %5890, %5891, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5892 = torch.aten.add.Scalar %result0_171, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5893 = torch.aten.rsqrt %5892 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5894 = torch.aten.sub.Tensor %5889, %result1_172, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5895 = torch.aten.mul.Tensor %5894, %5893 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %5896 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5897 = torch.aten.mul.Tensor %5895, %5896 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %5898 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5899 = torch.aten.add.Tensor %5897, %5898, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5900 = torch.prims.convert_element_type %5899, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5901 = torch.prims.convert_element_type %result1_172, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5902 = torch.prims.convert_element_type %5893, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %5903 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5904 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5905 = torch.aten.view %5900, %5904 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %5906 = torch_c.to_builtin_tensor %5905 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %5907 = torch_c.to_builtin_tensor %5903 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %5908 = tensor.empty() : tensor<2048x1280xf32>
    %5909 = linalg.fill ins(%cst : f32) outs(%5908 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5910 = tensor.empty() : tensor<2048x1280xf32>
    %5911 = linalg.fill ins(%cst : f32) outs(%5910 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %5912:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %5909, %5911, %5906, %5907, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5909, %5911)
    %5913 = arith.truncf %5912#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %5914 = torch_c.from_builtin_tensor %5913 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %5915 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5916 = torch.aten.view %5914, %5915 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %5917 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5918 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5919 = torch.aten.view %4, %5918 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5920 = torch_c.to_builtin_tensor %5919 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5921 = torch_c.to_builtin_tensor %5917 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5922 = tensor.empty() : tensor<128x1280xf32>
    %5923 = linalg.fill ins(%cst : f32) outs(%5922 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5924 = tensor.empty() : tensor<128x1280xf32>
    %5925 = linalg.fill ins(%cst : f32) outs(%5924 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5926:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5923, %5925, %5920, %5921, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5923, %5925)
    %5927 = arith.truncf %5926#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5928 = torch_c.from_builtin_tensor %5927 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5929 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5930 = torch.aten.view %5928, %5929 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %5931 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %5932 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %5933 = torch.aten.view %4, %5932 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %5934 = torch_c.to_builtin_tensor %5933 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %5935 = torch_c.to_builtin_tensor %5931 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %5936 = tensor.empty() : tensor<128x1280xf32>
    %5937 = linalg.fill ins(%cst : f32) outs(%5936 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5938 = tensor.empty() : tensor<128x1280xf32>
    %5939 = linalg.fill ins(%cst : f32) outs(%5938 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %5940:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %5937, %5939, %5934, %5935, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%5937, %5939)
    %5941 = arith.truncf %5940#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %5942 = torch_c.from_builtin_tensor %5941 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %5943 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5944 = torch.aten.view %5942, %5943 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %5945 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5946 = torch.aten.view %5916, %5945 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %5947 = torch.aten.transpose.int %5946, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %5948 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5949 = torch.aten.view %5930, %5948 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5950 = torch.aten.transpose.int %5949, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5951 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5952 = torch.aten.view %5944, %5951 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %5953 = torch.aten.transpose.int %5952, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %5954:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5947, %5950, %5953, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %5955 = torch.aten.transpose.int %5954#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %5956 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5957 = torch.aten.view %5955, %5956 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5958 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5959 = torch.aten.view %5957, %5958 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5960 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %5961 = torch.aten.transpose.int %5960, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %5962 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5963 = torch.prims.convert_element_type %5962, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %5964 = torch.prims.convert_element_type %5959, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5965 = torch.prims.convert_element_type %5961, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5966 = torch.aten.mm %5964, %5965 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %5967 = torch.aten.mul.Scalar %5966, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5968 = torch.aten.mul.Scalar %5963, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %5969 = torch.aten.add.Tensor %5967, %5968, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5970 = torch.prims.convert_element_type %5969, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %5971 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5972 = torch.aten.view %5970, %5971 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %5973 = torch.aten.div.Scalar %5972, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %5974 = torch.aten.add.Tensor %5973, %5889, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5975 = torch.prims.convert_element_type %5974, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5976 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_173, %result1_174 = torch.aten.var_mean.correction %5975, %5976, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %5977 = torch.aten.add.Scalar %result0_173, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %5978 = torch.aten.rsqrt %5977 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %5979 = torch.aten.sub.Tensor %5974, %result1_174, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5980 = torch.aten.mul.Tensor %5979, %5978 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %5981 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5982 = torch.aten.mul.Tensor %5980, %5981 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %5983 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5984 = torch.aten.add.Tensor %5982, %5983, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %5985 = torch.prims.convert_element_type %5984, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %5986 = torch.prims.convert_element_type %result1_174, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5987 = torch.prims.convert_element_type %5978, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %5988 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %5989 = torch.aten.view %5985, %5988 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5990 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %5991 = torch.aten.transpose.int %5990, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %5992 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %5993 = torch.prims.convert_element_type %5992, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %5994 = torch.prims.convert_element_type %5989, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %5995 = torch.prims.convert_element_type %5991, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5996 = torch.aten.mm %5994, %5995 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %5997 = torch.aten.mul.Scalar %5996, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %5998 = torch.aten.mul.Scalar %5993, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %5999 = torch.aten.add.Tensor %5997, %5998, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6000 = torch.prims.convert_element_type %5999, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6001 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6002 = torch.aten.view %6000, %6001 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6003 = torch.aten.slice.Tensor %6002, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6004 = torch.aten.slice.Tensor %6002, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6005 = torch.aten.gelu %6004, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6006 = torch.aten.mul.Tensor %6003, %6005 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6007 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6008 = torch.aten.view %6006, %6007 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %6009 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6010 = torch.aten.transpose.int %6009, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %6011 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6012 = torch.prims.convert_element_type %6011, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6013 = torch.prims.convert_element_type %6008, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6014 = torch.prims.convert_element_type %6010, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6015 = torch.aten.mm %6013, %6014 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6016 = torch.aten.mul.Scalar %6015, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6017 = torch.aten.mul.Scalar %6012, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6018 = torch.aten.add.Tensor %6016, %6017, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6019 = torch.prims.convert_element_type %6018, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6020 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6021 = torch.aten.view %6019, %6020 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6022 = torch.aten.add.Tensor %6021, %5974, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6023 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6024 = torch.aten.view %6022, %6023 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_out.weight = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_out.weight : tensor<1280x1280xf16>
    %6025 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6026 = torch.aten.transpose.int %6025, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.down_blocks.2.attentions.1.proj_out.bias = util.global.load @_params.unet.down_blocks.2.attentions.1.proj_out.bias : tensor<1280xf16>
    %6027 = torch_c.from_builtin_tensor %_params.unet.down_blocks.2.attentions.1.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6028 = torch.prims.convert_element_type %6027, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6029 = torch.prims.convert_element_type %6024, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6030 = torch.prims.convert_element_type %6026, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6031 = torch.aten.mm %6029, %6030 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6032 = torch.aten.mul.Scalar %6031, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6033 = torch.aten.mul.Scalar %6028, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6034 = torch.aten.add.Tensor %6032, %6033, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6035 = torch.prims.convert_element_type %6034, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6036 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6037 = torch.aten.view %6035, %6036 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6038 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6039 = torch.aten.view %6037, %6038 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %6040 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6041 = torch.aten.permute %6039, %6040 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %6042 = torch.aten.add.Tensor %6041, %3791, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6043 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6044 = torch.aten.view %6042, %6043 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %6045 = torch.prims.convert_element_type %6044, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %6046 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_175, %result1_176 = torch.aten.var_mean.correction %6045, %6046, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %6047 = torch.aten.add.Scalar %result0_175, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %6048 = torch.aten.rsqrt %6047 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %6049 = torch.aten.sub.Tensor %6044, %result1_176, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %6050 = torch.aten.mul.Tensor %6049, %6048 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %6051 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6052 = torch.aten.view %6050, %6051 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.0.norm1.bias = util.global.load @_params.unet.mid_block.resnets.0.norm1.bias : tensor<1280xf16>
    %6053 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6054 = torch.aten.unsqueeze %6053, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %6055 = torch.aten.unsqueeze %6054, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %6056 = torch.aten.unsqueeze %6055, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.0.norm1.weight = util.global.load @_params.unet.mid_block.resnets.0.norm1.weight : tensor<1280xf16>
    %6057 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6058 = torch.aten.unsqueeze %6057, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %6059 = torch.aten.unsqueeze %6058, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %6060 = torch.aten.unsqueeze %6059, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %6061 = torch.aten.mul.Tensor %6052, %6060 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %6062 = torch.aten.add.Tensor %6061, %6056, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %6063 = torch.prims.convert_element_type %6062, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6064 = torch.prims.convert_element_type %result1_176, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %6065 = torch.prims.convert_element_type %6048, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %6066 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %6067 = torch.prims.squeeze %6064, %6066 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %6068 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %6069 = torch.prims.squeeze %6067, %6068 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %6070 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %6071 = torch.prims.squeeze %6065, %6070 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %6072 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %6073 = torch.prims.squeeze %6071, %6072 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %6074 = torch.aten.silu %6063 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.0.conv1.weight = util.global.load @_params.unet.mid_block.resnets.0.conv1.weight : tensor<1280x1280x3x3xf16>
    %6075 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.0.conv1.bias = util.global.load @_params.unet.mid_block.resnets.0.conv1.bias : tensor<1280xf16>
    %6076 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6077 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6078 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6079 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6080 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %6081 = torch.aten.convolution %6074, %6075, %6076, %6077, %6078, %6079, %false, %6080, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6082 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.mid_block.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.mid_block.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %6083 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6084 = torch.aten.transpose.int %6083, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.mid_block.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %6085 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6086 = torch.prims.convert_element_type %6085, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6087 = torch.prims.convert_element_type %6082, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %6088 = torch.prims.convert_element_type %6084, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6089 = torch.aten.mm %6087, %6088 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %6090 = torch.aten.mul.Scalar %6089, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %6091 = torch.aten.mul.Scalar %6086, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6092 = torch.aten.add.Tensor %6090, %6091, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %6093 = torch.prims.convert_element_type %6092, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %6094 = torch.aten.unsqueeze %6093, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %6095 = torch.aten.unsqueeze %6094, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %6096 = torch.aten.add.Tensor %6081, %6095, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6097 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6098 = torch.aten.view %6096, %6097 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %6099 = torch.prims.convert_element_type %6098, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %6100 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_177, %result1_178 = torch.aten.var_mean.correction %6099, %6100, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %6101 = torch.aten.add.Scalar %result0_177, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %6102 = torch.aten.rsqrt %6101 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %6103 = torch.aten.sub.Tensor %6098, %result1_178, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %6104 = torch.aten.mul.Tensor %6103, %6102 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %6105 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6106 = torch.aten.view %6104, %6105 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.0.norm2.bias = util.global.load @_params.unet.mid_block.resnets.0.norm2.bias : tensor<1280xf16>
    %6107 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6108 = torch.aten.unsqueeze %6107, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %6109 = torch.aten.unsqueeze %6108, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %6110 = torch.aten.unsqueeze %6109, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.0.norm2.weight = util.global.load @_params.unet.mid_block.resnets.0.norm2.weight : tensor<1280xf16>
    %6111 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6112 = torch.aten.unsqueeze %6111, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %6113 = torch.aten.unsqueeze %6112, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %6114 = torch.aten.unsqueeze %6113, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %6115 = torch.aten.mul.Tensor %6106, %6114 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %6116 = torch.aten.add.Tensor %6115, %6110, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %6117 = torch.prims.convert_element_type %6116, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6118 = torch.prims.convert_element_type %result1_178, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %6119 = torch.prims.convert_element_type %6102, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %6120 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %6121 = torch.prims.squeeze %6118, %6120 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %6122 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %6123 = torch.prims.squeeze %6121, %6122 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %6124 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %6125 = torch.prims.squeeze %6119, %6124 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %6126 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %6127 = torch.prims.squeeze %6125, %6126 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %6128 = torch.aten.silu %6117 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.0.conv2.weight = util.global.load @_params.unet.mid_block.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %6129 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.0.conv2.bias = util.global.load @_params.unet.mid_block.resnets.0.conv2.bias : tensor<1280xf16>
    %6130 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6131 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6132 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6133 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %6134 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %6135 = torch.aten.convolution %6128, %6129, %6130, %6131, %6132, %6133, %false, %6134, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6136 = torch.aten.add.Tensor %6042, %6135, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6137 = torch.aten.div.Scalar %6136, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6138 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6139 = torch.aten.view %6137, %6138 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %6140 = torch.prims.convert_element_type %6139, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %6141 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_179, %result1_180 = torch.aten.var_mean.correction %6140, %6141, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %6142 = torch.aten.add.Scalar %result0_179, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %6143 = torch.aten.rsqrt %6142 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %6144 = torch.aten.sub.Tensor %6139, %result1_180, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %6145 = torch.aten.mul.Tensor %6144, %6143 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %6146 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6147 = torch.aten.view %6145, %6146 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.attentions.0.norm.bias = util.global.load @_params.unet.mid_block.attentions.0.norm.bias : tensor<1280xf16>
    %6148 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6149 = torch.aten.unsqueeze %6148, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %6150 = torch.aten.unsqueeze %6149, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %6151 = torch.aten.unsqueeze %6150, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.attentions.0.norm.weight = util.global.load @_params.unet.mid_block.attentions.0.norm.weight : tensor<1280xf16>
    %6152 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6153 = torch.aten.unsqueeze %6152, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %6154 = torch.aten.unsqueeze %6153, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %6155 = torch.aten.unsqueeze %6154, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %6156 = torch.aten.mul.Tensor %6147, %6155 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %6157 = torch.aten.add.Tensor %6156, %6151, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %6158 = torch.prims.convert_element_type %6157, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %6159 = torch.prims.convert_element_type %result1_180, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %6160 = torch.prims.convert_element_type %6143, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %6161 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %6162 = torch.prims.squeeze %6159, %6161 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %6163 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %6164 = torch.prims.squeeze %6162, %6163 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %6165 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %6166 = torch.prims.squeeze %6160, %6165 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %6167 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %6168 = torch.prims.squeeze %6166, %6167 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %6169 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6170 = torch.aten.permute %6158, %6169 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %6171 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6172 = torch.aten.view %6170, %6171 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_in.weight = util.global.load @_params.unet.mid_block.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %6173 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6174 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6175 = torch.aten._unsafe_view %6172, %6174 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6176 = torch_c.to_builtin_tensor %6175 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6177 = torch_c.to_builtin_tensor %6173 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6178 = tensor.empty() : tensor<2048x1280xf32>
    %6179 = linalg.fill ins(%cst : f32) outs(%6178 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6180 = tensor.empty() : tensor<2048x1280xf32>
    %6181 = linalg.fill ins(%cst : f32) outs(%6180 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6182:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6179, %6181, %6176, %6177, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6179, %6181)
    %6183 = arith.truncf %6182#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6184 = torch_c.from_builtin_tensor %6183 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6185 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6186 = torch.aten.view %6184, %6185 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_in.bias = util.global.load @_params.unet.mid_block.attentions.0.proj_in.bias : tensor<1280xf16>
    %6187 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6188 = torch.aten.add.Tensor %6186, %6187, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6189 = torch.prims.convert_element_type %6188, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6190 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_181, %result1_182 = torch.aten.var_mean.correction %6189, %6190, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6191 = torch.aten.add.Scalar %result0_181, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6192 = torch.aten.rsqrt %6191 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6193 = torch.aten.sub.Tensor %6188, %result1_182, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6194 = torch.aten.mul.Tensor %6193, %6192 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %6195 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6196 = torch.aten.mul.Tensor %6194, %6195 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %6197 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6198 = torch.aten.add.Tensor %6196, %6197, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6199 = torch.prims.convert_element_type %6198, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6200 = torch.prims.convert_element_type %result1_182, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6201 = torch.prims.convert_element_type %6192, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %6202 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6203 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6204 = torch.aten.view %6199, %6203 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6205 = torch_c.to_builtin_tensor %6204 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6206 = torch_c.to_builtin_tensor %6202 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6207 = tensor.empty() : tensor<2048x1280xf32>
    %6208 = linalg.fill ins(%cst : f32) outs(%6207 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6209 = tensor.empty() : tensor<2048x1280xf32>
    %6210 = linalg.fill ins(%cst : f32) outs(%6209 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6211:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6208, %6210, %6205, %6206, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6208, %6210)
    %6212 = arith.truncf %6211#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6213 = torch_c.from_builtin_tensor %6212 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6214 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6215 = torch.aten.view %6213, %6214 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %6216 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6217 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6218 = torch.aten.view %6199, %6217 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6219 = torch_c.to_builtin_tensor %6218 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6220 = torch_c.to_builtin_tensor %6216 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6221 = tensor.empty() : tensor<2048x1280xf32>
    %6222 = linalg.fill ins(%cst : f32) outs(%6221 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6223 = tensor.empty() : tensor<2048x1280xf32>
    %6224 = linalg.fill ins(%cst : f32) outs(%6223 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6225:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6222, %6224, %6219, %6220, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6222, %6224)
    %6226 = arith.truncf %6225#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6227 = torch_c.from_builtin_tensor %6226 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6228 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6229 = torch.aten.view %6227, %6228 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %6230 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6231 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6232 = torch.aten.view %6199, %6231 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6233 = torch_c.to_builtin_tensor %6232 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6234 = torch_c.to_builtin_tensor %6230 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6235 = tensor.empty() : tensor<2048x1280xf32>
    %6236 = linalg.fill ins(%cst : f32) outs(%6235 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6237 = tensor.empty() : tensor<2048x1280xf32>
    %6238 = linalg.fill ins(%cst : f32) outs(%6237 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6239:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6236, %6238, %6233, %6234, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6236, %6238)
    %6240 = arith.truncf %6239#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6241 = torch_c.from_builtin_tensor %6240 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6242 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6243 = torch.aten.view %6241, %6242 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6244 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6245 = torch.aten.view %6215, %6244 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6246 = torch.aten.transpose.int %6245, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6247 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6248 = torch.aten.view %6229, %6247 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6249 = torch.aten.transpose.int %6248, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6250 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6251 = torch.aten.view %6243, %6250 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6252 = torch.aten.transpose.int %6251, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6253:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6246, %6249, %6252, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6254 = torch.aten.transpose.int %6253#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6255 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6256 = torch.aten.view %6254, %6255 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6257 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6258 = torch.aten.view %6256, %6257 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6259 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6260 = torch.aten.transpose.int %6259, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %6261 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6262 = torch.prims.convert_element_type %6261, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6263 = torch.prims.convert_element_type %6258, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6264 = torch.prims.convert_element_type %6260, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6265 = torch.aten.mm %6263, %6264 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6266 = torch.aten.mul.Scalar %6265, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6267 = torch.aten.mul.Scalar %6262, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6268 = torch.aten.add.Tensor %6266, %6267, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6269 = torch.prims.convert_element_type %6268, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6270 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6271 = torch.aten.view %6269, %6270 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6272 = torch.aten.div.Scalar %6271, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6273 = torch.aten.add.Tensor %6272, %6188, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6274 = torch.prims.convert_element_type %6273, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6275 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_183, %result1_184 = torch.aten.var_mean.correction %6274, %6275, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6276 = torch.aten.add.Scalar %result0_183, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6277 = torch.aten.rsqrt %6276 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6278 = torch.aten.sub.Tensor %6273, %result1_184, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6279 = torch.aten.mul.Tensor %6278, %6277 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %6280 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6281 = torch.aten.mul.Tensor %6279, %6280 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %6282 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6283 = torch.aten.add.Tensor %6281, %6282, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6284 = torch.prims.convert_element_type %6283, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6285 = torch.prims.convert_element_type %result1_184, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6286 = torch.prims.convert_element_type %6277, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %6287 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6288 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6289 = torch.aten.view %6284, %6288 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6290 = torch_c.to_builtin_tensor %6289 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6291 = torch_c.to_builtin_tensor %6287 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6292 = tensor.empty() : tensor<2048x1280xf32>
    %6293 = linalg.fill ins(%cst : f32) outs(%6292 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6294 = tensor.empty() : tensor<2048x1280xf32>
    %6295 = linalg.fill ins(%cst : f32) outs(%6294 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6296:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6293, %6295, %6290, %6291, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6293, %6295)
    %6297 = arith.truncf %6296#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6298 = torch_c.from_builtin_tensor %6297 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6299 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6300 = torch.aten.view %6298, %6299 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %6301 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6302 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6303 = torch.aten.view %4, %6302 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6304 = torch_c.to_builtin_tensor %6303 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6305 = torch_c.to_builtin_tensor %6301 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6306 = tensor.empty() : tensor<128x1280xf32>
    %6307 = linalg.fill ins(%cst : f32) outs(%6306 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6308 = tensor.empty() : tensor<128x1280xf32>
    %6309 = linalg.fill ins(%cst : f32) outs(%6308 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6310:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6307, %6309, %6304, %6305, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6307, %6309)
    %6311 = arith.truncf %6310#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6312 = torch_c.from_builtin_tensor %6311 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6313 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6314 = torch.aten.view %6312, %6313 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %6315 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6316 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6317 = torch.aten.view %4, %6316 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6318 = torch_c.to_builtin_tensor %6317 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6319 = torch_c.to_builtin_tensor %6315 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6320 = tensor.empty() : tensor<128x1280xf32>
    %6321 = linalg.fill ins(%cst : f32) outs(%6320 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6322 = tensor.empty() : tensor<128x1280xf32>
    %6323 = linalg.fill ins(%cst : f32) outs(%6322 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6324:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6321, %6323, %6318, %6319, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6321, %6323)
    %6325 = arith.truncf %6324#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6326 = torch_c.from_builtin_tensor %6325 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6327 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6328 = torch.aten.view %6326, %6327 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6329 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6330 = torch.aten.view %6300, %6329 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6331 = torch.aten.transpose.int %6330, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6332 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6333 = torch.aten.view %6314, %6332 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6334 = torch.aten.transpose.int %6333, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6335 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6336 = torch.aten.view %6328, %6335 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6337 = torch.aten.transpose.int %6336, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6338:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6331, %6334, %6337, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6339 = torch.aten.transpose.int %6338#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6340 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6341 = torch.aten.view %6339, %6340 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6342 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6343 = torch.aten.view %6341, %6342 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6344 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6345 = torch.aten.transpose.int %6344, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %6346 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6347 = torch.prims.convert_element_type %6346, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6348 = torch.prims.convert_element_type %6343, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6349 = torch.prims.convert_element_type %6345, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6350 = torch.aten.mm %6348, %6349 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6351 = torch.aten.mul.Scalar %6350, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6352 = torch.aten.mul.Scalar %6347, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6353 = torch.aten.add.Tensor %6351, %6352, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6354 = torch.prims.convert_element_type %6353, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6355 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6356 = torch.aten.view %6354, %6355 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6357 = torch.aten.div.Scalar %6356, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6358 = torch.aten.add.Tensor %6357, %6273, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6359 = torch.prims.convert_element_type %6358, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6360 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_185, %result1_186 = torch.aten.var_mean.correction %6359, %6360, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6361 = torch.aten.add.Scalar %result0_185, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6362 = torch.aten.rsqrt %6361 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6363 = torch.aten.sub.Tensor %6358, %result1_186, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6364 = torch.aten.mul.Tensor %6363, %6362 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %6365 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6366 = torch.aten.mul.Tensor %6364, %6365 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %6367 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6368 = torch.aten.add.Tensor %6366, %6367, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6369 = torch.prims.convert_element_type %6368, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6370 = torch.prims.convert_element_type %result1_186, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6371 = torch.prims.convert_element_type %6362, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6372 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6373 = torch.aten.view %6369, %6372 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6374 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6375 = torch.aten.transpose.int %6374, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %6376 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6377 = torch.prims.convert_element_type %6376, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6378 = torch.prims.convert_element_type %6373, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6379 = torch.prims.convert_element_type %6375, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6380 = torch.aten.mm %6378, %6379 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6381 = torch.aten.mul.Scalar %6380, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6382 = torch.aten.mul.Scalar %6377, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6383 = torch.aten.add.Tensor %6381, %6382, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6384 = torch.prims.convert_element_type %6383, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6385 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6386 = torch.aten.view %6384, %6385 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6387 = torch.aten.slice.Tensor %6386, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6388 = torch.aten.slice.Tensor %6386, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6389 = torch.aten.gelu %6388, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6390 = torch.aten.mul.Tensor %6387, %6389 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6391 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6392 = torch.aten.view %6390, %6391 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %6393 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6394 = torch.aten.transpose.int %6393, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %6395 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6396 = torch.prims.convert_element_type %6395, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6397 = torch.prims.convert_element_type %6392, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6398 = torch.prims.convert_element_type %6394, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6399 = torch.aten.mm %6397, %6398 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6400 = torch.aten.mul.Scalar %6399, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6401 = torch.aten.mul.Scalar %6396, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6402 = torch.aten.add.Tensor %6400, %6401, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6403 = torch.prims.convert_element_type %6402, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6404 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6405 = torch.aten.view %6403, %6404 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6406 = torch.aten.add.Tensor %6405, %6358, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6407 = torch.prims.convert_element_type %6406, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6408 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_187, %result1_188 = torch.aten.var_mean.correction %6407, %6408, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6409 = torch.aten.add.Scalar %result0_187, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6410 = torch.aten.rsqrt %6409 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6411 = torch.aten.sub.Tensor %6406, %result1_188, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6412 = torch.aten.mul.Tensor %6411, %6410 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %6413 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6414 = torch.aten.mul.Tensor %6412, %6413 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %6415 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6416 = torch.aten.add.Tensor %6414, %6415, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6417 = torch.prims.convert_element_type %6416, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6418 = torch.prims.convert_element_type %result1_188, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6419 = torch.prims.convert_element_type %6410, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %6420 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6421 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6422 = torch.aten.view %6417, %6421 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6423 = torch_c.to_builtin_tensor %6422 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6424 = torch_c.to_builtin_tensor %6420 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6425 = tensor.empty() : tensor<2048x1280xf32>
    %6426 = linalg.fill ins(%cst : f32) outs(%6425 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6427 = tensor.empty() : tensor<2048x1280xf32>
    %6428 = linalg.fill ins(%cst : f32) outs(%6427 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6429:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6426, %6428, %6423, %6424, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6426, %6428)
    %6430 = arith.truncf %6429#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6431 = torch_c.from_builtin_tensor %6430 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6432 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6433 = torch.aten.view %6431, %6432 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %6434 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6435 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6436 = torch.aten.view %6417, %6435 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6437 = torch_c.to_builtin_tensor %6436 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6438 = torch_c.to_builtin_tensor %6434 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6439 = tensor.empty() : tensor<2048x1280xf32>
    %6440 = linalg.fill ins(%cst : f32) outs(%6439 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6441 = tensor.empty() : tensor<2048x1280xf32>
    %6442 = linalg.fill ins(%cst : f32) outs(%6441 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6443:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6440, %6442, %6437, %6438, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6440, %6442)
    %6444 = arith.truncf %6443#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6445 = torch_c.from_builtin_tensor %6444 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6446 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6447 = torch.aten.view %6445, %6446 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %6448 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6449 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6450 = torch.aten.view %6417, %6449 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6451 = torch_c.to_builtin_tensor %6450 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6452 = torch_c.to_builtin_tensor %6448 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6453 = tensor.empty() : tensor<2048x1280xf32>
    %6454 = linalg.fill ins(%cst : f32) outs(%6453 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6455 = tensor.empty() : tensor<2048x1280xf32>
    %6456 = linalg.fill ins(%cst : f32) outs(%6455 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6457:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6454, %6456, %6451, %6452, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6454, %6456)
    %6458 = arith.truncf %6457#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6459 = torch_c.from_builtin_tensor %6458 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6460 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6461 = torch.aten.view %6459, %6460 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6462 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6463 = torch.aten.view %6433, %6462 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6464 = torch.aten.transpose.int %6463, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6465 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6466 = torch.aten.view %6447, %6465 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6467 = torch.aten.transpose.int %6466, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6468 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6469 = torch.aten.view %6461, %6468 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6470 = torch.aten.transpose.int %6469, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6471:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6464, %6467, %6470, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6472 = torch.aten.transpose.int %6471#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6473 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6474 = torch.aten.view %6472, %6473 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6475 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6476 = torch.aten.view %6474, %6475 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6477 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6478 = torch.aten.transpose.int %6477, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %6479 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6480 = torch.prims.convert_element_type %6479, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6481 = torch.prims.convert_element_type %6476, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6482 = torch.prims.convert_element_type %6478, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6483 = torch.aten.mm %6481, %6482 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6484 = torch.aten.mul.Scalar %6483, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6485 = torch.aten.mul.Scalar %6480, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6486 = torch.aten.add.Tensor %6484, %6485, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6487 = torch.prims.convert_element_type %6486, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6488 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6489 = torch.aten.view %6487, %6488 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6490 = torch.aten.div.Scalar %6489, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6491 = torch.aten.add.Tensor %6490, %6406, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6492 = torch.prims.convert_element_type %6491, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6493 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_189, %result1_190 = torch.aten.var_mean.correction %6492, %6493, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6494 = torch.aten.add.Scalar %result0_189, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6495 = torch.aten.rsqrt %6494 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6496 = torch.aten.sub.Tensor %6491, %result1_190, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6497 = torch.aten.mul.Tensor %6496, %6495 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %6498 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6499 = torch.aten.mul.Tensor %6497, %6498 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %6500 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6501 = torch.aten.add.Tensor %6499, %6500, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6502 = torch.prims.convert_element_type %6501, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6503 = torch.prims.convert_element_type %result1_190, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6504 = torch.prims.convert_element_type %6495, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %6505 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6506 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6507 = torch.aten.view %6502, %6506 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6508 = torch_c.to_builtin_tensor %6507 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6509 = torch_c.to_builtin_tensor %6505 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6510 = tensor.empty() : tensor<2048x1280xf32>
    %6511 = linalg.fill ins(%cst : f32) outs(%6510 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6512 = tensor.empty() : tensor<2048x1280xf32>
    %6513 = linalg.fill ins(%cst : f32) outs(%6512 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6514:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6511, %6513, %6508, %6509, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6511, %6513)
    %6515 = arith.truncf %6514#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6516 = torch_c.from_builtin_tensor %6515 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6517 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6518 = torch.aten.view %6516, %6517 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %6519 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6520 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6521 = torch.aten.view %4, %6520 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6522 = torch_c.to_builtin_tensor %6521 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6523 = torch_c.to_builtin_tensor %6519 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6524 = tensor.empty() : tensor<128x1280xf32>
    %6525 = linalg.fill ins(%cst : f32) outs(%6524 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6526 = tensor.empty() : tensor<128x1280xf32>
    %6527 = linalg.fill ins(%cst : f32) outs(%6526 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6528:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6525, %6527, %6522, %6523, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6525, %6527)
    %6529 = arith.truncf %6528#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6530 = torch_c.from_builtin_tensor %6529 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6531 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6532 = torch.aten.view %6530, %6531 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %6533 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6534 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6535 = torch.aten.view %4, %6534 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6536 = torch_c.to_builtin_tensor %6535 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6537 = torch_c.to_builtin_tensor %6533 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6538 = tensor.empty() : tensor<128x1280xf32>
    %6539 = linalg.fill ins(%cst : f32) outs(%6538 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6540 = tensor.empty() : tensor<128x1280xf32>
    %6541 = linalg.fill ins(%cst : f32) outs(%6540 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6542:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6539, %6541, %6536, %6537, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6539, %6541)
    %6543 = arith.truncf %6542#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6544 = torch_c.from_builtin_tensor %6543 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6545 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6546 = torch.aten.view %6544, %6545 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6547 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6548 = torch.aten.view %6518, %6547 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6549 = torch.aten.transpose.int %6548, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6550 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6551 = torch.aten.view %6532, %6550 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6552 = torch.aten.transpose.int %6551, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6553 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6554 = torch.aten.view %6546, %6553 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6555 = torch.aten.transpose.int %6554, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6556:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6549, %6552, %6555, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6557 = torch.aten.transpose.int %6556#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6558 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6559 = torch.aten.view %6557, %6558 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6560 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6561 = torch.aten.view %6559, %6560 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6562 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6563 = torch.aten.transpose.int %6562, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %6564 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6565 = torch.prims.convert_element_type %6564, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6566 = torch.prims.convert_element_type %6561, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6567 = torch.prims.convert_element_type %6563, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6568 = torch.aten.mm %6566, %6567 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6569 = torch.aten.mul.Scalar %6568, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6570 = torch.aten.mul.Scalar %6565, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6571 = torch.aten.add.Tensor %6569, %6570, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6572 = torch.prims.convert_element_type %6571, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6573 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6574 = torch.aten.view %6572, %6573 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6575 = torch.aten.div.Scalar %6574, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6576 = torch.aten.add.Tensor %6575, %6491, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6577 = torch.prims.convert_element_type %6576, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6578 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_191, %result1_192 = torch.aten.var_mean.correction %6577, %6578, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6579 = torch.aten.add.Scalar %result0_191, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6580 = torch.aten.rsqrt %6579 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6581 = torch.aten.sub.Tensor %6576, %result1_192, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6582 = torch.aten.mul.Tensor %6581, %6580 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %6583 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6584 = torch.aten.mul.Tensor %6582, %6583 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %6585 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6586 = torch.aten.add.Tensor %6584, %6585, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6587 = torch.prims.convert_element_type %6586, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6588 = torch.prims.convert_element_type %result1_192, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6589 = torch.prims.convert_element_type %6580, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6590 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6591 = torch.aten.view %6587, %6590 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6592 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6593 = torch.aten.transpose.int %6592, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %6594 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6595 = torch.prims.convert_element_type %6594, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6596 = torch.prims.convert_element_type %6591, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6597 = torch.prims.convert_element_type %6593, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6598 = torch.aten.mm %6596, %6597 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6599 = torch.aten.mul.Scalar %6598, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6600 = torch.aten.mul.Scalar %6595, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6601 = torch.aten.add.Tensor %6599, %6600, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6602 = torch.prims.convert_element_type %6601, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6603 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6604 = torch.aten.view %6602, %6603 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6605 = torch.aten.slice.Tensor %6604, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6606 = torch.aten.slice.Tensor %6604, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6607 = torch.aten.gelu %6606, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6608 = torch.aten.mul.Tensor %6605, %6607 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6609 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6610 = torch.aten.view %6608, %6609 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %6611 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6612 = torch.aten.transpose.int %6611, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %6613 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6614 = torch.prims.convert_element_type %6613, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6615 = torch.prims.convert_element_type %6610, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6616 = torch.prims.convert_element_type %6612, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6617 = torch.aten.mm %6615, %6616 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6618 = torch.aten.mul.Scalar %6617, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6619 = torch.aten.mul.Scalar %6614, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6620 = torch.aten.add.Tensor %6618, %6619, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6621 = torch.prims.convert_element_type %6620, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6622 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6623 = torch.aten.view %6621, %6622 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6624 = torch.aten.add.Tensor %6623, %6576, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6625 = torch.prims.convert_element_type %6624, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6626 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_193, %result1_194 = torch.aten.var_mean.correction %6625, %6626, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6627 = torch.aten.add.Scalar %result0_193, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6628 = torch.aten.rsqrt %6627 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6629 = torch.aten.sub.Tensor %6624, %result1_194, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6630 = torch.aten.mul.Tensor %6629, %6628 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %6631 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6632 = torch.aten.mul.Tensor %6630, %6631 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %6633 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6634 = torch.aten.add.Tensor %6632, %6633, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6635 = torch.prims.convert_element_type %6634, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6636 = torch.prims.convert_element_type %result1_194, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6637 = torch.prims.convert_element_type %6628, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %6638 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6639 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6640 = torch.aten.view %6635, %6639 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6641 = torch_c.to_builtin_tensor %6640 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6642 = torch_c.to_builtin_tensor %6638 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6643 = tensor.empty() : tensor<2048x1280xf32>
    %6644 = linalg.fill ins(%cst : f32) outs(%6643 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6645 = tensor.empty() : tensor<2048x1280xf32>
    %6646 = linalg.fill ins(%cst : f32) outs(%6645 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6647:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6644, %6646, %6641, %6642, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6644, %6646)
    %6648 = arith.truncf %6647#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6649 = torch_c.from_builtin_tensor %6648 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6650 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6651 = torch.aten.view %6649, %6650 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %6652 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6653 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6654 = torch.aten.view %6635, %6653 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6655 = torch_c.to_builtin_tensor %6654 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6656 = torch_c.to_builtin_tensor %6652 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6657 = tensor.empty() : tensor<2048x1280xf32>
    %6658 = linalg.fill ins(%cst : f32) outs(%6657 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6659 = tensor.empty() : tensor<2048x1280xf32>
    %6660 = linalg.fill ins(%cst : f32) outs(%6659 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6661:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6658, %6660, %6655, %6656, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6658, %6660)
    %6662 = arith.truncf %6661#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6663 = torch_c.from_builtin_tensor %6662 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6664 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6665 = torch.aten.view %6663, %6664 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %6666 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6667 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6668 = torch.aten.view %6635, %6667 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6669 = torch_c.to_builtin_tensor %6668 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6670 = torch_c.to_builtin_tensor %6666 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6671 = tensor.empty() : tensor<2048x1280xf32>
    %6672 = linalg.fill ins(%cst : f32) outs(%6671 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6673 = tensor.empty() : tensor<2048x1280xf32>
    %6674 = linalg.fill ins(%cst : f32) outs(%6673 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6675:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6672, %6674, %6669, %6670, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6672, %6674)
    %6676 = arith.truncf %6675#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6677 = torch_c.from_builtin_tensor %6676 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6678 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6679 = torch.aten.view %6677, %6678 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6680 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6681 = torch.aten.view %6651, %6680 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6682 = torch.aten.transpose.int %6681, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6683 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6684 = torch.aten.view %6665, %6683 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6685 = torch.aten.transpose.int %6684, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6686 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6687 = torch.aten.view %6679, %6686 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6688 = torch.aten.transpose.int %6687, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6689:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6682, %6685, %6688, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6690 = torch.aten.transpose.int %6689#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6691 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6692 = torch.aten.view %6690, %6691 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6693 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6694 = torch.aten.view %6692, %6693 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6695 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6696 = torch.aten.transpose.int %6695, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %6697 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6698 = torch.prims.convert_element_type %6697, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6699 = torch.prims.convert_element_type %6694, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6700 = torch.prims.convert_element_type %6696, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6701 = torch.aten.mm %6699, %6700 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6702 = torch.aten.mul.Scalar %6701, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6703 = torch.aten.mul.Scalar %6698, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6704 = torch.aten.add.Tensor %6702, %6703, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6705 = torch.prims.convert_element_type %6704, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6706 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6707 = torch.aten.view %6705, %6706 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6708 = torch.aten.div.Scalar %6707, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6709 = torch.aten.add.Tensor %6708, %6624, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6710 = torch.prims.convert_element_type %6709, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6711 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_195, %result1_196 = torch.aten.var_mean.correction %6710, %6711, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6712 = torch.aten.add.Scalar %result0_195, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6713 = torch.aten.rsqrt %6712 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6714 = torch.aten.sub.Tensor %6709, %result1_196, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6715 = torch.aten.mul.Tensor %6714, %6713 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %6716 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6717 = torch.aten.mul.Tensor %6715, %6716 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %6718 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6719 = torch.aten.add.Tensor %6717, %6718, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6720 = torch.prims.convert_element_type %6719, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6721 = torch.prims.convert_element_type %result1_196, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6722 = torch.prims.convert_element_type %6713, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %6723 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6724 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6725 = torch.aten.view %6720, %6724 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6726 = torch_c.to_builtin_tensor %6725 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6727 = torch_c.to_builtin_tensor %6723 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6728 = tensor.empty() : tensor<2048x1280xf32>
    %6729 = linalg.fill ins(%cst : f32) outs(%6728 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6730 = tensor.empty() : tensor<2048x1280xf32>
    %6731 = linalg.fill ins(%cst : f32) outs(%6730 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6732:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6729, %6731, %6726, %6727, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6729, %6731)
    %6733 = arith.truncf %6732#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6734 = torch_c.from_builtin_tensor %6733 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6735 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6736 = torch.aten.view %6734, %6735 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %6737 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6738 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6739 = torch.aten.view %4, %6738 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6740 = torch_c.to_builtin_tensor %6739 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6741 = torch_c.to_builtin_tensor %6737 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6742 = tensor.empty() : tensor<128x1280xf32>
    %6743 = linalg.fill ins(%cst : f32) outs(%6742 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6744 = tensor.empty() : tensor<128x1280xf32>
    %6745 = linalg.fill ins(%cst : f32) outs(%6744 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6746:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6743, %6745, %6740, %6741, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6743, %6745)
    %6747 = arith.truncf %6746#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6748 = torch_c.from_builtin_tensor %6747 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6749 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6750 = torch.aten.view %6748, %6749 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %6751 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6752 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6753 = torch.aten.view %4, %6752 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6754 = torch_c.to_builtin_tensor %6753 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6755 = torch_c.to_builtin_tensor %6751 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6756 = tensor.empty() : tensor<128x1280xf32>
    %6757 = linalg.fill ins(%cst : f32) outs(%6756 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6758 = tensor.empty() : tensor<128x1280xf32>
    %6759 = linalg.fill ins(%cst : f32) outs(%6758 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6760:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6757, %6759, %6754, %6755, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6757, %6759)
    %6761 = arith.truncf %6760#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6762 = torch_c.from_builtin_tensor %6761 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6763 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6764 = torch.aten.view %6762, %6763 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6765 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6766 = torch.aten.view %6736, %6765 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6767 = torch.aten.transpose.int %6766, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6768 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6769 = torch.aten.view %6750, %6768 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6770 = torch.aten.transpose.int %6769, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6771 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6772 = torch.aten.view %6764, %6771 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6773 = torch.aten.transpose.int %6772, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6774:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6767, %6770, %6773, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6775 = torch.aten.transpose.int %6774#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6776 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6777 = torch.aten.view %6775, %6776 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6778 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6779 = torch.aten.view %6777, %6778 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6780 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6781 = torch.aten.transpose.int %6780, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %6782 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6783 = torch.prims.convert_element_type %6782, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6784 = torch.prims.convert_element_type %6779, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6785 = torch.prims.convert_element_type %6781, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6786 = torch.aten.mm %6784, %6785 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6787 = torch.aten.mul.Scalar %6786, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6788 = torch.aten.mul.Scalar %6783, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6789 = torch.aten.add.Tensor %6787, %6788, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6790 = torch.prims.convert_element_type %6789, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6791 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6792 = torch.aten.view %6790, %6791 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6793 = torch.aten.div.Scalar %6792, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6794 = torch.aten.add.Tensor %6793, %6709, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6795 = torch.prims.convert_element_type %6794, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6796 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_197, %result1_198 = torch.aten.var_mean.correction %6795, %6796, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6797 = torch.aten.add.Scalar %result0_197, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6798 = torch.aten.rsqrt %6797 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6799 = torch.aten.sub.Tensor %6794, %result1_198, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6800 = torch.aten.mul.Tensor %6799, %6798 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %6801 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6802 = torch.aten.mul.Tensor %6800, %6801 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %6803 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6804 = torch.aten.add.Tensor %6802, %6803, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6805 = torch.prims.convert_element_type %6804, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6806 = torch.prims.convert_element_type %result1_198, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6807 = torch.prims.convert_element_type %6798, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6808 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6809 = torch.aten.view %6805, %6808 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6810 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %6811 = torch.aten.transpose.int %6810, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %6812 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %6813 = torch.prims.convert_element_type %6812, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %6814 = torch.prims.convert_element_type %6809, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6815 = torch.prims.convert_element_type %6811, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6816 = torch.aten.mm %6814, %6815 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %6817 = torch.aten.mul.Scalar %6816, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6818 = torch.aten.mul.Scalar %6813, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %6819 = torch.aten.add.Tensor %6817, %6818, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %6820 = torch.prims.convert_element_type %6819, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %6821 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6822 = torch.aten.view %6820, %6821 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %6823 = torch.aten.slice.Tensor %6822, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6824 = torch.aten.slice.Tensor %6822, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %6825 = torch.aten.gelu %6824, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %6826 = torch.aten.mul.Tensor %6823, %6825 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %6827 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %6828 = torch.aten.view %6826, %6827 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %6829 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %6830 = torch.aten.transpose.int %6829, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %6831 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6832 = torch.prims.convert_element_type %6831, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6833 = torch.prims.convert_element_type %6828, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %6834 = torch.prims.convert_element_type %6830, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6835 = torch.aten.mm %6833, %6834 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6836 = torch.aten.mul.Scalar %6835, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6837 = torch.aten.mul.Scalar %6832, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6838 = torch.aten.add.Tensor %6836, %6837, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6839 = torch.prims.convert_element_type %6838, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6840 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6841 = torch.aten.view %6839, %6840 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6842 = torch.aten.add.Tensor %6841, %6794, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6843 = torch.prims.convert_element_type %6842, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6844 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_199, %result1_200 = torch.aten.var_mean.correction %6843, %6844, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6845 = torch.aten.add.Scalar %result0_199, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6846 = torch.aten.rsqrt %6845 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6847 = torch.aten.sub.Tensor %6842, %result1_200, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6848 = torch.aten.mul.Tensor %6847, %6846 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %6849 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6850 = torch.aten.mul.Tensor %6848, %6849 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %6851 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6852 = torch.aten.add.Tensor %6850, %6851, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6853 = torch.prims.convert_element_type %6852, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6854 = torch.prims.convert_element_type %result1_200, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6855 = torch.prims.convert_element_type %6846, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %6856 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6857 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6858 = torch.aten.view %6853, %6857 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6859 = torch_c.to_builtin_tensor %6858 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6860 = torch_c.to_builtin_tensor %6856 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6861 = tensor.empty() : tensor<2048x1280xf32>
    %6862 = linalg.fill ins(%cst : f32) outs(%6861 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6863 = tensor.empty() : tensor<2048x1280xf32>
    %6864 = linalg.fill ins(%cst : f32) outs(%6863 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6865:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6862, %6864, %6859, %6860, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6862, %6864)
    %6866 = arith.truncf %6865#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6867 = torch_c.from_builtin_tensor %6866 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6868 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6869 = torch.aten.view %6867, %6868 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %6870 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6871 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6872 = torch.aten.view %6853, %6871 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6873 = torch_c.to_builtin_tensor %6872 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6874 = torch_c.to_builtin_tensor %6870 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6875 = tensor.empty() : tensor<2048x1280xf32>
    %6876 = linalg.fill ins(%cst : f32) outs(%6875 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6877 = tensor.empty() : tensor<2048x1280xf32>
    %6878 = linalg.fill ins(%cst : f32) outs(%6877 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6879:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6876, %6878, %6873, %6874, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6876, %6878)
    %6880 = arith.truncf %6879#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6881 = torch_c.from_builtin_tensor %6880 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6882 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6883 = torch.aten.view %6881, %6882 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %6884 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6885 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6886 = torch.aten.view %6853, %6885 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6887 = torch_c.to_builtin_tensor %6886 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6888 = torch_c.to_builtin_tensor %6884 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6889 = tensor.empty() : tensor<2048x1280xf32>
    %6890 = linalg.fill ins(%cst : f32) outs(%6889 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6891 = tensor.empty() : tensor<2048x1280xf32>
    %6892 = linalg.fill ins(%cst : f32) outs(%6891 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6893:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6890, %6892, %6887, %6888, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6890, %6892)
    %6894 = arith.truncf %6893#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6895 = torch_c.from_builtin_tensor %6894 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6896 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6897 = torch.aten.view %6895, %6896 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6898 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6899 = torch.aten.view %6869, %6898 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6900 = torch.aten.transpose.int %6899, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6901 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6902 = torch.aten.view %6883, %6901 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6903 = torch.aten.transpose.int %6902, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6904 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6905 = torch.aten.view %6897, %6904 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6906 = torch.aten.transpose.int %6905, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6907:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6900, %6903, %6906, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6908 = torch.aten.transpose.int %6907#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6909 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6910 = torch.aten.view %6908, %6909 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6911 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6912 = torch.aten.view %6910, %6911 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6913 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6914 = torch.aten.transpose.int %6913, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %6915 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6916 = torch.prims.convert_element_type %6915, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %6917 = torch.prims.convert_element_type %6912, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6918 = torch.prims.convert_element_type %6914, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6919 = torch.aten.mm %6917, %6918 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %6920 = torch.aten.mul.Scalar %6919, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6921 = torch.aten.mul.Scalar %6916, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %6922 = torch.aten.add.Tensor %6920, %6921, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %6923 = torch.prims.convert_element_type %6922, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %6924 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6925 = torch.aten.view %6923, %6924 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6926 = torch.aten.div.Scalar %6925, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %6927 = torch.aten.add.Tensor %6926, %6842, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6928 = torch.prims.convert_element_type %6927, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6929 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_201, %result1_202 = torch.aten.var_mean.correction %6928, %6929, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %6930 = torch.aten.add.Scalar %result0_201, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %6931 = torch.aten.rsqrt %6930 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %6932 = torch.aten.sub.Tensor %6927, %result1_202, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6933 = torch.aten.mul.Tensor %6932, %6931 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %6934 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6935 = torch.aten.mul.Tensor %6933, %6934 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %6936 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6937 = torch.aten.add.Tensor %6935, %6936, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %6938 = torch.prims.convert_element_type %6937, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %6939 = torch.prims.convert_element_type %result1_202, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %6940 = torch.prims.convert_element_type %6931, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %6941 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6942 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6943 = torch.aten.view %6938, %6942 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %6944 = torch_c.to_builtin_tensor %6943 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %6945 = torch_c.to_builtin_tensor %6941 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %6946 = tensor.empty() : tensor<2048x1280xf32>
    %6947 = linalg.fill ins(%cst : f32) outs(%6946 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6948 = tensor.empty() : tensor<2048x1280xf32>
    %6949 = linalg.fill ins(%cst : f32) outs(%6948 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %6950:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %6947, %6949, %6944, %6945, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6947, %6949)
    %6951 = arith.truncf %6950#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %6952 = torch_c.from_builtin_tensor %6951 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %6953 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6954 = torch.aten.view %6952, %6953 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %6955 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6956 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6957 = torch.aten.view %4, %6956 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6958 = torch_c.to_builtin_tensor %6957 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6959 = torch_c.to_builtin_tensor %6955 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6960 = tensor.empty() : tensor<128x1280xf32>
    %6961 = linalg.fill ins(%cst : f32) outs(%6960 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6962 = tensor.empty() : tensor<128x1280xf32>
    %6963 = linalg.fill ins(%cst : f32) outs(%6962 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6964:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6961, %6963, %6958, %6959, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6961, %6963)
    %6965 = arith.truncf %6964#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6966 = torch_c.from_builtin_tensor %6965 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6967 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6968 = torch.aten.view %6966, %6967 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %6969 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %6970 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6971 = torch.aten.view %4, %6970 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %6972 = torch_c.to_builtin_tensor %6971 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %6973 = torch_c.to_builtin_tensor %6969 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %6974 = tensor.empty() : tensor<128x1280xf32>
    %6975 = linalg.fill ins(%cst : f32) outs(%6974 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6976 = tensor.empty() : tensor<128x1280xf32>
    %6977 = linalg.fill ins(%cst : f32) outs(%6976 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %6978:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %6975, %6977, %6972, %6973, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%6975, %6977)
    %6979 = arith.truncf %6978#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %6980 = torch_c.from_builtin_tensor %6979 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %6981 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6982 = torch.aten.view %6980, %6981 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %6983 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6984 = torch.aten.view %6954, %6983 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %6985 = torch.aten.transpose.int %6984, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %6986 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6987 = torch.aten.view %6968, %6986 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6988 = torch.aten.transpose.int %6987, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6989 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6990 = torch.aten.view %6982, %6989 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %6991 = torch.aten.transpose.int %6990, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %6992:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6985, %6988, %6991, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %6993 = torch.aten.transpose.int %6992#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %6994 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6995 = torch.aten.view %6993, %6994 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %6996 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %6997 = torch.aten.view %6995, %6996 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6998 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %6999 = torch.aten.transpose.int %6998, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %7000 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7001 = torch.prims.convert_element_type %7000, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7002 = torch.prims.convert_element_type %6997, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7003 = torch.prims.convert_element_type %6999, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7004 = torch.aten.mm %7002, %7003 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7005 = torch.aten.mul.Scalar %7004, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7006 = torch.aten.mul.Scalar %7001, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7007 = torch.aten.add.Tensor %7005, %7006, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7008 = torch.prims.convert_element_type %7007, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7009 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7010 = torch.aten.view %7008, %7009 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7011 = torch.aten.div.Scalar %7010, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7012 = torch.aten.add.Tensor %7011, %6927, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7013 = torch.prims.convert_element_type %7012, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7014 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_203, %result1_204 = torch.aten.var_mean.correction %7013, %7014, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7015 = torch.aten.add.Scalar %result0_203, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7016 = torch.aten.rsqrt %7015 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7017 = torch.aten.sub.Tensor %7012, %result1_204, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7018 = torch.aten.mul.Tensor %7017, %7016 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %7019 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7020 = torch.aten.mul.Tensor %7018, %7019 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %7021 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7022 = torch.aten.add.Tensor %7020, %7021, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7023 = torch.prims.convert_element_type %7022, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7024 = torch.prims.convert_element_type %result1_204, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7025 = torch.prims.convert_element_type %7016, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7026 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7027 = torch.aten.view %7023, %7026 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7028 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7029 = torch.aten.transpose.int %7028, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %7030 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7031 = torch.prims.convert_element_type %7030, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7032 = torch.prims.convert_element_type %7027, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7033 = torch.prims.convert_element_type %7029, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7034 = torch.aten.mm %7032, %7033 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7035 = torch.aten.mul.Scalar %7034, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7036 = torch.aten.mul.Scalar %7031, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7037 = torch.aten.add.Tensor %7035, %7036, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7038 = torch.prims.convert_element_type %7037, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7039 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7040 = torch.aten.view %7038, %7039 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7041 = torch.aten.slice.Tensor %7040, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7042 = torch.aten.slice.Tensor %7040, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7043 = torch.aten.gelu %7042, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7044 = torch.aten.mul.Tensor %7041, %7043 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7045 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7046 = torch.aten.view %7044, %7045 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %7047 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7048 = torch.aten.transpose.int %7047, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %7049 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7050 = torch.prims.convert_element_type %7049, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7051 = torch.prims.convert_element_type %7046, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7052 = torch.prims.convert_element_type %7048, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7053 = torch.aten.mm %7051, %7052 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7054 = torch.aten.mul.Scalar %7053, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7055 = torch.aten.mul.Scalar %7050, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7056 = torch.aten.add.Tensor %7054, %7055, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7057 = torch.prims.convert_element_type %7056, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7058 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7059 = torch.aten.view %7057, %7058 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7060 = torch.aten.add.Tensor %7059, %7012, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7061 = torch.prims.convert_element_type %7060, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7062 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_205, %result1_206 = torch.aten.var_mean.correction %7061, %7062, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7063 = torch.aten.add.Scalar %result0_205, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7064 = torch.aten.rsqrt %7063 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7065 = torch.aten.sub.Tensor %7060, %result1_206, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7066 = torch.aten.mul.Tensor %7065, %7064 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %7067 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7068 = torch.aten.mul.Tensor %7066, %7067 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %7069 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7070 = torch.aten.add.Tensor %7068, %7069, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7071 = torch.prims.convert_element_type %7070, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7072 = torch.prims.convert_element_type %result1_206, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7073 = torch.prims.convert_element_type %7064, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %7074 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7075 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7076 = torch.aten.view %7071, %7075 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7077 = torch_c.to_builtin_tensor %7076 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7078 = torch_c.to_builtin_tensor %7074 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7079 = tensor.empty() : tensor<2048x1280xf32>
    %7080 = linalg.fill ins(%cst : f32) outs(%7079 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7081 = tensor.empty() : tensor<2048x1280xf32>
    %7082 = linalg.fill ins(%cst : f32) outs(%7081 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7083:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7080, %7082, %7077, %7078, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7080, %7082)
    %7084 = arith.truncf %7083#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7085 = torch_c.from_builtin_tensor %7084 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7086 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7087 = torch.aten.view %7085, %7086 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %7088 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7089 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7090 = torch.aten.view %7071, %7089 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7091 = torch_c.to_builtin_tensor %7090 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7092 = torch_c.to_builtin_tensor %7088 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7093 = tensor.empty() : tensor<2048x1280xf32>
    %7094 = linalg.fill ins(%cst : f32) outs(%7093 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7095 = tensor.empty() : tensor<2048x1280xf32>
    %7096 = linalg.fill ins(%cst : f32) outs(%7095 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7097:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7094, %7096, %7091, %7092, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7094, %7096)
    %7098 = arith.truncf %7097#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7099 = torch_c.from_builtin_tensor %7098 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7100 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7101 = torch.aten.view %7099, %7100 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %7102 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7103 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7104 = torch.aten.view %7071, %7103 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7105 = torch_c.to_builtin_tensor %7104 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7106 = torch_c.to_builtin_tensor %7102 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7107 = tensor.empty() : tensor<2048x1280xf32>
    %7108 = linalg.fill ins(%cst : f32) outs(%7107 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7109 = tensor.empty() : tensor<2048x1280xf32>
    %7110 = linalg.fill ins(%cst : f32) outs(%7109 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7111:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7108, %7110, %7105, %7106, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7108, %7110)
    %7112 = arith.truncf %7111#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7113 = torch_c.from_builtin_tensor %7112 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7114 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7115 = torch.aten.view %7113, %7114 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7116 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7117 = torch.aten.view %7087, %7116 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7118 = torch.aten.transpose.int %7117, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7119 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7120 = torch.aten.view %7101, %7119 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7121 = torch.aten.transpose.int %7120, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7122 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7123 = torch.aten.view %7115, %7122 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7124 = torch.aten.transpose.int %7123, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7125:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7118, %7121, %7124, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7126 = torch.aten.transpose.int %7125#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7127 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7128 = torch.aten.view %7126, %7127 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7129 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7130 = torch.aten.view %7128, %7129 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7131 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7132 = torch.aten.transpose.int %7131, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %7133 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7134 = torch.prims.convert_element_type %7133, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7135 = torch.prims.convert_element_type %7130, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7136 = torch.prims.convert_element_type %7132, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7137 = torch.aten.mm %7135, %7136 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7138 = torch.aten.mul.Scalar %7137, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7139 = torch.aten.mul.Scalar %7134, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7140 = torch.aten.add.Tensor %7138, %7139, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7141 = torch.prims.convert_element_type %7140, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7142 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7143 = torch.aten.view %7141, %7142 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7144 = torch.aten.div.Scalar %7143, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7145 = torch.aten.add.Tensor %7144, %7060, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7146 = torch.prims.convert_element_type %7145, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7147 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_207, %result1_208 = torch.aten.var_mean.correction %7146, %7147, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7148 = torch.aten.add.Scalar %result0_207, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7149 = torch.aten.rsqrt %7148 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7150 = torch.aten.sub.Tensor %7145, %result1_208, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7151 = torch.aten.mul.Tensor %7150, %7149 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %7152 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7153 = torch.aten.mul.Tensor %7151, %7152 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %7154 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7155 = torch.aten.add.Tensor %7153, %7154, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7156 = torch.prims.convert_element_type %7155, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7157 = torch.prims.convert_element_type %result1_208, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7158 = torch.prims.convert_element_type %7149, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %7159 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7160 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7161 = torch.aten.view %7156, %7160 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7162 = torch_c.to_builtin_tensor %7161 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7163 = torch_c.to_builtin_tensor %7159 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7164 = tensor.empty() : tensor<2048x1280xf32>
    %7165 = linalg.fill ins(%cst : f32) outs(%7164 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7166 = tensor.empty() : tensor<2048x1280xf32>
    %7167 = linalg.fill ins(%cst : f32) outs(%7166 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7168:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7165, %7167, %7162, %7163, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7165, %7167)
    %7169 = arith.truncf %7168#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7170 = torch_c.from_builtin_tensor %7169 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7171 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7172 = torch.aten.view %7170, %7171 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %7173 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7174 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7175 = torch.aten.view %4, %7174 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7176 = torch_c.to_builtin_tensor %7175 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7177 = torch_c.to_builtin_tensor %7173 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7178 = tensor.empty() : tensor<128x1280xf32>
    %7179 = linalg.fill ins(%cst : f32) outs(%7178 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7180 = tensor.empty() : tensor<128x1280xf32>
    %7181 = linalg.fill ins(%cst : f32) outs(%7180 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7182:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7179, %7181, %7176, %7177, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7179, %7181)
    %7183 = arith.truncf %7182#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7184 = torch_c.from_builtin_tensor %7183 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7185 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7186 = torch.aten.view %7184, %7185 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %7187 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7188 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7189 = torch.aten.view %4, %7188 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7190 = torch_c.to_builtin_tensor %7189 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7191 = torch_c.to_builtin_tensor %7187 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7192 = tensor.empty() : tensor<128x1280xf32>
    %7193 = linalg.fill ins(%cst : f32) outs(%7192 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7194 = tensor.empty() : tensor<128x1280xf32>
    %7195 = linalg.fill ins(%cst : f32) outs(%7194 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7196:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7193, %7195, %7190, %7191, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7193, %7195)
    %7197 = arith.truncf %7196#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7198 = torch_c.from_builtin_tensor %7197 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7199 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7200 = torch.aten.view %7198, %7199 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7201 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7202 = torch.aten.view %7172, %7201 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7203 = torch.aten.transpose.int %7202, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7204 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7205 = torch.aten.view %7186, %7204 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7206 = torch.aten.transpose.int %7205, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7207 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7208 = torch.aten.view %7200, %7207 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7209 = torch.aten.transpose.int %7208, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7210:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7203, %7206, %7209, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7211 = torch.aten.transpose.int %7210#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7212 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7213 = torch.aten.view %7211, %7212 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7214 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7215 = torch.aten.view %7213, %7214 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7216 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7217 = torch.aten.transpose.int %7216, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %7218 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7219 = torch.prims.convert_element_type %7218, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7220 = torch.prims.convert_element_type %7215, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7221 = torch.prims.convert_element_type %7217, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7222 = torch.aten.mm %7220, %7221 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7223 = torch.aten.mul.Scalar %7222, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7224 = torch.aten.mul.Scalar %7219, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7225 = torch.aten.add.Tensor %7223, %7224, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7226 = torch.prims.convert_element_type %7225, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7227 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7228 = torch.aten.view %7226, %7227 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7229 = torch.aten.div.Scalar %7228, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7230 = torch.aten.add.Tensor %7229, %7145, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7231 = torch.prims.convert_element_type %7230, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7232 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_209, %result1_210 = torch.aten.var_mean.correction %7231, %7232, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7233 = torch.aten.add.Scalar %result0_209, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7234 = torch.aten.rsqrt %7233 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7235 = torch.aten.sub.Tensor %7230, %result1_210, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7236 = torch.aten.mul.Tensor %7235, %7234 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %7237 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7238 = torch.aten.mul.Tensor %7236, %7237 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %7239 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7240 = torch.aten.add.Tensor %7238, %7239, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7241 = torch.prims.convert_element_type %7240, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7242 = torch.prims.convert_element_type %result1_210, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7243 = torch.prims.convert_element_type %7234, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7244 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7245 = torch.aten.view %7241, %7244 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7246 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7247 = torch.aten.transpose.int %7246, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %7248 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7249 = torch.prims.convert_element_type %7248, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7250 = torch.prims.convert_element_type %7245, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7251 = torch.prims.convert_element_type %7247, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7252 = torch.aten.mm %7250, %7251 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7253 = torch.aten.mul.Scalar %7252, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7254 = torch.aten.mul.Scalar %7249, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7255 = torch.aten.add.Tensor %7253, %7254, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7256 = torch.prims.convert_element_type %7255, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7257 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7258 = torch.aten.view %7256, %7257 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7259 = torch.aten.slice.Tensor %7258, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7260 = torch.aten.slice.Tensor %7258, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7261 = torch.aten.gelu %7260, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7262 = torch.aten.mul.Tensor %7259, %7261 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7263 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7264 = torch.aten.view %7262, %7263 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %7265 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7266 = torch.aten.transpose.int %7265, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %7267 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7268 = torch.prims.convert_element_type %7267, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7269 = torch.prims.convert_element_type %7264, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7270 = torch.prims.convert_element_type %7266, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7271 = torch.aten.mm %7269, %7270 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7272 = torch.aten.mul.Scalar %7271, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7273 = torch.aten.mul.Scalar %7268, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7274 = torch.aten.add.Tensor %7272, %7273, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7275 = torch.prims.convert_element_type %7274, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7276 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7277 = torch.aten.view %7275, %7276 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7278 = torch.aten.add.Tensor %7277, %7230, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7279 = torch.prims.convert_element_type %7278, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7280 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_211, %result1_212 = torch.aten.var_mean.correction %7279, %7280, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7281 = torch.aten.add.Scalar %result0_211, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7282 = torch.aten.rsqrt %7281 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7283 = torch.aten.sub.Tensor %7278, %result1_212, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7284 = torch.aten.mul.Tensor %7283, %7282 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %7285 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7286 = torch.aten.mul.Tensor %7284, %7285 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %7287 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7288 = torch.aten.add.Tensor %7286, %7287, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7289 = torch.prims.convert_element_type %7288, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7290 = torch.prims.convert_element_type %result1_212, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7291 = torch.prims.convert_element_type %7282, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %7292 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7293 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7294 = torch.aten.view %7289, %7293 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7295 = torch_c.to_builtin_tensor %7294 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7296 = torch_c.to_builtin_tensor %7292 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7297 = tensor.empty() : tensor<2048x1280xf32>
    %7298 = linalg.fill ins(%cst : f32) outs(%7297 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7299 = tensor.empty() : tensor<2048x1280xf32>
    %7300 = linalg.fill ins(%cst : f32) outs(%7299 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7301:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7298, %7300, %7295, %7296, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7298, %7300)
    %7302 = arith.truncf %7301#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7303 = torch_c.from_builtin_tensor %7302 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7304 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7305 = torch.aten.view %7303, %7304 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %7306 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7307 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7308 = torch.aten.view %7289, %7307 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7309 = torch_c.to_builtin_tensor %7308 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7310 = torch_c.to_builtin_tensor %7306 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7311 = tensor.empty() : tensor<2048x1280xf32>
    %7312 = linalg.fill ins(%cst : f32) outs(%7311 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7313 = tensor.empty() : tensor<2048x1280xf32>
    %7314 = linalg.fill ins(%cst : f32) outs(%7313 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7315:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7312, %7314, %7309, %7310, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7312, %7314)
    %7316 = arith.truncf %7315#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7317 = torch_c.from_builtin_tensor %7316 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7318 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7319 = torch.aten.view %7317, %7318 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %7320 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7321 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7322 = torch.aten.view %7289, %7321 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7323 = torch_c.to_builtin_tensor %7322 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7324 = torch_c.to_builtin_tensor %7320 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7325 = tensor.empty() : tensor<2048x1280xf32>
    %7326 = linalg.fill ins(%cst : f32) outs(%7325 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7327 = tensor.empty() : tensor<2048x1280xf32>
    %7328 = linalg.fill ins(%cst : f32) outs(%7327 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7329:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7326, %7328, %7323, %7324, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7326, %7328)
    %7330 = arith.truncf %7329#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7331 = torch_c.from_builtin_tensor %7330 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7332 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7333 = torch.aten.view %7331, %7332 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7334 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7335 = torch.aten.view %7305, %7334 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7336 = torch.aten.transpose.int %7335, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7337 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7338 = torch.aten.view %7319, %7337 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7339 = torch.aten.transpose.int %7338, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7340 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7341 = torch.aten.view %7333, %7340 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7342 = torch.aten.transpose.int %7341, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7343:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7336, %7339, %7342, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7344 = torch.aten.transpose.int %7343#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7345 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7346 = torch.aten.view %7344, %7345 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7347 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7348 = torch.aten.view %7346, %7347 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7349 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7350 = torch.aten.transpose.int %7349, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %7351 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7352 = torch.prims.convert_element_type %7351, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7353 = torch.prims.convert_element_type %7348, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7354 = torch.prims.convert_element_type %7350, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7355 = torch.aten.mm %7353, %7354 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7356 = torch.aten.mul.Scalar %7355, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7357 = torch.aten.mul.Scalar %7352, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7358 = torch.aten.add.Tensor %7356, %7357, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7359 = torch.prims.convert_element_type %7358, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7360 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7361 = torch.aten.view %7359, %7360 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7362 = torch.aten.div.Scalar %7361, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7363 = torch.aten.add.Tensor %7362, %7278, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7364 = torch.prims.convert_element_type %7363, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7365 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_213, %result1_214 = torch.aten.var_mean.correction %7364, %7365, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7366 = torch.aten.add.Scalar %result0_213, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7367 = torch.aten.rsqrt %7366 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7368 = torch.aten.sub.Tensor %7363, %result1_214, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7369 = torch.aten.mul.Tensor %7368, %7367 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %7370 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7371 = torch.aten.mul.Tensor %7369, %7370 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %7372 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7373 = torch.aten.add.Tensor %7371, %7372, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7374 = torch.prims.convert_element_type %7373, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7375 = torch.prims.convert_element_type %result1_214, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7376 = torch.prims.convert_element_type %7367, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %7377 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7378 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7379 = torch.aten.view %7374, %7378 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7380 = torch_c.to_builtin_tensor %7379 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7381 = torch_c.to_builtin_tensor %7377 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7382 = tensor.empty() : tensor<2048x1280xf32>
    %7383 = linalg.fill ins(%cst : f32) outs(%7382 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7384 = tensor.empty() : tensor<2048x1280xf32>
    %7385 = linalg.fill ins(%cst : f32) outs(%7384 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7386:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7383, %7385, %7380, %7381, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7383, %7385)
    %7387 = arith.truncf %7386#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7388 = torch_c.from_builtin_tensor %7387 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7389 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7390 = torch.aten.view %7388, %7389 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %7391 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7392 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7393 = torch.aten.view %4, %7392 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7394 = torch_c.to_builtin_tensor %7393 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7395 = torch_c.to_builtin_tensor %7391 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7396 = tensor.empty() : tensor<128x1280xf32>
    %7397 = linalg.fill ins(%cst : f32) outs(%7396 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7398 = tensor.empty() : tensor<128x1280xf32>
    %7399 = linalg.fill ins(%cst : f32) outs(%7398 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7400:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7397, %7399, %7394, %7395, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7397, %7399)
    %7401 = arith.truncf %7400#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7402 = torch_c.from_builtin_tensor %7401 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7403 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7404 = torch.aten.view %7402, %7403 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %7405 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7406 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7407 = torch.aten.view %4, %7406 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7408 = torch_c.to_builtin_tensor %7407 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7409 = torch_c.to_builtin_tensor %7405 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7410 = tensor.empty() : tensor<128x1280xf32>
    %7411 = linalg.fill ins(%cst : f32) outs(%7410 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7412 = tensor.empty() : tensor<128x1280xf32>
    %7413 = linalg.fill ins(%cst : f32) outs(%7412 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7414:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7411, %7413, %7408, %7409, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7411, %7413)
    %7415 = arith.truncf %7414#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7416 = torch_c.from_builtin_tensor %7415 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7417 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7418 = torch.aten.view %7416, %7417 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7419 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7420 = torch.aten.view %7390, %7419 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7421 = torch.aten.transpose.int %7420, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7422 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7423 = torch.aten.view %7404, %7422 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7424 = torch.aten.transpose.int %7423, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7425 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7426 = torch.aten.view %7418, %7425 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7427 = torch.aten.transpose.int %7426, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7428:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7421, %7424, %7427, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7429 = torch.aten.transpose.int %7428#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7430 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7431 = torch.aten.view %7429, %7430 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7432 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7433 = torch.aten.view %7431, %7432 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7434 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7435 = torch.aten.transpose.int %7434, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %7436 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7437 = torch.prims.convert_element_type %7436, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7438 = torch.prims.convert_element_type %7433, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7439 = torch.prims.convert_element_type %7435, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7440 = torch.aten.mm %7438, %7439 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7441 = torch.aten.mul.Scalar %7440, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7442 = torch.aten.mul.Scalar %7437, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7443 = torch.aten.add.Tensor %7441, %7442, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7444 = torch.prims.convert_element_type %7443, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7445 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7446 = torch.aten.view %7444, %7445 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7447 = torch.aten.div.Scalar %7446, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7448 = torch.aten.add.Tensor %7447, %7363, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7449 = torch.prims.convert_element_type %7448, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7450 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_215, %result1_216 = torch.aten.var_mean.correction %7449, %7450, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7451 = torch.aten.add.Scalar %result0_215, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7452 = torch.aten.rsqrt %7451 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7453 = torch.aten.sub.Tensor %7448, %result1_216, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7454 = torch.aten.mul.Tensor %7453, %7452 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %7455 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7456 = torch.aten.mul.Tensor %7454, %7455 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %7457 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7458 = torch.aten.add.Tensor %7456, %7457, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7459 = torch.prims.convert_element_type %7458, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7460 = torch.prims.convert_element_type %result1_216, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7461 = torch.prims.convert_element_type %7452, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7462 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7463 = torch.aten.view %7459, %7462 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7464 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7465 = torch.aten.transpose.int %7464, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %7466 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7467 = torch.prims.convert_element_type %7466, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7468 = torch.prims.convert_element_type %7463, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7469 = torch.prims.convert_element_type %7465, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7470 = torch.aten.mm %7468, %7469 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7471 = torch.aten.mul.Scalar %7470, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7472 = torch.aten.mul.Scalar %7467, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7473 = torch.aten.add.Tensor %7471, %7472, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7474 = torch.prims.convert_element_type %7473, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7475 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7476 = torch.aten.view %7474, %7475 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7477 = torch.aten.slice.Tensor %7476, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7478 = torch.aten.slice.Tensor %7476, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7479 = torch.aten.gelu %7478, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7480 = torch.aten.mul.Tensor %7477, %7479 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7481 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7482 = torch.aten.view %7480, %7481 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %7483 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7484 = torch.aten.transpose.int %7483, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %7485 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7486 = torch.prims.convert_element_type %7485, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7487 = torch.prims.convert_element_type %7482, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7488 = torch.prims.convert_element_type %7484, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7489 = torch.aten.mm %7487, %7488 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7490 = torch.aten.mul.Scalar %7489, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7491 = torch.aten.mul.Scalar %7486, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7492 = torch.aten.add.Tensor %7490, %7491, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7493 = torch.prims.convert_element_type %7492, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7494 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7495 = torch.aten.view %7493, %7494 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7496 = torch.aten.add.Tensor %7495, %7448, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7497 = torch.prims.convert_element_type %7496, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7498 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_217, %result1_218 = torch.aten.var_mean.correction %7497, %7498, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7499 = torch.aten.add.Scalar %result0_217, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7500 = torch.aten.rsqrt %7499 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7501 = torch.aten.sub.Tensor %7496, %result1_218, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7502 = torch.aten.mul.Tensor %7501, %7500 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %7503 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7504 = torch.aten.mul.Tensor %7502, %7503 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %7505 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7506 = torch.aten.add.Tensor %7504, %7505, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7507 = torch.prims.convert_element_type %7506, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7508 = torch.prims.convert_element_type %result1_218, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7509 = torch.prims.convert_element_type %7500, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %7510 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7511 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7512 = torch.aten.view %7507, %7511 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7513 = torch_c.to_builtin_tensor %7512 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7514 = torch_c.to_builtin_tensor %7510 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7515 = tensor.empty() : tensor<2048x1280xf32>
    %7516 = linalg.fill ins(%cst : f32) outs(%7515 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7517 = tensor.empty() : tensor<2048x1280xf32>
    %7518 = linalg.fill ins(%cst : f32) outs(%7517 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7519:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7516, %7518, %7513, %7514, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7516, %7518)
    %7520 = arith.truncf %7519#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7521 = torch_c.from_builtin_tensor %7520 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7522 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7523 = torch.aten.view %7521, %7522 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %7524 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7525 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7526 = torch.aten.view %7507, %7525 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7527 = torch_c.to_builtin_tensor %7526 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7528 = torch_c.to_builtin_tensor %7524 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7529 = tensor.empty() : tensor<2048x1280xf32>
    %7530 = linalg.fill ins(%cst : f32) outs(%7529 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7531 = tensor.empty() : tensor<2048x1280xf32>
    %7532 = linalg.fill ins(%cst : f32) outs(%7531 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7533:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7530, %7532, %7527, %7528, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7530, %7532)
    %7534 = arith.truncf %7533#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7535 = torch_c.from_builtin_tensor %7534 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7536 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7537 = torch.aten.view %7535, %7536 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %7538 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7539 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7540 = torch.aten.view %7507, %7539 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7541 = torch_c.to_builtin_tensor %7540 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7542 = torch_c.to_builtin_tensor %7538 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7543 = tensor.empty() : tensor<2048x1280xf32>
    %7544 = linalg.fill ins(%cst : f32) outs(%7543 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7545 = tensor.empty() : tensor<2048x1280xf32>
    %7546 = linalg.fill ins(%cst : f32) outs(%7545 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7547:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7544, %7546, %7541, %7542, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7544, %7546)
    %7548 = arith.truncf %7547#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7549 = torch_c.from_builtin_tensor %7548 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7550 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7551 = torch.aten.view %7549, %7550 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7552 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7553 = torch.aten.view %7523, %7552 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7554 = torch.aten.transpose.int %7553, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7555 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7556 = torch.aten.view %7537, %7555 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7557 = torch.aten.transpose.int %7556, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7558 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7559 = torch.aten.view %7551, %7558 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7560 = torch.aten.transpose.int %7559, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7561:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7554, %7557, %7560, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7562 = torch.aten.transpose.int %7561#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7563 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7564 = torch.aten.view %7562, %7563 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7565 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7566 = torch.aten.view %7564, %7565 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7567 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7568 = torch.aten.transpose.int %7567, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %7569 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7570 = torch.prims.convert_element_type %7569, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7571 = torch.prims.convert_element_type %7566, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7572 = torch.prims.convert_element_type %7568, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7573 = torch.aten.mm %7571, %7572 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7574 = torch.aten.mul.Scalar %7573, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7575 = torch.aten.mul.Scalar %7570, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7576 = torch.aten.add.Tensor %7574, %7575, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7577 = torch.prims.convert_element_type %7576, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7578 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7579 = torch.aten.view %7577, %7578 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7580 = torch.aten.div.Scalar %7579, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7581 = torch.aten.add.Tensor %7580, %7496, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7582 = torch.prims.convert_element_type %7581, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7583 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_219, %result1_220 = torch.aten.var_mean.correction %7582, %7583, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7584 = torch.aten.add.Scalar %result0_219, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7585 = torch.aten.rsqrt %7584 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7586 = torch.aten.sub.Tensor %7581, %result1_220, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7587 = torch.aten.mul.Tensor %7586, %7585 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %7588 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7589 = torch.aten.mul.Tensor %7587, %7588 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %7590 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7591 = torch.aten.add.Tensor %7589, %7590, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7592 = torch.prims.convert_element_type %7591, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7593 = torch.prims.convert_element_type %result1_220, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7594 = torch.prims.convert_element_type %7585, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %7595 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7596 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7597 = torch.aten.view %7592, %7596 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7598 = torch_c.to_builtin_tensor %7597 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7599 = torch_c.to_builtin_tensor %7595 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7600 = tensor.empty() : tensor<2048x1280xf32>
    %7601 = linalg.fill ins(%cst : f32) outs(%7600 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7602 = tensor.empty() : tensor<2048x1280xf32>
    %7603 = linalg.fill ins(%cst : f32) outs(%7602 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7604:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7601, %7603, %7598, %7599, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7601, %7603)
    %7605 = arith.truncf %7604#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7606 = torch_c.from_builtin_tensor %7605 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7607 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7608 = torch.aten.view %7606, %7607 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %7609 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7610 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7611 = torch.aten.view %4, %7610 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7612 = torch_c.to_builtin_tensor %7611 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7613 = torch_c.to_builtin_tensor %7609 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7614 = tensor.empty() : tensor<128x1280xf32>
    %7615 = linalg.fill ins(%cst : f32) outs(%7614 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7616 = tensor.empty() : tensor<128x1280xf32>
    %7617 = linalg.fill ins(%cst : f32) outs(%7616 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7618:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7615, %7617, %7612, %7613, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7615, %7617)
    %7619 = arith.truncf %7618#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7620 = torch_c.from_builtin_tensor %7619 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7621 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7622 = torch.aten.view %7620, %7621 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %7623 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7624 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7625 = torch.aten.view %4, %7624 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7626 = torch_c.to_builtin_tensor %7625 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7627 = torch_c.to_builtin_tensor %7623 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7628 = tensor.empty() : tensor<128x1280xf32>
    %7629 = linalg.fill ins(%cst : f32) outs(%7628 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7630 = tensor.empty() : tensor<128x1280xf32>
    %7631 = linalg.fill ins(%cst : f32) outs(%7630 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7632:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7629, %7631, %7626, %7627, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7629, %7631)
    %7633 = arith.truncf %7632#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7634 = torch_c.from_builtin_tensor %7633 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7635 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7636 = torch.aten.view %7634, %7635 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7637 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7638 = torch.aten.view %7608, %7637 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7639 = torch.aten.transpose.int %7638, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7640 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7641 = torch.aten.view %7622, %7640 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7642 = torch.aten.transpose.int %7641, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7643 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7644 = torch.aten.view %7636, %7643 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7645 = torch.aten.transpose.int %7644, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7646:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7639, %7642, %7645, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7647 = torch.aten.transpose.int %7646#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7648 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7649 = torch.aten.view %7647, %7648 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7650 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7651 = torch.aten.view %7649, %7650 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7652 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7653 = torch.aten.transpose.int %7652, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %7654 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7655 = torch.prims.convert_element_type %7654, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7656 = torch.prims.convert_element_type %7651, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7657 = torch.prims.convert_element_type %7653, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7658 = torch.aten.mm %7656, %7657 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7659 = torch.aten.mul.Scalar %7658, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7660 = torch.aten.mul.Scalar %7655, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7661 = torch.aten.add.Tensor %7659, %7660, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7662 = torch.prims.convert_element_type %7661, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7663 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7664 = torch.aten.view %7662, %7663 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7665 = torch.aten.div.Scalar %7664, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7666 = torch.aten.add.Tensor %7665, %7581, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7667 = torch.prims.convert_element_type %7666, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7668 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_221, %result1_222 = torch.aten.var_mean.correction %7667, %7668, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7669 = torch.aten.add.Scalar %result0_221, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7670 = torch.aten.rsqrt %7669 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7671 = torch.aten.sub.Tensor %7666, %result1_222, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7672 = torch.aten.mul.Tensor %7671, %7670 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %7673 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7674 = torch.aten.mul.Tensor %7672, %7673 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %7675 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7676 = torch.aten.add.Tensor %7674, %7675, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7677 = torch.prims.convert_element_type %7676, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7678 = torch.prims.convert_element_type %result1_222, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7679 = torch.prims.convert_element_type %7670, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7680 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7681 = torch.aten.view %7677, %7680 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7682 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7683 = torch.aten.transpose.int %7682, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %7684 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7685 = torch.prims.convert_element_type %7684, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7686 = torch.prims.convert_element_type %7681, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7687 = torch.prims.convert_element_type %7683, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7688 = torch.aten.mm %7686, %7687 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7689 = torch.aten.mul.Scalar %7688, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7690 = torch.aten.mul.Scalar %7685, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7691 = torch.aten.add.Tensor %7689, %7690, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7692 = torch.prims.convert_element_type %7691, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7693 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7694 = torch.aten.view %7692, %7693 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7695 = torch.aten.slice.Tensor %7694, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7696 = torch.aten.slice.Tensor %7694, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7697 = torch.aten.gelu %7696, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7698 = torch.aten.mul.Tensor %7695, %7697 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7699 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7700 = torch.aten.view %7698, %7699 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %7701 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7702 = torch.aten.transpose.int %7701, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %7703 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7704 = torch.prims.convert_element_type %7703, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7705 = torch.prims.convert_element_type %7700, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7706 = torch.prims.convert_element_type %7702, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7707 = torch.aten.mm %7705, %7706 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7708 = torch.aten.mul.Scalar %7707, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7709 = torch.aten.mul.Scalar %7704, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7710 = torch.aten.add.Tensor %7708, %7709, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7711 = torch.prims.convert_element_type %7710, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7712 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7713 = torch.aten.view %7711, %7712 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7714 = torch.aten.add.Tensor %7713, %7666, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7715 = torch.prims.convert_element_type %7714, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7716 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_223, %result1_224 = torch.aten.var_mean.correction %7715, %7716, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7717 = torch.aten.add.Scalar %result0_223, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7718 = torch.aten.rsqrt %7717 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7719 = torch.aten.sub.Tensor %7714, %result1_224, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7720 = torch.aten.mul.Tensor %7719, %7718 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %7721 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7722 = torch.aten.mul.Tensor %7720, %7721 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %7723 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7724 = torch.aten.add.Tensor %7722, %7723, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7725 = torch.prims.convert_element_type %7724, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7726 = torch.prims.convert_element_type %result1_224, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7727 = torch.prims.convert_element_type %7718, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %7728 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7729 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7730 = torch.aten.view %7725, %7729 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7731 = torch_c.to_builtin_tensor %7730 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7732 = torch_c.to_builtin_tensor %7728 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7733 = tensor.empty() : tensor<2048x1280xf32>
    %7734 = linalg.fill ins(%cst : f32) outs(%7733 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7735 = tensor.empty() : tensor<2048x1280xf32>
    %7736 = linalg.fill ins(%cst : f32) outs(%7735 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7737:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7734, %7736, %7731, %7732, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7734, %7736)
    %7738 = arith.truncf %7737#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7739 = torch_c.from_builtin_tensor %7738 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7740 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7741 = torch.aten.view %7739, %7740 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %7742 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7743 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7744 = torch.aten.view %7725, %7743 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7745 = torch_c.to_builtin_tensor %7744 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7746 = torch_c.to_builtin_tensor %7742 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7747 = tensor.empty() : tensor<2048x1280xf32>
    %7748 = linalg.fill ins(%cst : f32) outs(%7747 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7749 = tensor.empty() : tensor<2048x1280xf32>
    %7750 = linalg.fill ins(%cst : f32) outs(%7749 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7751:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7748, %7750, %7745, %7746, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7748, %7750)
    %7752 = arith.truncf %7751#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7753 = torch_c.from_builtin_tensor %7752 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7754 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7755 = torch.aten.view %7753, %7754 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %7756 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7757 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7758 = torch.aten.view %7725, %7757 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7759 = torch_c.to_builtin_tensor %7758 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7760 = torch_c.to_builtin_tensor %7756 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7761 = tensor.empty() : tensor<2048x1280xf32>
    %7762 = linalg.fill ins(%cst : f32) outs(%7761 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7763 = tensor.empty() : tensor<2048x1280xf32>
    %7764 = linalg.fill ins(%cst : f32) outs(%7763 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7765:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7762, %7764, %7759, %7760, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7762, %7764)
    %7766 = arith.truncf %7765#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7767 = torch_c.from_builtin_tensor %7766 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7768 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7769 = torch.aten.view %7767, %7768 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7770 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7771 = torch.aten.view %7741, %7770 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7772 = torch.aten.transpose.int %7771, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7773 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7774 = torch.aten.view %7755, %7773 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7775 = torch.aten.transpose.int %7774, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7776 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7777 = torch.aten.view %7769, %7776 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7778 = torch.aten.transpose.int %7777, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7779:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7772, %7775, %7778, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7780 = torch.aten.transpose.int %7779#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7781 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7782 = torch.aten.view %7780, %7781 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7783 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7784 = torch.aten.view %7782, %7783 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %7785 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7786 = torch.aten.transpose.int %7785, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %7787 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7788 = torch.prims.convert_element_type %7787, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7789 = torch.prims.convert_element_type %7784, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7790 = torch.prims.convert_element_type %7786, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7791 = torch.aten.mm %7789, %7790 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7792 = torch.aten.mul.Scalar %7791, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7793 = torch.aten.mul.Scalar %7788, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7794 = torch.aten.add.Tensor %7792, %7793, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7795 = torch.prims.convert_element_type %7794, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7796 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7797 = torch.aten.view %7795, %7796 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7798 = torch.aten.div.Scalar %7797, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7799 = torch.aten.add.Tensor %7798, %7714, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7800 = torch.prims.convert_element_type %7799, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7801 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_225, %result1_226 = torch.aten.var_mean.correction %7800, %7801, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7802 = torch.aten.add.Scalar %result0_225, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7803 = torch.aten.rsqrt %7802 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7804 = torch.aten.sub.Tensor %7799, %result1_226, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7805 = torch.aten.mul.Tensor %7804, %7803 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %7806 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7807 = torch.aten.mul.Tensor %7805, %7806 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %7808 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7809 = torch.aten.add.Tensor %7807, %7808, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7810 = torch.prims.convert_element_type %7809, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7811 = torch.prims.convert_element_type %result1_226, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7812 = torch.prims.convert_element_type %7803, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %7813 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7814 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7815 = torch.aten.view %7810, %7814 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7816 = torch_c.to_builtin_tensor %7815 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7817 = torch_c.to_builtin_tensor %7813 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7818 = tensor.empty() : tensor<2048x1280xf32>
    %7819 = linalg.fill ins(%cst : f32) outs(%7818 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7820 = tensor.empty() : tensor<2048x1280xf32>
    %7821 = linalg.fill ins(%cst : f32) outs(%7820 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7822:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7819, %7821, %7816, %7817, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7819, %7821)
    %7823 = arith.truncf %7822#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7824 = torch_c.from_builtin_tensor %7823 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7825 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7826 = torch.aten.view %7824, %7825 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %7827 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7828 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7829 = torch.aten.view %4, %7828 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7830 = torch_c.to_builtin_tensor %7829 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7831 = torch_c.to_builtin_tensor %7827 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7832 = tensor.empty() : tensor<128x1280xf32>
    %7833 = linalg.fill ins(%cst : f32) outs(%7832 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7834 = tensor.empty() : tensor<128x1280xf32>
    %7835 = linalg.fill ins(%cst : f32) outs(%7834 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7836:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7833, %7835, %7830, %7831, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7833, %7835)
    %7837 = arith.truncf %7836#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7838 = torch_c.from_builtin_tensor %7837 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7839 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7840 = torch.aten.view %7838, %7839 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %7841 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %7842 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %7843 = torch.aten.view %4, %7842 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %7844 = torch_c.to_builtin_tensor %7843 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %7845 = torch_c.to_builtin_tensor %7841 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %7846 = tensor.empty() : tensor<128x1280xf32>
    %7847 = linalg.fill ins(%cst : f32) outs(%7846 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7848 = tensor.empty() : tensor<128x1280xf32>
    %7849 = linalg.fill ins(%cst : f32) outs(%7848 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %7850:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %7847, %7849, %7844, %7845, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7847, %7849)
    %7851 = arith.truncf %7850#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %7852 = torch_c.from_builtin_tensor %7851 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %7853 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7854 = torch.aten.view %7852, %7853 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %7855 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7856 = torch.aten.view %7826, %7855 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7857 = torch.aten.transpose.int %7856, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7858 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7859 = torch.aten.view %7840, %7858 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7860 = torch.aten.transpose.int %7859, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7861 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7862 = torch.aten.view %7854, %7861 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %7863 = torch.aten.transpose.int %7862, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %7864:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7857, %7860, %7863, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7865 = torch.aten.transpose.int %7864#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7866 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7867 = torch.aten.view %7865, %7866 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7868 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7869 = torch.aten.view %7867, %7868 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %7870 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7871 = torch.aten.transpose.int %7870, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %7872 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7873 = torch.prims.convert_element_type %7872, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7874 = torch.prims.convert_element_type %7869, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7875 = torch.prims.convert_element_type %7871, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7876 = torch.aten.mm %7874, %7875 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7877 = torch.aten.mul.Scalar %7876, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7878 = torch.aten.mul.Scalar %7873, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7879 = torch.aten.add.Tensor %7877, %7878, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7880 = torch.prims.convert_element_type %7879, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7881 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7882 = torch.aten.view %7880, %7881 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7883 = torch.aten.div.Scalar %7882, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %7884 = torch.aten.add.Tensor %7883, %7799, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7885 = torch.prims.convert_element_type %7884, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7886 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_227, %result1_228 = torch.aten.var_mean.correction %7885, %7886, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7887 = torch.aten.add.Scalar %result0_227, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7888 = torch.aten.rsqrt %7887 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7889 = torch.aten.sub.Tensor %7884, %result1_228, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7890 = torch.aten.mul.Tensor %7889, %7888 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %7891 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7892 = torch.aten.mul.Tensor %7890, %7891 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %7893 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7894 = torch.aten.add.Tensor %7892, %7893, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7895 = torch.prims.convert_element_type %7894, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7896 = torch.prims.convert_element_type %result1_228, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7897 = torch.prims.convert_element_type %7888, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7898 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7899 = torch.aten.view %7895, %7898 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %7900 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %7901 = torch.aten.transpose.int %7900, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %7902 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %7903 = torch.prims.convert_element_type %7902, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %7904 = torch.prims.convert_element_type %7899, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7905 = torch.prims.convert_element_type %7901, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %7906 = torch.aten.mm %7904, %7905 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %7907 = torch.aten.mul.Scalar %7906, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7908 = torch.aten.mul.Scalar %7903, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %7909 = torch.aten.add.Tensor %7907, %7908, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %7910 = torch.prims.convert_element_type %7909, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %7911 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7912 = torch.aten.view %7910, %7911 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %7913 = torch.aten.slice.Tensor %7912, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7914 = torch.aten.slice.Tensor %7912, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %7915 = torch.aten.gelu %7914, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %7916 = torch.aten.mul.Tensor %7913, %7915 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %7917 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %7918 = torch.aten.view %7916, %7917 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %7919 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %7920 = torch.aten.transpose.int %7919, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %7921 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7922 = torch.prims.convert_element_type %7921, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %7923 = torch.prims.convert_element_type %7918, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %7924 = torch.prims.convert_element_type %7920, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %7925 = torch.aten.mm %7923, %7924 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %7926 = torch.aten.mul.Scalar %7925, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7927 = torch.aten.mul.Scalar %7922, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %7928 = torch.aten.add.Tensor %7926, %7927, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %7929 = torch.prims.convert_element_type %7928, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %7930 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7931 = torch.aten.view %7929, %7930 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7932 = torch.aten.add.Tensor %7931, %7884, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7933 = torch.prims.convert_element_type %7932, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7934 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_229, %result1_230 = torch.aten.var_mean.correction %7933, %7934, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %7935 = torch.aten.add.Scalar %result0_229, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %7936 = torch.aten.rsqrt %7935 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %7937 = torch.aten.sub.Tensor %7932, %result1_230, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7938 = torch.aten.mul.Tensor %7937, %7936 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %7939 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7940 = torch.aten.mul.Tensor %7938, %7939 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %7941 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %7942 = torch.aten.add.Tensor %7940, %7941, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %7943 = torch.prims.convert_element_type %7942, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %7944 = torch.prims.convert_element_type %result1_230, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %7945 = torch.prims.convert_element_type %7936, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %7946 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7947 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7948 = torch.aten.view %7943, %7947 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7949 = torch_c.to_builtin_tensor %7948 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7950 = torch_c.to_builtin_tensor %7946 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7951 = tensor.empty() : tensor<2048x1280xf32>
    %7952 = linalg.fill ins(%cst : f32) outs(%7951 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7953 = tensor.empty() : tensor<2048x1280xf32>
    %7954 = linalg.fill ins(%cst : f32) outs(%7953 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7955:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7952, %7954, %7949, %7950, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7952, %7954)
    %7956 = arith.truncf %7955#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7957 = torch_c.from_builtin_tensor %7956 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7958 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7959 = torch.aten.view %7957, %7958 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %7960 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7961 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7962 = torch.aten.view %7943, %7961 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7963 = torch_c.to_builtin_tensor %7962 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7964 = torch_c.to_builtin_tensor %7960 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7965 = tensor.empty() : tensor<2048x1280xf32>
    %7966 = linalg.fill ins(%cst : f32) outs(%7965 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7967 = tensor.empty() : tensor<2048x1280xf32>
    %7968 = linalg.fill ins(%cst : f32) outs(%7967 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7969:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7966, %7968, %7963, %7964, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7966, %7968)
    %7970 = arith.truncf %7969#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7971 = torch_c.from_builtin_tensor %7970 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7972 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7973 = torch.aten.view %7971, %7972 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %7974 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %7975 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %7976 = torch.aten.view %7943, %7975 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %7977 = torch_c.to_builtin_tensor %7976 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %7978 = torch_c.to_builtin_tensor %7974 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %7979 = tensor.empty() : tensor<2048x1280xf32>
    %7980 = linalg.fill ins(%cst : f32) outs(%7979 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7981 = tensor.empty() : tensor<2048x1280xf32>
    %7982 = linalg.fill ins(%cst : f32) outs(%7981 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %7983:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %7980, %7982, %7977, %7978, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%7980, %7982)
    %7984 = arith.truncf %7983#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %7985 = torch_c.from_builtin_tensor %7984 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %7986 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7987 = torch.aten.view %7985, %7986 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %7988 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7989 = torch.aten.view %7959, %7988 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7990 = torch.aten.transpose.int %7989, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7991 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7992 = torch.aten.view %7973, %7991 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7993 = torch.aten.transpose.int %7992, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7994 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7995 = torch.aten.view %7987, %7994 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %7996 = torch.aten.transpose.int %7995, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %7997:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%7990, %7993, %7996, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %7998 = torch.aten.transpose.int %7997#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %7999 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8000 = torch.aten.view %7998, %7999 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8001 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8002 = torch.aten.view %8000, %8001 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8003 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8004 = torch.aten.transpose.int %8003, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %8005 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8006 = torch.prims.convert_element_type %8005, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8007 = torch.prims.convert_element_type %8002, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8008 = torch.prims.convert_element_type %8004, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8009 = torch.aten.mm %8007, %8008 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8010 = torch.aten.mul.Scalar %8009, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8011 = torch.aten.mul.Scalar %8006, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8012 = torch.aten.add.Tensor %8010, %8011, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8013 = torch.prims.convert_element_type %8012, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8014 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8015 = torch.aten.view %8013, %8014 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8016 = torch.aten.div.Scalar %8015, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8017 = torch.aten.add.Tensor %8016, %7932, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8018 = torch.prims.convert_element_type %8017, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8019 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_231, %result1_232 = torch.aten.var_mean.correction %8018, %8019, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8020 = torch.aten.add.Scalar %result0_231, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8021 = torch.aten.rsqrt %8020 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8022 = torch.aten.sub.Tensor %8017, %result1_232, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8023 = torch.aten.mul.Tensor %8022, %8021 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %8024 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8025 = torch.aten.mul.Tensor %8023, %8024 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %8026 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8027 = torch.aten.add.Tensor %8025, %8026, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8028 = torch.prims.convert_element_type %8027, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8029 = torch.prims.convert_element_type %result1_232, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8030 = torch.prims.convert_element_type %8021, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %8031 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8032 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8033 = torch.aten.view %8028, %8032 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8034 = torch_c.to_builtin_tensor %8033 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8035 = torch_c.to_builtin_tensor %8031 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8036 = tensor.empty() : tensor<2048x1280xf32>
    %8037 = linalg.fill ins(%cst : f32) outs(%8036 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8038 = tensor.empty() : tensor<2048x1280xf32>
    %8039 = linalg.fill ins(%cst : f32) outs(%8038 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8040:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8037, %8039, %8034, %8035, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8037, %8039)
    %8041 = arith.truncf %8040#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8042 = torch_c.from_builtin_tensor %8041 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8043 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8044 = torch.aten.view %8042, %8043 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %8045 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8046 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8047 = torch.aten.view %4, %8046 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8048 = torch_c.to_builtin_tensor %8047 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8049 = torch_c.to_builtin_tensor %8045 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8050 = tensor.empty() : tensor<128x1280xf32>
    %8051 = linalg.fill ins(%cst : f32) outs(%8050 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8052 = tensor.empty() : tensor<128x1280xf32>
    %8053 = linalg.fill ins(%cst : f32) outs(%8052 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8054:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8051, %8053, %8048, %8049, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8051, %8053)
    %8055 = arith.truncf %8054#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8056 = torch_c.from_builtin_tensor %8055 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8057 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8058 = torch.aten.view %8056, %8057 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %8059 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8060 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8061 = torch.aten.view %4, %8060 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8062 = torch_c.to_builtin_tensor %8061 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8063 = torch_c.to_builtin_tensor %8059 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8064 = tensor.empty() : tensor<128x1280xf32>
    %8065 = linalg.fill ins(%cst : f32) outs(%8064 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8066 = tensor.empty() : tensor<128x1280xf32>
    %8067 = linalg.fill ins(%cst : f32) outs(%8066 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8068:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8065, %8067, %8062, %8063, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8065, %8067)
    %8069 = arith.truncf %8068#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8070 = torch_c.from_builtin_tensor %8069 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8071 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8072 = torch.aten.view %8070, %8071 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8073 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8074 = torch.aten.view %8044, %8073 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8075 = torch.aten.transpose.int %8074, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8076 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8077 = torch.aten.view %8058, %8076 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8078 = torch.aten.transpose.int %8077, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8079 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8080 = torch.aten.view %8072, %8079 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8081 = torch.aten.transpose.int %8080, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8082:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8075, %8078, %8081, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8083 = torch.aten.transpose.int %8082#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8084 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8085 = torch.aten.view %8083, %8084 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8086 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8087 = torch.aten.view %8085, %8086 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %8088 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8089 = torch.aten.transpose.int %8088, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %8090 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8091 = torch.prims.convert_element_type %8090, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8092 = torch.prims.convert_element_type %8087, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8093 = torch.prims.convert_element_type %8089, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8094 = torch.aten.mm %8092, %8093 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8095 = torch.aten.mul.Scalar %8094, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8096 = torch.aten.mul.Scalar %8091, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8097 = torch.aten.add.Tensor %8095, %8096, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8098 = torch.prims.convert_element_type %8097, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8099 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8100 = torch.aten.view %8098, %8099 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8101 = torch.aten.div.Scalar %8100, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8102 = torch.aten.add.Tensor %8101, %8017, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8103 = torch.prims.convert_element_type %8102, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8104 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_233, %result1_234 = torch.aten.var_mean.correction %8103, %8104, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8105 = torch.aten.add.Scalar %result0_233, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8106 = torch.aten.rsqrt %8105 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8107 = torch.aten.sub.Tensor %8102, %result1_234, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8108 = torch.aten.mul.Tensor %8107, %8106 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %8109 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8110 = torch.aten.mul.Tensor %8108, %8109 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %8111 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8112 = torch.aten.add.Tensor %8110, %8111, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8113 = torch.prims.convert_element_type %8112, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8114 = torch.prims.convert_element_type %result1_234, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8115 = torch.prims.convert_element_type %8106, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8116 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8117 = torch.aten.view %8113, %8116 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %8118 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %8119 = torch.aten.transpose.int %8118, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %8120 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %8121 = torch.prims.convert_element_type %8120, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %8122 = torch.prims.convert_element_type %8117, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8123 = torch.prims.convert_element_type %8119, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %8124 = torch.aten.mm %8122, %8123 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %8125 = torch.aten.mul.Scalar %8124, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8126 = torch.aten.mul.Scalar %8121, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %8127 = torch.aten.add.Tensor %8125, %8126, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8128 = torch.prims.convert_element_type %8127, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %8129 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8130 = torch.aten.view %8128, %8129 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %8131 = torch.aten.slice.Tensor %8130, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8132 = torch.aten.slice.Tensor %8130, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8133 = torch.aten.gelu %8132, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %8134 = torch.aten.mul.Tensor %8131, %8133 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %8135 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %8136 = torch.aten.view %8134, %8135 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %8137 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %8138 = torch.aten.transpose.int %8137, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %8139 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8140 = torch.prims.convert_element_type %8139, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8141 = torch.prims.convert_element_type %8136, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %8142 = torch.prims.convert_element_type %8138, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %8143 = torch.aten.mm %8141, %8142 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8144 = torch.aten.mul.Scalar %8143, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8145 = torch.aten.mul.Scalar %8140, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8146 = torch.aten.add.Tensor %8144, %8145, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8147 = torch.prims.convert_element_type %8146, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8148 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8149 = torch.aten.view %8147, %8148 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8150 = torch.aten.add.Tensor %8149, %8102, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8151 = torch.prims.convert_element_type %8150, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8152 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_235, %result1_236 = torch.aten.var_mean.correction %8151, %8152, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8153 = torch.aten.add.Scalar %result0_235, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8154 = torch.aten.rsqrt %8153 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8155 = torch.aten.sub.Tensor %8150, %result1_236, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8156 = torch.aten.mul.Tensor %8155, %8154 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %8157 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8158 = torch.aten.mul.Tensor %8156, %8157 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %8159 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8160 = torch.aten.add.Tensor %8158, %8159, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8161 = torch.prims.convert_element_type %8160, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8162 = torch.prims.convert_element_type %result1_236, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8163 = torch.prims.convert_element_type %8154, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %8164 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8165 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8166 = torch.aten.view %8161, %8165 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8167 = torch_c.to_builtin_tensor %8166 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8168 = torch_c.to_builtin_tensor %8164 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8169 = tensor.empty() : tensor<2048x1280xf32>
    %8170 = linalg.fill ins(%cst : f32) outs(%8169 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8171 = tensor.empty() : tensor<2048x1280xf32>
    %8172 = linalg.fill ins(%cst : f32) outs(%8171 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8173:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8170, %8172, %8167, %8168, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8170, %8172)
    %8174 = arith.truncf %8173#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8175 = torch_c.from_builtin_tensor %8174 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8176 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8177 = torch.aten.view %8175, %8176 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %8178 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8179 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8180 = torch.aten.view %8161, %8179 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8181 = torch_c.to_builtin_tensor %8180 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8182 = torch_c.to_builtin_tensor %8178 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8183 = tensor.empty() : tensor<2048x1280xf32>
    %8184 = linalg.fill ins(%cst : f32) outs(%8183 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8185 = tensor.empty() : tensor<2048x1280xf32>
    %8186 = linalg.fill ins(%cst : f32) outs(%8185 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8187:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8184, %8186, %8181, %8182, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8184, %8186)
    %8188 = arith.truncf %8187#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8189 = torch_c.from_builtin_tensor %8188 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8190 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8191 = torch.aten.view %8189, %8190 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %8192 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8193 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8194 = torch.aten.view %8161, %8193 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8195 = torch_c.to_builtin_tensor %8194 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8196 = torch_c.to_builtin_tensor %8192 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8197 = tensor.empty() : tensor<2048x1280xf32>
    %8198 = linalg.fill ins(%cst : f32) outs(%8197 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8199 = tensor.empty() : tensor<2048x1280xf32>
    %8200 = linalg.fill ins(%cst : f32) outs(%8199 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8201:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8198, %8200, %8195, %8196, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8198, %8200)
    %8202 = arith.truncf %8201#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8203 = torch_c.from_builtin_tensor %8202 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8204 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8205 = torch.aten.view %8203, %8204 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8206 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8207 = torch.aten.view %8177, %8206 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8208 = torch.aten.transpose.int %8207, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8209 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8210 = torch.aten.view %8191, %8209 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8211 = torch.aten.transpose.int %8210, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8212 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8213 = torch.aten.view %8205, %8212 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8214 = torch.aten.transpose.int %8213, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8215:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8208, %8211, %8214, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8216 = torch.aten.transpose.int %8215#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8217 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8218 = torch.aten.view %8216, %8217 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8219 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8220 = torch.aten.view %8218, %8219 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8221 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8222 = torch.aten.transpose.int %8221, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %8223 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8224 = torch.prims.convert_element_type %8223, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8225 = torch.prims.convert_element_type %8220, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8226 = torch.prims.convert_element_type %8222, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8227 = torch.aten.mm %8225, %8226 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8228 = torch.aten.mul.Scalar %8227, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8229 = torch.aten.mul.Scalar %8224, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8230 = torch.aten.add.Tensor %8228, %8229, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8231 = torch.prims.convert_element_type %8230, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8232 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8233 = torch.aten.view %8231, %8232 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8234 = torch.aten.div.Scalar %8233, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8235 = torch.aten.add.Tensor %8234, %8150, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8236 = torch.prims.convert_element_type %8235, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8237 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_237, %result1_238 = torch.aten.var_mean.correction %8236, %8237, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8238 = torch.aten.add.Scalar %result0_237, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8239 = torch.aten.rsqrt %8238 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8240 = torch.aten.sub.Tensor %8235, %result1_238, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8241 = torch.aten.mul.Tensor %8240, %8239 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %8242 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8243 = torch.aten.mul.Tensor %8241, %8242 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %8244 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8245 = torch.aten.add.Tensor %8243, %8244, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8246 = torch.prims.convert_element_type %8245, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8247 = torch.prims.convert_element_type %result1_238, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8248 = torch.prims.convert_element_type %8239, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %8249 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8250 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8251 = torch.aten.view %8246, %8250 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8252 = torch_c.to_builtin_tensor %8251 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8253 = torch_c.to_builtin_tensor %8249 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8254 = tensor.empty() : tensor<2048x1280xf32>
    %8255 = linalg.fill ins(%cst : f32) outs(%8254 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8256 = tensor.empty() : tensor<2048x1280xf32>
    %8257 = linalg.fill ins(%cst : f32) outs(%8256 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8258:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8255, %8257, %8252, %8253, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8255, %8257)
    %8259 = arith.truncf %8258#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8260 = torch_c.from_builtin_tensor %8259 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8261 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8262 = torch.aten.view %8260, %8261 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %8263 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8264 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8265 = torch.aten.view %4, %8264 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8266 = torch_c.to_builtin_tensor %8265 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8267 = torch_c.to_builtin_tensor %8263 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8268 = tensor.empty() : tensor<128x1280xf32>
    %8269 = linalg.fill ins(%cst : f32) outs(%8268 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8270 = tensor.empty() : tensor<128x1280xf32>
    %8271 = linalg.fill ins(%cst : f32) outs(%8270 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8272:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8269, %8271, %8266, %8267, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8269, %8271)
    %8273 = arith.truncf %8272#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8274 = torch_c.from_builtin_tensor %8273 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8275 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8276 = torch.aten.view %8274, %8275 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %8277 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8278 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8279 = torch.aten.view %4, %8278 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8280 = torch_c.to_builtin_tensor %8279 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8281 = torch_c.to_builtin_tensor %8277 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8282 = tensor.empty() : tensor<128x1280xf32>
    %8283 = linalg.fill ins(%cst : f32) outs(%8282 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8284 = tensor.empty() : tensor<128x1280xf32>
    %8285 = linalg.fill ins(%cst : f32) outs(%8284 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8286:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8283, %8285, %8280, %8281, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8283, %8285)
    %8287 = arith.truncf %8286#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8288 = torch_c.from_builtin_tensor %8287 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8289 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8290 = torch.aten.view %8288, %8289 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8291 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8292 = torch.aten.view %8262, %8291 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8293 = torch.aten.transpose.int %8292, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8294 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8295 = torch.aten.view %8276, %8294 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8296 = torch.aten.transpose.int %8295, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8297 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8298 = torch.aten.view %8290, %8297 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8299 = torch.aten.transpose.int %8298, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8300:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8293, %8296, %8299, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8301 = torch.aten.transpose.int %8300#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8302 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8303 = torch.aten.view %8301, %8302 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8304 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8305 = torch.aten.view %8303, %8304 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %8306 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8307 = torch.aten.transpose.int %8306, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %8308 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8309 = torch.prims.convert_element_type %8308, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8310 = torch.prims.convert_element_type %8305, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8311 = torch.prims.convert_element_type %8307, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8312 = torch.aten.mm %8310, %8311 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8313 = torch.aten.mul.Scalar %8312, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8314 = torch.aten.mul.Scalar %8309, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8315 = torch.aten.add.Tensor %8313, %8314, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8316 = torch.prims.convert_element_type %8315, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8317 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8318 = torch.aten.view %8316, %8317 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8319 = torch.aten.div.Scalar %8318, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8320 = torch.aten.add.Tensor %8319, %8235, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8321 = torch.prims.convert_element_type %8320, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8322 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_239, %result1_240 = torch.aten.var_mean.correction %8321, %8322, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8323 = torch.aten.add.Scalar %result0_239, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8324 = torch.aten.rsqrt %8323 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8325 = torch.aten.sub.Tensor %8320, %result1_240, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8326 = torch.aten.mul.Tensor %8325, %8324 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %8327 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8328 = torch.aten.mul.Tensor %8326, %8327 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %8329 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8330 = torch.aten.add.Tensor %8328, %8329, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8331 = torch.prims.convert_element_type %8330, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8332 = torch.prims.convert_element_type %result1_240, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8333 = torch.prims.convert_element_type %8324, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8334 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8335 = torch.aten.view %8331, %8334 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %8336 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %8337 = torch.aten.transpose.int %8336, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %8338 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %8339 = torch.prims.convert_element_type %8338, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %8340 = torch.prims.convert_element_type %8335, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8341 = torch.prims.convert_element_type %8337, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %8342 = torch.aten.mm %8340, %8341 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %8343 = torch.aten.mul.Scalar %8342, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8344 = torch.aten.mul.Scalar %8339, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %8345 = torch.aten.add.Tensor %8343, %8344, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8346 = torch.prims.convert_element_type %8345, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %8347 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8348 = torch.aten.view %8346, %8347 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %8349 = torch.aten.slice.Tensor %8348, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8350 = torch.aten.slice.Tensor %8348, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8351 = torch.aten.gelu %8350, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %8352 = torch.aten.mul.Tensor %8349, %8351 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %8353 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %8354 = torch.aten.view %8352, %8353 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %8355 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %8356 = torch.aten.transpose.int %8355, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %8357 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8358 = torch.prims.convert_element_type %8357, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8359 = torch.prims.convert_element_type %8354, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %8360 = torch.prims.convert_element_type %8356, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %8361 = torch.aten.mm %8359, %8360 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8362 = torch.aten.mul.Scalar %8361, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8363 = torch.aten.mul.Scalar %8358, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8364 = torch.aten.add.Tensor %8362, %8363, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8365 = torch.prims.convert_element_type %8364, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8366 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8367 = torch.aten.view %8365, %8366 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8368 = torch.aten.add.Tensor %8367, %8320, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8369 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8370 = torch.aten.view %8368, %8369 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_out.weight = util.global.load @_params.unet.mid_block.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %8371 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8372 = torch.aten.transpose.int %8371, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.attentions.0.proj_out.bias = util.global.load @_params.unet.mid_block.attentions.0.proj_out.bias : tensor<1280xf16>
    %8373 = torch_c.from_builtin_tensor %_params.unet.mid_block.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8374 = torch.prims.convert_element_type %8373, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8375 = torch.prims.convert_element_type %8370, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8376 = torch.prims.convert_element_type %8372, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8377 = torch.aten.mm %8375, %8376 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8378 = torch.aten.mul.Scalar %8377, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8379 = torch.aten.mul.Scalar %8374, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8380 = torch.aten.add.Tensor %8378, %8379, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8381 = torch.prims.convert_element_type %8380, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8382 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8383 = torch.aten.view %8381, %8382 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8384 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8385 = torch.aten.view %8383, %8384 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %8386 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8387 = torch.aten.permute %8385, %8386 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %8388 = torch.aten.add.Tensor %8387, %6137, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8389 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8390 = torch.aten.view %8388, %8389 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %8391 = torch.prims.convert_element_type %8390, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8392 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_241, %result1_242 = torch.aten.var_mean.correction %8391, %8392, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8393 = torch.aten.add.Scalar %result0_241, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8394 = torch.aten.rsqrt %8393 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8395 = torch.aten.sub.Tensor %8390, %result1_242, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8396 = torch.aten.mul.Tensor %8395, %8394 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %8397 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8398 = torch.aten.view %8396, %8397 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.1.norm1.bias = util.global.load @_params.unet.mid_block.resnets.1.norm1.bias : tensor<1280xf16>
    %8399 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8400 = torch.aten.unsqueeze %8399, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8401 = torch.aten.unsqueeze %8400, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8402 = torch.aten.unsqueeze %8401, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.1.norm1.weight = util.global.load @_params.unet.mid_block.resnets.1.norm1.weight : tensor<1280xf16>
    %8403 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8404 = torch.aten.unsqueeze %8403, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8405 = torch.aten.unsqueeze %8404, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8406 = torch.aten.unsqueeze %8405, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %8407 = torch.aten.mul.Tensor %8398, %8406 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %8408 = torch.aten.add.Tensor %8407, %8402, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %8409 = torch.prims.convert_element_type %8408, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8410 = torch.prims.convert_element_type %result1_242, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8411 = torch.prims.convert_element_type %8394, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8412 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8413 = torch.prims.squeeze %8410, %8412 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8414 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8415 = torch.prims.squeeze %8413, %8414 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8416 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8417 = torch.prims.squeeze %8411, %8416 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8418 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8419 = torch.prims.squeeze %8417, %8418 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8420 = torch.aten.silu %8409 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.1.conv1.weight = util.global.load @_params.unet.mid_block.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16>
    %8421 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.1.conv1.bias = util.global.load @_params.unet.mid_block.resnets.1.conv1.bias : tensor<1280xf16>
    %8422 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8423 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8424 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8425 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8426 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8427 = torch.aten.convolution %8420, %8421, %8422, %8423, %8424, %8425, %false, %8426, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8428 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.mid_block.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.mid_block.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %8429 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8430 = torch.aten.transpose.int %8429, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.mid_block.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.mid_block.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %8431 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8432 = torch.prims.convert_element_type %8431, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8433 = torch.prims.convert_element_type %8428, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8434 = torch.prims.convert_element_type %8430, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8435 = torch.aten.mm %8433, %8434 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %8436 = torch.aten.mul.Scalar %8435, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8437 = torch.aten.mul.Scalar %8432, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8438 = torch.aten.add.Tensor %8436, %8437, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8439 = torch.prims.convert_element_type %8438, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %8440 = torch.aten.unsqueeze %8439, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %8441 = torch.aten.unsqueeze %8440, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %8442 = torch.aten.add.Tensor %8427, %8441, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8443 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8444 = torch.aten.view %8442, %8443 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %8445 = torch.prims.convert_element_type %8444, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8446 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_243, %result1_244 = torch.aten.var_mean.correction %8445, %8446, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8447 = torch.aten.add.Scalar %result0_243, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8448 = torch.aten.rsqrt %8447 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8449 = torch.aten.sub.Tensor %8444, %result1_244, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8450 = torch.aten.mul.Tensor %8449, %8448 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %8451 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8452 = torch.aten.view %8450, %8451 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.mid_block.resnets.1.norm2.bias = util.global.load @_params.unet.mid_block.resnets.1.norm2.bias : tensor<1280xf16>
    %8453 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8454 = torch.aten.unsqueeze %8453, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8455 = torch.aten.unsqueeze %8454, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8456 = torch.aten.unsqueeze %8455, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.mid_block.resnets.1.norm2.weight = util.global.load @_params.unet.mid_block.resnets.1.norm2.weight : tensor<1280xf16>
    %8457 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8458 = torch.aten.unsqueeze %8457, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8459 = torch.aten.unsqueeze %8458, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8460 = torch.aten.unsqueeze %8459, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %8461 = torch.aten.mul.Tensor %8452, %8460 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %8462 = torch.aten.add.Tensor %8461, %8456, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %8463 = torch.prims.convert_element_type %8462, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8464 = torch.prims.convert_element_type %result1_244, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8465 = torch.prims.convert_element_type %8448, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8466 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8467 = torch.prims.squeeze %8464, %8466 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8468 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8469 = torch.prims.squeeze %8467, %8468 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8470 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8471 = torch.prims.squeeze %8465, %8470 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8472 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8473 = torch.prims.squeeze %8471, %8472 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8474 = torch.aten.silu %8463 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.mid_block.resnets.1.conv2.weight = util.global.load @_params.unet.mid_block.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %8475 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.mid_block.resnets.1.conv2.bias = util.global.load @_params.unet.mid_block.resnets.1.conv2.bias : tensor<1280xf16>
    %8476 = torch_c.from_builtin_tensor %_params.unet.mid_block.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8477 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8478 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8479 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8480 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8481 = torch.aten.convolution %8474, %8475, %8476, %8477, %8478, %8479, %false, %8480, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8482 = torch.aten.add.Tensor %8388, %8481, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8483 = torch.aten.div.Scalar %8482, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8484 = torch.prim.ListConstruct %8483, %6042 : (!torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>) -> !torch.list<vtensor>
    %8485 = torch.aten.cat %8484, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %8486 = torch.prim.ListConstruct %int2, %int32, %int80, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8487 = torch.aten.view %8485, %8486 : !torch.vtensor<[2,2560,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,80,1024],f16>
    %8488 = torch.prims.convert_element_type %8487, %int6 : !torch.vtensor<[2,32,80,1024],f16>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %8489 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_245, %result1_246 = torch.aten.var_mean.correction %8488, %8489, %int0, %true : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8490 = torch.aten.add.Scalar %result0_245, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8491 = torch.aten.rsqrt %8490 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8492 = torch.aten.sub.Tensor %8487, %result1_246, %int1 : !torch.vtensor<[2,32,80,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %8493 = torch.aten.mul.Tensor %8492, %8491 : !torch.vtensor<[2,32,80,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,80,1024],f32>
    %8494 = torch.prim.ListConstruct %int2, %int2560, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8495 = torch.aten.view %8493, %8494 : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,2560,32,32],f32>
    %_params.unet.up_blocks.0.resnets.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.norm1.bias : tensor<2560xf16>
    %8496 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm1.bias : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %8497 = torch.aten.unsqueeze %8496, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %8498 = torch.aten.unsqueeze %8497, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %8499 = torch.aten.unsqueeze %8498, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.norm1.weight : tensor<2560xf16>
    %8500 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm1.weight : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %8501 = torch.aten.unsqueeze %8500, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %8502 = torch.aten.unsqueeze %8501, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %8503 = torch.aten.unsqueeze %8502, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %8504 = torch.aten.mul.Tensor %8495, %8503 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16> -> !torch.vtensor<[2,2560,32,32],f32>
    %8505 = torch.aten.add.Tensor %8504, %8499, %int1 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16>, !torch.int -> !torch.vtensor<[2,2560,32,32],f32>
    %8506 = torch.prims.convert_element_type %8505, %int5 : !torch.vtensor<[2,2560,32,32],f32>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %8507 = torch.prims.convert_element_type %result1_246, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8508 = torch.prims.convert_element_type %8491, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8509 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8510 = torch.prims.squeeze %8507, %8509 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8511 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8512 = torch.prims.squeeze %8510, %8511 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8513 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8514 = torch.prims.squeeze %8508, %8513 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8515 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8516 = torch.prims.squeeze %8514, %8515 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8517 = torch.aten.silu %8506 : !torch.vtensor<[2,2560,32,32],f16> -> !torch.vtensor<[2,2560,32,32],f16>
    %_params.unet.up_blocks.0.resnets.0.conv1.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.conv1.weight : tensor<1280x2560x3x3xf16>
    %8518 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv1.weight : tensor<1280x2560x3x3xf16> -> !torch.vtensor<[1280,2560,3,3],f16>
    %_params.unet.up_blocks.0.resnets.0.conv1.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.conv1.bias : tensor<1280xf16>
    %8519 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8520 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8521 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8522 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8523 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8524 = torch.aten.convolution %8517, %8518, %8519, %8520, %8521, %8522, %false, %8523, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8525 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %8526 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8527 = torch.aten.transpose.int %8526, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %8528 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8529 = torch.prims.convert_element_type %8528, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8530 = torch.prims.convert_element_type %8525, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8531 = torch.prims.convert_element_type %8527, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8532 = torch.aten.mm %8530, %8531 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %8533 = torch.aten.mul.Scalar %8532, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8534 = torch.aten.mul.Scalar %8529, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8535 = torch.aten.add.Tensor %8533, %8534, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %8536 = torch.prims.convert_element_type %8535, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %8537 = torch.aten.unsqueeze %8536, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %8538 = torch.aten.unsqueeze %8537, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %8539 = torch.aten.add.Tensor %8524, %8538, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8540 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8541 = torch.aten.view %8539, %8540 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %8542 = torch.prims.convert_element_type %8541, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8543 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_247, %result1_248 = torch.aten.var_mean.correction %8542, %8543, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8544 = torch.aten.add.Scalar %result0_247, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8545 = torch.aten.rsqrt %8544 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8546 = torch.aten.sub.Tensor %8541, %result1_248, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8547 = torch.aten.mul.Tensor %8546, %8545 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %8548 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8549 = torch.aten.view %8547, %8548 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.resnets.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.norm2.bias : tensor<1280xf16>
    %8550 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8551 = torch.aten.unsqueeze %8550, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8552 = torch.aten.unsqueeze %8551, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8553 = torch.aten.unsqueeze %8552, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.resnets.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.norm2.weight : tensor<1280xf16>
    %8554 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8555 = torch.aten.unsqueeze %8554, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8556 = torch.aten.unsqueeze %8555, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8557 = torch.aten.unsqueeze %8556, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %8558 = torch.aten.mul.Tensor %8549, %8557 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %8559 = torch.aten.add.Tensor %8558, %8553, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %8560 = torch.prims.convert_element_type %8559, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8561 = torch.prims.convert_element_type %result1_248, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8562 = torch.prims.convert_element_type %8545, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8563 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8564 = torch.prims.squeeze %8561, %8563 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8565 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8566 = torch.prims.squeeze %8564, %8565 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8567 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8568 = torch.prims.squeeze %8562, %8567 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8569 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8570 = torch.prims.squeeze %8568, %8569 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8571 = torch.aten.silu %8560 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.0.conv2.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %8572 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.resnets.0.conv2.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.conv2.bias : tensor<1280xf16>
    %8573 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8574 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8575 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8576 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8577 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8578 = torch.aten.convolution %8571, %8572, %8573, %8574, %8575, %8576, %false, %8577, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight : tensor<1280x2560x1x1xf16>
    %8579 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv_shortcut.weight : tensor<1280x2560x1x1xf16> -> !torch.vtensor<[1280,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias : tensor<1280xf16>
    %8580 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.0.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8581 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8582 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8583 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %8584 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %8585 = torch.aten.convolution %8485, %8579, %8580, %8581, %8582, %8583, %false, %8584, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8586 = torch.aten.add.Tensor %8585, %8578, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8587 = torch.aten.div.Scalar %8586, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %8588 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8589 = torch.aten.view %8587, %8588 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %8590 = torch.prims.convert_element_type %8589, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8591 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_249, %result1_250 = torch.aten.var_mean.correction %8590, %8591, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %8592 = torch.aten.add.Scalar %result0_249, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %8593 = torch.aten.rsqrt %8592 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %8594 = torch.aten.sub.Tensor %8589, %result1_250, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %8595 = torch.aten.mul.Tensor %8594, %8593 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %8596 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8597 = torch.aten.view %8595, %8596 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.attentions.0.norm.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.norm.bias : tensor<1280xf16>
    %8598 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8599 = torch.aten.unsqueeze %8598, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8600 = torch.aten.unsqueeze %8599, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8601 = torch.aten.unsqueeze %8600, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.attentions.0.norm.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.norm.weight : tensor<1280xf16>
    %8602 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8603 = torch.aten.unsqueeze %8602, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %8604 = torch.aten.unsqueeze %8603, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %8605 = torch.aten.unsqueeze %8604, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %8606 = torch.aten.mul.Tensor %8597, %8605 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %8607 = torch.aten.add.Tensor %8606, %8601, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %8608 = torch.prims.convert_element_type %8607, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %8609 = torch.prims.convert_element_type %result1_250, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8610 = torch.prims.convert_element_type %8593, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %8611 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8612 = torch.prims.squeeze %8609, %8611 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8613 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8614 = torch.prims.squeeze %8612, %8613 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8615 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %8616 = torch.prims.squeeze %8610, %8615 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %8617 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %8618 = torch.prims.squeeze %8616, %8617 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %8619 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8620 = torch.aten.permute %8608, %8619 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %8621 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8622 = torch.aten.view %8620, %8621 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_in.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %8623 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8624 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8625 = torch.aten._unsafe_view %8622, %8624 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8626 = torch_c.to_builtin_tensor %8625 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8627 = torch_c.to_builtin_tensor %8623 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8628 = tensor.empty() : tensor<2048x1280xf32>
    %8629 = linalg.fill ins(%cst : f32) outs(%8628 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8630 = tensor.empty() : tensor<2048x1280xf32>
    %8631 = linalg.fill ins(%cst : f32) outs(%8630 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8632:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8629, %8631, %8626, %8627, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8629, %8631)
    %8633 = arith.truncf %8632#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8634 = torch_c.from_builtin_tensor %8633 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8635 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8636 = torch.aten.view %8634, %8635 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_in.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_in.bias : tensor<1280xf16>
    %8637 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8638 = torch.aten.add.Tensor %8636, %8637, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8639 = torch.prims.convert_element_type %8638, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8640 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_251, %result1_252 = torch.aten.var_mean.correction %8639, %8640, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8641 = torch.aten.add.Scalar %result0_251, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8642 = torch.aten.rsqrt %8641 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8643 = torch.aten.sub.Tensor %8638, %result1_252, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8644 = torch.aten.mul.Tensor %8643, %8642 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %8645 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8646 = torch.aten.mul.Tensor %8644, %8645 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %8647 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8648 = torch.aten.add.Tensor %8646, %8647, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8649 = torch.prims.convert_element_type %8648, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8650 = torch.prims.convert_element_type %result1_252, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8651 = torch.prims.convert_element_type %8642, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %8652 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8653 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8654 = torch.aten.view %8649, %8653 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8655 = torch_c.to_builtin_tensor %8654 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8656 = torch_c.to_builtin_tensor %8652 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8657 = tensor.empty() : tensor<2048x1280xf32>
    %8658 = linalg.fill ins(%cst : f32) outs(%8657 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8659 = tensor.empty() : tensor<2048x1280xf32>
    %8660 = linalg.fill ins(%cst : f32) outs(%8659 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8661:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8658, %8660, %8655, %8656, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8658, %8660)
    %8662 = arith.truncf %8661#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8663 = torch_c.from_builtin_tensor %8662 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8664 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8665 = torch.aten.view %8663, %8664 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %8666 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8667 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8668 = torch.aten.view %8649, %8667 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8669 = torch_c.to_builtin_tensor %8668 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8670 = torch_c.to_builtin_tensor %8666 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8671 = tensor.empty() : tensor<2048x1280xf32>
    %8672 = linalg.fill ins(%cst : f32) outs(%8671 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8673 = tensor.empty() : tensor<2048x1280xf32>
    %8674 = linalg.fill ins(%cst : f32) outs(%8673 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8675:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8672, %8674, %8669, %8670, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8672, %8674)
    %8676 = arith.truncf %8675#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8677 = torch_c.from_builtin_tensor %8676 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8678 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8679 = torch.aten.view %8677, %8678 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %8680 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8681 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8682 = torch.aten.view %8649, %8681 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8683 = torch_c.to_builtin_tensor %8682 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8684 = torch_c.to_builtin_tensor %8680 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8685 = tensor.empty() : tensor<2048x1280xf32>
    %8686 = linalg.fill ins(%cst : f32) outs(%8685 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8687 = tensor.empty() : tensor<2048x1280xf32>
    %8688 = linalg.fill ins(%cst : f32) outs(%8687 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8689:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8686, %8688, %8683, %8684, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8686, %8688)
    %8690 = arith.truncf %8689#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8691 = torch_c.from_builtin_tensor %8690 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8692 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8693 = torch.aten.view %8691, %8692 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8694 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8695 = torch.aten.view %8665, %8694 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8696 = torch.aten.transpose.int %8695, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8697 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8698 = torch.aten.view %8679, %8697 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8699 = torch.aten.transpose.int %8698, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8700 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8701 = torch.aten.view %8693, %8700 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8702 = torch.aten.transpose.int %8701, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8703:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8696, %8699, %8702, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8704 = torch.aten.transpose.int %8703#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8705 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8706 = torch.aten.view %8704, %8705 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8707 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8708 = torch.aten.view %8706, %8707 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8709 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8710 = torch.aten.transpose.int %8709, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %8711 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8712 = torch.prims.convert_element_type %8711, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8713 = torch.prims.convert_element_type %8708, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8714 = torch.prims.convert_element_type %8710, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8715 = torch.aten.mm %8713, %8714 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8716 = torch.aten.mul.Scalar %8715, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8717 = torch.aten.mul.Scalar %8712, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8718 = torch.aten.add.Tensor %8716, %8717, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8719 = torch.prims.convert_element_type %8718, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8720 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8721 = torch.aten.view %8719, %8720 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8722 = torch.aten.div.Scalar %8721, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8723 = torch.aten.add.Tensor %8722, %8638, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8724 = torch.prims.convert_element_type %8723, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8725 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_253, %result1_254 = torch.aten.var_mean.correction %8724, %8725, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8726 = torch.aten.add.Scalar %result0_253, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8727 = torch.aten.rsqrt %8726 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8728 = torch.aten.sub.Tensor %8723, %result1_254, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8729 = torch.aten.mul.Tensor %8728, %8727 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %8730 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8731 = torch.aten.mul.Tensor %8729, %8730 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %8732 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8733 = torch.aten.add.Tensor %8731, %8732, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8734 = torch.prims.convert_element_type %8733, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8735 = torch.prims.convert_element_type %result1_254, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8736 = torch.prims.convert_element_type %8727, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %8737 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8738 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8739 = torch.aten.view %8734, %8738 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8740 = torch_c.to_builtin_tensor %8739 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8741 = torch_c.to_builtin_tensor %8737 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8742 = tensor.empty() : tensor<2048x1280xf32>
    %8743 = linalg.fill ins(%cst : f32) outs(%8742 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8744 = tensor.empty() : tensor<2048x1280xf32>
    %8745 = linalg.fill ins(%cst : f32) outs(%8744 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8746:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8743, %8745, %8740, %8741, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8743, %8745)
    %8747 = arith.truncf %8746#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8748 = torch_c.from_builtin_tensor %8747 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8749 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8750 = torch.aten.view %8748, %8749 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %8751 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8752 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8753 = torch.aten.view %4, %8752 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8754 = torch_c.to_builtin_tensor %8753 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8755 = torch_c.to_builtin_tensor %8751 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8756 = tensor.empty() : tensor<128x1280xf32>
    %8757 = linalg.fill ins(%cst : f32) outs(%8756 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8758 = tensor.empty() : tensor<128x1280xf32>
    %8759 = linalg.fill ins(%cst : f32) outs(%8758 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8760:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8757, %8759, %8754, %8755, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8757, %8759)
    %8761 = arith.truncf %8760#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8762 = torch_c.from_builtin_tensor %8761 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8763 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8764 = torch.aten.view %8762, %8763 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %8765 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8766 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8767 = torch.aten.view %4, %8766 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8768 = torch_c.to_builtin_tensor %8767 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8769 = torch_c.to_builtin_tensor %8765 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8770 = tensor.empty() : tensor<128x1280xf32>
    %8771 = linalg.fill ins(%cst : f32) outs(%8770 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8772 = tensor.empty() : tensor<128x1280xf32>
    %8773 = linalg.fill ins(%cst : f32) outs(%8772 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8774:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8771, %8773, %8768, %8769, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8771, %8773)
    %8775 = arith.truncf %8774#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8776 = torch_c.from_builtin_tensor %8775 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8777 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8778 = torch.aten.view %8776, %8777 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8779 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8780 = torch.aten.view %8750, %8779 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8781 = torch.aten.transpose.int %8780, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8782 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8783 = torch.aten.view %8764, %8782 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8784 = torch.aten.transpose.int %8783, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8785 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8786 = torch.aten.view %8778, %8785 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %8787 = torch.aten.transpose.int %8786, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %8788:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8781, %8784, %8787, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8789 = torch.aten.transpose.int %8788#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8790 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8791 = torch.aten.view %8789, %8790 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8792 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8793 = torch.aten.view %8791, %8792 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %8794 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8795 = torch.aten.transpose.int %8794, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %8796 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8797 = torch.prims.convert_element_type %8796, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8798 = torch.prims.convert_element_type %8793, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8799 = torch.prims.convert_element_type %8795, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8800 = torch.aten.mm %8798, %8799 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8801 = torch.aten.mul.Scalar %8800, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8802 = torch.aten.mul.Scalar %8797, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8803 = torch.aten.add.Tensor %8801, %8802, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8804 = torch.prims.convert_element_type %8803, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8805 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8806 = torch.aten.view %8804, %8805 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8807 = torch.aten.div.Scalar %8806, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8808 = torch.aten.add.Tensor %8807, %8723, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8809 = torch.prims.convert_element_type %8808, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8810 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_255, %result1_256 = torch.aten.var_mean.correction %8809, %8810, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8811 = torch.aten.add.Scalar %result0_255, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8812 = torch.aten.rsqrt %8811 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8813 = torch.aten.sub.Tensor %8808, %result1_256, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8814 = torch.aten.mul.Tensor %8813, %8812 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %8815 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8816 = torch.aten.mul.Tensor %8814, %8815 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %8817 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8818 = torch.aten.add.Tensor %8816, %8817, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8819 = torch.prims.convert_element_type %8818, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8820 = torch.prims.convert_element_type %result1_256, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8821 = torch.prims.convert_element_type %8812, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8822 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8823 = torch.aten.view %8819, %8822 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %8824 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %8825 = torch.aten.transpose.int %8824, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %8826 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %8827 = torch.prims.convert_element_type %8826, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %8828 = torch.prims.convert_element_type %8823, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8829 = torch.prims.convert_element_type %8825, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %8830 = torch.aten.mm %8828, %8829 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %8831 = torch.aten.mul.Scalar %8830, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8832 = torch.aten.mul.Scalar %8827, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %8833 = torch.aten.add.Tensor %8831, %8832, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %8834 = torch.prims.convert_element_type %8833, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %8835 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8836 = torch.aten.view %8834, %8835 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %8837 = torch.aten.slice.Tensor %8836, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8838 = torch.aten.slice.Tensor %8836, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %8839 = torch.aten.gelu %8838, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %8840 = torch.aten.mul.Tensor %8837, %8839 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %8841 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %8842 = torch.aten.view %8840, %8841 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %8843 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %8844 = torch.aten.transpose.int %8843, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %8845 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8846 = torch.prims.convert_element_type %8845, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8847 = torch.prims.convert_element_type %8842, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %8848 = torch.prims.convert_element_type %8844, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %8849 = torch.aten.mm %8847, %8848 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8850 = torch.aten.mul.Scalar %8849, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8851 = torch.aten.mul.Scalar %8846, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8852 = torch.aten.add.Tensor %8850, %8851, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8853 = torch.prims.convert_element_type %8852, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8854 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8855 = torch.aten.view %8853, %8854 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8856 = torch.aten.add.Tensor %8855, %8808, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8857 = torch.prims.convert_element_type %8856, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8858 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_257, %result1_258 = torch.aten.var_mean.correction %8857, %8858, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8859 = torch.aten.add.Scalar %result0_257, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8860 = torch.aten.rsqrt %8859 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8861 = torch.aten.sub.Tensor %8856, %result1_258, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8862 = torch.aten.mul.Tensor %8861, %8860 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %8863 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8864 = torch.aten.mul.Tensor %8862, %8863 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %8865 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8866 = torch.aten.add.Tensor %8864, %8865, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8867 = torch.prims.convert_element_type %8866, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8868 = torch.prims.convert_element_type %result1_258, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8869 = torch.prims.convert_element_type %8860, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %8870 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8871 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8872 = torch.aten.view %8867, %8871 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8873 = torch_c.to_builtin_tensor %8872 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8874 = torch_c.to_builtin_tensor %8870 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8875 = tensor.empty() : tensor<2048x1280xf32>
    %8876 = linalg.fill ins(%cst : f32) outs(%8875 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8877 = tensor.empty() : tensor<2048x1280xf32>
    %8878 = linalg.fill ins(%cst : f32) outs(%8877 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8879:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8876, %8878, %8873, %8874, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8876, %8878)
    %8880 = arith.truncf %8879#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8881 = torch_c.from_builtin_tensor %8880 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8882 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8883 = torch.aten.view %8881, %8882 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %8884 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8885 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8886 = torch.aten.view %8867, %8885 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8887 = torch_c.to_builtin_tensor %8886 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8888 = torch_c.to_builtin_tensor %8884 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8889 = tensor.empty() : tensor<2048x1280xf32>
    %8890 = linalg.fill ins(%cst : f32) outs(%8889 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8891 = tensor.empty() : tensor<2048x1280xf32>
    %8892 = linalg.fill ins(%cst : f32) outs(%8891 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8893:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8890, %8892, %8887, %8888, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8890, %8892)
    %8894 = arith.truncf %8893#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8895 = torch_c.from_builtin_tensor %8894 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8896 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8897 = torch.aten.view %8895, %8896 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %8898 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8899 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8900 = torch.aten.view %8867, %8899 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8901 = torch_c.to_builtin_tensor %8900 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8902 = torch_c.to_builtin_tensor %8898 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8903 = tensor.empty() : tensor<2048x1280xf32>
    %8904 = linalg.fill ins(%cst : f32) outs(%8903 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8905 = tensor.empty() : tensor<2048x1280xf32>
    %8906 = linalg.fill ins(%cst : f32) outs(%8905 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8907:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8904, %8906, %8901, %8902, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8904, %8906)
    %8908 = arith.truncf %8907#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8909 = torch_c.from_builtin_tensor %8908 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8910 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8911 = torch.aten.view %8909, %8910 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8912 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8913 = torch.aten.view %8883, %8912 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8914 = torch.aten.transpose.int %8913, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8915 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8916 = torch.aten.view %8897, %8915 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8917 = torch.aten.transpose.int %8916, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8918 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8919 = torch.aten.view %8911, %8918 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8920 = torch.aten.transpose.int %8919, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %8921:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8914, %8917, %8920, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %8922 = torch.aten.transpose.int %8921#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %8923 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8924 = torch.aten.view %8922, %8923 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8925 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8926 = torch.aten.view %8924, %8925 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %8927 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8928 = torch.aten.transpose.int %8927, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %8929 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8930 = torch.prims.convert_element_type %8929, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %8931 = torch.prims.convert_element_type %8926, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8932 = torch.prims.convert_element_type %8928, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %8933 = torch.aten.mm %8931, %8932 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %8934 = torch.aten.mul.Scalar %8933, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8935 = torch.aten.mul.Scalar %8930, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %8936 = torch.aten.add.Tensor %8934, %8935, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %8937 = torch.prims.convert_element_type %8936, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %8938 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8939 = torch.aten.view %8937, %8938 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %8940 = torch.aten.div.Scalar %8939, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %8941 = torch.aten.add.Tensor %8940, %8856, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8942 = torch.prims.convert_element_type %8941, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8943 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_259, %result1_260 = torch.aten.var_mean.correction %8942, %8943, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %8944 = torch.aten.add.Scalar %result0_259, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %8945 = torch.aten.rsqrt %8944 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %8946 = torch.aten.sub.Tensor %8941, %result1_260, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8947 = torch.aten.mul.Tensor %8946, %8945 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %8948 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8949 = torch.aten.mul.Tensor %8947, %8948 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %8950 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %8951 = torch.aten.add.Tensor %8949, %8950, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %8952 = torch.prims.convert_element_type %8951, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %8953 = torch.prims.convert_element_type %result1_260, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %8954 = torch.prims.convert_element_type %8945, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %8955 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %8956 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %8957 = torch.aten.view %8952, %8956 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %8958 = torch_c.to_builtin_tensor %8957 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %8959 = torch_c.to_builtin_tensor %8955 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %8960 = tensor.empty() : tensor<2048x1280xf32>
    %8961 = linalg.fill ins(%cst : f32) outs(%8960 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8962 = tensor.empty() : tensor<2048x1280xf32>
    %8963 = linalg.fill ins(%cst : f32) outs(%8962 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %8964:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %8961, %8963, %8958, %8959, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8961, %8963)
    %8965 = arith.truncf %8964#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %8966 = torch_c.from_builtin_tensor %8965 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %8967 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8968 = torch.aten.view %8966, %8967 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %8969 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8970 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8971 = torch.aten.view %4, %8970 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8972 = torch_c.to_builtin_tensor %8971 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8973 = torch_c.to_builtin_tensor %8969 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8974 = tensor.empty() : tensor<128x1280xf32>
    %8975 = linalg.fill ins(%cst : f32) outs(%8974 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8976 = tensor.empty() : tensor<128x1280xf32>
    %8977 = linalg.fill ins(%cst : f32) outs(%8976 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8978:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8975, %8977, %8972, %8973, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8975, %8977)
    %8979 = arith.truncf %8978#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8980 = torch_c.from_builtin_tensor %8979 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8981 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8982 = torch.aten.view %8980, %8981 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %8983 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %8984 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %8985 = torch.aten.view %4, %8984 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %8986 = torch_c.to_builtin_tensor %8985 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %8987 = torch_c.to_builtin_tensor %8983 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %8988 = tensor.empty() : tensor<128x1280xf32>
    %8989 = linalg.fill ins(%cst : f32) outs(%8988 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8990 = tensor.empty() : tensor<128x1280xf32>
    %8991 = linalg.fill ins(%cst : f32) outs(%8990 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %8992:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %8989, %8991, %8986, %8987, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%8989, %8991)
    %8993 = arith.truncf %8992#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %8994 = torch_c.from_builtin_tensor %8993 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %8995 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8996 = torch.aten.view %8994, %8995 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %8997 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %8998 = torch.aten.view %8968, %8997 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %8999 = torch.aten.transpose.int %8998, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9000 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9001 = torch.aten.view %8982, %9000 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9002 = torch.aten.transpose.int %9001, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9003 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9004 = torch.aten.view %8996, %9003 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9005 = torch.aten.transpose.int %9004, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9006:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%8999, %9002, %9005, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9007 = torch.aten.transpose.int %9006#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9008 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9009 = torch.aten.view %9007, %9008 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9010 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9011 = torch.aten.view %9009, %9010 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9012 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9013 = torch.aten.transpose.int %9012, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %9014 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9015 = torch.prims.convert_element_type %9014, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9016 = torch.prims.convert_element_type %9011, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9017 = torch.prims.convert_element_type %9013, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9018 = torch.aten.mm %9016, %9017 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9019 = torch.aten.mul.Scalar %9018, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9020 = torch.aten.mul.Scalar %9015, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9021 = torch.aten.add.Tensor %9019, %9020, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9022 = torch.prims.convert_element_type %9021, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9023 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9024 = torch.aten.view %9022, %9023 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9025 = torch.aten.div.Scalar %9024, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9026 = torch.aten.add.Tensor %9025, %8941, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9027 = torch.prims.convert_element_type %9026, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9028 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_261, %result1_262 = torch.aten.var_mean.correction %9027, %9028, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9029 = torch.aten.add.Scalar %result0_261, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9030 = torch.aten.rsqrt %9029 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9031 = torch.aten.sub.Tensor %9026, %result1_262, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9032 = torch.aten.mul.Tensor %9031, %9030 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %9033 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9034 = torch.aten.mul.Tensor %9032, %9033 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %9035 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9036 = torch.aten.add.Tensor %9034, %9035, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9037 = torch.prims.convert_element_type %9036, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9038 = torch.prims.convert_element_type %result1_262, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9039 = torch.prims.convert_element_type %9030, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9040 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9041 = torch.aten.view %9037, %9040 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9042 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9043 = torch.aten.transpose.int %9042, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %9044 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9045 = torch.prims.convert_element_type %9044, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9046 = torch.prims.convert_element_type %9041, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9047 = torch.prims.convert_element_type %9043, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9048 = torch.aten.mm %9046, %9047 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9049 = torch.aten.mul.Scalar %9048, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9050 = torch.aten.mul.Scalar %9045, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9051 = torch.aten.add.Tensor %9049, %9050, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9052 = torch.prims.convert_element_type %9051, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9053 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9054 = torch.aten.view %9052, %9053 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9055 = torch.aten.slice.Tensor %9054, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9056 = torch.aten.slice.Tensor %9054, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9057 = torch.aten.gelu %9056, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9058 = torch.aten.mul.Tensor %9055, %9057 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9059 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9060 = torch.aten.view %9058, %9059 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %9061 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9062 = torch.aten.transpose.int %9061, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %9063 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9064 = torch.prims.convert_element_type %9063, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9065 = torch.prims.convert_element_type %9060, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9066 = torch.prims.convert_element_type %9062, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9067 = torch.aten.mm %9065, %9066 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9068 = torch.aten.mul.Scalar %9067, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9069 = torch.aten.mul.Scalar %9064, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9070 = torch.aten.add.Tensor %9068, %9069, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9071 = torch.prims.convert_element_type %9070, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9072 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9073 = torch.aten.view %9071, %9072 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9074 = torch.aten.add.Tensor %9073, %9026, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9075 = torch.prims.convert_element_type %9074, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9076 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_263, %result1_264 = torch.aten.var_mean.correction %9075, %9076, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9077 = torch.aten.add.Scalar %result0_263, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9078 = torch.aten.rsqrt %9077 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9079 = torch.aten.sub.Tensor %9074, %result1_264, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9080 = torch.aten.mul.Tensor %9079, %9078 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %9081 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9082 = torch.aten.mul.Tensor %9080, %9081 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %9083 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9084 = torch.aten.add.Tensor %9082, %9083, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9085 = torch.prims.convert_element_type %9084, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9086 = torch.prims.convert_element_type %result1_264, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9087 = torch.prims.convert_element_type %9078, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %9088 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9089 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9090 = torch.aten.view %9085, %9089 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9091 = torch_c.to_builtin_tensor %9090 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9092 = torch_c.to_builtin_tensor %9088 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9093 = tensor.empty() : tensor<2048x1280xf32>
    %9094 = linalg.fill ins(%cst : f32) outs(%9093 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9095 = tensor.empty() : tensor<2048x1280xf32>
    %9096 = linalg.fill ins(%cst : f32) outs(%9095 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9097:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9094, %9096, %9091, %9092, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9094, %9096)
    %9098 = arith.truncf %9097#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9099 = torch_c.from_builtin_tensor %9098 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9100 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9101 = torch.aten.view %9099, %9100 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %9102 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9103 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9104 = torch.aten.view %9085, %9103 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9105 = torch_c.to_builtin_tensor %9104 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9106 = torch_c.to_builtin_tensor %9102 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9107 = tensor.empty() : tensor<2048x1280xf32>
    %9108 = linalg.fill ins(%cst : f32) outs(%9107 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9109 = tensor.empty() : tensor<2048x1280xf32>
    %9110 = linalg.fill ins(%cst : f32) outs(%9109 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9111:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9108, %9110, %9105, %9106, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9108, %9110)
    %9112 = arith.truncf %9111#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9113 = torch_c.from_builtin_tensor %9112 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9114 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9115 = torch.aten.view %9113, %9114 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %9116 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9117 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9118 = torch.aten.view %9085, %9117 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9119 = torch_c.to_builtin_tensor %9118 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9120 = torch_c.to_builtin_tensor %9116 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9121 = tensor.empty() : tensor<2048x1280xf32>
    %9122 = linalg.fill ins(%cst : f32) outs(%9121 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9123 = tensor.empty() : tensor<2048x1280xf32>
    %9124 = linalg.fill ins(%cst : f32) outs(%9123 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9125:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9122, %9124, %9119, %9120, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9122, %9124)
    %9126 = arith.truncf %9125#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9127 = torch_c.from_builtin_tensor %9126 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9128 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9129 = torch.aten.view %9127, %9128 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9130 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9131 = torch.aten.view %9101, %9130 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9132 = torch.aten.transpose.int %9131, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9133 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9134 = torch.aten.view %9115, %9133 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9135 = torch.aten.transpose.int %9134, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9136 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9137 = torch.aten.view %9129, %9136 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9138 = torch.aten.transpose.int %9137, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9139:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9132, %9135, %9138, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9140 = torch.aten.transpose.int %9139#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9141 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9142 = torch.aten.view %9140, %9141 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9143 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9144 = torch.aten.view %9142, %9143 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9145 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9146 = torch.aten.transpose.int %9145, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %9147 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9148 = torch.prims.convert_element_type %9147, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9149 = torch.prims.convert_element_type %9144, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9150 = torch.prims.convert_element_type %9146, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9151 = torch.aten.mm %9149, %9150 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9152 = torch.aten.mul.Scalar %9151, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9153 = torch.aten.mul.Scalar %9148, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9154 = torch.aten.add.Tensor %9152, %9153, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9155 = torch.prims.convert_element_type %9154, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9156 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9157 = torch.aten.view %9155, %9156 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9158 = torch.aten.div.Scalar %9157, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9159 = torch.aten.add.Tensor %9158, %9074, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9160 = torch.prims.convert_element_type %9159, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9161 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_265, %result1_266 = torch.aten.var_mean.correction %9160, %9161, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9162 = torch.aten.add.Scalar %result0_265, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9163 = torch.aten.rsqrt %9162 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9164 = torch.aten.sub.Tensor %9159, %result1_266, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9165 = torch.aten.mul.Tensor %9164, %9163 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %9166 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9167 = torch.aten.mul.Tensor %9165, %9166 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %9168 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9169 = torch.aten.add.Tensor %9167, %9168, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9170 = torch.prims.convert_element_type %9169, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9171 = torch.prims.convert_element_type %result1_266, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9172 = torch.prims.convert_element_type %9163, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %9173 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9174 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9175 = torch.aten.view %9170, %9174 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9176 = torch_c.to_builtin_tensor %9175 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9177 = torch_c.to_builtin_tensor %9173 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9178 = tensor.empty() : tensor<2048x1280xf32>
    %9179 = linalg.fill ins(%cst : f32) outs(%9178 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9180 = tensor.empty() : tensor<2048x1280xf32>
    %9181 = linalg.fill ins(%cst : f32) outs(%9180 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9182:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9179, %9181, %9176, %9177, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9179, %9181)
    %9183 = arith.truncf %9182#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9184 = torch_c.from_builtin_tensor %9183 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9185 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9186 = torch.aten.view %9184, %9185 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %9187 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9188 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9189 = torch.aten.view %4, %9188 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9190 = torch_c.to_builtin_tensor %9189 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9191 = torch_c.to_builtin_tensor %9187 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9192 = tensor.empty() : tensor<128x1280xf32>
    %9193 = linalg.fill ins(%cst : f32) outs(%9192 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9194 = tensor.empty() : tensor<128x1280xf32>
    %9195 = linalg.fill ins(%cst : f32) outs(%9194 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9196:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9193, %9195, %9190, %9191, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9193, %9195)
    %9197 = arith.truncf %9196#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9198 = torch_c.from_builtin_tensor %9197 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9199 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9200 = torch.aten.view %9198, %9199 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %9201 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9202 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9203 = torch.aten.view %4, %9202 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9204 = torch_c.to_builtin_tensor %9203 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9205 = torch_c.to_builtin_tensor %9201 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9206 = tensor.empty() : tensor<128x1280xf32>
    %9207 = linalg.fill ins(%cst : f32) outs(%9206 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9208 = tensor.empty() : tensor<128x1280xf32>
    %9209 = linalg.fill ins(%cst : f32) outs(%9208 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9210:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9207, %9209, %9204, %9205, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9207, %9209)
    %9211 = arith.truncf %9210#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9212 = torch_c.from_builtin_tensor %9211 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9213 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9214 = torch.aten.view %9212, %9213 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9215 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9216 = torch.aten.view %9186, %9215 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9217 = torch.aten.transpose.int %9216, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9218 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9219 = torch.aten.view %9200, %9218 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9220 = torch.aten.transpose.int %9219, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9221 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9222 = torch.aten.view %9214, %9221 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9223 = torch.aten.transpose.int %9222, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9224:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9217, %9220, %9223, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9225 = torch.aten.transpose.int %9224#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9226 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9227 = torch.aten.view %9225, %9226 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9228 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9229 = torch.aten.view %9227, %9228 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9230 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9231 = torch.aten.transpose.int %9230, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %9232 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9233 = torch.prims.convert_element_type %9232, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9234 = torch.prims.convert_element_type %9229, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9235 = torch.prims.convert_element_type %9231, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9236 = torch.aten.mm %9234, %9235 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9237 = torch.aten.mul.Scalar %9236, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9238 = torch.aten.mul.Scalar %9233, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9239 = torch.aten.add.Tensor %9237, %9238, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9240 = torch.prims.convert_element_type %9239, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9241 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9242 = torch.aten.view %9240, %9241 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9243 = torch.aten.div.Scalar %9242, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9244 = torch.aten.add.Tensor %9243, %9159, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9245 = torch.prims.convert_element_type %9244, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9246 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_267, %result1_268 = torch.aten.var_mean.correction %9245, %9246, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9247 = torch.aten.add.Scalar %result0_267, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9248 = torch.aten.rsqrt %9247 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9249 = torch.aten.sub.Tensor %9244, %result1_268, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9250 = torch.aten.mul.Tensor %9249, %9248 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %9251 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9252 = torch.aten.mul.Tensor %9250, %9251 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %9253 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9254 = torch.aten.add.Tensor %9252, %9253, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9255 = torch.prims.convert_element_type %9254, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9256 = torch.prims.convert_element_type %result1_268, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9257 = torch.prims.convert_element_type %9248, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9258 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9259 = torch.aten.view %9255, %9258 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9260 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9261 = torch.aten.transpose.int %9260, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %9262 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9263 = torch.prims.convert_element_type %9262, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9264 = torch.prims.convert_element_type %9259, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9265 = torch.prims.convert_element_type %9261, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9266 = torch.aten.mm %9264, %9265 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9267 = torch.aten.mul.Scalar %9266, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9268 = torch.aten.mul.Scalar %9263, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9269 = torch.aten.add.Tensor %9267, %9268, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9270 = torch.prims.convert_element_type %9269, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9271 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9272 = torch.aten.view %9270, %9271 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9273 = torch.aten.slice.Tensor %9272, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9274 = torch.aten.slice.Tensor %9272, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9275 = torch.aten.gelu %9274, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9276 = torch.aten.mul.Tensor %9273, %9275 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9277 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9278 = torch.aten.view %9276, %9277 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %9279 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9280 = torch.aten.transpose.int %9279, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %9281 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9282 = torch.prims.convert_element_type %9281, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9283 = torch.prims.convert_element_type %9278, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9284 = torch.prims.convert_element_type %9280, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9285 = torch.aten.mm %9283, %9284 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9286 = torch.aten.mul.Scalar %9285, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9287 = torch.aten.mul.Scalar %9282, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9288 = torch.aten.add.Tensor %9286, %9287, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9289 = torch.prims.convert_element_type %9288, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9290 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9291 = torch.aten.view %9289, %9290 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9292 = torch.aten.add.Tensor %9291, %9244, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9293 = torch.prims.convert_element_type %9292, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9294 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_269, %result1_270 = torch.aten.var_mean.correction %9293, %9294, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9295 = torch.aten.add.Scalar %result0_269, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9296 = torch.aten.rsqrt %9295 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9297 = torch.aten.sub.Tensor %9292, %result1_270, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9298 = torch.aten.mul.Tensor %9297, %9296 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %9299 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9300 = torch.aten.mul.Tensor %9298, %9299 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %9301 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9302 = torch.aten.add.Tensor %9300, %9301, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9303 = torch.prims.convert_element_type %9302, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9304 = torch.prims.convert_element_type %result1_270, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9305 = torch.prims.convert_element_type %9296, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %9306 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9307 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9308 = torch.aten.view %9303, %9307 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9309 = torch_c.to_builtin_tensor %9308 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9310 = torch_c.to_builtin_tensor %9306 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9311 = tensor.empty() : tensor<2048x1280xf32>
    %9312 = linalg.fill ins(%cst : f32) outs(%9311 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9313 = tensor.empty() : tensor<2048x1280xf32>
    %9314 = linalg.fill ins(%cst : f32) outs(%9313 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9315:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9312, %9314, %9309, %9310, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9312, %9314)
    %9316 = arith.truncf %9315#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9317 = torch_c.from_builtin_tensor %9316 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9318 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9319 = torch.aten.view %9317, %9318 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %9320 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9321 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9322 = torch.aten.view %9303, %9321 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9323 = torch_c.to_builtin_tensor %9322 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9324 = torch_c.to_builtin_tensor %9320 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9325 = tensor.empty() : tensor<2048x1280xf32>
    %9326 = linalg.fill ins(%cst : f32) outs(%9325 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9327 = tensor.empty() : tensor<2048x1280xf32>
    %9328 = linalg.fill ins(%cst : f32) outs(%9327 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9329:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9326, %9328, %9323, %9324, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9326, %9328)
    %9330 = arith.truncf %9329#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9331 = torch_c.from_builtin_tensor %9330 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9332 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9333 = torch.aten.view %9331, %9332 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %9334 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9335 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9336 = torch.aten.view %9303, %9335 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9337 = torch_c.to_builtin_tensor %9336 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9338 = torch_c.to_builtin_tensor %9334 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9339 = tensor.empty() : tensor<2048x1280xf32>
    %9340 = linalg.fill ins(%cst : f32) outs(%9339 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9341 = tensor.empty() : tensor<2048x1280xf32>
    %9342 = linalg.fill ins(%cst : f32) outs(%9341 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9343:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9340, %9342, %9337, %9338, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9340, %9342)
    %9344 = arith.truncf %9343#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9345 = torch_c.from_builtin_tensor %9344 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9346 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9347 = torch.aten.view %9345, %9346 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9348 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9349 = torch.aten.view %9319, %9348 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9350 = torch.aten.transpose.int %9349, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9351 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9352 = torch.aten.view %9333, %9351 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9353 = torch.aten.transpose.int %9352, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9354 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9355 = torch.aten.view %9347, %9354 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9356 = torch.aten.transpose.int %9355, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9357:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9350, %9353, %9356, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9358 = torch.aten.transpose.int %9357#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9359 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9360 = torch.aten.view %9358, %9359 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9361 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9362 = torch.aten.view %9360, %9361 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9363 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9364 = torch.aten.transpose.int %9363, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %9365 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9366 = torch.prims.convert_element_type %9365, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9367 = torch.prims.convert_element_type %9362, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9368 = torch.prims.convert_element_type %9364, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9369 = torch.aten.mm %9367, %9368 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9370 = torch.aten.mul.Scalar %9369, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9371 = torch.aten.mul.Scalar %9366, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9372 = torch.aten.add.Tensor %9370, %9371, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9373 = torch.prims.convert_element_type %9372, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9374 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9375 = torch.aten.view %9373, %9374 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9376 = torch.aten.div.Scalar %9375, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9377 = torch.aten.add.Tensor %9376, %9292, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9378 = torch.prims.convert_element_type %9377, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9379 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_271, %result1_272 = torch.aten.var_mean.correction %9378, %9379, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9380 = torch.aten.add.Scalar %result0_271, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9381 = torch.aten.rsqrt %9380 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9382 = torch.aten.sub.Tensor %9377, %result1_272, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9383 = torch.aten.mul.Tensor %9382, %9381 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %9384 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9385 = torch.aten.mul.Tensor %9383, %9384 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %9386 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9387 = torch.aten.add.Tensor %9385, %9386, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9388 = torch.prims.convert_element_type %9387, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9389 = torch.prims.convert_element_type %result1_272, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9390 = torch.prims.convert_element_type %9381, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %9391 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9392 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9393 = torch.aten.view %9388, %9392 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9394 = torch_c.to_builtin_tensor %9393 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9395 = torch_c.to_builtin_tensor %9391 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9396 = tensor.empty() : tensor<2048x1280xf32>
    %9397 = linalg.fill ins(%cst : f32) outs(%9396 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9398 = tensor.empty() : tensor<2048x1280xf32>
    %9399 = linalg.fill ins(%cst : f32) outs(%9398 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9400:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9397, %9399, %9394, %9395, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9397, %9399)
    %9401 = arith.truncf %9400#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9402 = torch_c.from_builtin_tensor %9401 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9403 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9404 = torch.aten.view %9402, %9403 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %9405 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9406 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9407 = torch.aten.view %4, %9406 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9408 = torch_c.to_builtin_tensor %9407 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9409 = torch_c.to_builtin_tensor %9405 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9410 = tensor.empty() : tensor<128x1280xf32>
    %9411 = linalg.fill ins(%cst : f32) outs(%9410 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9412 = tensor.empty() : tensor<128x1280xf32>
    %9413 = linalg.fill ins(%cst : f32) outs(%9412 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9414:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9411, %9413, %9408, %9409, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9411, %9413)
    %9415 = arith.truncf %9414#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9416 = torch_c.from_builtin_tensor %9415 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9417 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9418 = torch.aten.view %9416, %9417 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %9419 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9420 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9421 = torch.aten.view %4, %9420 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9422 = torch_c.to_builtin_tensor %9421 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9423 = torch_c.to_builtin_tensor %9419 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9424 = tensor.empty() : tensor<128x1280xf32>
    %9425 = linalg.fill ins(%cst : f32) outs(%9424 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9426 = tensor.empty() : tensor<128x1280xf32>
    %9427 = linalg.fill ins(%cst : f32) outs(%9426 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9428:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9425, %9427, %9422, %9423, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9425, %9427)
    %9429 = arith.truncf %9428#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9430 = torch_c.from_builtin_tensor %9429 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9431 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9432 = torch.aten.view %9430, %9431 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9433 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9434 = torch.aten.view %9404, %9433 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9435 = torch.aten.transpose.int %9434, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9436 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9437 = torch.aten.view %9418, %9436 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9438 = torch.aten.transpose.int %9437, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9439 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9440 = torch.aten.view %9432, %9439 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9441 = torch.aten.transpose.int %9440, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9442:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9435, %9438, %9441, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9443 = torch.aten.transpose.int %9442#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9444 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9445 = torch.aten.view %9443, %9444 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9446 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9447 = torch.aten.view %9445, %9446 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9448 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9449 = torch.aten.transpose.int %9448, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %9450 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9451 = torch.prims.convert_element_type %9450, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9452 = torch.prims.convert_element_type %9447, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9453 = torch.prims.convert_element_type %9449, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9454 = torch.aten.mm %9452, %9453 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9455 = torch.aten.mul.Scalar %9454, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9456 = torch.aten.mul.Scalar %9451, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9457 = torch.aten.add.Tensor %9455, %9456, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9458 = torch.prims.convert_element_type %9457, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9459 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9460 = torch.aten.view %9458, %9459 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9461 = torch.aten.div.Scalar %9460, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9462 = torch.aten.add.Tensor %9461, %9377, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9463 = torch.prims.convert_element_type %9462, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9464 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_273, %result1_274 = torch.aten.var_mean.correction %9463, %9464, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9465 = torch.aten.add.Scalar %result0_273, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9466 = torch.aten.rsqrt %9465 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9467 = torch.aten.sub.Tensor %9462, %result1_274, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9468 = torch.aten.mul.Tensor %9467, %9466 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %9469 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9470 = torch.aten.mul.Tensor %9468, %9469 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %9471 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9472 = torch.aten.add.Tensor %9470, %9471, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9473 = torch.prims.convert_element_type %9472, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9474 = torch.prims.convert_element_type %result1_274, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9475 = torch.prims.convert_element_type %9466, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9476 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9477 = torch.aten.view %9473, %9476 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9478 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9479 = torch.aten.transpose.int %9478, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %9480 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9481 = torch.prims.convert_element_type %9480, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9482 = torch.prims.convert_element_type %9477, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9483 = torch.prims.convert_element_type %9479, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9484 = torch.aten.mm %9482, %9483 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9485 = torch.aten.mul.Scalar %9484, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9486 = torch.aten.mul.Scalar %9481, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9487 = torch.aten.add.Tensor %9485, %9486, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9488 = torch.prims.convert_element_type %9487, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9489 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9490 = torch.aten.view %9488, %9489 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9491 = torch.aten.slice.Tensor %9490, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9492 = torch.aten.slice.Tensor %9490, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9493 = torch.aten.gelu %9492, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9494 = torch.aten.mul.Tensor %9491, %9493 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9495 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9496 = torch.aten.view %9494, %9495 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %9497 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9498 = torch.aten.transpose.int %9497, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %9499 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9500 = torch.prims.convert_element_type %9499, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9501 = torch.prims.convert_element_type %9496, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9502 = torch.prims.convert_element_type %9498, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9503 = torch.aten.mm %9501, %9502 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9504 = torch.aten.mul.Scalar %9503, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9505 = torch.aten.mul.Scalar %9500, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9506 = torch.aten.add.Tensor %9504, %9505, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9507 = torch.prims.convert_element_type %9506, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9508 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9509 = torch.aten.view %9507, %9508 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9510 = torch.aten.add.Tensor %9509, %9462, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9511 = torch.prims.convert_element_type %9510, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9512 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_275, %result1_276 = torch.aten.var_mean.correction %9511, %9512, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9513 = torch.aten.add.Scalar %result0_275, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9514 = torch.aten.rsqrt %9513 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9515 = torch.aten.sub.Tensor %9510, %result1_276, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9516 = torch.aten.mul.Tensor %9515, %9514 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %9517 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9518 = torch.aten.mul.Tensor %9516, %9517 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %9519 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9520 = torch.aten.add.Tensor %9518, %9519, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9521 = torch.prims.convert_element_type %9520, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9522 = torch.prims.convert_element_type %result1_276, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9523 = torch.prims.convert_element_type %9514, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %9524 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9525 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9526 = torch.aten.view %9521, %9525 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9527 = torch_c.to_builtin_tensor %9526 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9528 = torch_c.to_builtin_tensor %9524 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9529 = tensor.empty() : tensor<2048x1280xf32>
    %9530 = linalg.fill ins(%cst : f32) outs(%9529 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9531 = tensor.empty() : tensor<2048x1280xf32>
    %9532 = linalg.fill ins(%cst : f32) outs(%9531 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9533:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9530, %9532, %9527, %9528, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9530, %9532)
    %9534 = arith.truncf %9533#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9535 = torch_c.from_builtin_tensor %9534 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9536 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9537 = torch.aten.view %9535, %9536 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %9538 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9539 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9540 = torch.aten.view %9521, %9539 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9541 = torch_c.to_builtin_tensor %9540 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9542 = torch_c.to_builtin_tensor %9538 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9543 = tensor.empty() : tensor<2048x1280xf32>
    %9544 = linalg.fill ins(%cst : f32) outs(%9543 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9545 = tensor.empty() : tensor<2048x1280xf32>
    %9546 = linalg.fill ins(%cst : f32) outs(%9545 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9547:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9544, %9546, %9541, %9542, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9544, %9546)
    %9548 = arith.truncf %9547#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9549 = torch_c.from_builtin_tensor %9548 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9550 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9551 = torch.aten.view %9549, %9550 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %9552 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9553 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9554 = torch.aten.view %9521, %9553 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9555 = torch_c.to_builtin_tensor %9554 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9556 = torch_c.to_builtin_tensor %9552 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9557 = tensor.empty() : tensor<2048x1280xf32>
    %9558 = linalg.fill ins(%cst : f32) outs(%9557 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9559 = tensor.empty() : tensor<2048x1280xf32>
    %9560 = linalg.fill ins(%cst : f32) outs(%9559 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9561:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9558, %9560, %9555, %9556, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9558, %9560)
    %9562 = arith.truncf %9561#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9563 = torch_c.from_builtin_tensor %9562 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9564 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9565 = torch.aten.view %9563, %9564 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9566 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9567 = torch.aten.view %9537, %9566 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9568 = torch.aten.transpose.int %9567, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9569 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9570 = torch.aten.view %9551, %9569 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9571 = torch.aten.transpose.int %9570, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9572 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9573 = torch.aten.view %9565, %9572 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9574 = torch.aten.transpose.int %9573, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9575:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9568, %9571, %9574, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9576 = torch.aten.transpose.int %9575#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9577 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9578 = torch.aten.view %9576, %9577 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9579 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9580 = torch.aten.view %9578, %9579 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9581 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9582 = torch.aten.transpose.int %9581, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %9583 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9584 = torch.prims.convert_element_type %9583, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9585 = torch.prims.convert_element_type %9580, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9586 = torch.prims.convert_element_type %9582, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9587 = torch.aten.mm %9585, %9586 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9588 = torch.aten.mul.Scalar %9587, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9589 = torch.aten.mul.Scalar %9584, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9590 = torch.aten.add.Tensor %9588, %9589, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9591 = torch.prims.convert_element_type %9590, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9592 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9593 = torch.aten.view %9591, %9592 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9594 = torch.aten.div.Scalar %9593, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9595 = torch.aten.add.Tensor %9594, %9510, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9596 = torch.prims.convert_element_type %9595, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9597 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_277, %result1_278 = torch.aten.var_mean.correction %9596, %9597, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9598 = torch.aten.add.Scalar %result0_277, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9599 = torch.aten.rsqrt %9598 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9600 = torch.aten.sub.Tensor %9595, %result1_278, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9601 = torch.aten.mul.Tensor %9600, %9599 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %9602 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9603 = torch.aten.mul.Tensor %9601, %9602 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %9604 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9605 = torch.aten.add.Tensor %9603, %9604, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9606 = torch.prims.convert_element_type %9605, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9607 = torch.prims.convert_element_type %result1_278, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9608 = torch.prims.convert_element_type %9599, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %9609 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9610 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9611 = torch.aten.view %9606, %9610 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9612 = torch_c.to_builtin_tensor %9611 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9613 = torch_c.to_builtin_tensor %9609 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9614 = tensor.empty() : tensor<2048x1280xf32>
    %9615 = linalg.fill ins(%cst : f32) outs(%9614 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9616 = tensor.empty() : tensor<2048x1280xf32>
    %9617 = linalg.fill ins(%cst : f32) outs(%9616 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9618:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9615, %9617, %9612, %9613, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9615, %9617)
    %9619 = arith.truncf %9618#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9620 = torch_c.from_builtin_tensor %9619 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9621 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9622 = torch.aten.view %9620, %9621 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %9623 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9624 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9625 = torch.aten.view %4, %9624 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9626 = torch_c.to_builtin_tensor %9625 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9627 = torch_c.to_builtin_tensor %9623 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9628 = tensor.empty() : tensor<128x1280xf32>
    %9629 = linalg.fill ins(%cst : f32) outs(%9628 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9630 = tensor.empty() : tensor<128x1280xf32>
    %9631 = linalg.fill ins(%cst : f32) outs(%9630 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9632:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9629, %9631, %9626, %9627, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9629, %9631)
    %9633 = arith.truncf %9632#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9634 = torch_c.from_builtin_tensor %9633 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9635 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9636 = torch.aten.view %9634, %9635 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %9637 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9638 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9639 = torch.aten.view %4, %9638 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9640 = torch_c.to_builtin_tensor %9639 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9641 = torch_c.to_builtin_tensor %9637 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9642 = tensor.empty() : tensor<128x1280xf32>
    %9643 = linalg.fill ins(%cst : f32) outs(%9642 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9644 = tensor.empty() : tensor<128x1280xf32>
    %9645 = linalg.fill ins(%cst : f32) outs(%9644 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9646:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9643, %9645, %9640, %9641, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9643, %9645)
    %9647 = arith.truncf %9646#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9648 = torch_c.from_builtin_tensor %9647 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9649 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9650 = torch.aten.view %9648, %9649 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9651 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9652 = torch.aten.view %9622, %9651 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9653 = torch.aten.transpose.int %9652, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9654 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9655 = torch.aten.view %9636, %9654 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9656 = torch.aten.transpose.int %9655, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9657 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9658 = torch.aten.view %9650, %9657 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9659 = torch.aten.transpose.int %9658, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9660:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9653, %9656, %9659, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9661 = torch.aten.transpose.int %9660#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9662 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9663 = torch.aten.view %9661, %9662 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9664 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9665 = torch.aten.view %9663, %9664 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9666 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9667 = torch.aten.transpose.int %9666, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %9668 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9669 = torch.prims.convert_element_type %9668, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9670 = torch.prims.convert_element_type %9665, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9671 = torch.prims.convert_element_type %9667, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9672 = torch.aten.mm %9670, %9671 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9673 = torch.aten.mul.Scalar %9672, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9674 = torch.aten.mul.Scalar %9669, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9675 = torch.aten.add.Tensor %9673, %9674, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9676 = torch.prims.convert_element_type %9675, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9677 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9678 = torch.aten.view %9676, %9677 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9679 = torch.aten.div.Scalar %9678, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9680 = torch.aten.add.Tensor %9679, %9595, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9681 = torch.prims.convert_element_type %9680, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9682 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_279, %result1_280 = torch.aten.var_mean.correction %9681, %9682, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9683 = torch.aten.add.Scalar %result0_279, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9684 = torch.aten.rsqrt %9683 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9685 = torch.aten.sub.Tensor %9680, %result1_280, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9686 = torch.aten.mul.Tensor %9685, %9684 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %9687 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9688 = torch.aten.mul.Tensor %9686, %9687 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %9689 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9690 = torch.aten.add.Tensor %9688, %9689, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9691 = torch.prims.convert_element_type %9690, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9692 = torch.prims.convert_element_type %result1_280, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9693 = torch.prims.convert_element_type %9684, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9694 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9695 = torch.aten.view %9691, %9694 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9696 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9697 = torch.aten.transpose.int %9696, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %9698 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9699 = torch.prims.convert_element_type %9698, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9700 = torch.prims.convert_element_type %9695, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9701 = torch.prims.convert_element_type %9697, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9702 = torch.aten.mm %9700, %9701 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9703 = torch.aten.mul.Scalar %9702, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9704 = torch.aten.mul.Scalar %9699, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9705 = torch.aten.add.Tensor %9703, %9704, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9706 = torch.prims.convert_element_type %9705, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9707 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9708 = torch.aten.view %9706, %9707 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9709 = torch.aten.slice.Tensor %9708, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9710 = torch.aten.slice.Tensor %9708, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9711 = torch.aten.gelu %9710, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9712 = torch.aten.mul.Tensor %9709, %9711 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9713 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9714 = torch.aten.view %9712, %9713 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %9715 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9716 = torch.aten.transpose.int %9715, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %9717 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9718 = torch.prims.convert_element_type %9717, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9719 = torch.prims.convert_element_type %9714, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9720 = torch.prims.convert_element_type %9716, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9721 = torch.aten.mm %9719, %9720 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9722 = torch.aten.mul.Scalar %9721, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9723 = torch.aten.mul.Scalar %9718, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9724 = torch.aten.add.Tensor %9722, %9723, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9725 = torch.prims.convert_element_type %9724, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9726 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9727 = torch.aten.view %9725, %9726 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9728 = torch.aten.add.Tensor %9727, %9680, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9729 = torch.prims.convert_element_type %9728, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9730 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_281, %result1_282 = torch.aten.var_mean.correction %9729, %9730, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9731 = torch.aten.add.Scalar %result0_281, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9732 = torch.aten.rsqrt %9731 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9733 = torch.aten.sub.Tensor %9728, %result1_282, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9734 = torch.aten.mul.Tensor %9733, %9732 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %9735 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9736 = torch.aten.mul.Tensor %9734, %9735 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %9737 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9738 = torch.aten.add.Tensor %9736, %9737, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9739 = torch.prims.convert_element_type %9738, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9740 = torch.prims.convert_element_type %result1_282, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9741 = torch.prims.convert_element_type %9732, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %9742 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9743 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9744 = torch.aten.view %9739, %9743 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9745 = torch_c.to_builtin_tensor %9744 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9746 = torch_c.to_builtin_tensor %9742 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9747 = tensor.empty() : tensor<2048x1280xf32>
    %9748 = linalg.fill ins(%cst : f32) outs(%9747 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9749 = tensor.empty() : tensor<2048x1280xf32>
    %9750 = linalg.fill ins(%cst : f32) outs(%9749 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9751:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9748, %9750, %9745, %9746, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9748, %9750)
    %9752 = arith.truncf %9751#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9753 = torch_c.from_builtin_tensor %9752 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9754 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9755 = torch.aten.view %9753, %9754 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %9756 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9757 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9758 = torch.aten.view %9739, %9757 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9759 = torch_c.to_builtin_tensor %9758 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9760 = torch_c.to_builtin_tensor %9756 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9761 = tensor.empty() : tensor<2048x1280xf32>
    %9762 = linalg.fill ins(%cst : f32) outs(%9761 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9763 = tensor.empty() : tensor<2048x1280xf32>
    %9764 = linalg.fill ins(%cst : f32) outs(%9763 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9765:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9762, %9764, %9759, %9760, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9762, %9764)
    %9766 = arith.truncf %9765#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9767 = torch_c.from_builtin_tensor %9766 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9768 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9769 = torch.aten.view %9767, %9768 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %9770 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9771 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9772 = torch.aten.view %9739, %9771 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9773 = torch_c.to_builtin_tensor %9772 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9774 = torch_c.to_builtin_tensor %9770 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9775 = tensor.empty() : tensor<2048x1280xf32>
    %9776 = linalg.fill ins(%cst : f32) outs(%9775 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9777 = tensor.empty() : tensor<2048x1280xf32>
    %9778 = linalg.fill ins(%cst : f32) outs(%9777 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9779:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9776, %9778, %9773, %9774, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9776, %9778)
    %9780 = arith.truncf %9779#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9781 = torch_c.from_builtin_tensor %9780 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9782 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9783 = torch.aten.view %9781, %9782 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9784 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9785 = torch.aten.view %9755, %9784 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9786 = torch.aten.transpose.int %9785, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9787 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9788 = torch.aten.view %9769, %9787 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9789 = torch.aten.transpose.int %9788, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9790 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9791 = torch.aten.view %9783, %9790 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9792 = torch.aten.transpose.int %9791, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9793:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9786, %9789, %9792, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9794 = torch.aten.transpose.int %9793#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9795 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9796 = torch.aten.view %9794, %9795 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9797 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9798 = torch.aten.view %9796, %9797 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %9799 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9800 = torch.aten.transpose.int %9799, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %9801 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9802 = torch.prims.convert_element_type %9801, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9803 = torch.prims.convert_element_type %9798, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9804 = torch.prims.convert_element_type %9800, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9805 = torch.aten.mm %9803, %9804 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9806 = torch.aten.mul.Scalar %9805, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9807 = torch.aten.mul.Scalar %9802, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9808 = torch.aten.add.Tensor %9806, %9807, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9809 = torch.prims.convert_element_type %9808, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9810 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9811 = torch.aten.view %9809, %9810 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9812 = torch.aten.div.Scalar %9811, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9813 = torch.aten.add.Tensor %9812, %9728, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9814 = torch.prims.convert_element_type %9813, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9815 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_283, %result1_284 = torch.aten.var_mean.correction %9814, %9815, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9816 = torch.aten.add.Scalar %result0_283, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9817 = torch.aten.rsqrt %9816 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9818 = torch.aten.sub.Tensor %9813, %result1_284, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9819 = torch.aten.mul.Tensor %9818, %9817 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %9820 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9821 = torch.aten.mul.Tensor %9819, %9820 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %9822 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9823 = torch.aten.add.Tensor %9821, %9822, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9824 = torch.prims.convert_element_type %9823, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9825 = torch.prims.convert_element_type %result1_284, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9826 = torch.prims.convert_element_type %9817, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %9827 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9828 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9829 = torch.aten.view %9824, %9828 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9830 = torch_c.to_builtin_tensor %9829 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9831 = torch_c.to_builtin_tensor %9827 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9832 = tensor.empty() : tensor<2048x1280xf32>
    %9833 = linalg.fill ins(%cst : f32) outs(%9832 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9834 = tensor.empty() : tensor<2048x1280xf32>
    %9835 = linalg.fill ins(%cst : f32) outs(%9834 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9836:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9833, %9835, %9830, %9831, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9833, %9835)
    %9837 = arith.truncf %9836#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9838 = torch_c.from_builtin_tensor %9837 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9839 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9840 = torch.aten.view %9838, %9839 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %9841 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9842 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9843 = torch.aten.view %4, %9842 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9844 = torch_c.to_builtin_tensor %9843 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9845 = torch_c.to_builtin_tensor %9841 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9846 = tensor.empty() : tensor<128x1280xf32>
    %9847 = linalg.fill ins(%cst : f32) outs(%9846 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9848 = tensor.empty() : tensor<128x1280xf32>
    %9849 = linalg.fill ins(%cst : f32) outs(%9848 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9850:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9847, %9849, %9844, %9845, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9847, %9849)
    %9851 = arith.truncf %9850#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9852 = torch_c.from_builtin_tensor %9851 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9853 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9854 = torch.aten.view %9852, %9853 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %9855 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %9856 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %9857 = torch.aten.view %4, %9856 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %9858 = torch_c.to_builtin_tensor %9857 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %9859 = torch_c.to_builtin_tensor %9855 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %9860 = tensor.empty() : tensor<128x1280xf32>
    %9861 = linalg.fill ins(%cst : f32) outs(%9860 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9862 = tensor.empty() : tensor<128x1280xf32>
    %9863 = linalg.fill ins(%cst : f32) outs(%9862 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %9864:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %9861, %9863, %9858, %9859, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9861, %9863)
    %9865 = arith.truncf %9864#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %9866 = torch_c.from_builtin_tensor %9865 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %9867 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9868 = torch.aten.view %9866, %9867 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %9869 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9870 = torch.aten.view %9840, %9869 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %9871 = torch.aten.transpose.int %9870, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %9872 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9873 = torch.aten.view %9854, %9872 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9874 = torch.aten.transpose.int %9873, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9875 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9876 = torch.aten.view %9868, %9875 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %9877 = torch.aten.transpose.int %9876, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %9878:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%9871, %9874, %9877, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %9879 = torch.aten.transpose.int %9878#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %9880 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9881 = torch.aten.view %9879, %9880 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9882 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9883 = torch.aten.view %9881, %9882 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %9884 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9885 = torch.aten.transpose.int %9884, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %9886 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9887 = torch.prims.convert_element_type %9886, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9888 = torch.prims.convert_element_type %9883, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9889 = torch.prims.convert_element_type %9885, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %9890 = torch.aten.mm %9888, %9889 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9891 = torch.aten.mul.Scalar %9890, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9892 = torch.aten.mul.Scalar %9887, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9893 = torch.aten.add.Tensor %9891, %9892, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9894 = torch.prims.convert_element_type %9893, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9895 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9896 = torch.aten.view %9894, %9895 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9897 = torch.aten.div.Scalar %9896, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %9898 = torch.aten.add.Tensor %9897, %9813, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9899 = torch.prims.convert_element_type %9898, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9900 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_285, %result1_286 = torch.aten.var_mean.correction %9899, %9900, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9901 = torch.aten.add.Scalar %result0_285, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9902 = torch.aten.rsqrt %9901 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9903 = torch.aten.sub.Tensor %9898, %result1_286, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9904 = torch.aten.mul.Tensor %9903, %9902 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %9905 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9906 = torch.aten.mul.Tensor %9904, %9905 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %9907 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9908 = torch.aten.add.Tensor %9906, %9907, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9909 = torch.prims.convert_element_type %9908, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9910 = torch.prims.convert_element_type %result1_286, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9911 = torch.prims.convert_element_type %9902, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9912 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9913 = torch.aten.view %9909, %9912 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %9914 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %9915 = torch.aten.transpose.int %9914, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %9916 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %9917 = torch.prims.convert_element_type %9916, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %9918 = torch.prims.convert_element_type %9913, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9919 = torch.prims.convert_element_type %9915, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %9920 = torch.aten.mm %9918, %9919 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %9921 = torch.aten.mul.Scalar %9920, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9922 = torch.aten.mul.Scalar %9917, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %9923 = torch.aten.add.Tensor %9921, %9922, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %9924 = torch.prims.convert_element_type %9923, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %9925 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9926 = torch.aten.view %9924, %9925 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %9927 = torch.aten.slice.Tensor %9926, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9928 = torch.aten.slice.Tensor %9926, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %9929 = torch.aten.gelu %9928, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %9930 = torch.aten.mul.Tensor %9927, %9929 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %9931 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %9932 = torch.aten.view %9930, %9931 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %9933 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %9934 = torch.aten.transpose.int %9933, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %9935 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9936 = torch.prims.convert_element_type %9935, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %9937 = torch.prims.convert_element_type %9932, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %9938 = torch.prims.convert_element_type %9934, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %9939 = torch.aten.mm %9937, %9938 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %9940 = torch.aten.mul.Scalar %9939, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9941 = torch.aten.mul.Scalar %9936, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %9942 = torch.aten.add.Tensor %9940, %9941, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %9943 = torch.prims.convert_element_type %9942, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %9944 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9945 = torch.aten.view %9943, %9944 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %9946 = torch.aten.add.Tensor %9945, %9898, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9947 = torch.prims.convert_element_type %9946, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9948 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_287, %result1_288 = torch.aten.var_mean.correction %9947, %9948, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %9949 = torch.aten.add.Scalar %result0_287, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %9950 = torch.aten.rsqrt %9949 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %9951 = torch.aten.sub.Tensor %9946, %result1_288, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9952 = torch.aten.mul.Tensor %9951, %9950 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %9953 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9954 = torch.aten.mul.Tensor %9952, %9953 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %9955 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %9956 = torch.aten.add.Tensor %9954, %9955, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %9957 = torch.prims.convert_element_type %9956, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %9958 = torch.prims.convert_element_type %result1_288, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %9959 = torch.prims.convert_element_type %9950, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %9960 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9961 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9962 = torch.aten.view %9957, %9961 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9963 = torch_c.to_builtin_tensor %9962 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9964 = torch_c.to_builtin_tensor %9960 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9965 = tensor.empty() : tensor<2048x1280xf32>
    %9966 = linalg.fill ins(%cst : f32) outs(%9965 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9967 = tensor.empty() : tensor<2048x1280xf32>
    %9968 = linalg.fill ins(%cst : f32) outs(%9967 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9969:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9966, %9968, %9963, %9964, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9966, %9968)
    %9970 = arith.truncf %9969#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9971 = torch_c.from_builtin_tensor %9970 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9972 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9973 = torch.aten.view %9971, %9972 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %9974 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9975 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9976 = torch.aten.view %9957, %9975 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9977 = torch_c.to_builtin_tensor %9976 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9978 = torch_c.to_builtin_tensor %9974 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9979 = tensor.empty() : tensor<2048x1280xf32>
    %9980 = linalg.fill ins(%cst : f32) outs(%9979 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9981 = tensor.empty() : tensor<2048x1280xf32>
    %9982 = linalg.fill ins(%cst : f32) outs(%9981 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9983:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9980, %9982, %9977, %9978, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9980, %9982)
    %9984 = arith.truncf %9983#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9985 = torch_c.from_builtin_tensor %9984 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %9986 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %9987 = torch.aten.view %9985, %9986 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %9988 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %9989 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %9990 = torch.aten.view %9957, %9989 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %9991 = torch_c.to_builtin_tensor %9990 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %9992 = torch_c.to_builtin_tensor %9988 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %9993 = tensor.empty() : tensor<2048x1280xf32>
    %9994 = linalg.fill ins(%cst : f32) outs(%9993 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9995 = tensor.empty() : tensor<2048x1280xf32>
    %9996 = linalg.fill ins(%cst : f32) outs(%9995 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %9997:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %9994, %9996, %9991, %9992, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%9994, %9996)
    %9998 = arith.truncf %9997#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %9999 = torch_c.from_builtin_tensor %9998 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10000 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10001 = torch.aten.view %9999, %10000 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10002 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10003 = torch.aten.view %9973, %10002 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10004 = torch.aten.transpose.int %10003, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10005 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10006 = torch.aten.view %9987, %10005 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10007 = torch.aten.transpose.int %10006, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10008 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10009 = torch.aten.view %10001, %10008 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10010 = torch.aten.transpose.int %10009, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10011:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10004, %10007, %10010, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10012 = torch.aten.transpose.int %10011#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10013 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10014 = torch.aten.view %10012, %10013 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10015 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10016 = torch.aten.view %10014, %10015 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10017 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10018 = torch.aten.transpose.int %10017, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %10019 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10020 = torch.prims.convert_element_type %10019, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10021 = torch.prims.convert_element_type %10016, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10022 = torch.prims.convert_element_type %10018, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10023 = torch.aten.mm %10021, %10022 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10024 = torch.aten.mul.Scalar %10023, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10025 = torch.aten.mul.Scalar %10020, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10026 = torch.aten.add.Tensor %10024, %10025, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10027 = torch.prims.convert_element_type %10026, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10028 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10029 = torch.aten.view %10027, %10028 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10030 = torch.aten.div.Scalar %10029, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10031 = torch.aten.add.Tensor %10030, %9946, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10032 = torch.prims.convert_element_type %10031, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10033 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_289, %result1_290 = torch.aten.var_mean.correction %10032, %10033, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10034 = torch.aten.add.Scalar %result0_289, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10035 = torch.aten.rsqrt %10034 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10036 = torch.aten.sub.Tensor %10031, %result1_290, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10037 = torch.aten.mul.Tensor %10036, %10035 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %10038 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10039 = torch.aten.mul.Tensor %10037, %10038 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %10040 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10041 = torch.aten.add.Tensor %10039, %10040, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10042 = torch.prims.convert_element_type %10041, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10043 = torch.prims.convert_element_type %result1_290, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10044 = torch.prims.convert_element_type %10035, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %10045 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10046 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10047 = torch.aten.view %10042, %10046 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10048 = torch_c.to_builtin_tensor %10047 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10049 = torch_c.to_builtin_tensor %10045 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10050 = tensor.empty() : tensor<2048x1280xf32>
    %10051 = linalg.fill ins(%cst : f32) outs(%10050 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10052 = tensor.empty() : tensor<2048x1280xf32>
    %10053 = linalg.fill ins(%cst : f32) outs(%10052 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10054:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10051, %10053, %10048, %10049, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10051, %10053)
    %10055 = arith.truncf %10054#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10056 = torch_c.from_builtin_tensor %10055 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10057 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10058 = torch.aten.view %10056, %10057 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %10059 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10060 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10061 = torch.aten.view %4, %10060 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10062 = torch_c.to_builtin_tensor %10061 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10063 = torch_c.to_builtin_tensor %10059 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10064 = tensor.empty() : tensor<128x1280xf32>
    %10065 = linalg.fill ins(%cst : f32) outs(%10064 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10066 = tensor.empty() : tensor<128x1280xf32>
    %10067 = linalg.fill ins(%cst : f32) outs(%10066 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10068:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10065, %10067, %10062, %10063, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10065, %10067)
    %10069 = arith.truncf %10068#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10070 = torch_c.from_builtin_tensor %10069 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10071 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10072 = torch.aten.view %10070, %10071 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %10073 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10074 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10075 = torch.aten.view %4, %10074 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10076 = torch_c.to_builtin_tensor %10075 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10077 = torch_c.to_builtin_tensor %10073 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10078 = tensor.empty() : tensor<128x1280xf32>
    %10079 = linalg.fill ins(%cst : f32) outs(%10078 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10080 = tensor.empty() : tensor<128x1280xf32>
    %10081 = linalg.fill ins(%cst : f32) outs(%10080 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10082:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10079, %10081, %10076, %10077, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10079, %10081)
    %10083 = arith.truncf %10082#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10084 = torch_c.from_builtin_tensor %10083 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10085 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10086 = torch.aten.view %10084, %10085 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10087 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10088 = torch.aten.view %10058, %10087 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10089 = torch.aten.transpose.int %10088, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10090 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10091 = torch.aten.view %10072, %10090 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10092 = torch.aten.transpose.int %10091, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10093 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10094 = torch.aten.view %10086, %10093 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10095 = torch.aten.transpose.int %10094, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10096:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10089, %10092, %10095, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10097 = torch.aten.transpose.int %10096#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10098 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10099 = torch.aten.view %10097, %10098 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10100 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10101 = torch.aten.view %10099, %10100 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10102 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10103 = torch.aten.transpose.int %10102, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %10104 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10105 = torch.prims.convert_element_type %10104, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10106 = torch.prims.convert_element_type %10101, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10107 = torch.prims.convert_element_type %10103, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10108 = torch.aten.mm %10106, %10107 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10109 = torch.aten.mul.Scalar %10108, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10110 = torch.aten.mul.Scalar %10105, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10111 = torch.aten.add.Tensor %10109, %10110, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10112 = torch.prims.convert_element_type %10111, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10113 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10114 = torch.aten.view %10112, %10113 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10115 = torch.aten.div.Scalar %10114, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10116 = torch.aten.add.Tensor %10115, %10031, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10117 = torch.prims.convert_element_type %10116, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10118 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_291, %result1_292 = torch.aten.var_mean.correction %10117, %10118, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10119 = torch.aten.add.Scalar %result0_291, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10120 = torch.aten.rsqrt %10119 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10121 = torch.aten.sub.Tensor %10116, %result1_292, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10122 = torch.aten.mul.Tensor %10121, %10120 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %10123 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10124 = torch.aten.mul.Tensor %10122, %10123 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %10125 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10126 = torch.aten.add.Tensor %10124, %10125, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10127 = torch.prims.convert_element_type %10126, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10128 = torch.prims.convert_element_type %result1_292, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10129 = torch.prims.convert_element_type %10120, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10130 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10131 = torch.aten.view %10127, %10130 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10132 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10133 = torch.aten.transpose.int %10132, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %10134 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10135 = torch.prims.convert_element_type %10134, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10136 = torch.prims.convert_element_type %10131, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10137 = torch.prims.convert_element_type %10133, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10138 = torch.aten.mm %10136, %10137 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10139 = torch.aten.mul.Scalar %10138, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10140 = torch.aten.mul.Scalar %10135, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10141 = torch.aten.add.Tensor %10139, %10140, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10142 = torch.prims.convert_element_type %10141, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10143 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10144 = torch.aten.view %10142, %10143 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10145 = torch.aten.slice.Tensor %10144, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10146 = torch.aten.slice.Tensor %10144, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10147 = torch.aten.gelu %10146, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10148 = torch.aten.mul.Tensor %10145, %10147 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10149 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10150 = torch.aten.view %10148, %10149 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %10151 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10152 = torch.aten.transpose.int %10151, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %10153 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10154 = torch.prims.convert_element_type %10153, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10155 = torch.prims.convert_element_type %10150, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10156 = torch.prims.convert_element_type %10152, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10157 = torch.aten.mm %10155, %10156 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10158 = torch.aten.mul.Scalar %10157, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10159 = torch.aten.mul.Scalar %10154, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10160 = torch.aten.add.Tensor %10158, %10159, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10161 = torch.prims.convert_element_type %10160, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10162 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10163 = torch.aten.view %10161, %10162 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10164 = torch.aten.add.Tensor %10163, %10116, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10165 = torch.prims.convert_element_type %10164, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10166 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_293, %result1_294 = torch.aten.var_mean.correction %10165, %10166, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10167 = torch.aten.add.Scalar %result0_293, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10168 = torch.aten.rsqrt %10167 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10169 = torch.aten.sub.Tensor %10164, %result1_294, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10170 = torch.aten.mul.Tensor %10169, %10168 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %10171 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10172 = torch.aten.mul.Tensor %10170, %10171 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %10173 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10174 = torch.aten.add.Tensor %10172, %10173, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10175 = torch.prims.convert_element_type %10174, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10176 = torch.prims.convert_element_type %result1_294, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10177 = torch.prims.convert_element_type %10168, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %10178 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10179 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10180 = torch.aten.view %10175, %10179 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10181 = torch_c.to_builtin_tensor %10180 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10182 = torch_c.to_builtin_tensor %10178 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10183 = tensor.empty() : tensor<2048x1280xf32>
    %10184 = linalg.fill ins(%cst : f32) outs(%10183 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10185 = tensor.empty() : tensor<2048x1280xf32>
    %10186 = linalg.fill ins(%cst : f32) outs(%10185 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10187:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10184, %10186, %10181, %10182, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10184, %10186)
    %10188 = arith.truncf %10187#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10189 = torch_c.from_builtin_tensor %10188 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10190 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10191 = torch.aten.view %10189, %10190 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %10192 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10193 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10194 = torch.aten.view %10175, %10193 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10195 = torch_c.to_builtin_tensor %10194 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10196 = torch_c.to_builtin_tensor %10192 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10197 = tensor.empty() : tensor<2048x1280xf32>
    %10198 = linalg.fill ins(%cst : f32) outs(%10197 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10199 = tensor.empty() : tensor<2048x1280xf32>
    %10200 = linalg.fill ins(%cst : f32) outs(%10199 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10201:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10198, %10200, %10195, %10196, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10198, %10200)
    %10202 = arith.truncf %10201#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10203 = torch_c.from_builtin_tensor %10202 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10204 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10205 = torch.aten.view %10203, %10204 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %10206 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10207 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10208 = torch.aten.view %10175, %10207 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10209 = torch_c.to_builtin_tensor %10208 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10210 = torch_c.to_builtin_tensor %10206 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10211 = tensor.empty() : tensor<2048x1280xf32>
    %10212 = linalg.fill ins(%cst : f32) outs(%10211 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10213 = tensor.empty() : tensor<2048x1280xf32>
    %10214 = linalg.fill ins(%cst : f32) outs(%10213 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10215:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10212, %10214, %10209, %10210, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10212, %10214)
    %10216 = arith.truncf %10215#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10217 = torch_c.from_builtin_tensor %10216 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10218 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10219 = torch.aten.view %10217, %10218 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10220 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10221 = torch.aten.view %10191, %10220 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10222 = torch.aten.transpose.int %10221, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10223 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10224 = torch.aten.view %10205, %10223 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10225 = torch.aten.transpose.int %10224, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10226 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10227 = torch.aten.view %10219, %10226 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10228 = torch.aten.transpose.int %10227, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10229:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10222, %10225, %10228, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10230 = torch.aten.transpose.int %10229#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10231 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10232 = torch.aten.view %10230, %10231 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10233 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10234 = torch.aten.view %10232, %10233 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10235 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10236 = torch.aten.transpose.int %10235, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %10237 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10238 = torch.prims.convert_element_type %10237, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10239 = torch.prims.convert_element_type %10234, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10240 = torch.prims.convert_element_type %10236, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10241 = torch.aten.mm %10239, %10240 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10242 = torch.aten.mul.Scalar %10241, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10243 = torch.aten.mul.Scalar %10238, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10244 = torch.aten.add.Tensor %10242, %10243, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10245 = torch.prims.convert_element_type %10244, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10246 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10247 = torch.aten.view %10245, %10246 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10248 = torch.aten.div.Scalar %10247, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10249 = torch.aten.add.Tensor %10248, %10164, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10250 = torch.prims.convert_element_type %10249, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10251 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_295, %result1_296 = torch.aten.var_mean.correction %10250, %10251, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10252 = torch.aten.add.Scalar %result0_295, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10253 = torch.aten.rsqrt %10252 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10254 = torch.aten.sub.Tensor %10249, %result1_296, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10255 = torch.aten.mul.Tensor %10254, %10253 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %10256 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10257 = torch.aten.mul.Tensor %10255, %10256 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %10258 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10259 = torch.aten.add.Tensor %10257, %10258, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10260 = torch.prims.convert_element_type %10259, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10261 = torch.prims.convert_element_type %result1_296, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10262 = torch.prims.convert_element_type %10253, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %10263 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10264 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10265 = torch.aten.view %10260, %10264 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10266 = torch_c.to_builtin_tensor %10265 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10267 = torch_c.to_builtin_tensor %10263 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10268 = tensor.empty() : tensor<2048x1280xf32>
    %10269 = linalg.fill ins(%cst : f32) outs(%10268 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10270 = tensor.empty() : tensor<2048x1280xf32>
    %10271 = linalg.fill ins(%cst : f32) outs(%10270 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10272:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10269, %10271, %10266, %10267, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10269, %10271)
    %10273 = arith.truncf %10272#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10274 = torch_c.from_builtin_tensor %10273 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10275 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10276 = torch.aten.view %10274, %10275 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %10277 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10278 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10279 = torch.aten.view %4, %10278 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10280 = torch_c.to_builtin_tensor %10279 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10281 = torch_c.to_builtin_tensor %10277 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10282 = tensor.empty() : tensor<128x1280xf32>
    %10283 = linalg.fill ins(%cst : f32) outs(%10282 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10284 = tensor.empty() : tensor<128x1280xf32>
    %10285 = linalg.fill ins(%cst : f32) outs(%10284 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10286:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10283, %10285, %10280, %10281, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10283, %10285)
    %10287 = arith.truncf %10286#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10288 = torch_c.from_builtin_tensor %10287 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10289 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10290 = torch.aten.view %10288, %10289 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %10291 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10292 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10293 = torch.aten.view %4, %10292 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10294 = torch_c.to_builtin_tensor %10293 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10295 = torch_c.to_builtin_tensor %10291 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10296 = tensor.empty() : tensor<128x1280xf32>
    %10297 = linalg.fill ins(%cst : f32) outs(%10296 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10298 = tensor.empty() : tensor<128x1280xf32>
    %10299 = linalg.fill ins(%cst : f32) outs(%10298 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10300:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10297, %10299, %10294, %10295, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10297, %10299)
    %10301 = arith.truncf %10300#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10302 = torch_c.from_builtin_tensor %10301 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10303 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10304 = torch.aten.view %10302, %10303 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10305 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10306 = torch.aten.view %10276, %10305 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10307 = torch.aten.transpose.int %10306, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10308 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10309 = torch.aten.view %10290, %10308 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10310 = torch.aten.transpose.int %10309, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10311 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10312 = torch.aten.view %10304, %10311 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10313 = torch.aten.transpose.int %10312, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10314:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10307, %10310, %10313, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10315 = torch.aten.transpose.int %10314#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10316 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10317 = torch.aten.view %10315, %10316 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10318 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10319 = torch.aten.view %10317, %10318 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10320 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10321 = torch.aten.transpose.int %10320, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %10322 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10323 = torch.prims.convert_element_type %10322, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10324 = torch.prims.convert_element_type %10319, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10325 = torch.prims.convert_element_type %10321, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10326 = torch.aten.mm %10324, %10325 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10327 = torch.aten.mul.Scalar %10326, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10328 = torch.aten.mul.Scalar %10323, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10329 = torch.aten.add.Tensor %10327, %10328, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10330 = torch.prims.convert_element_type %10329, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10331 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10332 = torch.aten.view %10330, %10331 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10333 = torch.aten.div.Scalar %10332, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10334 = torch.aten.add.Tensor %10333, %10249, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10335 = torch.prims.convert_element_type %10334, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10336 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_297, %result1_298 = torch.aten.var_mean.correction %10335, %10336, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10337 = torch.aten.add.Scalar %result0_297, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10338 = torch.aten.rsqrt %10337 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10339 = torch.aten.sub.Tensor %10334, %result1_298, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10340 = torch.aten.mul.Tensor %10339, %10338 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %10341 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10342 = torch.aten.mul.Tensor %10340, %10341 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %10343 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10344 = torch.aten.add.Tensor %10342, %10343, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10345 = torch.prims.convert_element_type %10344, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10346 = torch.prims.convert_element_type %result1_298, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10347 = torch.prims.convert_element_type %10338, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10348 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10349 = torch.aten.view %10345, %10348 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10350 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10351 = torch.aten.transpose.int %10350, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %10352 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10353 = torch.prims.convert_element_type %10352, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10354 = torch.prims.convert_element_type %10349, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10355 = torch.prims.convert_element_type %10351, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10356 = torch.aten.mm %10354, %10355 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10357 = torch.aten.mul.Scalar %10356, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10358 = torch.aten.mul.Scalar %10353, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10359 = torch.aten.add.Tensor %10357, %10358, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10360 = torch.prims.convert_element_type %10359, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10361 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10362 = torch.aten.view %10360, %10361 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10363 = torch.aten.slice.Tensor %10362, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10364 = torch.aten.slice.Tensor %10362, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10365 = torch.aten.gelu %10364, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10366 = torch.aten.mul.Tensor %10363, %10365 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10367 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10368 = torch.aten.view %10366, %10367 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %10369 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10370 = torch.aten.transpose.int %10369, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %10371 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10372 = torch.prims.convert_element_type %10371, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10373 = torch.prims.convert_element_type %10368, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10374 = torch.prims.convert_element_type %10370, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10375 = torch.aten.mm %10373, %10374 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10376 = torch.aten.mul.Scalar %10375, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10377 = torch.aten.mul.Scalar %10372, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10378 = torch.aten.add.Tensor %10376, %10377, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10379 = torch.prims.convert_element_type %10378, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10380 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10381 = torch.aten.view %10379, %10380 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10382 = torch.aten.add.Tensor %10381, %10334, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10383 = torch.prims.convert_element_type %10382, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10384 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_299, %result1_300 = torch.aten.var_mean.correction %10383, %10384, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10385 = torch.aten.add.Scalar %result0_299, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10386 = torch.aten.rsqrt %10385 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10387 = torch.aten.sub.Tensor %10382, %result1_300, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10388 = torch.aten.mul.Tensor %10387, %10386 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %10389 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10390 = torch.aten.mul.Tensor %10388, %10389 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %10391 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10392 = torch.aten.add.Tensor %10390, %10391, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10393 = torch.prims.convert_element_type %10392, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10394 = torch.prims.convert_element_type %result1_300, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10395 = torch.prims.convert_element_type %10386, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %10396 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10397 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10398 = torch.aten.view %10393, %10397 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10399 = torch_c.to_builtin_tensor %10398 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10400 = torch_c.to_builtin_tensor %10396 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10401 = tensor.empty() : tensor<2048x1280xf32>
    %10402 = linalg.fill ins(%cst : f32) outs(%10401 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10403 = tensor.empty() : tensor<2048x1280xf32>
    %10404 = linalg.fill ins(%cst : f32) outs(%10403 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10405:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10402, %10404, %10399, %10400, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10402, %10404)
    %10406 = arith.truncf %10405#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10407 = torch_c.from_builtin_tensor %10406 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10408 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10409 = torch.aten.view %10407, %10408 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %10410 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10411 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10412 = torch.aten.view %10393, %10411 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10413 = torch_c.to_builtin_tensor %10412 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10414 = torch_c.to_builtin_tensor %10410 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10415 = tensor.empty() : tensor<2048x1280xf32>
    %10416 = linalg.fill ins(%cst : f32) outs(%10415 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10417 = tensor.empty() : tensor<2048x1280xf32>
    %10418 = linalg.fill ins(%cst : f32) outs(%10417 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10419:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10416, %10418, %10413, %10414, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10416, %10418)
    %10420 = arith.truncf %10419#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10421 = torch_c.from_builtin_tensor %10420 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10422 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10423 = torch.aten.view %10421, %10422 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %10424 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10425 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10426 = torch.aten.view %10393, %10425 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10427 = torch_c.to_builtin_tensor %10426 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10428 = torch_c.to_builtin_tensor %10424 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10429 = tensor.empty() : tensor<2048x1280xf32>
    %10430 = linalg.fill ins(%cst : f32) outs(%10429 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10431 = tensor.empty() : tensor<2048x1280xf32>
    %10432 = linalg.fill ins(%cst : f32) outs(%10431 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10433:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10430, %10432, %10427, %10428, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10430, %10432)
    %10434 = arith.truncf %10433#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10435 = torch_c.from_builtin_tensor %10434 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10436 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10437 = torch.aten.view %10435, %10436 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10438 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10439 = torch.aten.view %10409, %10438 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10440 = torch.aten.transpose.int %10439, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10441 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10442 = torch.aten.view %10423, %10441 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10443 = torch.aten.transpose.int %10442, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10444 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10445 = torch.aten.view %10437, %10444 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10446 = torch.aten.transpose.int %10445, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10447:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10440, %10443, %10446, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10448 = torch.aten.transpose.int %10447#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10449 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10450 = torch.aten.view %10448, %10449 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10451 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10452 = torch.aten.view %10450, %10451 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10453 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10454 = torch.aten.transpose.int %10453, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %10455 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10456 = torch.prims.convert_element_type %10455, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10457 = torch.prims.convert_element_type %10452, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10458 = torch.prims.convert_element_type %10454, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10459 = torch.aten.mm %10457, %10458 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10460 = torch.aten.mul.Scalar %10459, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10461 = torch.aten.mul.Scalar %10456, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10462 = torch.aten.add.Tensor %10460, %10461, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10463 = torch.prims.convert_element_type %10462, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10464 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10465 = torch.aten.view %10463, %10464 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10466 = torch.aten.div.Scalar %10465, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10467 = torch.aten.add.Tensor %10466, %10382, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10468 = torch.prims.convert_element_type %10467, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10469 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_301, %result1_302 = torch.aten.var_mean.correction %10468, %10469, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10470 = torch.aten.add.Scalar %result0_301, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10471 = torch.aten.rsqrt %10470 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10472 = torch.aten.sub.Tensor %10467, %result1_302, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10473 = torch.aten.mul.Tensor %10472, %10471 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %10474 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10475 = torch.aten.mul.Tensor %10473, %10474 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %10476 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10477 = torch.aten.add.Tensor %10475, %10476, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10478 = torch.prims.convert_element_type %10477, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10479 = torch.prims.convert_element_type %result1_302, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10480 = torch.prims.convert_element_type %10471, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %10481 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10482 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10483 = torch.aten.view %10478, %10482 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10484 = torch_c.to_builtin_tensor %10483 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10485 = torch_c.to_builtin_tensor %10481 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10486 = tensor.empty() : tensor<2048x1280xf32>
    %10487 = linalg.fill ins(%cst : f32) outs(%10486 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10488 = tensor.empty() : tensor<2048x1280xf32>
    %10489 = linalg.fill ins(%cst : f32) outs(%10488 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10490:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10487, %10489, %10484, %10485, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10487, %10489)
    %10491 = arith.truncf %10490#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10492 = torch_c.from_builtin_tensor %10491 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10493 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10494 = torch.aten.view %10492, %10493 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %10495 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10496 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10497 = torch.aten.view %4, %10496 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10498 = torch_c.to_builtin_tensor %10497 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10499 = torch_c.to_builtin_tensor %10495 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10500 = tensor.empty() : tensor<128x1280xf32>
    %10501 = linalg.fill ins(%cst : f32) outs(%10500 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10502 = tensor.empty() : tensor<128x1280xf32>
    %10503 = linalg.fill ins(%cst : f32) outs(%10502 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10504:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10501, %10503, %10498, %10499, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10501, %10503)
    %10505 = arith.truncf %10504#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10506 = torch_c.from_builtin_tensor %10505 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10507 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10508 = torch.aten.view %10506, %10507 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %10509 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10510 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10511 = torch.aten.view %4, %10510 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10512 = torch_c.to_builtin_tensor %10511 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10513 = torch_c.to_builtin_tensor %10509 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10514 = tensor.empty() : tensor<128x1280xf32>
    %10515 = linalg.fill ins(%cst : f32) outs(%10514 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10516 = tensor.empty() : tensor<128x1280xf32>
    %10517 = linalg.fill ins(%cst : f32) outs(%10516 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10518:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10515, %10517, %10512, %10513, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10515, %10517)
    %10519 = arith.truncf %10518#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10520 = torch_c.from_builtin_tensor %10519 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10521 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10522 = torch.aten.view %10520, %10521 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10523 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10524 = torch.aten.view %10494, %10523 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10525 = torch.aten.transpose.int %10524, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10526 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10527 = torch.aten.view %10508, %10526 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10528 = torch.aten.transpose.int %10527, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10529 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10530 = torch.aten.view %10522, %10529 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10531 = torch.aten.transpose.int %10530, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10532:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10525, %10528, %10531, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10533 = torch.aten.transpose.int %10532#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10534 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10535 = torch.aten.view %10533, %10534 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10536 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10537 = torch.aten.view %10535, %10536 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10538 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10539 = torch.aten.transpose.int %10538, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %10540 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10541 = torch.prims.convert_element_type %10540, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10542 = torch.prims.convert_element_type %10537, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10543 = torch.prims.convert_element_type %10539, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10544 = torch.aten.mm %10542, %10543 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10545 = torch.aten.mul.Scalar %10544, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10546 = torch.aten.mul.Scalar %10541, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10547 = torch.aten.add.Tensor %10545, %10546, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10548 = torch.prims.convert_element_type %10547, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10549 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10550 = torch.aten.view %10548, %10549 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10551 = torch.aten.div.Scalar %10550, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10552 = torch.aten.add.Tensor %10551, %10467, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10553 = torch.prims.convert_element_type %10552, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10554 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_303, %result1_304 = torch.aten.var_mean.correction %10553, %10554, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10555 = torch.aten.add.Scalar %result0_303, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10556 = torch.aten.rsqrt %10555 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10557 = torch.aten.sub.Tensor %10552, %result1_304, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10558 = torch.aten.mul.Tensor %10557, %10556 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %10559 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10560 = torch.aten.mul.Tensor %10558, %10559 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %10561 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10562 = torch.aten.add.Tensor %10560, %10561, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10563 = torch.prims.convert_element_type %10562, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10564 = torch.prims.convert_element_type %result1_304, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10565 = torch.prims.convert_element_type %10556, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10566 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10567 = torch.aten.view %10563, %10566 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10568 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10569 = torch.aten.transpose.int %10568, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %10570 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10571 = torch.prims.convert_element_type %10570, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10572 = torch.prims.convert_element_type %10567, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10573 = torch.prims.convert_element_type %10569, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10574 = torch.aten.mm %10572, %10573 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10575 = torch.aten.mul.Scalar %10574, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10576 = torch.aten.mul.Scalar %10571, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10577 = torch.aten.add.Tensor %10575, %10576, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10578 = torch.prims.convert_element_type %10577, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10579 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10580 = torch.aten.view %10578, %10579 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10581 = torch.aten.slice.Tensor %10580, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10582 = torch.aten.slice.Tensor %10580, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10583 = torch.aten.gelu %10582, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10584 = torch.aten.mul.Tensor %10581, %10583 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10585 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10586 = torch.aten.view %10584, %10585 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %10587 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10588 = torch.aten.transpose.int %10587, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %10589 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10590 = torch.prims.convert_element_type %10589, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10591 = torch.prims.convert_element_type %10586, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10592 = torch.prims.convert_element_type %10588, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10593 = torch.aten.mm %10591, %10592 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10594 = torch.aten.mul.Scalar %10593, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10595 = torch.aten.mul.Scalar %10590, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10596 = torch.aten.add.Tensor %10594, %10595, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10597 = torch.prims.convert_element_type %10596, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10598 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10599 = torch.aten.view %10597, %10598 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10600 = torch.aten.add.Tensor %10599, %10552, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10601 = torch.prims.convert_element_type %10600, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10602 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_305, %result1_306 = torch.aten.var_mean.correction %10601, %10602, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10603 = torch.aten.add.Scalar %result0_305, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10604 = torch.aten.rsqrt %10603 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10605 = torch.aten.sub.Tensor %10600, %result1_306, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10606 = torch.aten.mul.Tensor %10605, %10604 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %10607 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10608 = torch.aten.mul.Tensor %10606, %10607 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %10609 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10610 = torch.aten.add.Tensor %10608, %10609, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10611 = torch.prims.convert_element_type %10610, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10612 = torch.prims.convert_element_type %result1_306, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10613 = torch.prims.convert_element_type %10604, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %10614 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10615 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10616 = torch.aten.view %10611, %10615 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10617 = torch_c.to_builtin_tensor %10616 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10618 = torch_c.to_builtin_tensor %10614 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10619 = tensor.empty() : tensor<2048x1280xf32>
    %10620 = linalg.fill ins(%cst : f32) outs(%10619 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10621 = tensor.empty() : tensor<2048x1280xf32>
    %10622 = linalg.fill ins(%cst : f32) outs(%10621 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10623:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10620, %10622, %10617, %10618, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10620, %10622)
    %10624 = arith.truncf %10623#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10625 = torch_c.from_builtin_tensor %10624 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10626 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10627 = torch.aten.view %10625, %10626 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %10628 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10629 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10630 = torch.aten.view %10611, %10629 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10631 = torch_c.to_builtin_tensor %10630 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10632 = torch_c.to_builtin_tensor %10628 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10633 = tensor.empty() : tensor<2048x1280xf32>
    %10634 = linalg.fill ins(%cst : f32) outs(%10633 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10635 = tensor.empty() : tensor<2048x1280xf32>
    %10636 = linalg.fill ins(%cst : f32) outs(%10635 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10637:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10634, %10636, %10631, %10632, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10634, %10636)
    %10638 = arith.truncf %10637#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10639 = torch_c.from_builtin_tensor %10638 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10640 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10641 = torch.aten.view %10639, %10640 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %10642 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10643 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10644 = torch.aten.view %10611, %10643 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10645 = torch_c.to_builtin_tensor %10644 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10646 = torch_c.to_builtin_tensor %10642 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10647 = tensor.empty() : tensor<2048x1280xf32>
    %10648 = linalg.fill ins(%cst : f32) outs(%10647 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10649 = tensor.empty() : tensor<2048x1280xf32>
    %10650 = linalg.fill ins(%cst : f32) outs(%10649 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10651:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10648, %10650, %10645, %10646, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10648, %10650)
    %10652 = arith.truncf %10651#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10653 = torch_c.from_builtin_tensor %10652 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10654 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10655 = torch.aten.view %10653, %10654 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10656 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10657 = torch.aten.view %10627, %10656 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10658 = torch.aten.transpose.int %10657, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10659 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10660 = torch.aten.view %10641, %10659 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10661 = torch.aten.transpose.int %10660, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10662 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10663 = torch.aten.view %10655, %10662 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10664 = torch.aten.transpose.int %10663, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10665:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10658, %10661, %10664, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10666 = torch.aten.transpose.int %10665#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10667 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10668 = torch.aten.view %10666, %10667 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10669 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10670 = torch.aten.view %10668, %10669 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %10671 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10672 = torch.aten.transpose.int %10671, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %10673 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10674 = torch.prims.convert_element_type %10673, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10675 = torch.prims.convert_element_type %10670, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10676 = torch.prims.convert_element_type %10672, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10677 = torch.aten.mm %10675, %10676 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10678 = torch.aten.mul.Scalar %10677, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10679 = torch.aten.mul.Scalar %10674, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10680 = torch.aten.add.Tensor %10678, %10679, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10681 = torch.prims.convert_element_type %10680, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10682 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10683 = torch.aten.view %10681, %10682 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10684 = torch.aten.div.Scalar %10683, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10685 = torch.aten.add.Tensor %10684, %10600, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10686 = torch.prims.convert_element_type %10685, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10687 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_307, %result1_308 = torch.aten.var_mean.correction %10686, %10687, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10688 = torch.aten.add.Scalar %result0_307, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10689 = torch.aten.rsqrt %10688 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10690 = torch.aten.sub.Tensor %10685, %result1_308, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10691 = torch.aten.mul.Tensor %10690, %10689 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %10692 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10693 = torch.aten.mul.Tensor %10691, %10692 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %10694 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10695 = torch.aten.add.Tensor %10693, %10694, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10696 = torch.prims.convert_element_type %10695, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10697 = torch.prims.convert_element_type %result1_308, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10698 = torch.prims.convert_element_type %10689, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %10699 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10700 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10701 = torch.aten.view %10696, %10700 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10702 = torch_c.to_builtin_tensor %10701 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10703 = torch_c.to_builtin_tensor %10699 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10704 = tensor.empty() : tensor<2048x1280xf32>
    %10705 = linalg.fill ins(%cst : f32) outs(%10704 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10706 = tensor.empty() : tensor<2048x1280xf32>
    %10707 = linalg.fill ins(%cst : f32) outs(%10706 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10708:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10705, %10707, %10702, %10703, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10705, %10707)
    %10709 = arith.truncf %10708#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10710 = torch_c.from_builtin_tensor %10709 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10711 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10712 = torch.aten.view %10710, %10711 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %10713 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10714 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10715 = torch.aten.view %4, %10714 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10716 = torch_c.to_builtin_tensor %10715 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10717 = torch_c.to_builtin_tensor %10713 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10718 = tensor.empty() : tensor<128x1280xf32>
    %10719 = linalg.fill ins(%cst : f32) outs(%10718 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10720 = tensor.empty() : tensor<128x1280xf32>
    %10721 = linalg.fill ins(%cst : f32) outs(%10720 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10722:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10719, %10721, %10716, %10717, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10719, %10721)
    %10723 = arith.truncf %10722#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10724 = torch_c.from_builtin_tensor %10723 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10725 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10726 = torch.aten.view %10724, %10725 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %10727 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %10728 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %10729 = torch.aten.view %4, %10728 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %10730 = torch_c.to_builtin_tensor %10729 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %10731 = torch_c.to_builtin_tensor %10727 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %10732 = tensor.empty() : tensor<128x1280xf32>
    %10733 = linalg.fill ins(%cst : f32) outs(%10732 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10734 = tensor.empty() : tensor<128x1280xf32>
    %10735 = linalg.fill ins(%cst : f32) outs(%10734 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %10736:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %10733, %10735, %10730, %10731, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10733, %10735)
    %10737 = arith.truncf %10736#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %10738 = torch_c.from_builtin_tensor %10737 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %10739 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10740 = torch.aten.view %10738, %10739 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %10741 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10742 = torch.aten.view %10712, %10741 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %10743 = torch.aten.transpose.int %10742, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %10744 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10745 = torch.aten.view %10726, %10744 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10746 = torch.aten.transpose.int %10745, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10747 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10748 = torch.aten.view %10740, %10747 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %10749 = torch.aten.transpose.int %10748, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %10750:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%10743, %10746, %10749, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %10751 = torch.aten.transpose.int %10750#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %10752 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10753 = torch.aten.view %10751, %10752 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10754 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10755 = torch.aten.view %10753, %10754 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %10756 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10757 = torch.aten.transpose.int %10756, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %10758 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10759 = torch.prims.convert_element_type %10758, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10760 = torch.prims.convert_element_type %10755, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10761 = torch.prims.convert_element_type %10757, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10762 = torch.aten.mm %10760, %10761 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10763 = torch.aten.mul.Scalar %10762, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10764 = torch.aten.mul.Scalar %10759, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10765 = torch.aten.add.Tensor %10763, %10764, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10766 = torch.prims.convert_element_type %10765, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10767 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10768 = torch.aten.view %10766, %10767 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10769 = torch.aten.div.Scalar %10768, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %10770 = torch.aten.add.Tensor %10769, %10685, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10771 = torch.prims.convert_element_type %10770, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10772 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_309, %result1_310 = torch.aten.var_mean.correction %10771, %10772, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10773 = torch.aten.add.Scalar %result0_309, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10774 = torch.aten.rsqrt %10773 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10775 = torch.aten.sub.Tensor %10770, %result1_310, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10776 = torch.aten.mul.Tensor %10775, %10774 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %10777 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10778 = torch.aten.mul.Tensor %10776, %10777 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %10779 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10780 = torch.aten.add.Tensor %10778, %10779, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10781 = torch.prims.convert_element_type %10780, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10782 = torch.prims.convert_element_type %result1_310, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10783 = torch.prims.convert_element_type %10774, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %10784 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10785 = torch.aten.view %10781, %10784 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %10786 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %10787 = torch.aten.transpose.int %10786, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %10788 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %10789 = torch.prims.convert_element_type %10788, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %10790 = torch.prims.convert_element_type %10785, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10791 = torch.prims.convert_element_type %10787, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %10792 = torch.aten.mm %10790, %10791 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %10793 = torch.aten.mul.Scalar %10792, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10794 = torch.aten.mul.Scalar %10789, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %10795 = torch.aten.add.Tensor %10793, %10794, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %10796 = torch.prims.convert_element_type %10795, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %10797 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10798 = torch.aten.view %10796, %10797 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %10799 = torch.aten.slice.Tensor %10798, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10800 = torch.aten.slice.Tensor %10798, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %10801 = torch.aten.gelu %10800, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %10802 = torch.aten.mul.Tensor %10799, %10801 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %10803 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %10804 = torch.aten.view %10802, %10803 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %10805 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %10806 = torch.aten.transpose.int %10805, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %10807 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10808 = torch.prims.convert_element_type %10807, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10809 = torch.prims.convert_element_type %10804, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %10810 = torch.prims.convert_element_type %10806, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %10811 = torch.aten.mm %10809, %10810 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10812 = torch.aten.mul.Scalar %10811, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10813 = torch.aten.mul.Scalar %10808, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10814 = torch.aten.add.Tensor %10812, %10813, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10815 = torch.prims.convert_element_type %10814, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10816 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10817 = torch.aten.view %10815, %10816 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10818 = torch.aten.add.Tensor %10817, %10770, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10819 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10820 = torch.aten.view %10818, %10819 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_out.weight = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %10821 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10822 = torch.aten.transpose.int %10821, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.0.proj_out.bias = util.global.load @_params.unet.up_blocks.0.attentions.0.proj_out.bias : tensor<1280xf16>
    %10823 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10824 = torch.prims.convert_element_type %10823, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10825 = torch.prims.convert_element_type %10820, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10826 = torch.prims.convert_element_type %10822, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10827 = torch.aten.mm %10825, %10826 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %10828 = torch.aten.mul.Scalar %10827, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10829 = torch.aten.mul.Scalar %10824, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10830 = torch.aten.add.Tensor %10828, %10829, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %10831 = torch.prims.convert_element_type %10830, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %10832 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10833 = torch.aten.view %10831, %10832 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %10834 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10835 = torch.aten.view %10833, %10834 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %10836 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10837 = torch.aten.permute %10835, %10836 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %10838 = torch.aten.add.Tensor %10837, %8587, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10839 = torch.prim.ListConstruct %10838, %3696 : (!torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>) -> !torch.list<vtensor>
    %10840 = torch.aten.cat %10839, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %10841 = torch.prim.ListConstruct %int2, %int32, %int80, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10842 = torch.aten.view %10840, %10841 : !torch.vtensor<[2,2560,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,80,1024],f16>
    %10843 = torch.prims.convert_element_type %10842, %int6 : !torch.vtensor<[2,32,80,1024],f16>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %10844 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_311, %result1_312 = torch.aten.var_mean.correction %10843, %10844, %int0, %true : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %10845 = torch.aten.add.Scalar %result0_311, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %10846 = torch.aten.rsqrt %10845 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %10847 = torch.aten.sub.Tensor %10842, %result1_312, %int1 : !torch.vtensor<[2,32,80,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,80,1024],f32>
    %10848 = torch.aten.mul.Tensor %10847, %10846 : !torch.vtensor<[2,32,80,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,80,1024],f32>
    %10849 = torch.prim.ListConstruct %int2, %int2560, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10850 = torch.aten.view %10848, %10849 : !torch.vtensor<[2,32,80,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,2560,32,32],f32>
    %_params.unet.up_blocks.0.resnets.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.norm1.bias : tensor<2560xf16>
    %10851 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm1.bias : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %10852 = torch.aten.unsqueeze %10851, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %10853 = torch.aten.unsqueeze %10852, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %10854 = torch.aten.unsqueeze %10853, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.norm1.weight : tensor<2560xf16>
    %10855 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm1.weight : tensor<2560xf16> -> !torch.vtensor<[2560],f16>
    %10856 = torch.aten.unsqueeze %10855, %int0 : !torch.vtensor<[2560],f16>, !torch.int -> !torch.vtensor<[1,2560],f16>
    %10857 = torch.aten.unsqueeze %10856, %int2 : !torch.vtensor<[1,2560],f16>, !torch.int -> !torch.vtensor<[1,2560,1],f16>
    %10858 = torch.aten.unsqueeze %10857, %int3 : !torch.vtensor<[1,2560,1],f16>, !torch.int -> !torch.vtensor<[1,2560,1,1],f16>
    %10859 = torch.aten.mul.Tensor %10850, %10858 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16> -> !torch.vtensor<[2,2560,32,32],f32>
    %10860 = torch.aten.add.Tensor %10859, %10854, %int1 : !torch.vtensor<[2,2560,32,32],f32>, !torch.vtensor<[1,2560,1,1],f16>, !torch.int -> !torch.vtensor<[2,2560,32,32],f32>
    %10861 = torch.prims.convert_element_type %10860, %int5 : !torch.vtensor<[2,2560,32,32],f32>, !torch.int -> !torch.vtensor<[2,2560,32,32],f16>
    %10862 = torch.prims.convert_element_type %result1_312, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10863 = torch.prims.convert_element_type %10846, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10864 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10865 = torch.prims.squeeze %10862, %10864 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10866 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10867 = torch.prims.squeeze %10865, %10866 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10868 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10869 = torch.prims.squeeze %10863, %10868 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10870 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10871 = torch.prims.squeeze %10869, %10870 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10872 = torch.aten.silu %10861 : !torch.vtensor<[2,2560,32,32],f16> -> !torch.vtensor<[2,2560,32,32],f16>
    %_params.unet.up_blocks.0.resnets.1.conv1.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.conv1.weight : tensor<1280x2560x3x3xf16>
    %10873 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv1.weight : tensor<1280x2560x3x3xf16> -> !torch.vtensor<[1280,2560,3,3],f16>
    %_params.unet.up_blocks.0.resnets.1.conv1.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.conv1.bias : tensor<1280xf16>
    %10874 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10875 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10876 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10877 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10878 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10879 = torch.aten.convolution %10872, %10873, %10874, %10875, %10876, %10877, %false, %10878, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10880 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %10881 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10882 = torch.aten.transpose.int %10881, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %10883 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10884 = torch.prims.convert_element_type %10883, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %10885 = torch.prims.convert_element_type %10880, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %10886 = torch.prims.convert_element_type %10882, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %10887 = torch.aten.mm %10885, %10886 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %10888 = torch.aten.mul.Scalar %10887, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %10889 = torch.aten.mul.Scalar %10884, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %10890 = torch.aten.add.Tensor %10888, %10889, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %10891 = torch.prims.convert_element_type %10890, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %10892 = torch.aten.unsqueeze %10891, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %10893 = torch.aten.unsqueeze %10892, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %10894 = torch.aten.add.Tensor %10879, %10893, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10895 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10896 = torch.aten.view %10894, %10895 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %10897 = torch.prims.convert_element_type %10896, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10898 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_313, %result1_314 = torch.aten.var_mean.correction %10897, %10898, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %10899 = torch.aten.add.Scalar %result0_313, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %10900 = torch.aten.rsqrt %10899 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %10901 = torch.aten.sub.Tensor %10896, %result1_314, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10902 = torch.aten.mul.Tensor %10901, %10900 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %10903 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10904 = torch.aten.view %10902, %10903 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.resnets.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.norm2.bias : tensor<1280xf16>
    %10905 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10906 = torch.aten.unsqueeze %10905, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10907 = torch.aten.unsqueeze %10906, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10908 = torch.aten.unsqueeze %10907, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.resnets.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.norm2.weight : tensor<1280xf16>
    %10909 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10910 = torch.aten.unsqueeze %10909, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10911 = torch.aten.unsqueeze %10910, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10912 = torch.aten.unsqueeze %10911, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %10913 = torch.aten.mul.Tensor %10904, %10912 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %10914 = torch.aten.add.Tensor %10913, %10908, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %10915 = torch.prims.convert_element_type %10914, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10916 = torch.prims.convert_element_type %result1_314, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10917 = torch.prims.convert_element_type %10900, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10918 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10919 = torch.prims.squeeze %10916, %10918 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10920 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10921 = torch.prims.squeeze %10919, %10920 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10922 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10923 = torch.prims.squeeze %10917, %10922 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10924 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10925 = torch.prims.squeeze %10923, %10924 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10926 = torch.aten.silu %10915 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.1.conv2.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %10927 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.resnets.1.conv2.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.conv2.bias : tensor<1280xf16>
    %10928 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10929 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10930 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10931 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10932 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10933 = torch.aten.convolution %10926, %10927, %10928, %10929, %10930, %10931, %false, %10932, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight : tensor<1280x2560x1x1xf16>
    %10934 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv_shortcut.weight : tensor<1280x2560x1x1xf16> -> !torch.vtensor<[1280,2560,1,1],f16>
    %_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias : tensor<1280xf16>
    %10935 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.1.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10936 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10937 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10938 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %10939 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %10940 = torch.aten.convolution %10840, %10934, %10935, %10936, %10937, %10938, %false, %10939, %int1 : !torch.vtensor<[2,2560,32,32],f16>, !torch.vtensor<[1280,2560,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10941 = torch.aten.add.Tensor %10940, %10933, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10942 = torch.aten.div.Scalar %10941, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %10943 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10944 = torch.aten.view %10942, %10943 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %10945 = torch.prims.convert_element_type %10944, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10946 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_315, %result1_316 = torch.aten.var_mean.correction %10945, %10946, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %10947 = torch.aten.add.Scalar %result0_315, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %10948 = torch.aten.rsqrt %10947 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %10949 = torch.aten.sub.Tensor %10944, %result1_316, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %10950 = torch.aten.mul.Tensor %10949, %10948 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %10951 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10952 = torch.aten.view %10950, %10951 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.attentions.1.norm.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.norm.bias : tensor<1280xf16>
    %10953 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10954 = torch.aten.unsqueeze %10953, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10955 = torch.aten.unsqueeze %10954, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10956 = torch.aten.unsqueeze %10955, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.attentions.1.norm.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.norm.weight : tensor<1280xf16>
    %10957 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10958 = torch.aten.unsqueeze %10957, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %10959 = torch.aten.unsqueeze %10958, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %10960 = torch.aten.unsqueeze %10959, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %10961 = torch.aten.mul.Tensor %10952, %10960 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %10962 = torch.aten.add.Tensor %10961, %10956, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %10963 = torch.prims.convert_element_type %10962, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %10964 = torch.prims.convert_element_type %result1_316, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10965 = torch.prims.convert_element_type %10948, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %10966 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10967 = torch.prims.squeeze %10964, %10966 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10968 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10969 = torch.prims.squeeze %10967, %10968 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10970 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %10971 = torch.prims.squeeze %10965, %10970 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %10972 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %10973 = torch.prims.squeeze %10971, %10972 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %10974 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10975 = torch.aten.permute %10963, %10974 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %10976 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10977 = torch.aten.view %10975, %10976 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_in.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_in.weight : tensor<1280x1280xf16>
    %10978 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %10979 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %10980 = torch.aten._unsafe_view %10977, %10979 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %10981 = torch_c.to_builtin_tensor %10980 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %10982 = torch_c.to_builtin_tensor %10978 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %10983 = tensor.empty() : tensor<2048x1280xf32>
    %10984 = linalg.fill ins(%cst : f32) outs(%10983 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10985 = tensor.empty() : tensor<2048x1280xf32>
    %10986 = linalg.fill ins(%cst : f32) outs(%10985 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %10987:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %10984, %10986, %10981, %10982, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%10984, %10986)
    %10988 = arith.truncf %10987#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %10989 = torch_c.from_builtin_tensor %10988 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %10990 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %10991 = torch.aten.view %10989, %10990 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_in.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_in.bias : tensor<1280xf16>
    %10992 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %10993 = torch.aten.add.Tensor %10991, %10992, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %10994 = torch.prims.convert_element_type %10993, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10995 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_317, %result1_318 = torch.aten.var_mean.correction %10994, %10995, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %10996 = torch.aten.add.Scalar %result0_317, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %10997 = torch.aten.rsqrt %10996 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %10998 = torch.aten.sub.Tensor %10993, %result1_318, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %10999 = torch.aten.mul.Tensor %10998, %10997 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %11000 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11001 = torch.aten.mul.Tensor %10999, %11000 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %11002 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11003 = torch.aten.add.Tensor %11001, %11002, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11004 = torch.prims.convert_element_type %11003, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11005 = torch.prims.convert_element_type %result1_318, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11006 = torch.prims.convert_element_type %10997, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %11007 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11008 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11009 = torch.aten.view %11004, %11008 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11010 = torch_c.to_builtin_tensor %11009 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11011 = torch_c.to_builtin_tensor %11007 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11012 = tensor.empty() : tensor<2048x1280xf32>
    %11013 = linalg.fill ins(%cst : f32) outs(%11012 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11014 = tensor.empty() : tensor<2048x1280xf32>
    %11015 = linalg.fill ins(%cst : f32) outs(%11014 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11016:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11013, %11015, %11010, %11011, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11013, %11015)
    %11017 = arith.truncf %11016#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11018 = torch_c.from_builtin_tensor %11017 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11019 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11020 = torch.aten.view %11018, %11019 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %11021 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11022 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11023 = torch.aten.view %11004, %11022 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11024 = torch_c.to_builtin_tensor %11023 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11025 = torch_c.to_builtin_tensor %11021 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11026 = tensor.empty() : tensor<2048x1280xf32>
    %11027 = linalg.fill ins(%cst : f32) outs(%11026 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11028 = tensor.empty() : tensor<2048x1280xf32>
    %11029 = linalg.fill ins(%cst : f32) outs(%11028 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11030:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11027, %11029, %11024, %11025, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11027, %11029)
    %11031 = arith.truncf %11030#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11032 = torch_c.from_builtin_tensor %11031 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11033 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11034 = torch.aten.view %11032, %11033 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %11035 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11036 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11037 = torch.aten.view %11004, %11036 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11038 = torch_c.to_builtin_tensor %11037 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11039 = torch_c.to_builtin_tensor %11035 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11040 = tensor.empty() : tensor<2048x1280xf32>
    %11041 = linalg.fill ins(%cst : f32) outs(%11040 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11042 = tensor.empty() : tensor<2048x1280xf32>
    %11043 = linalg.fill ins(%cst : f32) outs(%11042 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11044:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11041, %11043, %11038, %11039, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11041, %11043)
    %11045 = arith.truncf %11044#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11046 = torch_c.from_builtin_tensor %11045 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11047 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11048 = torch.aten.view %11046, %11047 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11049 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11050 = torch.aten.view %11020, %11049 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11051 = torch.aten.transpose.int %11050, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11052 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11053 = torch.aten.view %11034, %11052 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11054 = torch.aten.transpose.int %11053, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11055 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11056 = torch.aten.view %11048, %11055 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11057 = torch.aten.transpose.int %11056, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11058:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11051, %11054, %11057, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11059 = torch.aten.transpose.int %11058#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11060 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11061 = torch.aten.view %11059, %11060 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11062 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11063 = torch.aten.view %11061, %11062 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11064 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11065 = torch.aten.transpose.int %11064, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %11066 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11067 = torch.prims.convert_element_type %11066, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11068 = torch.prims.convert_element_type %11063, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11069 = torch.prims.convert_element_type %11065, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11070 = torch.aten.mm %11068, %11069 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11071 = torch.aten.mul.Scalar %11070, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11072 = torch.aten.mul.Scalar %11067, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11073 = torch.aten.add.Tensor %11071, %11072, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11074 = torch.prims.convert_element_type %11073, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11075 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11076 = torch.aten.view %11074, %11075 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11077 = torch.aten.div.Scalar %11076, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11078 = torch.aten.add.Tensor %11077, %10993, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11079 = torch.prims.convert_element_type %11078, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11080 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_319, %result1_320 = torch.aten.var_mean.correction %11079, %11080, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11081 = torch.aten.add.Scalar %result0_319, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11082 = torch.aten.rsqrt %11081 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11083 = torch.aten.sub.Tensor %11078, %result1_320, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11084 = torch.aten.mul.Tensor %11083, %11082 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %11085 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11086 = torch.aten.mul.Tensor %11084, %11085 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %11087 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11088 = torch.aten.add.Tensor %11086, %11087, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11089 = torch.prims.convert_element_type %11088, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11090 = torch.prims.convert_element_type %result1_320, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11091 = torch.prims.convert_element_type %11082, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %11092 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11093 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11094 = torch.aten.view %11089, %11093 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11095 = torch_c.to_builtin_tensor %11094 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11096 = torch_c.to_builtin_tensor %11092 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11097 = tensor.empty() : tensor<2048x1280xf32>
    %11098 = linalg.fill ins(%cst : f32) outs(%11097 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11099 = tensor.empty() : tensor<2048x1280xf32>
    %11100 = linalg.fill ins(%cst : f32) outs(%11099 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11101:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11098, %11100, %11095, %11096, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11098, %11100)
    %11102 = arith.truncf %11101#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11103 = torch_c.from_builtin_tensor %11102 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11104 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11105 = torch.aten.view %11103, %11104 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %11106 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11107 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11108 = torch.aten.view %4, %11107 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11109 = torch_c.to_builtin_tensor %11108 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11110 = torch_c.to_builtin_tensor %11106 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11111 = tensor.empty() : tensor<128x1280xf32>
    %11112 = linalg.fill ins(%cst : f32) outs(%11111 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11113 = tensor.empty() : tensor<128x1280xf32>
    %11114 = linalg.fill ins(%cst : f32) outs(%11113 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11115:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11112, %11114, %11109, %11110, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11112, %11114)
    %11116 = arith.truncf %11115#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11117 = torch_c.from_builtin_tensor %11116 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11118 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11119 = torch.aten.view %11117, %11118 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %11120 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11121 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11122 = torch.aten.view %4, %11121 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11123 = torch_c.to_builtin_tensor %11122 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11124 = torch_c.to_builtin_tensor %11120 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11125 = tensor.empty() : tensor<128x1280xf32>
    %11126 = linalg.fill ins(%cst : f32) outs(%11125 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11127 = tensor.empty() : tensor<128x1280xf32>
    %11128 = linalg.fill ins(%cst : f32) outs(%11127 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11129:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11126, %11128, %11123, %11124, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11126, %11128)
    %11130 = arith.truncf %11129#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11131 = torch_c.from_builtin_tensor %11130 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11132 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11133 = torch.aten.view %11131, %11132 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11134 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11135 = torch.aten.view %11105, %11134 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11136 = torch.aten.transpose.int %11135, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11137 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11138 = torch.aten.view %11119, %11137 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11139 = torch.aten.transpose.int %11138, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11140 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11141 = torch.aten.view %11133, %11140 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11142 = torch.aten.transpose.int %11141, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11143:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11136, %11139, %11142, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11144 = torch.aten.transpose.int %11143#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11145 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11146 = torch.aten.view %11144, %11145 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11147 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11148 = torch.aten.view %11146, %11147 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11149 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11150 = torch.aten.transpose.int %11149, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %11151 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11152 = torch.prims.convert_element_type %11151, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11153 = torch.prims.convert_element_type %11148, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11154 = torch.prims.convert_element_type %11150, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11155 = torch.aten.mm %11153, %11154 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11156 = torch.aten.mul.Scalar %11155, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11157 = torch.aten.mul.Scalar %11152, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11158 = torch.aten.add.Tensor %11156, %11157, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11159 = torch.prims.convert_element_type %11158, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11160 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11161 = torch.aten.view %11159, %11160 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11162 = torch.aten.div.Scalar %11161, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11163 = torch.aten.add.Tensor %11162, %11078, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11164 = torch.prims.convert_element_type %11163, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11165 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_321, %result1_322 = torch.aten.var_mean.correction %11164, %11165, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11166 = torch.aten.add.Scalar %result0_321, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11167 = torch.aten.rsqrt %11166 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11168 = torch.aten.sub.Tensor %11163, %result1_322, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11169 = torch.aten.mul.Tensor %11168, %11167 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %11170 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11171 = torch.aten.mul.Tensor %11169, %11170 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %11172 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11173 = torch.aten.add.Tensor %11171, %11172, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11174 = torch.prims.convert_element_type %11173, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11175 = torch.prims.convert_element_type %result1_322, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11176 = torch.prims.convert_element_type %11167, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11177 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11178 = torch.aten.view %11174, %11177 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11179 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11180 = torch.aten.transpose.int %11179, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %11181 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11182 = torch.prims.convert_element_type %11181, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11183 = torch.prims.convert_element_type %11178, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11184 = torch.prims.convert_element_type %11180, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11185 = torch.aten.mm %11183, %11184 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11186 = torch.aten.mul.Scalar %11185, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11187 = torch.aten.mul.Scalar %11182, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11188 = torch.aten.add.Tensor %11186, %11187, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11189 = torch.prims.convert_element_type %11188, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11190 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11191 = torch.aten.view %11189, %11190 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11192 = torch.aten.slice.Tensor %11191, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11193 = torch.aten.slice.Tensor %11191, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11194 = torch.aten.gelu %11193, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11195 = torch.aten.mul.Tensor %11192, %11194 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11196 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11197 = torch.aten.view %11195, %11196 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %11198 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11199 = torch.aten.transpose.int %11198, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %11200 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11201 = torch.prims.convert_element_type %11200, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11202 = torch.prims.convert_element_type %11197, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11203 = torch.prims.convert_element_type %11199, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11204 = torch.aten.mm %11202, %11203 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11205 = torch.aten.mul.Scalar %11204, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11206 = torch.aten.mul.Scalar %11201, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11207 = torch.aten.add.Tensor %11205, %11206, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11208 = torch.prims.convert_element_type %11207, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11209 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11210 = torch.aten.view %11208, %11209 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11211 = torch.aten.add.Tensor %11210, %11163, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11212 = torch.prims.convert_element_type %11211, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11213 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_323, %result1_324 = torch.aten.var_mean.correction %11212, %11213, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11214 = torch.aten.add.Scalar %result0_323, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11215 = torch.aten.rsqrt %11214 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11216 = torch.aten.sub.Tensor %11211, %result1_324, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11217 = torch.aten.mul.Tensor %11216, %11215 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %11218 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11219 = torch.aten.mul.Tensor %11217, %11218 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %11220 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11221 = torch.aten.add.Tensor %11219, %11220, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11222 = torch.prims.convert_element_type %11221, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11223 = torch.prims.convert_element_type %result1_324, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11224 = torch.prims.convert_element_type %11215, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %11225 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11226 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11227 = torch.aten.view %11222, %11226 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11228 = torch_c.to_builtin_tensor %11227 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11229 = torch_c.to_builtin_tensor %11225 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11230 = tensor.empty() : tensor<2048x1280xf32>
    %11231 = linalg.fill ins(%cst : f32) outs(%11230 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11232 = tensor.empty() : tensor<2048x1280xf32>
    %11233 = linalg.fill ins(%cst : f32) outs(%11232 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11234:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11231, %11233, %11228, %11229, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11231, %11233)
    %11235 = arith.truncf %11234#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11236 = torch_c.from_builtin_tensor %11235 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11237 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11238 = torch.aten.view %11236, %11237 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %11239 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11240 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11241 = torch.aten.view %11222, %11240 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11242 = torch_c.to_builtin_tensor %11241 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11243 = torch_c.to_builtin_tensor %11239 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11244 = tensor.empty() : tensor<2048x1280xf32>
    %11245 = linalg.fill ins(%cst : f32) outs(%11244 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11246 = tensor.empty() : tensor<2048x1280xf32>
    %11247 = linalg.fill ins(%cst : f32) outs(%11246 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11248:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11245, %11247, %11242, %11243, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11245, %11247)
    %11249 = arith.truncf %11248#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11250 = torch_c.from_builtin_tensor %11249 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11251 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11252 = torch.aten.view %11250, %11251 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %11253 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11254 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11255 = torch.aten.view %11222, %11254 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11256 = torch_c.to_builtin_tensor %11255 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11257 = torch_c.to_builtin_tensor %11253 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11258 = tensor.empty() : tensor<2048x1280xf32>
    %11259 = linalg.fill ins(%cst : f32) outs(%11258 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11260 = tensor.empty() : tensor<2048x1280xf32>
    %11261 = linalg.fill ins(%cst : f32) outs(%11260 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11262:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11259, %11261, %11256, %11257, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11259, %11261)
    %11263 = arith.truncf %11262#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11264 = torch_c.from_builtin_tensor %11263 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11265 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11266 = torch.aten.view %11264, %11265 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11267 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11268 = torch.aten.view %11238, %11267 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11269 = torch.aten.transpose.int %11268, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11270 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11271 = torch.aten.view %11252, %11270 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11272 = torch.aten.transpose.int %11271, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11273 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11274 = torch.aten.view %11266, %11273 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11275 = torch.aten.transpose.int %11274, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11276:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11269, %11272, %11275, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11277 = torch.aten.transpose.int %11276#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11278 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11279 = torch.aten.view %11277, %11278 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11280 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11281 = torch.aten.view %11279, %11280 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11282 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11283 = torch.aten.transpose.int %11282, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %11284 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11285 = torch.prims.convert_element_type %11284, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11286 = torch.prims.convert_element_type %11281, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11287 = torch.prims.convert_element_type %11283, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11288 = torch.aten.mm %11286, %11287 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11289 = torch.aten.mul.Scalar %11288, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11290 = torch.aten.mul.Scalar %11285, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11291 = torch.aten.add.Tensor %11289, %11290, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11292 = torch.prims.convert_element_type %11291, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11293 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11294 = torch.aten.view %11292, %11293 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11295 = torch.aten.div.Scalar %11294, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11296 = torch.aten.add.Tensor %11295, %11211, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11297 = torch.prims.convert_element_type %11296, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11298 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_325, %result1_326 = torch.aten.var_mean.correction %11297, %11298, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11299 = torch.aten.add.Scalar %result0_325, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11300 = torch.aten.rsqrt %11299 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11301 = torch.aten.sub.Tensor %11296, %result1_326, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11302 = torch.aten.mul.Tensor %11301, %11300 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %11303 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11304 = torch.aten.mul.Tensor %11302, %11303 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %11305 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11306 = torch.aten.add.Tensor %11304, %11305, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11307 = torch.prims.convert_element_type %11306, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11308 = torch.prims.convert_element_type %result1_326, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11309 = torch.prims.convert_element_type %11300, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %11310 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11311 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11312 = torch.aten.view %11307, %11311 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11313 = torch_c.to_builtin_tensor %11312 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11314 = torch_c.to_builtin_tensor %11310 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11315 = tensor.empty() : tensor<2048x1280xf32>
    %11316 = linalg.fill ins(%cst : f32) outs(%11315 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11317 = tensor.empty() : tensor<2048x1280xf32>
    %11318 = linalg.fill ins(%cst : f32) outs(%11317 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11319:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11316, %11318, %11313, %11314, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11316, %11318)
    %11320 = arith.truncf %11319#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11321 = torch_c.from_builtin_tensor %11320 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11322 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11323 = torch.aten.view %11321, %11322 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %11324 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11325 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11326 = torch.aten.view %4, %11325 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11327 = torch_c.to_builtin_tensor %11326 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11328 = torch_c.to_builtin_tensor %11324 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11329 = tensor.empty() : tensor<128x1280xf32>
    %11330 = linalg.fill ins(%cst : f32) outs(%11329 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11331 = tensor.empty() : tensor<128x1280xf32>
    %11332 = linalg.fill ins(%cst : f32) outs(%11331 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11333:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11330, %11332, %11327, %11328, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11330, %11332)
    %11334 = arith.truncf %11333#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11335 = torch_c.from_builtin_tensor %11334 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11336 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11337 = torch.aten.view %11335, %11336 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %11338 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11339 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11340 = torch.aten.view %4, %11339 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11341 = torch_c.to_builtin_tensor %11340 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11342 = torch_c.to_builtin_tensor %11338 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11343 = tensor.empty() : tensor<128x1280xf32>
    %11344 = linalg.fill ins(%cst : f32) outs(%11343 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11345 = tensor.empty() : tensor<128x1280xf32>
    %11346 = linalg.fill ins(%cst : f32) outs(%11345 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11347:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11344, %11346, %11341, %11342, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11344, %11346)
    %11348 = arith.truncf %11347#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11349 = torch_c.from_builtin_tensor %11348 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11350 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11351 = torch.aten.view %11349, %11350 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11352 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11353 = torch.aten.view %11323, %11352 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11354 = torch.aten.transpose.int %11353, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11355 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11356 = torch.aten.view %11337, %11355 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11357 = torch.aten.transpose.int %11356, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11358 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11359 = torch.aten.view %11351, %11358 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11360 = torch.aten.transpose.int %11359, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11361:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11354, %11357, %11360, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11362 = torch.aten.transpose.int %11361#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11363 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11364 = torch.aten.view %11362, %11363 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11365 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11366 = torch.aten.view %11364, %11365 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11367 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11368 = torch.aten.transpose.int %11367, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %11369 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11370 = torch.prims.convert_element_type %11369, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11371 = torch.prims.convert_element_type %11366, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11372 = torch.prims.convert_element_type %11368, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11373 = torch.aten.mm %11371, %11372 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11374 = torch.aten.mul.Scalar %11373, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11375 = torch.aten.mul.Scalar %11370, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11376 = torch.aten.add.Tensor %11374, %11375, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11377 = torch.prims.convert_element_type %11376, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11378 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11379 = torch.aten.view %11377, %11378 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11380 = torch.aten.div.Scalar %11379, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11381 = torch.aten.add.Tensor %11380, %11296, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11382 = torch.prims.convert_element_type %11381, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11383 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_327, %result1_328 = torch.aten.var_mean.correction %11382, %11383, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11384 = torch.aten.add.Scalar %result0_327, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11385 = torch.aten.rsqrt %11384 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11386 = torch.aten.sub.Tensor %11381, %result1_328, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11387 = torch.aten.mul.Tensor %11386, %11385 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %11388 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11389 = torch.aten.mul.Tensor %11387, %11388 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %11390 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11391 = torch.aten.add.Tensor %11389, %11390, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11392 = torch.prims.convert_element_type %11391, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11393 = torch.prims.convert_element_type %result1_328, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11394 = torch.prims.convert_element_type %11385, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11395 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11396 = torch.aten.view %11392, %11395 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11397 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11398 = torch.aten.transpose.int %11397, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %11399 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11400 = torch.prims.convert_element_type %11399, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11401 = torch.prims.convert_element_type %11396, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11402 = torch.prims.convert_element_type %11398, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11403 = torch.aten.mm %11401, %11402 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11404 = torch.aten.mul.Scalar %11403, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11405 = torch.aten.mul.Scalar %11400, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11406 = torch.aten.add.Tensor %11404, %11405, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11407 = torch.prims.convert_element_type %11406, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11408 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11409 = torch.aten.view %11407, %11408 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11410 = torch.aten.slice.Tensor %11409, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11411 = torch.aten.slice.Tensor %11409, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11412 = torch.aten.gelu %11411, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11413 = torch.aten.mul.Tensor %11410, %11412 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11414 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11415 = torch.aten.view %11413, %11414 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %11416 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11417 = torch.aten.transpose.int %11416, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %11418 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11419 = torch.prims.convert_element_type %11418, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11420 = torch.prims.convert_element_type %11415, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11421 = torch.prims.convert_element_type %11417, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11422 = torch.aten.mm %11420, %11421 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11423 = torch.aten.mul.Scalar %11422, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11424 = torch.aten.mul.Scalar %11419, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11425 = torch.aten.add.Tensor %11423, %11424, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11426 = torch.prims.convert_element_type %11425, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11427 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11428 = torch.aten.view %11426, %11427 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11429 = torch.aten.add.Tensor %11428, %11381, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11430 = torch.prims.convert_element_type %11429, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11431 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_329, %result1_330 = torch.aten.var_mean.correction %11430, %11431, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11432 = torch.aten.add.Scalar %result0_329, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11433 = torch.aten.rsqrt %11432 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11434 = torch.aten.sub.Tensor %11429, %result1_330, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11435 = torch.aten.mul.Tensor %11434, %11433 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %11436 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11437 = torch.aten.mul.Tensor %11435, %11436 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %11438 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11439 = torch.aten.add.Tensor %11437, %11438, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11440 = torch.prims.convert_element_type %11439, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11441 = torch.prims.convert_element_type %result1_330, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11442 = torch.prims.convert_element_type %11433, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %11443 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11444 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11445 = torch.aten.view %11440, %11444 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11446 = torch_c.to_builtin_tensor %11445 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11447 = torch_c.to_builtin_tensor %11443 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11448 = tensor.empty() : tensor<2048x1280xf32>
    %11449 = linalg.fill ins(%cst : f32) outs(%11448 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11450 = tensor.empty() : tensor<2048x1280xf32>
    %11451 = linalg.fill ins(%cst : f32) outs(%11450 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11452:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11449, %11451, %11446, %11447, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11449, %11451)
    %11453 = arith.truncf %11452#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11454 = torch_c.from_builtin_tensor %11453 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11455 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11456 = torch.aten.view %11454, %11455 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %11457 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11458 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11459 = torch.aten.view %11440, %11458 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11460 = torch_c.to_builtin_tensor %11459 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11461 = torch_c.to_builtin_tensor %11457 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11462 = tensor.empty() : tensor<2048x1280xf32>
    %11463 = linalg.fill ins(%cst : f32) outs(%11462 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11464 = tensor.empty() : tensor<2048x1280xf32>
    %11465 = linalg.fill ins(%cst : f32) outs(%11464 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11466:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11463, %11465, %11460, %11461, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11463, %11465)
    %11467 = arith.truncf %11466#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11468 = torch_c.from_builtin_tensor %11467 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11469 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11470 = torch.aten.view %11468, %11469 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %11471 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11472 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11473 = torch.aten.view %11440, %11472 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11474 = torch_c.to_builtin_tensor %11473 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11475 = torch_c.to_builtin_tensor %11471 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11476 = tensor.empty() : tensor<2048x1280xf32>
    %11477 = linalg.fill ins(%cst : f32) outs(%11476 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11478 = tensor.empty() : tensor<2048x1280xf32>
    %11479 = linalg.fill ins(%cst : f32) outs(%11478 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11480:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11477, %11479, %11474, %11475, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11477, %11479)
    %11481 = arith.truncf %11480#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11482 = torch_c.from_builtin_tensor %11481 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11483 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11484 = torch.aten.view %11482, %11483 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11485 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11486 = torch.aten.view %11456, %11485 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11487 = torch.aten.transpose.int %11486, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11488 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11489 = torch.aten.view %11470, %11488 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11490 = torch.aten.transpose.int %11489, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11491 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11492 = torch.aten.view %11484, %11491 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11493 = torch.aten.transpose.int %11492, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11494:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11487, %11490, %11493, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11495 = torch.aten.transpose.int %11494#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11496 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11497 = torch.aten.view %11495, %11496 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11498 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11499 = torch.aten.view %11497, %11498 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11500 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11501 = torch.aten.transpose.int %11500, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %11502 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11503 = torch.prims.convert_element_type %11502, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11504 = torch.prims.convert_element_type %11499, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11505 = torch.prims.convert_element_type %11501, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11506 = torch.aten.mm %11504, %11505 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11507 = torch.aten.mul.Scalar %11506, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11508 = torch.aten.mul.Scalar %11503, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11509 = torch.aten.add.Tensor %11507, %11508, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11510 = torch.prims.convert_element_type %11509, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11511 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11512 = torch.aten.view %11510, %11511 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11513 = torch.aten.div.Scalar %11512, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11514 = torch.aten.add.Tensor %11513, %11429, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11515 = torch.prims.convert_element_type %11514, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11516 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_331, %result1_332 = torch.aten.var_mean.correction %11515, %11516, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11517 = torch.aten.add.Scalar %result0_331, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11518 = torch.aten.rsqrt %11517 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11519 = torch.aten.sub.Tensor %11514, %result1_332, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11520 = torch.aten.mul.Tensor %11519, %11518 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %11521 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11522 = torch.aten.mul.Tensor %11520, %11521 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %11523 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11524 = torch.aten.add.Tensor %11522, %11523, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11525 = torch.prims.convert_element_type %11524, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11526 = torch.prims.convert_element_type %result1_332, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11527 = torch.prims.convert_element_type %11518, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %11528 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11529 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11530 = torch.aten.view %11525, %11529 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11531 = torch_c.to_builtin_tensor %11530 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11532 = torch_c.to_builtin_tensor %11528 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11533 = tensor.empty() : tensor<2048x1280xf32>
    %11534 = linalg.fill ins(%cst : f32) outs(%11533 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11535 = tensor.empty() : tensor<2048x1280xf32>
    %11536 = linalg.fill ins(%cst : f32) outs(%11535 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11537:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11534, %11536, %11531, %11532, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11534, %11536)
    %11538 = arith.truncf %11537#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11539 = torch_c.from_builtin_tensor %11538 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11540 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11541 = torch.aten.view %11539, %11540 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %11542 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11543 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11544 = torch.aten.view %4, %11543 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11545 = torch_c.to_builtin_tensor %11544 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11546 = torch_c.to_builtin_tensor %11542 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11547 = tensor.empty() : tensor<128x1280xf32>
    %11548 = linalg.fill ins(%cst : f32) outs(%11547 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11549 = tensor.empty() : tensor<128x1280xf32>
    %11550 = linalg.fill ins(%cst : f32) outs(%11549 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11551:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11548, %11550, %11545, %11546, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11548, %11550)
    %11552 = arith.truncf %11551#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11553 = torch_c.from_builtin_tensor %11552 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11554 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11555 = torch.aten.view %11553, %11554 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %11556 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11557 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11558 = torch.aten.view %4, %11557 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11559 = torch_c.to_builtin_tensor %11558 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11560 = torch_c.to_builtin_tensor %11556 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11561 = tensor.empty() : tensor<128x1280xf32>
    %11562 = linalg.fill ins(%cst : f32) outs(%11561 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11563 = tensor.empty() : tensor<128x1280xf32>
    %11564 = linalg.fill ins(%cst : f32) outs(%11563 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11565:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11562, %11564, %11559, %11560, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11562, %11564)
    %11566 = arith.truncf %11565#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11567 = torch_c.from_builtin_tensor %11566 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11568 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11569 = torch.aten.view %11567, %11568 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11570 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11571 = torch.aten.view %11541, %11570 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11572 = torch.aten.transpose.int %11571, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11573 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11574 = torch.aten.view %11555, %11573 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11575 = torch.aten.transpose.int %11574, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11576 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11577 = torch.aten.view %11569, %11576 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11578 = torch.aten.transpose.int %11577, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11579:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11572, %11575, %11578, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11580 = torch.aten.transpose.int %11579#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11581 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11582 = torch.aten.view %11580, %11581 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11583 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11584 = torch.aten.view %11582, %11583 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11585 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11586 = torch.aten.transpose.int %11585, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %11587 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11588 = torch.prims.convert_element_type %11587, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11589 = torch.prims.convert_element_type %11584, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11590 = torch.prims.convert_element_type %11586, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11591 = torch.aten.mm %11589, %11590 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11592 = torch.aten.mul.Scalar %11591, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11593 = torch.aten.mul.Scalar %11588, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11594 = torch.aten.add.Tensor %11592, %11593, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11595 = torch.prims.convert_element_type %11594, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11596 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11597 = torch.aten.view %11595, %11596 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11598 = torch.aten.div.Scalar %11597, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11599 = torch.aten.add.Tensor %11598, %11514, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11600 = torch.prims.convert_element_type %11599, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11601 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_333, %result1_334 = torch.aten.var_mean.correction %11600, %11601, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11602 = torch.aten.add.Scalar %result0_333, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11603 = torch.aten.rsqrt %11602 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11604 = torch.aten.sub.Tensor %11599, %result1_334, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11605 = torch.aten.mul.Tensor %11604, %11603 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %11606 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11607 = torch.aten.mul.Tensor %11605, %11606 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %11608 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11609 = torch.aten.add.Tensor %11607, %11608, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11610 = torch.prims.convert_element_type %11609, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11611 = torch.prims.convert_element_type %result1_334, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11612 = torch.prims.convert_element_type %11603, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11613 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11614 = torch.aten.view %11610, %11613 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11615 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11616 = torch.aten.transpose.int %11615, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %11617 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11618 = torch.prims.convert_element_type %11617, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11619 = torch.prims.convert_element_type %11614, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11620 = torch.prims.convert_element_type %11616, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11621 = torch.aten.mm %11619, %11620 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11622 = torch.aten.mul.Scalar %11621, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11623 = torch.aten.mul.Scalar %11618, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11624 = torch.aten.add.Tensor %11622, %11623, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11625 = torch.prims.convert_element_type %11624, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11626 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11627 = torch.aten.view %11625, %11626 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11628 = torch.aten.slice.Tensor %11627, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11629 = torch.aten.slice.Tensor %11627, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11630 = torch.aten.gelu %11629, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11631 = torch.aten.mul.Tensor %11628, %11630 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11632 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11633 = torch.aten.view %11631, %11632 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %11634 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11635 = torch.aten.transpose.int %11634, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %11636 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11637 = torch.prims.convert_element_type %11636, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11638 = torch.prims.convert_element_type %11633, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11639 = torch.prims.convert_element_type %11635, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11640 = torch.aten.mm %11638, %11639 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11641 = torch.aten.mul.Scalar %11640, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11642 = torch.aten.mul.Scalar %11637, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11643 = torch.aten.add.Tensor %11641, %11642, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11644 = torch.prims.convert_element_type %11643, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11645 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11646 = torch.aten.view %11644, %11645 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11647 = torch.aten.add.Tensor %11646, %11599, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11648 = torch.prims.convert_element_type %11647, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11649 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_335, %result1_336 = torch.aten.var_mean.correction %11648, %11649, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11650 = torch.aten.add.Scalar %result0_335, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11651 = torch.aten.rsqrt %11650 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11652 = torch.aten.sub.Tensor %11647, %result1_336, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11653 = torch.aten.mul.Tensor %11652, %11651 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %11654 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11655 = torch.aten.mul.Tensor %11653, %11654 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %11656 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11657 = torch.aten.add.Tensor %11655, %11656, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11658 = torch.prims.convert_element_type %11657, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11659 = torch.prims.convert_element_type %result1_336, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11660 = torch.prims.convert_element_type %11651, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %11661 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11662 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11663 = torch.aten.view %11658, %11662 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11664 = torch_c.to_builtin_tensor %11663 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11665 = torch_c.to_builtin_tensor %11661 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11666 = tensor.empty() : tensor<2048x1280xf32>
    %11667 = linalg.fill ins(%cst : f32) outs(%11666 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11668 = tensor.empty() : tensor<2048x1280xf32>
    %11669 = linalg.fill ins(%cst : f32) outs(%11668 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11670:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11667, %11669, %11664, %11665, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11667, %11669)
    %11671 = arith.truncf %11670#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11672 = torch_c.from_builtin_tensor %11671 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11673 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11674 = torch.aten.view %11672, %11673 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %11675 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11676 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11677 = torch.aten.view %11658, %11676 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11678 = torch_c.to_builtin_tensor %11677 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11679 = torch_c.to_builtin_tensor %11675 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11680 = tensor.empty() : tensor<2048x1280xf32>
    %11681 = linalg.fill ins(%cst : f32) outs(%11680 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11682 = tensor.empty() : tensor<2048x1280xf32>
    %11683 = linalg.fill ins(%cst : f32) outs(%11682 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11684:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11681, %11683, %11678, %11679, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11681, %11683)
    %11685 = arith.truncf %11684#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11686 = torch_c.from_builtin_tensor %11685 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11687 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11688 = torch.aten.view %11686, %11687 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %11689 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11690 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11691 = torch.aten.view %11658, %11690 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11692 = torch_c.to_builtin_tensor %11691 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11693 = torch_c.to_builtin_tensor %11689 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11694 = tensor.empty() : tensor<2048x1280xf32>
    %11695 = linalg.fill ins(%cst : f32) outs(%11694 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11696 = tensor.empty() : tensor<2048x1280xf32>
    %11697 = linalg.fill ins(%cst : f32) outs(%11696 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11698:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11695, %11697, %11692, %11693, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11695, %11697)
    %11699 = arith.truncf %11698#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11700 = torch_c.from_builtin_tensor %11699 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11701 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11702 = torch.aten.view %11700, %11701 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11703 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11704 = torch.aten.view %11674, %11703 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11705 = torch.aten.transpose.int %11704, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11706 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11707 = torch.aten.view %11688, %11706 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11708 = torch.aten.transpose.int %11707, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11709 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11710 = torch.aten.view %11702, %11709 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11711 = torch.aten.transpose.int %11710, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11712:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11705, %11708, %11711, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11713 = torch.aten.transpose.int %11712#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11714 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11715 = torch.aten.view %11713, %11714 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11716 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11717 = torch.aten.view %11715, %11716 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11718 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11719 = torch.aten.transpose.int %11718, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %11720 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11721 = torch.prims.convert_element_type %11720, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11722 = torch.prims.convert_element_type %11717, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11723 = torch.prims.convert_element_type %11719, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11724 = torch.aten.mm %11722, %11723 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11725 = torch.aten.mul.Scalar %11724, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11726 = torch.aten.mul.Scalar %11721, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11727 = torch.aten.add.Tensor %11725, %11726, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11728 = torch.prims.convert_element_type %11727, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11729 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11730 = torch.aten.view %11728, %11729 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11731 = torch.aten.div.Scalar %11730, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11732 = torch.aten.add.Tensor %11731, %11647, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11733 = torch.prims.convert_element_type %11732, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11734 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_337, %result1_338 = torch.aten.var_mean.correction %11733, %11734, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11735 = torch.aten.add.Scalar %result0_337, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11736 = torch.aten.rsqrt %11735 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11737 = torch.aten.sub.Tensor %11732, %result1_338, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11738 = torch.aten.mul.Tensor %11737, %11736 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %11739 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11740 = torch.aten.mul.Tensor %11738, %11739 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %11741 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11742 = torch.aten.add.Tensor %11740, %11741, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11743 = torch.prims.convert_element_type %11742, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11744 = torch.prims.convert_element_type %result1_338, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11745 = torch.prims.convert_element_type %11736, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %11746 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11747 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11748 = torch.aten.view %11743, %11747 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11749 = torch_c.to_builtin_tensor %11748 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11750 = torch_c.to_builtin_tensor %11746 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11751 = tensor.empty() : tensor<2048x1280xf32>
    %11752 = linalg.fill ins(%cst : f32) outs(%11751 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11753 = tensor.empty() : tensor<2048x1280xf32>
    %11754 = linalg.fill ins(%cst : f32) outs(%11753 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11755:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11752, %11754, %11749, %11750, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11752, %11754)
    %11756 = arith.truncf %11755#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11757 = torch_c.from_builtin_tensor %11756 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11758 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11759 = torch.aten.view %11757, %11758 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %11760 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11761 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11762 = torch.aten.view %4, %11761 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11763 = torch_c.to_builtin_tensor %11762 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11764 = torch_c.to_builtin_tensor %11760 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11765 = tensor.empty() : tensor<128x1280xf32>
    %11766 = linalg.fill ins(%cst : f32) outs(%11765 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11767 = tensor.empty() : tensor<128x1280xf32>
    %11768 = linalg.fill ins(%cst : f32) outs(%11767 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11769:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11766, %11768, %11763, %11764, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11766, %11768)
    %11770 = arith.truncf %11769#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11771 = torch_c.from_builtin_tensor %11770 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11772 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11773 = torch.aten.view %11771, %11772 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %11774 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11775 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11776 = torch.aten.view %4, %11775 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11777 = torch_c.to_builtin_tensor %11776 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11778 = torch_c.to_builtin_tensor %11774 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11779 = tensor.empty() : tensor<128x1280xf32>
    %11780 = linalg.fill ins(%cst : f32) outs(%11779 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11781 = tensor.empty() : tensor<128x1280xf32>
    %11782 = linalg.fill ins(%cst : f32) outs(%11781 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11783:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11780, %11782, %11777, %11778, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11780, %11782)
    %11784 = arith.truncf %11783#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11785 = torch_c.from_builtin_tensor %11784 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11786 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11787 = torch.aten.view %11785, %11786 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %11788 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11789 = torch.aten.view %11759, %11788 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11790 = torch.aten.transpose.int %11789, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11791 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11792 = torch.aten.view %11773, %11791 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11793 = torch.aten.transpose.int %11792, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11794 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11795 = torch.aten.view %11787, %11794 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %11796 = torch.aten.transpose.int %11795, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %11797:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11790, %11793, %11796, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11798 = torch.aten.transpose.int %11797#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11799 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11800 = torch.aten.view %11798, %11799 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11801 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11802 = torch.aten.view %11800, %11801 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %11803 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11804 = torch.aten.transpose.int %11803, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %11805 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11806 = torch.prims.convert_element_type %11805, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11807 = torch.prims.convert_element_type %11802, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11808 = torch.prims.convert_element_type %11804, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11809 = torch.aten.mm %11807, %11808 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11810 = torch.aten.mul.Scalar %11809, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11811 = torch.aten.mul.Scalar %11806, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11812 = torch.aten.add.Tensor %11810, %11811, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11813 = torch.prims.convert_element_type %11812, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11814 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11815 = torch.aten.view %11813, %11814 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11816 = torch.aten.div.Scalar %11815, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11817 = torch.aten.add.Tensor %11816, %11732, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11818 = torch.prims.convert_element_type %11817, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11819 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_339, %result1_340 = torch.aten.var_mean.correction %11818, %11819, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11820 = torch.aten.add.Scalar %result0_339, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11821 = torch.aten.rsqrt %11820 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11822 = torch.aten.sub.Tensor %11817, %result1_340, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11823 = torch.aten.mul.Tensor %11822, %11821 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %11824 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11825 = torch.aten.mul.Tensor %11823, %11824 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %11826 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11827 = torch.aten.add.Tensor %11825, %11826, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11828 = torch.prims.convert_element_type %11827, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11829 = torch.prims.convert_element_type %result1_340, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11830 = torch.prims.convert_element_type %11821, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11831 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11832 = torch.aten.view %11828, %11831 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %11833 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %11834 = torch.aten.transpose.int %11833, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %11835 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %11836 = torch.prims.convert_element_type %11835, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %11837 = torch.prims.convert_element_type %11832, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11838 = torch.prims.convert_element_type %11834, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %11839 = torch.aten.mm %11837, %11838 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %11840 = torch.aten.mul.Scalar %11839, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11841 = torch.aten.mul.Scalar %11836, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %11842 = torch.aten.add.Tensor %11840, %11841, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %11843 = torch.prims.convert_element_type %11842, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %11844 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11845 = torch.aten.view %11843, %11844 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %11846 = torch.aten.slice.Tensor %11845, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11847 = torch.aten.slice.Tensor %11845, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %11848 = torch.aten.gelu %11847, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %11849 = torch.aten.mul.Tensor %11846, %11848 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %11850 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %11851 = torch.aten.view %11849, %11850 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %11852 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %11853 = torch.aten.transpose.int %11852, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %11854 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11855 = torch.prims.convert_element_type %11854, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11856 = torch.prims.convert_element_type %11851, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %11857 = torch.prims.convert_element_type %11853, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %11858 = torch.aten.mm %11856, %11857 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11859 = torch.aten.mul.Scalar %11858, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11860 = torch.aten.mul.Scalar %11855, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11861 = torch.aten.add.Tensor %11859, %11860, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11862 = torch.prims.convert_element_type %11861, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11863 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11864 = torch.aten.view %11862, %11863 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11865 = torch.aten.add.Tensor %11864, %11817, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11866 = torch.prims.convert_element_type %11865, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11867 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_341, %result1_342 = torch.aten.var_mean.correction %11866, %11867, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11868 = torch.aten.add.Scalar %result0_341, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11869 = torch.aten.rsqrt %11868 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11870 = torch.aten.sub.Tensor %11865, %result1_342, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11871 = torch.aten.mul.Tensor %11870, %11869 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %11872 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11873 = torch.aten.mul.Tensor %11871, %11872 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %11874 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11875 = torch.aten.add.Tensor %11873, %11874, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11876 = torch.prims.convert_element_type %11875, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11877 = torch.prims.convert_element_type %result1_342, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11878 = torch.prims.convert_element_type %11869, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %11879 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11880 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11881 = torch.aten.view %11876, %11880 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11882 = torch_c.to_builtin_tensor %11881 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11883 = torch_c.to_builtin_tensor %11879 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11884 = tensor.empty() : tensor<2048x1280xf32>
    %11885 = linalg.fill ins(%cst : f32) outs(%11884 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11886 = tensor.empty() : tensor<2048x1280xf32>
    %11887 = linalg.fill ins(%cst : f32) outs(%11886 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11888:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11885, %11887, %11882, %11883, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11885, %11887)
    %11889 = arith.truncf %11888#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11890 = torch_c.from_builtin_tensor %11889 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11891 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11892 = torch.aten.view %11890, %11891 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %11893 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11894 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11895 = torch.aten.view %11876, %11894 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11896 = torch_c.to_builtin_tensor %11895 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11897 = torch_c.to_builtin_tensor %11893 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11898 = tensor.empty() : tensor<2048x1280xf32>
    %11899 = linalg.fill ins(%cst : f32) outs(%11898 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11900 = tensor.empty() : tensor<2048x1280xf32>
    %11901 = linalg.fill ins(%cst : f32) outs(%11900 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11902:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11899, %11901, %11896, %11897, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11899, %11901)
    %11903 = arith.truncf %11902#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11904 = torch_c.from_builtin_tensor %11903 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11905 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11906 = torch.aten.view %11904, %11905 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %11907 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11908 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11909 = torch.aten.view %11876, %11908 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11910 = torch_c.to_builtin_tensor %11909 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11911 = torch_c.to_builtin_tensor %11907 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11912 = tensor.empty() : tensor<2048x1280xf32>
    %11913 = linalg.fill ins(%cst : f32) outs(%11912 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11914 = tensor.empty() : tensor<2048x1280xf32>
    %11915 = linalg.fill ins(%cst : f32) outs(%11914 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11916:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11913, %11915, %11910, %11911, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11913, %11915)
    %11917 = arith.truncf %11916#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11918 = torch_c.from_builtin_tensor %11917 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11919 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11920 = torch.aten.view %11918, %11919 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11921 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11922 = torch.aten.view %11892, %11921 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11923 = torch.aten.transpose.int %11922, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11924 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11925 = torch.aten.view %11906, %11924 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11926 = torch.aten.transpose.int %11925, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11927 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11928 = torch.aten.view %11920, %11927 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %11929 = torch.aten.transpose.int %11928, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %11930:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%11923, %11926, %11929, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %11931 = torch.aten.transpose.int %11930#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %11932 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11933 = torch.aten.view %11931, %11932 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11934 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11935 = torch.aten.view %11933, %11934 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %11936 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11937 = torch.aten.transpose.int %11936, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %11938 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11939 = torch.prims.convert_element_type %11938, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %11940 = torch.prims.convert_element_type %11935, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11941 = torch.prims.convert_element_type %11937, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %11942 = torch.aten.mm %11940, %11941 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %11943 = torch.aten.mul.Scalar %11942, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11944 = torch.aten.mul.Scalar %11939, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %11945 = torch.aten.add.Tensor %11943, %11944, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %11946 = torch.prims.convert_element_type %11945, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %11947 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11948 = torch.aten.view %11946, %11947 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %11949 = torch.aten.div.Scalar %11948, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %11950 = torch.aten.add.Tensor %11949, %11865, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11951 = torch.prims.convert_element_type %11950, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11952 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_343, %result1_344 = torch.aten.var_mean.correction %11951, %11952, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %11953 = torch.aten.add.Scalar %result0_343, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %11954 = torch.aten.rsqrt %11953 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %11955 = torch.aten.sub.Tensor %11950, %result1_344, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11956 = torch.aten.mul.Tensor %11955, %11954 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %11957 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11958 = torch.aten.mul.Tensor %11956, %11957 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %11959 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %11960 = torch.aten.add.Tensor %11958, %11959, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %11961 = torch.prims.convert_element_type %11960, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %11962 = torch.prims.convert_element_type %result1_344, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %11963 = torch.prims.convert_element_type %11954, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %11964 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %11965 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %11966 = torch.aten.view %11961, %11965 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %11967 = torch_c.to_builtin_tensor %11966 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %11968 = torch_c.to_builtin_tensor %11964 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %11969 = tensor.empty() : tensor<2048x1280xf32>
    %11970 = linalg.fill ins(%cst : f32) outs(%11969 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11971 = tensor.empty() : tensor<2048x1280xf32>
    %11972 = linalg.fill ins(%cst : f32) outs(%11971 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %11973:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %11970, %11972, %11967, %11968, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11970, %11972)
    %11974 = arith.truncf %11973#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %11975 = torch_c.from_builtin_tensor %11974 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %11976 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11977 = torch.aten.view %11975, %11976 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %11978 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11979 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11980 = torch.aten.view %4, %11979 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11981 = torch_c.to_builtin_tensor %11980 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11982 = torch_c.to_builtin_tensor %11978 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11983 = tensor.empty() : tensor<128x1280xf32>
    %11984 = linalg.fill ins(%cst : f32) outs(%11983 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11985 = tensor.empty() : tensor<128x1280xf32>
    %11986 = linalg.fill ins(%cst : f32) outs(%11985 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11987:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11984, %11986, %11981, %11982, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11984, %11986)
    %11988 = arith.truncf %11987#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %11989 = torch_c.from_builtin_tensor %11988 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %11990 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %11991 = torch.aten.view %11989, %11990 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %11992 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %11993 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %11994 = torch.aten.view %4, %11993 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %11995 = torch_c.to_builtin_tensor %11994 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %11996 = torch_c.to_builtin_tensor %11992 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %11997 = tensor.empty() : tensor<128x1280xf32>
    %11998 = linalg.fill ins(%cst : f32) outs(%11997 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %11999 = tensor.empty() : tensor<128x1280xf32>
    %12000 = linalg.fill ins(%cst : f32) outs(%11999 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12001:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %11998, %12000, %11995, %11996, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%11998, %12000)
    %12002 = arith.truncf %12001#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12003 = torch_c.from_builtin_tensor %12002 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12004 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12005 = torch.aten.view %12003, %12004 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12006 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12007 = torch.aten.view %11977, %12006 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12008 = torch.aten.transpose.int %12007, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12009 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12010 = torch.aten.view %11991, %12009 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12011 = torch.aten.transpose.int %12010, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12012 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12013 = torch.aten.view %12005, %12012 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12014 = torch.aten.transpose.int %12013, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12015:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12008, %12011, %12014, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12016 = torch.aten.transpose.int %12015#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12017 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12018 = torch.aten.view %12016, %12017 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12019 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12020 = torch.aten.view %12018, %12019 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12021 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12022 = torch.aten.transpose.int %12021, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %12023 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12024 = torch.prims.convert_element_type %12023, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12025 = torch.prims.convert_element_type %12020, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12026 = torch.prims.convert_element_type %12022, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12027 = torch.aten.mm %12025, %12026 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12028 = torch.aten.mul.Scalar %12027, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12029 = torch.aten.mul.Scalar %12024, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12030 = torch.aten.add.Tensor %12028, %12029, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12031 = torch.prims.convert_element_type %12030, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12032 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12033 = torch.aten.view %12031, %12032 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12034 = torch.aten.div.Scalar %12033, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12035 = torch.aten.add.Tensor %12034, %11950, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12036 = torch.prims.convert_element_type %12035, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12037 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_345, %result1_346 = torch.aten.var_mean.correction %12036, %12037, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12038 = torch.aten.add.Scalar %result0_345, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12039 = torch.aten.rsqrt %12038 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12040 = torch.aten.sub.Tensor %12035, %result1_346, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12041 = torch.aten.mul.Tensor %12040, %12039 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %12042 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12043 = torch.aten.mul.Tensor %12041, %12042 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %12044 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12045 = torch.aten.add.Tensor %12043, %12044, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12046 = torch.prims.convert_element_type %12045, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12047 = torch.prims.convert_element_type %result1_346, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12048 = torch.prims.convert_element_type %12039, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12049 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12050 = torch.aten.view %12046, %12049 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12051 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12052 = torch.aten.transpose.int %12051, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %12053 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12054 = torch.prims.convert_element_type %12053, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12055 = torch.prims.convert_element_type %12050, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12056 = torch.prims.convert_element_type %12052, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12057 = torch.aten.mm %12055, %12056 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12058 = torch.aten.mul.Scalar %12057, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12059 = torch.aten.mul.Scalar %12054, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12060 = torch.aten.add.Tensor %12058, %12059, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12061 = torch.prims.convert_element_type %12060, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12062 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12063 = torch.aten.view %12061, %12062 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12064 = torch.aten.slice.Tensor %12063, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12065 = torch.aten.slice.Tensor %12063, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12066 = torch.aten.gelu %12065, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12067 = torch.aten.mul.Tensor %12064, %12066 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12068 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12069 = torch.aten.view %12067, %12068 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %12070 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12071 = torch.aten.transpose.int %12070, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %12072 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12073 = torch.prims.convert_element_type %12072, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12074 = torch.prims.convert_element_type %12069, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12075 = torch.prims.convert_element_type %12071, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12076 = torch.aten.mm %12074, %12075 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12077 = torch.aten.mul.Scalar %12076, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12078 = torch.aten.mul.Scalar %12073, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12079 = torch.aten.add.Tensor %12077, %12078, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12080 = torch.prims.convert_element_type %12079, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12081 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12082 = torch.aten.view %12080, %12081 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12083 = torch.aten.add.Tensor %12082, %12035, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12084 = torch.prims.convert_element_type %12083, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12085 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_347, %result1_348 = torch.aten.var_mean.correction %12084, %12085, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12086 = torch.aten.add.Scalar %result0_347, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12087 = torch.aten.rsqrt %12086 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12088 = torch.aten.sub.Tensor %12083, %result1_348, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12089 = torch.aten.mul.Tensor %12088, %12087 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %12090 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12091 = torch.aten.mul.Tensor %12089, %12090 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %12092 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12093 = torch.aten.add.Tensor %12091, %12092, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12094 = torch.prims.convert_element_type %12093, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12095 = torch.prims.convert_element_type %result1_348, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12096 = torch.prims.convert_element_type %12087, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %12097 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12098 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12099 = torch.aten.view %12094, %12098 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12100 = torch_c.to_builtin_tensor %12099 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12101 = torch_c.to_builtin_tensor %12097 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12102 = tensor.empty() : tensor<2048x1280xf32>
    %12103 = linalg.fill ins(%cst : f32) outs(%12102 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12104 = tensor.empty() : tensor<2048x1280xf32>
    %12105 = linalg.fill ins(%cst : f32) outs(%12104 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12106:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12103, %12105, %12100, %12101, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12103, %12105)
    %12107 = arith.truncf %12106#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12108 = torch_c.from_builtin_tensor %12107 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12109 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12110 = torch.aten.view %12108, %12109 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %12111 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12112 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12113 = torch.aten.view %12094, %12112 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12114 = torch_c.to_builtin_tensor %12113 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12115 = torch_c.to_builtin_tensor %12111 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12116 = tensor.empty() : tensor<2048x1280xf32>
    %12117 = linalg.fill ins(%cst : f32) outs(%12116 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12118 = tensor.empty() : tensor<2048x1280xf32>
    %12119 = linalg.fill ins(%cst : f32) outs(%12118 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12120:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12117, %12119, %12114, %12115, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12117, %12119)
    %12121 = arith.truncf %12120#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12122 = torch_c.from_builtin_tensor %12121 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12123 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12124 = torch.aten.view %12122, %12123 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %12125 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12126 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12127 = torch.aten.view %12094, %12126 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12128 = torch_c.to_builtin_tensor %12127 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12129 = torch_c.to_builtin_tensor %12125 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12130 = tensor.empty() : tensor<2048x1280xf32>
    %12131 = linalg.fill ins(%cst : f32) outs(%12130 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12132 = tensor.empty() : tensor<2048x1280xf32>
    %12133 = linalg.fill ins(%cst : f32) outs(%12132 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12134:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12131, %12133, %12128, %12129, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12131, %12133)
    %12135 = arith.truncf %12134#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12136 = torch_c.from_builtin_tensor %12135 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12137 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12138 = torch.aten.view %12136, %12137 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12139 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12140 = torch.aten.view %12110, %12139 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12141 = torch.aten.transpose.int %12140, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12142 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12143 = torch.aten.view %12124, %12142 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12144 = torch.aten.transpose.int %12143, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12145 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12146 = torch.aten.view %12138, %12145 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12147 = torch.aten.transpose.int %12146, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12148:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12141, %12144, %12147, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12149 = torch.aten.transpose.int %12148#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12150 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12151 = torch.aten.view %12149, %12150 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12152 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12153 = torch.aten.view %12151, %12152 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12154 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12155 = torch.aten.transpose.int %12154, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %12156 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12157 = torch.prims.convert_element_type %12156, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12158 = torch.prims.convert_element_type %12153, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12159 = torch.prims.convert_element_type %12155, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12160 = torch.aten.mm %12158, %12159 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12161 = torch.aten.mul.Scalar %12160, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12162 = torch.aten.mul.Scalar %12157, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12163 = torch.aten.add.Tensor %12161, %12162, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12164 = torch.prims.convert_element_type %12163, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12165 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12166 = torch.aten.view %12164, %12165 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12167 = torch.aten.div.Scalar %12166, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12168 = torch.aten.add.Tensor %12167, %12083, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12169 = torch.prims.convert_element_type %12168, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12170 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_349, %result1_350 = torch.aten.var_mean.correction %12169, %12170, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12171 = torch.aten.add.Scalar %result0_349, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12172 = torch.aten.rsqrt %12171 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12173 = torch.aten.sub.Tensor %12168, %result1_350, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12174 = torch.aten.mul.Tensor %12173, %12172 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %12175 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12176 = torch.aten.mul.Tensor %12174, %12175 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %12177 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12178 = torch.aten.add.Tensor %12176, %12177, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12179 = torch.prims.convert_element_type %12178, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12180 = torch.prims.convert_element_type %result1_350, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12181 = torch.prims.convert_element_type %12172, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %12182 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12183 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12184 = torch.aten.view %12179, %12183 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12185 = torch_c.to_builtin_tensor %12184 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12186 = torch_c.to_builtin_tensor %12182 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12187 = tensor.empty() : tensor<2048x1280xf32>
    %12188 = linalg.fill ins(%cst : f32) outs(%12187 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12189 = tensor.empty() : tensor<2048x1280xf32>
    %12190 = linalg.fill ins(%cst : f32) outs(%12189 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12191:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12188, %12190, %12185, %12186, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12188, %12190)
    %12192 = arith.truncf %12191#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12193 = torch_c.from_builtin_tensor %12192 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12194 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12195 = torch.aten.view %12193, %12194 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %12196 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12197 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12198 = torch.aten.view %4, %12197 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12199 = torch_c.to_builtin_tensor %12198 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12200 = torch_c.to_builtin_tensor %12196 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12201 = tensor.empty() : tensor<128x1280xf32>
    %12202 = linalg.fill ins(%cst : f32) outs(%12201 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12203 = tensor.empty() : tensor<128x1280xf32>
    %12204 = linalg.fill ins(%cst : f32) outs(%12203 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12205:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12202, %12204, %12199, %12200, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12202, %12204)
    %12206 = arith.truncf %12205#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12207 = torch_c.from_builtin_tensor %12206 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12208 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12209 = torch.aten.view %12207, %12208 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %12210 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12211 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12212 = torch.aten.view %4, %12211 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12213 = torch_c.to_builtin_tensor %12212 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12214 = torch_c.to_builtin_tensor %12210 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12215 = tensor.empty() : tensor<128x1280xf32>
    %12216 = linalg.fill ins(%cst : f32) outs(%12215 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12217 = tensor.empty() : tensor<128x1280xf32>
    %12218 = linalg.fill ins(%cst : f32) outs(%12217 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12219:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12216, %12218, %12213, %12214, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12216, %12218)
    %12220 = arith.truncf %12219#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12221 = torch_c.from_builtin_tensor %12220 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12222 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12223 = torch.aten.view %12221, %12222 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12224 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12225 = torch.aten.view %12195, %12224 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12226 = torch.aten.transpose.int %12225, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12227 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12228 = torch.aten.view %12209, %12227 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12229 = torch.aten.transpose.int %12228, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12230 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12231 = torch.aten.view %12223, %12230 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12232 = torch.aten.transpose.int %12231, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12233:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12226, %12229, %12232, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12234 = torch.aten.transpose.int %12233#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12235 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12236 = torch.aten.view %12234, %12235 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12237 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12238 = torch.aten.view %12236, %12237 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12239 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12240 = torch.aten.transpose.int %12239, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %12241 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12242 = torch.prims.convert_element_type %12241, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12243 = torch.prims.convert_element_type %12238, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12244 = torch.prims.convert_element_type %12240, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12245 = torch.aten.mm %12243, %12244 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12246 = torch.aten.mul.Scalar %12245, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12247 = torch.aten.mul.Scalar %12242, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12248 = torch.aten.add.Tensor %12246, %12247, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12249 = torch.prims.convert_element_type %12248, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12250 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12251 = torch.aten.view %12249, %12250 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12252 = torch.aten.div.Scalar %12251, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12253 = torch.aten.add.Tensor %12252, %12168, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12254 = torch.prims.convert_element_type %12253, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12255 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_351, %result1_352 = torch.aten.var_mean.correction %12254, %12255, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12256 = torch.aten.add.Scalar %result0_351, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12257 = torch.aten.rsqrt %12256 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12258 = torch.aten.sub.Tensor %12253, %result1_352, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12259 = torch.aten.mul.Tensor %12258, %12257 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %12260 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12261 = torch.aten.mul.Tensor %12259, %12260 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %12262 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12263 = torch.aten.add.Tensor %12261, %12262, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12264 = torch.prims.convert_element_type %12263, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12265 = torch.prims.convert_element_type %result1_352, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12266 = torch.prims.convert_element_type %12257, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12267 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12268 = torch.aten.view %12264, %12267 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12269 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12270 = torch.aten.transpose.int %12269, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %12271 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12272 = torch.prims.convert_element_type %12271, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12273 = torch.prims.convert_element_type %12268, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12274 = torch.prims.convert_element_type %12270, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12275 = torch.aten.mm %12273, %12274 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12276 = torch.aten.mul.Scalar %12275, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12277 = torch.aten.mul.Scalar %12272, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12278 = torch.aten.add.Tensor %12276, %12277, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12279 = torch.prims.convert_element_type %12278, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12280 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12281 = torch.aten.view %12279, %12280 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12282 = torch.aten.slice.Tensor %12281, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12283 = torch.aten.slice.Tensor %12281, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12284 = torch.aten.gelu %12283, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12285 = torch.aten.mul.Tensor %12282, %12284 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12286 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12287 = torch.aten.view %12285, %12286 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %12288 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12289 = torch.aten.transpose.int %12288, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %12290 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12291 = torch.prims.convert_element_type %12290, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12292 = torch.prims.convert_element_type %12287, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12293 = torch.prims.convert_element_type %12289, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12294 = torch.aten.mm %12292, %12293 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12295 = torch.aten.mul.Scalar %12294, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12296 = torch.aten.mul.Scalar %12291, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12297 = torch.aten.add.Tensor %12295, %12296, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12298 = torch.prims.convert_element_type %12297, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12299 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12300 = torch.aten.view %12298, %12299 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12301 = torch.aten.add.Tensor %12300, %12253, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12302 = torch.prims.convert_element_type %12301, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12303 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_353, %result1_354 = torch.aten.var_mean.correction %12302, %12303, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12304 = torch.aten.add.Scalar %result0_353, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12305 = torch.aten.rsqrt %12304 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12306 = torch.aten.sub.Tensor %12301, %result1_354, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12307 = torch.aten.mul.Tensor %12306, %12305 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %12308 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12309 = torch.aten.mul.Tensor %12307, %12308 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %12310 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12311 = torch.aten.add.Tensor %12309, %12310, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12312 = torch.prims.convert_element_type %12311, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12313 = torch.prims.convert_element_type %result1_354, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12314 = torch.prims.convert_element_type %12305, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %12315 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12316 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12317 = torch.aten.view %12312, %12316 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12318 = torch_c.to_builtin_tensor %12317 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12319 = torch_c.to_builtin_tensor %12315 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12320 = tensor.empty() : tensor<2048x1280xf32>
    %12321 = linalg.fill ins(%cst : f32) outs(%12320 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12322 = tensor.empty() : tensor<2048x1280xf32>
    %12323 = linalg.fill ins(%cst : f32) outs(%12322 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12324:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12321, %12323, %12318, %12319, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12321, %12323)
    %12325 = arith.truncf %12324#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12326 = torch_c.from_builtin_tensor %12325 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12327 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12328 = torch.aten.view %12326, %12327 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %12329 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12330 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12331 = torch.aten.view %12312, %12330 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12332 = torch_c.to_builtin_tensor %12331 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12333 = torch_c.to_builtin_tensor %12329 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12334 = tensor.empty() : tensor<2048x1280xf32>
    %12335 = linalg.fill ins(%cst : f32) outs(%12334 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12336 = tensor.empty() : tensor<2048x1280xf32>
    %12337 = linalg.fill ins(%cst : f32) outs(%12336 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12338:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12335, %12337, %12332, %12333, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12335, %12337)
    %12339 = arith.truncf %12338#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12340 = torch_c.from_builtin_tensor %12339 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12341 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12342 = torch.aten.view %12340, %12341 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %12343 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12344 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12345 = torch.aten.view %12312, %12344 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12346 = torch_c.to_builtin_tensor %12345 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12347 = torch_c.to_builtin_tensor %12343 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12348 = tensor.empty() : tensor<2048x1280xf32>
    %12349 = linalg.fill ins(%cst : f32) outs(%12348 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12350 = tensor.empty() : tensor<2048x1280xf32>
    %12351 = linalg.fill ins(%cst : f32) outs(%12350 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12352:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12349, %12351, %12346, %12347, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12349, %12351)
    %12353 = arith.truncf %12352#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12354 = torch_c.from_builtin_tensor %12353 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12355 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12356 = torch.aten.view %12354, %12355 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12357 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12358 = torch.aten.view %12328, %12357 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12359 = torch.aten.transpose.int %12358, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12360 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12361 = torch.aten.view %12342, %12360 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12362 = torch.aten.transpose.int %12361, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12363 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12364 = torch.aten.view %12356, %12363 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12365 = torch.aten.transpose.int %12364, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12366:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12359, %12362, %12365, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12367 = torch.aten.transpose.int %12366#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12368 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12369 = torch.aten.view %12367, %12368 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12370 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12371 = torch.aten.view %12369, %12370 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12372 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12373 = torch.aten.transpose.int %12372, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %12374 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12375 = torch.prims.convert_element_type %12374, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12376 = torch.prims.convert_element_type %12371, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12377 = torch.prims.convert_element_type %12373, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12378 = torch.aten.mm %12376, %12377 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12379 = torch.aten.mul.Scalar %12378, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12380 = torch.aten.mul.Scalar %12375, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12381 = torch.aten.add.Tensor %12379, %12380, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12382 = torch.prims.convert_element_type %12381, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12383 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12384 = torch.aten.view %12382, %12383 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12385 = torch.aten.div.Scalar %12384, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12386 = torch.aten.add.Tensor %12385, %12301, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12387 = torch.prims.convert_element_type %12386, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12388 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_355, %result1_356 = torch.aten.var_mean.correction %12387, %12388, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12389 = torch.aten.add.Scalar %result0_355, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12390 = torch.aten.rsqrt %12389 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12391 = torch.aten.sub.Tensor %12386, %result1_356, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12392 = torch.aten.mul.Tensor %12391, %12390 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %12393 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12394 = torch.aten.mul.Tensor %12392, %12393 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %12395 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12396 = torch.aten.add.Tensor %12394, %12395, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12397 = torch.prims.convert_element_type %12396, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12398 = torch.prims.convert_element_type %result1_356, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12399 = torch.prims.convert_element_type %12390, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %12400 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12401 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12402 = torch.aten.view %12397, %12401 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12403 = torch_c.to_builtin_tensor %12402 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12404 = torch_c.to_builtin_tensor %12400 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12405 = tensor.empty() : tensor<2048x1280xf32>
    %12406 = linalg.fill ins(%cst : f32) outs(%12405 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12407 = tensor.empty() : tensor<2048x1280xf32>
    %12408 = linalg.fill ins(%cst : f32) outs(%12407 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12409:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12406, %12408, %12403, %12404, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12406, %12408)
    %12410 = arith.truncf %12409#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12411 = torch_c.from_builtin_tensor %12410 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12412 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12413 = torch.aten.view %12411, %12412 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %12414 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12415 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12416 = torch.aten.view %4, %12415 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12417 = torch_c.to_builtin_tensor %12416 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12418 = torch_c.to_builtin_tensor %12414 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12419 = tensor.empty() : tensor<128x1280xf32>
    %12420 = linalg.fill ins(%cst : f32) outs(%12419 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12421 = tensor.empty() : tensor<128x1280xf32>
    %12422 = linalg.fill ins(%cst : f32) outs(%12421 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12423:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12420, %12422, %12417, %12418, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12420, %12422)
    %12424 = arith.truncf %12423#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12425 = torch_c.from_builtin_tensor %12424 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12426 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12427 = torch.aten.view %12425, %12426 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %12428 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12429 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12430 = torch.aten.view %4, %12429 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12431 = torch_c.to_builtin_tensor %12430 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12432 = torch_c.to_builtin_tensor %12428 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12433 = tensor.empty() : tensor<128x1280xf32>
    %12434 = linalg.fill ins(%cst : f32) outs(%12433 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12435 = tensor.empty() : tensor<128x1280xf32>
    %12436 = linalg.fill ins(%cst : f32) outs(%12435 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12437:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12434, %12436, %12431, %12432, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12434, %12436)
    %12438 = arith.truncf %12437#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12439 = torch_c.from_builtin_tensor %12438 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12440 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12441 = torch.aten.view %12439, %12440 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12442 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12443 = torch.aten.view %12413, %12442 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12444 = torch.aten.transpose.int %12443, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12445 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12446 = torch.aten.view %12427, %12445 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12447 = torch.aten.transpose.int %12446, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12448 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12449 = torch.aten.view %12441, %12448 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12450 = torch.aten.transpose.int %12449, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12451:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12444, %12447, %12450, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12452 = torch.aten.transpose.int %12451#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12453 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12454 = torch.aten.view %12452, %12453 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12455 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12456 = torch.aten.view %12454, %12455 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12457 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12458 = torch.aten.transpose.int %12457, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %12459 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12460 = torch.prims.convert_element_type %12459, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12461 = torch.prims.convert_element_type %12456, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12462 = torch.prims.convert_element_type %12458, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12463 = torch.aten.mm %12461, %12462 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12464 = torch.aten.mul.Scalar %12463, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12465 = torch.aten.mul.Scalar %12460, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12466 = torch.aten.add.Tensor %12464, %12465, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12467 = torch.prims.convert_element_type %12466, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12468 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12469 = torch.aten.view %12467, %12468 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12470 = torch.aten.div.Scalar %12469, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12471 = torch.aten.add.Tensor %12470, %12386, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12472 = torch.prims.convert_element_type %12471, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12473 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_357, %result1_358 = torch.aten.var_mean.correction %12472, %12473, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12474 = torch.aten.add.Scalar %result0_357, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12475 = torch.aten.rsqrt %12474 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12476 = torch.aten.sub.Tensor %12471, %result1_358, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12477 = torch.aten.mul.Tensor %12476, %12475 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %12478 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12479 = torch.aten.mul.Tensor %12477, %12478 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %12480 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12481 = torch.aten.add.Tensor %12479, %12480, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12482 = torch.prims.convert_element_type %12481, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12483 = torch.prims.convert_element_type %result1_358, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12484 = torch.prims.convert_element_type %12475, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12485 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12486 = torch.aten.view %12482, %12485 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12487 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12488 = torch.aten.transpose.int %12487, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %12489 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12490 = torch.prims.convert_element_type %12489, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12491 = torch.prims.convert_element_type %12486, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12492 = torch.prims.convert_element_type %12488, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12493 = torch.aten.mm %12491, %12492 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12494 = torch.aten.mul.Scalar %12493, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12495 = torch.aten.mul.Scalar %12490, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12496 = torch.aten.add.Tensor %12494, %12495, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12497 = torch.prims.convert_element_type %12496, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12498 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12499 = torch.aten.view %12497, %12498 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12500 = torch.aten.slice.Tensor %12499, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12501 = torch.aten.slice.Tensor %12499, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12502 = torch.aten.gelu %12501, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12503 = torch.aten.mul.Tensor %12500, %12502 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12504 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12505 = torch.aten.view %12503, %12504 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %12506 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12507 = torch.aten.transpose.int %12506, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %12508 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12509 = torch.prims.convert_element_type %12508, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12510 = torch.prims.convert_element_type %12505, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12511 = torch.prims.convert_element_type %12507, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12512 = torch.aten.mm %12510, %12511 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12513 = torch.aten.mul.Scalar %12512, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12514 = torch.aten.mul.Scalar %12509, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12515 = torch.aten.add.Tensor %12513, %12514, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12516 = torch.prims.convert_element_type %12515, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12517 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12518 = torch.aten.view %12516, %12517 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12519 = torch.aten.add.Tensor %12518, %12471, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12520 = torch.prims.convert_element_type %12519, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12521 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_359, %result1_360 = torch.aten.var_mean.correction %12520, %12521, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12522 = torch.aten.add.Scalar %result0_359, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12523 = torch.aten.rsqrt %12522 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12524 = torch.aten.sub.Tensor %12519, %result1_360, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12525 = torch.aten.mul.Tensor %12524, %12523 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %12526 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12527 = torch.aten.mul.Tensor %12525, %12526 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %12528 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12529 = torch.aten.add.Tensor %12527, %12528, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12530 = torch.prims.convert_element_type %12529, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12531 = torch.prims.convert_element_type %result1_360, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12532 = torch.prims.convert_element_type %12523, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %12533 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12534 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12535 = torch.aten.view %12530, %12534 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12536 = torch_c.to_builtin_tensor %12535 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12537 = torch_c.to_builtin_tensor %12533 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12538 = tensor.empty() : tensor<2048x1280xf32>
    %12539 = linalg.fill ins(%cst : f32) outs(%12538 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12540 = tensor.empty() : tensor<2048x1280xf32>
    %12541 = linalg.fill ins(%cst : f32) outs(%12540 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12542:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12539, %12541, %12536, %12537, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12539, %12541)
    %12543 = arith.truncf %12542#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12544 = torch_c.from_builtin_tensor %12543 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12545 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12546 = torch.aten.view %12544, %12545 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %12547 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12548 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12549 = torch.aten.view %12530, %12548 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12550 = torch_c.to_builtin_tensor %12549 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12551 = torch_c.to_builtin_tensor %12547 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12552 = tensor.empty() : tensor<2048x1280xf32>
    %12553 = linalg.fill ins(%cst : f32) outs(%12552 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12554 = tensor.empty() : tensor<2048x1280xf32>
    %12555 = linalg.fill ins(%cst : f32) outs(%12554 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12556:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12553, %12555, %12550, %12551, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12553, %12555)
    %12557 = arith.truncf %12556#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12558 = torch_c.from_builtin_tensor %12557 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12559 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12560 = torch.aten.view %12558, %12559 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %12561 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12562 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12563 = torch.aten.view %12530, %12562 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12564 = torch_c.to_builtin_tensor %12563 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12565 = torch_c.to_builtin_tensor %12561 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12566 = tensor.empty() : tensor<2048x1280xf32>
    %12567 = linalg.fill ins(%cst : f32) outs(%12566 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12568 = tensor.empty() : tensor<2048x1280xf32>
    %12569 = linalg.fill ins(%cst : f32) outs(%12568 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12570:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12567, %12569, %12564, %12565, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12567, %12569)
    %12571 = arith.truncf %12570#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12572 = torch_c.from_builtin_tensor %12571 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12573 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12574 = torch.aten.view %12572, %12573 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12575 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12576 = torch.aten.view %12546, %12575 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12577 = torch.aten.transpose.int %12576, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12578 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12579 = torch.aten.view %12560, %12578 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12580 = torch.aten.transpose.int %12579, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12581 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12582 = torch.aten.view %12574, %12581 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12583 = torch.aten.transpose.int %12582, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12584:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12577, %12580, %12583, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12585 = torch.aten.transpose.int %12584#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12586 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12587 = torch.aten.view %12585, %12586 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12588 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12589 = torch.aten.view %12587, %12588 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12590 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12591 = torch.aten.transpose.int %12590, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %12592 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12593 = torch.prims.convert_element_type %12592, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12594 = torch.prims.convert_element_type %12589, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12595 = torch.prims.convert_element_type %12591, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12596 = torch.aten.mm %12594, %12595 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12597 = torch.aten.mul.Scalar %12596, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12598 = torch.aten.mul.Scalar %12593, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12599 = torch.aten.add.Tensor %12597, %12598, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12600 = torch.prims.convert_element_type %12599, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12601 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12602 = torch.aten.view %12600, %12601 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12603 = torch.aten.div.Scalar %12602, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12604 = torch.aten.add.Tensor %12603, %12519, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12605 = torch.prims.convert_element_type %12604, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12606 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_361, %result1_362 = torch.aten.var_mean.correction %12605, %12606, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12607 = torch.aten.add.Scalar %result0_361, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12608 = torch.aten.rsqrt %12607 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12609 = torch.aten.sub.Tensor %12604, %result1_362, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12610 = torch.aten.mul.Tensor %12609, %12608 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %12611 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12612 = torch.aten.mul.Tensor %12610, %12611 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %12613 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12614 = torch.aten.add.Tensor %12612, %12613, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12615 = torch.prims.convert_element_type %12614, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12616 = torch.prims.convert_element_type %result1_362, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12617 = torch.prims.convert_element_type %12608, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %12618 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12619 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12620 = torch.aten.view %12615, %12619 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12621 = torch_c.to_builtin_tensor %12620 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12622 = torch_c.to_builtin_tensor %12618 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12623 = tensor.empty() : tensor<2048x1280xf32>
    %12624 = linalg.fill ins(%cst : f32) outs(%12623 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12625 = tensor.empty() : tensor<2048x1280xf32>
    %12626 = linalg.fill ins(%cst : f32) outs(%12625 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12627:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12624, %12626, %12621, %12622, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12624, %12626)
    %12628 = arith.truncf %12627#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12629 = torch_c.from_builtin_tensor %12628 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12630 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12631 = torch.aten.view %12629, %12630 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %12632 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12633 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12634 = torch.aten.view %4, %12633 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12635 = torch_c.to_builtin_tensor %12634 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12636 = torch_c.to_builtin_tensor %12632 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12637 = tensor.empty() : tensor<128x1280xf32>
    %12638 = linalg.fill ins(%cst : f32) outs(%12637 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12639 = tensor.empty() : tensor<128x1280xf32>
    %12640 = linalg.fill ins(%cst : f32) outs(%12639 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12641:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12638, %12640, %12635, %12636, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12638, %12640)
    %12642 = arith.truncf %12641#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12643 = torch_c.from_builtin_tensor %12642 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12644 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12645 = torch.aten.view %12643, %12644 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %12646 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12647 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12648 = torch.aten.view %4, %12647 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12649 = torch_c.to_builtin_tensor %12648 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12650 = torch_c.to_builtin_tensor %12646 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12651 = tensor.empty() : tensor<128x1280xf32>
    %12652 = linalg.fill ins(%cst : f32) outs(%12651 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12653 = tensor.empty() : tensor<128x1280xf32>
    %12654 = linalg.fill ins(%cst : f32) outs(%12653 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12655:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12652, %12654, %12649, %12650, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12652, %12654)
    %12656 = arith.truncf %12655#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12657 = torch_c.from_builtin_tensor %12656 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12658 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12659 = torch.aten.view %12657, %12658 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12660 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12661 = torch.aten.view %12631, %12660 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12662 = torch.aten.transpose.int %12661, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12663 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12664 = torch.aten.view %12645, %12663 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12665 = torch.aten.transpose.int %12664, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12666 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12667 = torch.aten.view %12659, %12666 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12668 = torch.aten.transpose.int %12667, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12669:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12662, %12665, %12668, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12670 = torch.aten.transpose.int %12669#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12671 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12672 = torch.aten.view %12670, %12671 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12673 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12674 = torch.aten.view %12672, %12673 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12675 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12676 = torch.aten.transpose.int %12675, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %12677 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12678 = torch.prims.convert_element_type %12677, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12679 = torch.prims.convert_element_type %12674, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12680 = torch.prims.convert_element_type %12676, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12681 = torch.aten.mm %12679, %12680 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12682 = torch.aten.mul.Scalar %12681, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12683 = torch.aten.mul.Scalar %12678, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12684 = torch.aten.add.Tensor %12682, %12683, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12685 = torch.prims.convert_element_type %12684, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12686 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12687 = torch.aten.view %12685, %12686 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12688 = torch.aten.div.Scalar %12687, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12689 = torch.aten.add.Tensor %12688, %12604, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12690 = torch.prims.convert_element_type %12689, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12691 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_363, %result1_364 = torch.aten.var_mean.correction %12690, %12691, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12692 = torch.aten.add.Scalar %result0_363, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12693 = torch.aten.rsqrt %12692 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12694 = torch.aten.sub.Tensor %12689, %result1_364, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12695 = torch.aten.mul.Tensor %12694, %12693 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %12696 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12697 = torch.aten.mul.Tensor %12695, %12696 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %12698 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12699 = torch.aten.add.Tensor %12697, %12698, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12700 = torch.prims.convert_element_type %12699, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12701 = torch.prims.convert_element_type %result1_364, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12702 = torch.prims.convert_element_type %12693, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12703 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12704 = torch.aten.view %12700, %12703 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12705 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12706 = torch.aten.transpose.int %12705, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %12707 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12708 = torch.prims.convert_element_type %12707, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12709 = torch.prims.convert_element_type %12704, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12710 = torch.prims.convert_element_type %12706, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12711 = torch.aten.mm %12709, %12710 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12712 = torch.aten.mul.Scalar %12711, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12713 = torch.aten.mul.Scalar %12708, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12714 = torch.aten.add.Tensor %12712, %12713, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12715 = torch.prims.convert_element_type %12714, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12716 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12717 = torch.aten.view %12715, %12716 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12718 = torch.aten.slice.Tensor %12717, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12719 = torch.aten.slice.Tensor %12717, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12720 = torch.aten.gelu %12719, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12721 = torch.aten.mul.Tensor %12718, %12720 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12722 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12723 = torch.aten.view %12721, %12722 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %12724 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12725 = torch.aten.transpose.int %12724, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %12726 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12727 = torch.prims.convert_element_type %12726, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12728 = torch.prims.convert_element_type %12723, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12729 = torch.prims.convert_element_type %12725, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12730 = torch.aten.mm %12728, %12729 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12731 = torch.aten.mul.Scalar %12730, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12732 = torch.aten.mul.Scalar %12727, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12733 = torch.aten.add.Tensor %12731, %12732, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12734 = torch.prims.convert_element_type %12733, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12735 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12736 = torch.aten.view %12734, %12735 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12737 = torch.aten.add.Tensor %12736, %12689, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12738 = torch.prims.convert_element_type %12737, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12739 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_365, %result1_366 = torch.aten.var_mean.correction %12738, %12739, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12740 = torch.aten.add.Scalar %result0_365, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12741 = torch.aten.rsqrt %12740 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12742 = torch.aten.sub.Tensor %12737, %result1_366, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12743 = torch.aten.mul.Tensor %12742, %12741 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %12744 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12745 = torch.aten.mul.Tensor %12743, %12744 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %12746 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12747 = torch.aten.add.Tensor %12745, %12746, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12748 = torch.prims.convert_element_type %12747, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12749 = torch.prims.convert_element_type %result1_366, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12750 = torch.prims.convert_element_type %12741, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %12751 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12752 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12753 = torch.aten.view %12748, %12752 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12754 = torch_c.to_builtin_tensor %12753 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12755 = torch_c.to_builtin_tensor %12751 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12756 = tensor.empty() : tensor<2048x1280xf32>
    %12757 = linalg.fill ins(%cst : f32) outs(%12756 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12758 = tensor.empty() : tensor<2048x1280xf32>
    %12759 = linalg.fill ins(%cst : f32) outs(%12758 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12760:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12757, %12759, %12754, %12755, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12757, %12759)
    %12761 = arith.truncf %12760#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12762 = torch_c.from_builtin_tensor %12761 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12763 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12764 = torch.aten.view %12762, %12763 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %12765 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12766 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12767 = torch.aten.view %12748, %12766 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12768 = torch_c.to_builtin_tensor %12767 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12769 = torch_c.to_builtin_tensor %12765 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12770 = tensor.empty() : tensor<2048x1280xf32>
    %12771 = linalg.fill ins(%cst : f32) outs(%12770 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12772 = tensor.empty() : tensor<2048x1280xf32>
    %12773 = linalg.fill ins(%cst : f32) outs(%12772 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12774:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12771, %12773, %12768, %12769, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12771, %12773)
    %12775 = arith.truncf %12774#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12776 = torch_c.from_builtin_tensor %12775 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12777 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12778 = torch.aten.view %12776, %12777 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %12779 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12780 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12781 = torch.aten.view %12748, %12780 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12782 = torch_c.to_builtin_tensor %12781 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12783 = torch_c.to_builtin_tensor %12779 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12784 = tensor.empty() : tensor<2048x1280xf32>
    %12785 = linalg.fill ins(%cst : f32) outs(%12784 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12786 = tensor.empty() : tensor<2048x1280xf32>
    %12787 = linalg.fill ins(%cst : f32) outs(%12786 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12788:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12785, %12787, %12782, %12783, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12785, %12787)
    %12789 = arith.truncf %12788#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12790 = torch_c.from_builtin_tensor %12789 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12791 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12792 = torch.aten.view %12790, %12791 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12793 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12794 = torch.aten.view %12764, %12793 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12795 = torch.aten.transpose.int %12794, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12796 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12797 = torch.aten.view %12778, %12796 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12798 = torch.aten.transpose.int %12797, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12799 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12800 = torch.aten.view %12792, %12799 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12801 = torch.aten.transpose.int %12800, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12802:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12795, %12798, %12801, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12803 = torch.aten.transpose.int %12802#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12804 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12805 = torch.aten.view %12803, %12804 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12806 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12807 = torch.aten.view %12805, %12806 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %12808 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12809 = torch.aten.transpose.int %12808, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %12810 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12811 = torch.prims.convert_element_type %12810, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12812 = torch.prims.convert_element_type %12807, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12813 = torch.prims.convert_element_type %12809, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12814 = torch.aten.mm %12812, %12813 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12815 = torch.aten.mul.Scalar %12814, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12816 = torch.aten.mul.Scalar %12811, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12817 = torch.aten.add.Tensor %12815, %12816, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12818 = torch.prims.convert_element_type %12817, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12819 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12820 = torch.aten.view %12818, %12819 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12821 = torch.aten.div.Scalar %12820, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12822 = torch.aten.add.Tensor %12821, %12737, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12823 = torch.prims.convert_element_type %12822, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12824 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_367, %result1_368 = torch.aten.var_mean.correction %12823, %12824, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12825 = torch.aten.add.Scalar %result0_367, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12826 = torch.aten.rsqrt %12825 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12827 = torch.aten.sub.Tensor %12822, %result1_368, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12828 = torch.aten.mul.Tensor %12827, %12826 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %12829 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12830 = torch.aten.mul.Tensor %12828, %12829 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %12831 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12832 = torch.aten.add.Tensor %12830, %12831, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12833 = torch.prims.convert_element_type %12832, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12834 = torch.prims.convert_element_type %result1_368, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12835 = torch.prims.convert_element_type %12826, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %12836 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12837 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12838 = torch.aten.view %12833, %12837 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12839 = torch_c.to_builtin_tensor %12838 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12840 = torch_c.to_builtin_tensor %12836 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12841 = tensor.empty() : tensor<2048x1280xf32>
    %12842 = linalg.fill ins(%cst : f32) outs(%12841 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12843 = tensor.empty() : tensor<2048x1280xf32>
    %12844 = linalg.fill ins(%cst : f32) outs(%12843 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12845:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12842, %12844, %12839, %12840, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12842, %12844)
    %12846 = arith.truncf %12845#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12847 = torch_c.from_builtin_tensor %12846 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12848 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12849 = torch.aten.view %12847, %12848 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %12850 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12851 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12852 = torch.aten.view %4, %12851 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12853 = torch_c.to_builtin_tensor %12852 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12854 = torch_c.to_builtin_tensor %12850 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12855 = tensor.empty() : tensor<128x1280xf32>
    %12856 = linalg.fill ins(%cst : f32) outs(%12855 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12857 = tensor.empty() : tensor<128x1280xf32>
    %12858 = linalg.fill ins(%cst : f32) outs(%12857 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12859:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12856, %12858, %12853, %12854, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12856, %12858)
    %12860 = arith.truncf %12859#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12861 = torch_c.from_builtin_tensor %12860 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12862 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12863 = torch.aten.view %12861, %12862 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %12864 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %12865 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %12866 = torch.aten.view %4, %12865 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %12867 = torch_c.to_builtin_tensor %12866 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %12868 = torch_c.to_builtin_tensor %12864 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %12869 = tensor.empty() : tensor<128x1280xf32>
    %12870 = linalg.fill ins(%cst : f32) outs(%12869 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12871 = tensor.empty() : tensor<128x1280xf32>
    %12872 = linalg.fill ins(%cst : f32) outs(%12871 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %12873:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %12870, %12872, %12867, %12868, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12870, %12872)
    %12874 = arith.truncf %12873#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %12875 = torch_c.from_builtin_tensor %12874 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %12876 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12877 = torch.aten.view %12875, %12876 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %12878 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12879 = torch.aten.view %12849, %12878 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %12880 = torch.aten.transpose.int %12879, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %12881 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12882 = torch.aten.view %12863, %12881 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12883 = torch.aten.transpose.int %12882, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12884 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12885 = torch.aten.view %12877, %12884 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %12886 = torch.aten.transpose.int %12885, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %12887:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%12880, %12883, %12886, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %12888 = torch.aten.transpose.int %12887#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %12889 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12890 = torch.aten.view %12888, %12889 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12891 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12892 = torch.aten.view %12890, %12891 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %12893 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12894 = torch.aten.transpose.int %12893, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %12895 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12896 = torch.prims.convert_element_type %12895, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12897 = torch.prims.convert_element_type %12892, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12898 = torch.prims.convert_element_type %12894, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %12899 = torch.aten.mm %12897, %12898 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12900 = torch.aten.mul.Scalar %12899, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12901 = torch.aten.mul.Scalar %12896, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12902 = torch.aten.add.Tensor %12900, %12901, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12903 = torch.prims.convert_element_type %12902, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12904 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12905 = torch.aten.view %12903, %12904 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12906 = torch.aten.div.Scalar %12905, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %12907 = torch.aten.add.Tensor %12906, %12822, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12908 = torch.prims.convert_element_type %12907, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12909 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_369, %result1_370 = torch.aten.var_mean.correction %12908, %12909, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12910 = torch.aten.add.Scalar %result0_369, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12911 = torch.aten.rsqrt %12910 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12912 = torch.aten.sub.Tensor %12907, %result1_370, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12913 = torch.aten.mul.Tensor %12912, %12911 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %12914 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12915 = torch.aten.mul.Tensor %12913, %12914 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %12916 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12917 = torch.aten.add.Tensor %12915, %12916, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12918 = torch.prims.convert_element_type %12917, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12919 = torch.prims.convert_element_type %result1_370, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12920 = torch.prims.convert_element_type %12911, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12921 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12922 = torch.aten.view %12918, %12921 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %12923 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %12924 = torch.aten.transpose.int %12923, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %12925 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %12926 = torch.prims.convert_element_type %12925, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %12927 = torch.prims.convert_element_type %12922, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12928 = torch.prims.convert_element_type %12924, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %12929 = torch.aten.mm %12927, %12928 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %12930 = torch.aten.mul.Scalar %12929, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12931 = torch.aten.mul.Scalar %12926, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %12932 = torch.aten.add.Tensor %12930, %12931, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %12933 = torch.prims.convert_element_type %12932, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %12934 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12935 = torch.aten.view %12933, %12934 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %12936 = torch.aten.slice.Tensor %12935, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12937 = torch.aten.slice.Tensor %12935, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %12938 = torch.aten.gelu %12937, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %12939 = torch.aten.mul.Tensor %12936, %12938 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %12940 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %12941 = torch.aten.view %12939, %12940 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %12942 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %12943 = torch.aten.transpose.int %12942, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %12944 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12945 = torch.prims.convert_element_type %12944, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %12946 = torch.prims.convert_element_type %12941, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %12947 = torch.prims.convert_element_type %12943, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %12948 = torch.aten.mm %12946, %12947 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %12949 = torch.aten.mul.Scalar %12948, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12950 = torch.aten.mul.Scalar %12945, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %12951 = torch.aten.add.Tensor %12949, %12950, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %12952 = torch.prims.convert_element_type %12951, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %12953 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12954 = torch.aten.view %12952, %12953 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %12955 = torch.aten.add.Tensor %12954, %12907, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12956 = torch.prims.convert_element_type %12955, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12957 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_371, %result1_372 = torch.aten.var_mean.correction %12956, %12957, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %12958 = torch.aten.add.Scalar %result0_371, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %12959 = torch.aten.rsqrt %12958 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %12960 = torch.aten.sub.Tensor %12955, %result1_372, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12961 = torch.aten.mul.Tensor %12960, %12959 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %12962 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12963 = torch.aten.mul.Tensor %12961, %12962 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %12964 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %12965 = torch.aten.add.Tensor %12963, %12964, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %12966 = torch.prims.convert_element_type %12965, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %12967 = torch.prims.convert_element_type %result1_372, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %12968 = torch.prims.convert_element_type %12959, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %12969 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12970 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12971 = torch.aten.view %12966, %12970 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12972 = torch_c.to_builtin_tensor %12971 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12973 = torch_c.to_builtin_tensor %12969 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12974 = tensor.empty() : tensor<2048x1280xf32>
    %12975 = linalg.fill ins(%cst : f32) outs(%12974 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12976 = tensor.empty() : tensor<2048x1280xf32>
    %12977 = linalg.fill ins(%cst : f32) outs(%12976 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12978:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12975, %12977, %12972, %12973, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12975, %12977)
    %12979 = arith.truncf %12978#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12980 = torch_c.from_builtin_tensor %12979 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12981 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12982 = torch.aten.view %12980, %12981 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %12983 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12984 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12985 = torch.aten.view %12966, %12984 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %12986 = torch_c.to_builtin_tensor %12985 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %12987 = torch_c.to_builtin_tensor %12983 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %12988 = tensor.empty() : tensor<2048x1280xf32>
    %12989 = linalg.fill ins(%cst : f32) outs(%12988 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12990 = tensor.empty() : tensor<2048x1280xf32>
    %12991 = linalg.fill ins(%cst : f32) outs(%12990 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %12992:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %12989, %12991, %12986, %12987, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%12989, %12991)
    %12993 = arith.truncf %12992#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %12994 = torch_c.from_builtin_tensor %12993 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %12995 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12996 = torch.aten.view %12994, %12995 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %12997 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %12998 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %12999 = torch.aten.view %12966, %12998 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13000 = torch_c.to_builtin_tensor %12999 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13001 = torch_c.to_builtin_tensor %12997 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13002 = tensor.empty() : tensor<2048x1280xf32>
    %13003 = linalg.fill ins(%cst : f32) outs(%13002 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13004 = tensor.empty() : tensor<2048x1280xf32>
    %13005 = linalg.fill ins(%cst : f32) outs(%13004 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13006:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13003, %13005, %13000, %13001, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13003, %13005)
    %13007 = arith.truncf %13006#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13008 = torch_c.from_builtin_tensor %13007 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13009 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13010 = torch.aten.view %13008, %13009 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13011 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13012 = torch.aten.view %12982, %13011 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13013 = torch.aten.transpose.int %13012, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13014 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13015 = torch.aten.view %12996, %13014 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13016 = torch.aten.transpose.int %13015, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13017 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13018 = torch.aten.view %13010, %13017 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13019 = torch.aten.transpose.int %13018, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13020:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13013, %13016, %13019, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13021 = torch.aten.transpose.int %13020#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13022 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13023 = torch.aten.view %13021, %13022 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13024 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13025 = torch.aten.view %13023, %13024 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13026 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13027 = torch.aten.transpose.int %13026, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %13028 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13029 = torch.prims.convert_element_type %13028, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13030 = torch.prims.convert_element_type %13025, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13031 = torch.prims.convert_element_type %13027, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13032 = torch.aten.mm %13030, %13031 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13033 = torch.aten.mul.Scalar %13032, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13034 = torch.aten.mul.Scalar %13029, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13035 = torch.aten.add.Tensor %13033, %13034, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13036 = torch.prims.convert_element_type %13035, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13037 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13038 = torch.aten.view %13036, %13037 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13039 = torch.aten.div.Scalar %13038, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13040 = torch.aten.add.Tensor %13039, %12955, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13041 = torch.prims.convert_element_type %13040, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13042 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_373, %result1_374 = torch.aten.var_mean.correction %13041, %13042, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13043 = torch.aten.add.Scalar %result0_373, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13044 = torch.aten.rsqrt %13043 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13045 = torch.aten.sub.Tensor %13040, %result1_374, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13046 = torch.aten.mul.Tensor %13045, %13044 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %13047 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13048 = torch.aten.mul.Tensor %13046, %13047 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %13049 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13050 = torch.aten.add.Tensor %13048, %13049, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13051 = torch.prims.convert_element_type %13050, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13052 = torch.prims.convert_element_type %result1_374, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13053 = torch.prims.convert_element_type %13044, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %13054 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13055 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13056 = torch.aten.view %13051, %13055 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13057 = torch_c.to_builtin_tensor %13056 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13058 = torch_c.to_builtin_tensor %13054 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13059 = tensor.empty() : tensor<2048x1280xf32>
    %13060 = linalg.fill ins(%cst : f32) outs(%13059 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13061 = tensor.empty() : tensor<2048x1280xf32>
    %13062 = linalg.fill ins(%cst : f32) outs(%13061 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13063:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13060, %13062, %13057, %13058, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13060, %13062)
    %13064 = arith.truncf %13063#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13065 = torch_c.from_builtin_tensor %13064 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13066 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13067 = torch.aten.view %13065, %13066 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %13068 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13069 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13070 = torch.aten.view %4, %13069 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13071 = torch_c.to_builtin_tensor %13070 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13072 = torch_c.to_builtin_tensor %13068 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13073 = tensor.empty() : tensor<128x1280xf32>
    %13074 = linalg.fill ins(%cst : f32) outs(%13073 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13075 = tensor.empty() : tensor<128x1280xf32>
    %13076 = linalg.fill ins(%cst : f32) outs(%13075 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13077:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13074, %13076, %13071, %13072, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13074, %13076)
    %13078 = arith.truncf %13077#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13079 = torch_c.from_builtin_tensor %13078 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13080 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13081 = torch.aten.view %13079, %13080 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %13082 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13083 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13084 = torch.aten.view %4, %13083 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13085 = torch_c.to_builtin_tensor %13084 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13086 = torch_c.to_builtin_tensor %13082 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13087 = tensor.empty() : tensor<128x1280xf32>
    %13088 = linalg.fill ins(%cst : f32) outs(%13087 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13089 = tensor.empty() : tensor<128x1280xf32>
    %13090 = linalg.fill ins(%cst : f32) outs(%13089 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13091:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13088, %13090, %13085, %13086, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13088, %13090)
    %13092 = arith.truncf %13091#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13093 = torch_c.from_builtin_tensor %13092 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13094 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13095 = torch.aten.view %13093, %13094 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13096 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13097 = torch.aten.view %13067, %13096 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13098 = torch.aten.transpose.int %13097, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13099 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13100 = torch.aten.view %13081, %13099 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13101 = torch.aten.transpose.int %13100, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13102 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13103 = torch.aten.view %13095, %13102 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13104 = torch.aten.transpose.int %13103, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13105:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13098, %13101, %13104, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13106 = torch.aten.transpose.int %13105#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13107 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13108 = torch.aten.view %13106, %13107 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13109 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13110 = torch.aten.view %13108, %13109 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13111 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13112 = torch.aten.transpose.int %13111, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %13113 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13114 = torch.prims.convert_element_type %13113, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13115 = torch.prims.convert_element_type %13110, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13116 = torch.prims.convert_element_type %13112, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13117 = torch.aten.mm %13115, %13116 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13118 = torch.aten.mul.Scalar %13117, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13119 = torch.aten.mul.Scalar %13114, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13120 = torch.aten.add.Tensor %13118, %13119, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13121 = torch.prims.convert_element_type %13120, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13122 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13123 = torch.aten.view %13121, %13122 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13124 = torch.aten.div.Scalar %13123, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13125 = torch.aten.add.Tensor %13124, %13040, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13126 = torch.prims.convert_element_type %13125, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13127 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_375, %result1_376 = torch.aten.var_mean.correction %13126, %13127, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13128 = torch.aten.add.Scalar %result0_375, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13129 = torch.aten.rsqrt %13128 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13130 = torch.aten.sub.Tensor %13125, %result1_376, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13131 = torch.aten.mul.Tensor %13130, %13129 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %13132 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13133 = torch.aten.mul.Tensor %13131, %13132 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %13134 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13135 = torch.aten.add.Tensor %13133, %13134, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13136 = torch.prims.convert_element_type %13135, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13137 = torch.prims.convert_element_type %result1_376, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13138 = torch.prims.convert_element_type %13129, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13139 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13140 = torch.aten.view %13136, %13139 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13141 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13142 = torch.aten.transpose.int %13141, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %13143 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13144 = torch.prims.convert_element_type %13143, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13145 = torch.prims.convert_element_type %13140, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13146 = torch.prims.convert_element_type %13142, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13147 = torch.aten.mm %13145, %13146 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13148 = torch.aten.mul.Scalar %13147, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13149 = torch.aten.mul.Scalar %13144, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13150 = torch.aten.add.Tensor %13148, %13149, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13151 = torch.prims.convert_element_type %13150, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13152 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13153 = torch.aten.view %13151, %13152 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13154 = torch.aten.slice.Tensor %13153, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13155 = torch.aten.slice.Tensor %13153, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13156 = torch.aten.gelu %13155, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13157 = torch.aten.mul.Tensor %13154, %13156 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13158 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13159 = torch.aten.view %13157, %13158 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %13160 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13161 = torch.aten.transpose.int %13160, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %13162 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13163 = torch.prims.convert_element_type %13162, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13164 = torch.prims.convert_element_type %13159, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13165 = torch.prims.convert_element_type %13161, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13166 = torch.aten.mm %13164, %13165 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13167 = torch.aten.mul.Scalar %13166, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13168 = torch.aten.mul.Scalar %13163, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13169 = torch.aten.add.Tensor %13167, %13168, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13170 = torch.prims.convert_element_type %13169, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13171 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13172 = torch.aten.view %13170, %13171 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13173 = torch.aten.add.Tensor %13172, %13125, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13174 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13175 = torch.aten.view %13173, %13174 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_out.weight = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_out.weight : tensor<1280x1280xf16>
    %13176 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13177 = torch.aten.transpose.int %13176, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.1.proj_out.bias = util.global.load @_params.unet.up_blocks.0.attentions.1.proj_out.bias : tensor<1280xf16>
    %13178 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.1.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13179 = torch.prims.convert_element_type %13178, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13180 = torch.prims.convert_element_type %13175, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13181 = torch.prims.convert_element_type %13177, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13182 = torch.aten.mm %13180, %13181 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13183 = torch.aten.mul.Scalar %13182, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13184 = torch.aten.mul.Scalar %13179, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13185 = torch.aten.add.Tensor %13183, %13184, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13186 = torch.prims.convert_element_type %13185, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13187 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13188 = torch.aten.view %13186, %13187 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13189 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13190 = torch.aten.view %13188, %13189 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %13191 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13192 = torch.aten.permute %13190, %13191 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %13193 = torch.aten.add.Tensor %13192, %10942, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %13194 = torch.prim.ListConstruct %13193, %1343 : (!torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,640,32,32],f16>) -> !torch.list<vtensor>
    %13195 = torch.aten.cat %13194, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,1920,32,32],f16>
    %13196 = torch.prim.ListConstruct %int2, %int32, %int60, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13197 = torch.aten.view %13195, %13196 : !torch.vtensor<[2,1920,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,60,1024],f16>
    %13198 = torch.prims.convert_element_type %13197, %int6 : !torch.vtensor<[2,32,60,1024],f16>, !torch.int -> !torch.vtensor<[2,32,60,1024],f32>
    %13199 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_377, %result1_378 = torch.aten.var_mean.correction %13198, %13199, %int0, %true : !torch.vtensor<[2,32,60,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %13200 = torch.aten.add.Scalar %result0_377, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %13201 = torch.aten.rsqrt %13200 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %13202 = torch.aten.sub.Tensor %13197, %result1_378, %int1 : !torch.vtensor<[2,32,60,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,60,1024],f32>
    %13203 = torch.aten.mul.Tensor %13202, %13201 : !torch.vtensor<[2,32,60,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,60,1024],f32>
    %13204 = torch.prim.ListConstruct %int2, %int1920, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13205 = torch.aten.view %13203, %13204 : !torch.vtensor<[2,32,60,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1920,32,32],f32>
    %_params.unet.up_blocks.0.resnets.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.norm1.bias : tensor<1920xf16>
    %13206 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm1.bias : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %13207 = torch.aten.unsqueeze %13206, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %13208 = torch.aten.unsqueeze %13207, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %13209 = torch.aten.unsqueeze %13208, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %_params.unet.up_blocks.0.resnets.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.norm1.weight : tensor<1920xf16>
    %13210 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm1.weight : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %13211 = torch.aten.unsqueeze %13210, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %13212 = torch.aten.unsqueeze %13211, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %13213 = torch.aten.unsqueeze %13212, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %13214 = torch.aten.mul.Tensor %13205, %13213 : !torch.vtensor<[2,1920,32,32],f32>, !torch.vtensor<[1,1920,1,1],f16> -> !torch.vtensor<[2,1920,32,32],f32>
    %13215 = torch.aten.add.Tensor %13214, %13209, %int1 : !torch.vtensor<[2,1920,32,32],f32>, !torch.vtensor<[1,1920,1,1],f16>, !torch.int -> !torch.vtensor<[2,1920,32,32],f32>
    %13216 = torch.prims.convert_element_type %13215, %int5 : !torch.vtensor<[2,1920,32,32],f32>, !torch.int -> !torch.vtensor<[2,1920,32,32],f16>
    %13217 = torch.prims.convert_element_type %result1_378, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %13218 = torch.prims.convert_element_type %13201, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %13219 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %13220 = torch.prims.squeeze %13217, %13219 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %13221 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %13222 = torch.prims.squeeze %13220, %13221 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %13223 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %13224 = torch.prims.squeeze %13218, %13223 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %13225 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %13226 = torch.prims.squeeze %13224, %13225 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %13227 = torch.aten.silu %13216 : !torch.vtensor<[2,1920,32,32],f16> -> !torch.vtensor<[2,1920,32,32],f16>
    %_params.unet.up_blocks.0.resnets.2.conv1.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.conv1.weight : tensor<1280x1920x3x3xf16>
    %13228 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv1.weight : tensor<1280x1920x3x3xf16> -> !torch.vtensor<[1280,1920,3,3],f16>
    %_params.unet.up_blocks.0.resnets.2.conv1.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.conv1.bias : tensor<1280xf16>
    %13229 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13230 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13231 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13232 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13233 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %13234 = torch.aten.convolution %13227, %13228, %13229, %13230, %13231, %13232, %false, %13233, %int1 : !torch.vtensor<[2,1920,32,32],f16>, !torch.vtensor<[1280,1920,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %13235 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight : tensor<1280x1280xf16>
    %13236 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13237 = torch.aten.transpose.int %13236, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias : tensor<1280xf16>
    %13238 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13239 = torch.prims.convert_element_type %13238, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13240 = torch.prims.convert_element_type %13235, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %13241 = torch.prims.convert_element_type %13237, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13242 = torch.aten.mm %13240, %13241 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %13243 = torch.aten.mul.Scalar %13242, %int1 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %13244 = torch.aten.mul.Scalar %13239, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13245 = torch.aten.add.Tensor %13243, %13244, %int1 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %13246 = torch.prims.convert_element_type %13245, %int5 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %13247 = torch.aten.unsqueeze %13246, %int2 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %13248 = torch.aten.unsqueeze %13247, %int3 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %13249 = torch.aten.add.Tensor %13234, %13248, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %13250 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13251 = torch.aten.view %13249, %13250 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %13252 = torch.prims.convert_element_type %13251, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %13253 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_379, %result1_380 = torch.aten.var_mean.correction %13252, %13253, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %13254 = torch.aten.add.Scalar %result0_379, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %13255 = torch.aten.rsqrt %13254 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %13256 = torch.aten.sub.Tensor %13251, %result1_380, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %13257 = torch.aten.mul.Tensor %13256, %13255 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %13258 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13259 = torch.aten.view %13257, %13258 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.resnets.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.norm2.bias : tensor<1280xf16>
    %13260 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13261 = torch.aten.unsqueeze %13260, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %13262 = torch.aten.unsqueeze %13261, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %13263 = torch.aten.unsqueeze %13262, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.resnets.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.norm2.weight : tensor<1280xf16>
    %13264 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13265 = torch.aten.unsqueeze %13264, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %13266 = torch.aten.unsqueeze %13265, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %13267 = torch.aten.unsqueeze %13266, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %13268 = torch.aten.mul.Tensor %13259, %13267 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %13269 = torch.aten.add.Tensor %13268, %13263, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %13270 = torch.prims.convert_element_type %13269, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %13271 = torch.prims.convert_element_type %result1_380, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %13272 = torch.prims.convert_element_type %13255, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %13273 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %13274 = torch.prims.squeeze %13271, %13273 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %13275 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %13276 = torch.prims.squeeze %13274, %13275 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %13277 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %13278 = torch.prims.squeeze %13272, %13277 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %13279 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %13280 = torch.prims.squeeze %13278, %13279 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %13281 = torch.aten.silu %13270 : !torch.vtensor<[2,1280,32,32],f16> -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.2.conv2.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.conv2.weight : tensor<1280x1280x3x3xf16>
    %13282 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.resnets.2.conv2.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.conv2.bias : tensor<1280xf16>
    %13283 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13284 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13285 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13286 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13287 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %13288 = torch.aten.convolution %13281, %13282, %13283, %13284, %13285, %13286, %false, %13287, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight : tensor<1280x1920x1x1xf16>
    %13289 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv_shortcut.weight : tensor<1280x1920x1x1xf16> -> !torch.vtensor<[1280,1920,1,1],f16>
    %_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias : tensor<1280xf16>
    %13290 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.resnets.2.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13291 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13292 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %13293 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %13294 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %13295 = torch.aten.convolution %13195, %13289, %13290, %13291, %13292, %13293, %false, %13294, %int1 : !torch.vtensor<[2,1920,32,32],f16>, !torch.vtensor<[1280,1920,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %13296 = torch.aten.add.Tensor %13295, %13288, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %13297 = torch.aten.div.Scalar %13296, %float1.000000e00 : !torch.vtensor<[2,1280,32,32],f16>, !torch.float -> !torch.vtensor<[2,1280,32,32],f16>
    %13298 = torch.prim.ListConstruct %int2, %int32, %int40, %int1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13299 = torch.aten.view %13297, %13298 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,1024],f16>
    %13300 = torch.prims.convert_element_type %13299, %int6 : !torch.vtensor<[2,32,40,1024],f16>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %13301 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_381, %result1_382 = torch.aten.var_mean.correction %13300, %13301, %int0, %true : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %13302 = torch.aten.add.Scalar %result0_381, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %13303 = torch.aten.rsqrt %13302 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %13304 = torch.aten.sub.Tensor %13299, %result1_382, %int1 : !torch.vtensor<[2,32,40,1024],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,1024],f32>
    %13305 = torch.aten.mul.Tensor %13304, %13303 : !torch.vtensor<[2,32,40,1024],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,1024],f32>
    %13306 = torch.prim.ListConstruct %int2, %int1280, %int32, %int32 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13307 = torch.aten.view %13305, %13306 : !torch.vtensor<[2,32,40,1024],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f32>
    %_params.unet.up_blocks.0.attentions.2.norm.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.norm.bias : tensor<1280xf16>
    %13308 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13309 = torch.aten.unsqueeze %13308, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %13310 = torch.aten.unsqueeze %13309, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %13311 = torch.aten.unsqueeze %13310, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.0.attentions.2.norm.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.norm.weight : tensor<1280xf16>
    %13312 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13313 = torch.aten.unsqueeze %13312, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %13314 = torch.aten.unsqueeze %13313, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %13315 = torch.aten.unsqueeze %13314, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %13316 = torch.aten.mul.Tensor %13307, %13315 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,32,32],f32>
    %13317 = torch.aten.add.Tensor %13316, %13311, %int1 : !torch.vtensor<[2,1280,32,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %13318 = torch.prims.convert_element_type %13317, %int5 : !torch.vtensor<[2,1280,32,32],f32>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %13319 = torch.prims.convert_element_type %result1_382, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %13320 = torch.prims.convert_element_type %13303, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %13321 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %13322 = torch.prims.squeeze %13319, %13321 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %13323 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %13324 = torch.prims.squeeze %13322, %13323 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %13325 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %13326 = torch.prims.squeeze %13320, %13325 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %13327 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %13328 = torch.prims.squeeze %13326, %13327 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %13329 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13330 = torch.aten.permute %13318, %13329 : !torch.vtensor<[2,1280,32,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %13331 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13332 = torch.aten.view %13330, %13331 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_in.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_in.weight : tensor<1280x1280xf16>
    %13333 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13334 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13335 = torch.aten._unsafe_view %13332, %13334 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13336 = torch_c.to_builtin_tensor %13335 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13337 = torch_c.to_builtin_tensor %13333 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13338 = tensor.empty() : tensor<2048x1280xf32>
    %13339 = linalg.fill ins(%cst : f32) outs(%13338 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13340 = tensor.empty() : tensor<2048x1280xf32>
    %13341 = linalg.fill ins(%cst : f32) outs(%13340 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13342:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13339, %13341, %13336, %13337, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13339, %13341)
    %13343 = arith.truncf %13342#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13344 = torch_c.from_builtin_tensor %13343 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13345 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13346 = torch.aten.view %13344, %13345 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_in.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_in.bias : tensor<1280xf16>
    %13347 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13348 = torch.aten.add.Tensor %13346, %13347, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13349 = torch.prims.convert_element_type %13348, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13350 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_383, %result1_384 = torch.aten.var_mean.correction %13349, %13350, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13351 = torch.aten.add.Scalar %result0_383, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13352 = torch.aten.rsqrt %13351 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13353 = torch.aten.sub.Tensor %13348, %result1_384, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13354 = torch.aten.mul.Tensor %13353, %13352 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %13355 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13356 = torch.aten.mul.Tensor %13354, %13355 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %13357 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13358 = torch.aten.add.Tensor %13356, %13357, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13359 = torch.prims.convert_element_type %13358, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13360 = torch.prims.convert_element_type %result1_384, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13361 = torch.prims.convert_element_type %13352, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %13362 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13363 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13364 = torch.aten.view %13359, %13363 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13365 = torch_c.to_builtin_tensor %13364 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13366 = torch_c.to_builtin_tensor %13362 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13367 = tensor.empty() : tensor<2048x1280xf32>
    %13368 = linalg.fill ins(%cst : f32) outs(%13367 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13369 = tensor.empty() : tensor<2048x1280xf32>
    %13370 = linalg.fill ins(%cst : f32) outs(%13369 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13371:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13368, %13370, %13365, %13366, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13368, %13370)
    %13372 = arith.truncf %13371#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13373 = torch_c.from_builtin_tensor %13372 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13374 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13375 = torch.aten.view %13373, %13374 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %13376 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13377 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13378 = torch.aten.view %13359, %13377 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13379 = torch_c.to_builtin_tensor %13378 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13380 = torch_c.to_builtin_tensor %13376 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13381 = tensor.empty() : tensor<2048x1280xf32>
    %13382 = linalg.fill ins(%cst : f32) outs(%13381 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13383 = tensor.empty() : tensor<2048x1280xf32>
    %13384 = linalg.fill ins(%cst : f32) outs(%13383 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13385:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13382, %13384, %13379, %13380, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13382, %13384)
    %13386 = arith.truncf %13385#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13387 = torch_c.from_builtin_tensor %13386 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13388 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13389 = torch.aten.view %13387, %13388 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %13390 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13391 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13392 = torch.aten.view %13359, %13391 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13393 = torch_c.to_builtin_tensor %13392 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13394 = torch_c.to_builtin_tensor %13390 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13395 = tensor.empty() : tensor<2048x1280xf32>
    %13396 = linalg.fill ins(%cst : f32) outs(%13395 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13397 = tensor.empty() : tensor<2048x1280xf32>
    %13398 = linalg.fill ins(%cst : f32) outs(%13397 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13399:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13396, %13398, %13393, %13394, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13396, %13398)
    %13400 = arith.truncf %13399#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13401 = torch_c.from_builtin_tensor %13400 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13402 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13403 = torch.aten.view %13401, %13402 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13404 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13405 = torch.aten.view %13375, %13404 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13406 = torch.aten.transpose.int %13405, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13407 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13408 = torch.aten.view %13389, %13407 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13409 = torch.aten.transpose.int %13408, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13410 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13411 = torch.aten.view %13403, %13410 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13412 = torch.aten.transpose.int %13411, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13413:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13406, %13409, %13412, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13414 = torch.aten.transpose.int %13413#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13415 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13416 = torch.aten.view %13414, %13415 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13417 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13418 = torch.aten.view %13416, %13417 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13419 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13420 = torch.aten.transpose.int %13419, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %13421 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13422 = torch.prims.convert_element_type %13421, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13423 = torch.prims.convert_element_type %13418, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13424 = torch.prims.convert_element_type %13420, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13425 = torch.aten.mm %13423, %13424 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13426 = torch.aten.mul.Scalar %13425, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13427 = torch.aten.mul.Scalar %13422, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13428 = torch.aten.add.Tensor %13426, %13427, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13429 = torch.prims.convert_element_type %13428, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13430 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13431 = torch.aten.view %13429, %13430 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13432 = torch.aten.div.Scalar %13431, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13433 = torch.aten.add.Tensor %13432, %13348, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13434 = torch.prims.convert_element_type %13433, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13435 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_385, %result1_386 = torch.aten.var_mean.correction %13434, %13435, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13436 = torch.aten.add.Scalar %result0_385, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13437 = torch.aten.rsqrt %13436 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13438 = torch.aten.sub.Tensor %13433, %result1_386, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13439 = torch.aten.mul.Tensor %13438, %13437 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %13440 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13441 = torch.aten.mul.Tensor %13439, %13440 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %13442 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13443 = torch.aten.add.Tensor %13441, %13442, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13444 = torch.prims.convert_element_type %13443, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13445 = torch.prims.convert_element_type %result1_386, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13446 = torch.prims.convert_element_type %13437, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %13447 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13448 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13449 = torch.aten.view %13444, %13448 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13450 = torch_c.to_builtin_tensor %13449 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13451 = torch_c.to_builtin_tensor %13447 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13452 = tensor.empty() : tensor<2048x1280xf32>
    %13453 = linalg.fill ins(%cst : f32) outs(%13452 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13454 = tensor.empty() : tensor<2048x1280xf32>
    %13455 = linalg.fill ins(%cst : f32) outs(%13454 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13456:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13453, %13455, %13450, %13451, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13453, %13455)
    %13457 = arith.truncf %13456#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13458 = torch_c.from_builtin_tensor %13457 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13459 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13460 = torch.aten.view %13458, %13459 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %13461 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13462 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13463 = torch.aten.view %4, %13462 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13464 = torch_c.to_builtin_tensor %13463 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13465 = torch_c.to_builtin_tensor %13461 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13466 = tensor.empty() : tensor<128x1280xf32>
    %13467 = linalg.fill ins(%cst : f32) outs(%13466 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13468 = tensor.empty() : tensor<128x1280xf32>
    %13469 = linalg.fill ins(%cst : f32) outs(%13468 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13470:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13467, %13469, %13464, %13465, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13467, %13469)
    %13471 = arith.truncf %13470#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13472 = torch_c.from_builtin_tensor %13471 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13473 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13474 = torch.aten.view %13472, %13473 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %13475 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13476 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13477 = torch.aten.view %4, %13476 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13478 = torch_c.to_builtin_tensor %13477 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13479 = torch_c.to_builtin_tensor %13475 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13480 = tensor.empty() : tensor<128x1280xf32>
    %13481 = linalg.fill ins(%cst : f32) outs(%13480 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13482 = tensor.empty() : tensor<128x1280xf32>
    %13483 = linalg.fill ins(%cst : f32) outs(%13482 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13484:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13481, %13483, %13478, %13479, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13481, %13483)
    %13485 = arith.truncf %13484#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13486 = torch_c.from_builtin_tensor %13485 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13487 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13488 = torch.aten.view %13486, %13487 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13489 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13490 = torch.aten.view %13460, %13489 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13491 = torch.aten.transpose.int %13490, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13492 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13493 = torch.aten.view %13474, %13492 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13494 = torch.aten.transpose.int %13493, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13495 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13496 = torch.aten.view %13488, %13495 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13497 = torch.aten.transpose.int %13496, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13498:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13491, %13494, %13497, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13499 = torch.aten.transpose.int %13498#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13500 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13501 = torch.aten.view %13499, %13500 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13502 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13503 = torch.aten.view %13501, %13502 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13504 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13505 = torch.aten.transpose.int %13504, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %13506 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13507 = torch.prims.convert_element_type %13506, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13508 = torch.prims.convert_element_type %13503, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13509 = torch.prims.convert_element_type %13505, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13510 = torch.aten.mm %13508, %13509 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13511 = torch.aten.mul.Scalar %13510, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13512 = torch.aten.mul.Scalar %13507, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13513 = torch.aten.add.Tensor %13511, %13512, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13514 = torch.prims.convert_element_type %13513, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13515 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13516 = torch.aten.view %13514, %13515 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13517 = torch.aten.div.Scalar %13516, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13518 = torch.aten.add.Tensor %13517, %13433, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13519 = torch.prims.convert_element_type %13518, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13520 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_387, %result1_388 = torch.aten.var_mean.correction %13519, %13520, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13521 = torch.aten.add.Scalar %result0_387, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13522 = torch.aten.rsqrt %13521 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13523 = torch.aten.sub.Tensor %13518, %result1_388, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13524 = torch.aten.mul.Tensor %13523, %13522 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %13525 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13526 = torch.aten.mul.Tensor %13524, %13525 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %13527 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13528 = torch.aten.add.Tensor %13526, %13527, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13529 = torch.prims.convert_element_type %13528, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13530 = torch.prims.convert_element_type %result1_388, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13531 = torch.prims.convert_element_type %13522, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13532 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13533 = torch.aten.view %13529, %13532 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13534 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13535 = torch.aten.transpose.int %13534, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %13536 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13537 = torch.prims.convert_element_type %13536, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13538 = torch.prims.convert_element_type %13533, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13539 = torch.prims.convert_element_type %13535, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13540 = torch.aten.mm %13538, %13539 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13541 = torch.aten.mul.Scalar %13540, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13542 = torch.aten.mul.Scalar %13537, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13543 = torch.aten.add.Tensor %13541, %13542, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13544 = torch.prims.convert_element_type %13543, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13545 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13546 = torch.aten.view %13544, %13545 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13547 = torch.aten.slice.Tensor %13546, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13548 = torch.aten.slice.Tensor %13546, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13549 = torch.aten.gelu %13548, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13550 = torch.aten.mul.Tensor %13547, %13549 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13551 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13552 = torch.aten.view %13550, %13551 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %13553 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13554 = torch.aten.transpose.int %13553, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %13555 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13556 = torch.prims.convert_element_type %13555, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13557 = torch.prims.convert_element_type %13552, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13558 = torch.prims.convert_element_type %13554, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13559 = torch.aten.mm %13557, %13558 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13560 = torch.aten.mul.Scalar %13559, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13561 = torch.aten.mul.Scalar %13556, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13562 = torch.aten.add.Tensor %13560, %13561, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13563 = torch.prims.convert_element_type %13562, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13564 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13565 = torch.aten.view %13563, %13564 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13566 = torch.aten.add.Tensor %13565, %13518, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13567 = torch.prims.convert_element_type %13566, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13568 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_389, %result1_390 = torch.aten.var_mean.correction %13567, %13568, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13569 = torch.aten.add.Scalar %result0_389, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13570 = torch.aten.rsqrt %13569 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13571 = torch.aten.sub.Tensor %13566, %result1_390, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13572 = torch.aten.mul.Tensor %13571, %13570 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %13573 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13574 = torch.aten.mul.Tensor %13572, %13573 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %13575 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13576 = torch.aten.add.Tensor %13574, %13575, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13577 = torch.prims.convert_element_type %13576, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13578 = torch.prims.convert_element_type %result1_390, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13579 = torch.prims.convert_element_type %13570, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %13580 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13581 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13582 = torch.aten.view %13577, %13581 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13583 = torch_c.to_builtin_tensor %13582 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13584 = torch_c.to_builtin_tensor %13580 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13585 = tensor.empty() : tensor<2048x1280xf32>
    %13586 = linalg.fill ins(%cst : f32) outs(%13585 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13587 = tensor.empty() : tensor<2048x1280xf32>
    %13588 = linalg.fill ins(%cst : f32) outs(%13587 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13589:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13586, %13588, %13583, %13584, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13586, %13588)
    %13590 = arith.truncf %13589#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13591 = torch_c.from_builtin_tensor %13590 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13592 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13593 = torch.aten.view %13591, %13592 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %13594 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13595 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13596 = torch.aten.view %13577, %13595 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13597 = torch_c.to_builtin_tensor %13596 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13598 = torch_c.to_builtin_tensor %13594 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13599 = tensor.empty() : tensor<2048x1280xf32>
    %13600 = linalg.fill ins(%cst : f32) outs(%13599 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13601 = tensor.empty() : tensor<2048x1280xf32>
    %13602 = linalg.fill ins(%cst : f32) outs(%13601 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13603:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13600, %13602, %13597, %13598, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13600, %13602)
    %13604 = arith.truncf %13603#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13605 = torch_c.from_builtin_tensor %13604 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13606 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13607 = torch.aten.view %13605, %13606 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %13608 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13609 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13610 = torch.aten.view %13577, %13609 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13611 = torch_c.to_builtin_tensor %13610 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13612 = torch_c.to_builtin_tensor %13608 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13613 = tensor.empty() : tensor<2048x1280xf32>
    %13614 = linalg.fill ins(%cst : f32) outs(%13613 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13615 = tensor.empty() : tensor<2048x1280xf32>
    %13616 = linalg.fill ins(%cst : f32) outs(%13615 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13617:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13614, %13616, %13611, %13612, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13614, %13616)
    %13618 = arith.truncf %13617#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13619 = torch_c.from_builtin_tensor %13618 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13620 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13621 = torch.aten.view %13619, %13620 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13622 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13623 = torch.aten.view %13593, %13622 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13624 = torch.aten.transpose.int %13623, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13625 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13626 = torch.aten.view %13607, %13625 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13627 = torch.aten.transpose.int %13626, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13628 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13629 = torch.aten.view %13621, %13628 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13630 = torch.aten.transpose.int %13629, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13631:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13624, %13627, %13630, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13632 = torch.aten.transpose.int %13631#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13633 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13634 = torch.aten.view %13632, %13633 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13635 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13636 = torch.aten.view %13634, %13635 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13637 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13638 = torch.aten.transpose.int %13637, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %13639 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13640 = torch.prims.convert_element_type %13639, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13641 = torch.prims.convert_element_type %13636, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13642 = torch.prims.convert_element_type %13638, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13643 = torch.aten.mm %13641, %13642 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13644 = torch.aten.mul.Scalar %13643, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13645 = torch.aten.mul.Scalar %13640, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13646 = torch.aten.add.Tensor %13644, %13645, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13647 = torch.prims.convert_element_type %13646, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13648 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13649 = torch.aten.view %13647, %13648 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13650 = torch.aten.div.Scalar %13649, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13651 = torch.aten.add.Tensor %13650, %13566, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13652 = torch.prims.convert_element_type %13651, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13653 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_391, %result1_392 = torch.aten.var_mean.correction %13652, %13653, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13654 = torch.aten.add.Scalar %result0_391, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13655 = torch.aten.rsqrt %13654 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13656 = torch.aten.sub.Tensor %13651, %result1_392, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13657 = torch.aten.mul.Tensor %13656, %13655 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %13658 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13659 = torch.aten.mul.Tensor %13657, %13658 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %13660 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13661 = torch.aten.add.Tensor %13659, %13660, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13662 = torch.prims.convert_element_type %13661, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13663 = torch.prims.convert_element_type %result1_392, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13664 = torch.prims.convert_element_type %13655, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %13665 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13666 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13667 = torch.aten.view %13662, %13666 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13668 = torch_c.to_builtin_tensor %13667 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13669 = torch_c.to_builtin_tensor %13665 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13670 = tensor.empty() : tensor<2048x1280xf32>
    %13671 = linalg.fill ins(%cst : f32) outs(%13670 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13672 = tensor.empty() : tensor<2048x1280xf32>
    %13673 = linalg.fill ins(%cst : f32) outs(%13672 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13674:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13671, %13673, %13668, %13669, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13671, %13673)
    %13675 = arith.truncf %13674#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13676 = torch_c.from_builtin_tensor %13675 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13677 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13678 = torch.aten.view %13676, %13677 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %13679 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13680 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13681 = torch.aten.view %4, %13680 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13682 = torch_c.to_builtin_tensor %13681 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13683 = torch_c.to_builtin_tensor %13679 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13684 = tensor.empty() : tensor<128x1280xf32>
    %13685 = linalg.fill ins(%cst : f32) outs(%13684 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13686 = tensor.empty() : tensor<128x1280xf32>
    %13687 = linalg.fill ins(%cst : f32) outs(%13686 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13688:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13685, %13687, %13682, %13683, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13685, %13687)
    %13689 = arith.truncf %13688#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13690 = torch_c.from_builtin_tensor %13689 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13691 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13692 = torch.aten.view %13690, %13691 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %13693 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13694 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13695 = torch.aten.view %4, %13694 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13696 = torch_c.to_builtin_tensor %13695 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13697 = torch_c.to_builtin_tensor %13693 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13698 = tensor.empty() : tensor<128x1280xf32>
    %13699 = linalg.fill ins(%cst : f32) outs(%13698 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13700 = tensor.empty() : tensor<128x1280xf32>
    %13701 = linalg.fill ins(%cst : f32) outs(%13700 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13702:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13699, %13701, %13696, %13697, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13699, %13701)
    %13703 = arith.truncf %13702#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13704 = torch_c.from_builtin_tensor %13703 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13705 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13706 = torch.aten.view %13704, %13705 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13707 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13708 = torch.aten.view %13678, %13707 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13709 = torch.aten.transpose.int %13708, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13710 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13711 = torch.aten.view %13692, %13710 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13712 = torch.aten.transpose.int %13711, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13713 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13714 = torch.aten.view %13706, %13713 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13715 = torch.aten.transpose.int %13714, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13716:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13709, %13712, %13715, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13717 = torch.aten.transpose.int %13716#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13718 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13719 = torch.aten.view %13717, %13718 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13720 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13721 = torch.aten.view %13719, %13720 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13722 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13723 = torch.aten.transpose.int %13722, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %13724 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13725 = torch.prims.convert_element_type %13724, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13726 = torch.prims.convert_element_type %13721, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13727 = torch.prims.convert_element_type %13723, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13728 = torch.aten.mm %13726, %13727 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13729 = torch.aten.mul.Scalar %13728, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13730 = torch.aten.mul.Scalar %13725, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13731 = torch.aten.add.Tensor %13729, %13730, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13732 = torch.prims.convert_element_type %13731, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13733 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13734 = torch.aten.view %13732, %13733 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13735 = torch.aten.div.Scalar %13734, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13736 = torch.aten.add.Tensor %13735, %13651, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13737 = torch.prims.convert_element_type %13736, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13738 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_393, %result1_394 = torch.aten.var_mean.correction %13737, %13738, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13739 = torch.aten.add.Scalar %result0_393, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13740 = torch.aten.rsqrt %13739 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13741 = torch.aten.sub.Tensor %13736, %result1_394, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13742 = torch.aten.mul.Tensor %13741, %13740 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %13743 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13744 = torch.aten.mul.Tensor %13742, %13743 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %13745 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13746 = torch.aten.add.Tensor %13744, %13745, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13747 = torch.prims.convert_element_type %13746, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13748 = torch.prims.convert_element_type %result1_394, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13749 = torch.prims.convert_element_type %13740, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13750 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13751 = torch.aten.view %13747, %13750 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13752 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13753 = torch.aten.transpose.int %13752, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %13754 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13755 = torch.prims.convert_element_type %13754, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13756 = torch.prims.convert_element_type %13751, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13757 = torch.prims.convert_element_type %13753, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13758 = torch.aten.mm %13756, %13757 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13759 = torch.aten.mul.Scalar %13758, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13760 = torch.aten.mul.Scalar %13755, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13761 = torch.aten.add.Tensor %13759, %13760, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13762 = torch.prims.convert_element_type %13761, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13763 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13764 = torch.aten.view %13762, %13763 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13765 = torch.aten.slice.Tensor %13764, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13766 = torch.aten.slice.Tensor %13764, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13767 = torch.aten.gelu %13766, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13768 = torch.aten.mul.Tensor %13765, %13767 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13769 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13770 = torch.aten.view %13768, %13769 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %13771 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13772 = torch.aten.transpose.int %13771, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %13773 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13774 = torch.prims.convert_element_type %13773, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13775 = torch.prims.convert_element_type %13770, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13776 = torch.prims.convert_element_type %13772, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13777 = torch.aten.mm %13775, %13776 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13778 = torch.aten.mul.Scalar %13777, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13779 = torch.aten.mul.Scalar %13774, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13780 = torch.aten.add.Tensor %13778, %13779, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13781 = torch.prims.convert_element_type %13780, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13782 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13783 = torch.aten.view %13781, %13782 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13784 = torch.aten.add.Tensor %13783, %13736, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13785 = torch.prims.convert_element_type %13784, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13786 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_395, %result1_396 = torch.aten.var_mean.correction %13785, %13786, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13787 = torch.aten.add.Scalar %result0_395, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13788 = torch.aten.rsqrt %13787 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13789 = torch.aten.sub.Tensor %13784, %result1_396, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13790 = torch.aten.mul.Tensor %13789, %13788 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %13791 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13792 = torch.aten.mul.Tensor %13790, %13791 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %13793 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13794 = torch.aten.add.Tensor %13792, %13793, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13795 = torch.prims.convert_element_type %13794, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13796 = torch.prims.convert_element_type %result1_396, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13797 = torch.prims.convert_element_type %13788, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %13798 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13799 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13800 = torch.aten.view %13795, %13799 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13801 = torch_c.to_builtin_tensor %13800 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13802 = torch_c.to_builtin_tensor %13798 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13803 = tensor.empty() : tensor<2048x1280xf32>
    %13804 = linalg.fill ins(%cst : f32) outs(%13803 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13805 = tensor.empty() : tensor<2048x1280xf32>
    %13806 = linalg.fill ins(%cst : f32) outs(%13805 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13807:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13804, %13806, %13801, %13802, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13804, %13806)
    %13808 = arith.truncf %13807#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13809 = torch_c.from_builtin_tensor %13808 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13810 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13811 = torch.aten.view %13809, %13810 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %13812 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13813 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13814 = torch.aten.view %13795, %13813 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13815 = torch_c.to_builtin_tensor %13814 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13816 = torch_c.to_builtin_tensor %13812 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13817 = tensor.empty() : tensor<2048x1280xf32>
    %13818 = linalg.fill ins(%cst : f32) outs(%13817 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13819 = tensor.empty() : tensor<2048x1280xf32>
    %13820 = linalg.fill ins(%cst : f32) outs(%13819 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13821:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13818, %13820, %13815, %13816, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13818, %13820)
    %13822 = arith.truncf %13821#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13823 = torch_c.from_builtin_tensor %13822 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13824 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13825 = torch.aten.view %13823, %13824 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %13826 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13827 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13828 = torch.aten.view %13795, %13827 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13829 = torch_c.to_builtin_tensor %13828 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13830 = torch_c.to_builtin_tensor %13826 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13831 = tensor.empty() : tensor<2048x1280xf32>
    %13832 = linalg.fill ins(%cst : f32) outs(%13831 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13833 = tensor.empty() : tensor<2048x1280xf32>
    %13834 = linalg.fill ins(%cst : f32) outs(%13833 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13835:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13832, %13834, %13829, %13830, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13832, %13834)
    %13836 = arith.truncf %13835#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13837 = torch_c.from_builtin_tensor %13836 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13838 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13839 = torch.aten.view %13837, %13838 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13840 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13841 = torch.aten.view %13811, %13840 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13842 = torch.aten.transpose.int %13841, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13843 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13844 = torch.aten.view %13825, %13843 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13845 = torch.aten.transpose.int %13844, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13846 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13847 = torch.aten.view %13839, %13846 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13848 = torch.aten.transpose.int %13847, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13849:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13842, %13845, %13848, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13850 = torch.aten.transpose.int %13849#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13851 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13852 = torch.aten.view %13850, %13851 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13853 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13854 = torch.aten.view %13852, %13853 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %13855 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13856 = torch.aten.transpose.int %13855, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %13857 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13858 = torch.prims.convert_element_type %13857, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13859 = torch.prims.convert_element_type %13854, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13860 = torch.prims.convert_element_type %13856, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13861 = torch.aten.mm %13859, %13860 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13862 = torch.aten.mul.Scalar %13861, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13863 = torch.aten.mul.Scalar %13858, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13864 = torch.aten.add.Tensor %13862, %13863, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13865 = torch.prims.convert_element_type %13864, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13866 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13867 = torch.aten.view %13865, %13866 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13868 = torch.aten.div.Scalar %13867, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13869 = torch.aten.add.Tensor %13868, %13784, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13870 = torch.prims.convert_element_type %13869, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13871 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_397, %result1_398 = torch.aten.var_mean.correction %13870, %13871, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13872 = torch.aten.add.Scalar %result0_397, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13873 = torch.aten.rsqrt %13872 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13874 = torch.aten.sub.Tensor %13869, %result1_398, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13875 = torch.aten.mul.Tensor %13874, %13873 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %13876 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13877 = torch.aten.mul.Tensor %13875, %13876 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %13878 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13879 = torch.aten.add.Tensor %13877, %13878, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13880 = torch.prims.convert_element_type %13879, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13881 = torch.prims.convert_element_type %result1_398, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13882 = torch.prims.convert_element_type %13873, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %13883 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13884 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13885 = torch.aten.view %13880, %13884 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %13886 = torch_c.to_builtin_tensor %13885 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %13887 = torch_c.to_builtin_tensor %13883 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %13888 = tensor.empty() : tensor<2048x1280xf32>
    %13889 = linalg.fill ins(%cst : f32) outs(%13888 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13890 = tensor.empty() : tensor<2048x1280xf32>
    %13891 = linalg.fill ins(%cst : f32) outs(%13890 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %13892:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %13889, %13891, %13886, %13887, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13889, %13891)
    %13893 = arith.truncf %13892#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %13894 = torch_c.from_builtin_tensor %13893 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %13895 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13896 = torch.aten.view %13894, %13895 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %13897 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13898 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13899 = torch.aten.view %4, %13898 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13900 = torch_c.to_builtin_tensor %13899 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13901 = torch_c.to_builtin_tensor %13897 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13902 = tensor.empty() : tensor<128x1280xf32>
    %13903 = linalg.fill ins(%cst : f32) outs(%13902 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13904 = tensor.empty() : tensor<128x1280xf32>
    %13905 = linalg.fill ins(%cst : f32) outs(%13904 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13906:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13903, %13905, %13900, %13901, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13903, %13905)
    %13907 = arith.truncf %13906#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13908 = torch_c.from_builtin_tensor %13907 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13909 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13910 = torch.aten.view %13908, %13909 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %13911 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %13912 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %13913 = torch.aten.view %4, %13912 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %13914 = torch_c.to_builtin_tensor %13913 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %13915 = torch_c.to_builtin_tensor %13911 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %13916 = tensor.empty() : tensor<128x1280xf32>
    %13917 = linalg.fill ins(%cst : f32) outs(%13916 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13918 = tensor.empty() : tensor<128x1280xf32>
    %13919 = linalg.fill ins(%cst : f32) outs(%13918 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %13920:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %13917, %13919, %13914, %13915, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%13917, %13919)
    %13921 = arith.truncf %13920#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %13922 = torch_c.from_builtin_tensor %13921 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %13923 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13924 = torch.aten.view %13922, %13923 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %13925 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13926 = torch.aten.view %13896, %13925 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %13927 = torch.aten.transpose.int %13926, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %13928 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13929 = torch.aten.view %13910, %13928 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13930 = torch.aten.transpose.int %13929, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13931 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13932 = torch.aten.view %13924, %13931 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %13933 = torch.aten.transpose.int %13932, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %13934:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%13927, %13930, %13933, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %13935 = torch.aten.transpose.int %13934#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %13936 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13937 = torch.aten.view %13935, %13936 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13938 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13939 = torch.aten.view %13937, %13938 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %13940 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %13941 = torch.aten.transpose.int %13940, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %13942 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13943 = torch.prims.convert_element_type %13942, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13944 = torch.prims.convert_element_type %13939, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13945 = torch.prims.convert_element_type %13941, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %13946 = torch.aten.mm %13944, %13945 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13947 = torch.aten.mul.Scalar %13946, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13948 = torch.aten.mul.Scalar %13943, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13949 = torch.aten.add.Tensor %13947, %13948, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13950 = torch.prims.convert_element_type %13949, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %13951 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13952 = torch.aten.view %13950, %13951 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %13953 = torch.aten.div.Scalar %13952, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %13954 = torch.aten.add.Tensor %13953, %13869, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13955 = torch.prims.convert_element_type %13954, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13956 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_399, %result1_400 = torch.aten.var_mean.correction %13955, %13956, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %13957 = torch.aten.add.Scalar %result0_399, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %13958 = torch.aten.rsqrt %13957 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %13959 = torch.aten.sub.Tensor %13954, %result1_400, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13960 = torch.aten.mul.Tensor %13959, %13958 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %13961 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13962 = torch.aten.mul.Tensor %13960, %13961 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %13963 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13964 = torch.aten.add.Tensor %13962, %13963, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %13965 = torch.prims.convert_element_type %13964, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %13966 = torch.prims.convert_element_type %result1_400, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13967 = torch.prims.convert_element_type %13958, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %13968 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %13969 = torch.aten.view %13965, %13968 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %13970 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %13971 = torch.aten.transpose.int %13970, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %13972 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %13973 = torch.prims.convert_element_type %13972, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %13974 = torch.prims.convert_element_type %13969, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13975 = torch.prims.convert_element_type %13971, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %13976 = torch.aten.mm %13974, %13975 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %13977 = torch.aten.mul.Scalar %13976, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13978 = torch.aten.mul.Scalar %13973, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %13979 = torch.aten.add.Tensor %13977, %13978, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %13980 = torch.prims.convert_element_type %13979, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %13981 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %13982 = torch.aten.view %13980, %13981 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %13983 = torch.aten.slice.Tensor %13982, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13984 = torch.aten.slice.Tensor %13982, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %13985 = torch.aten.gelu %13984, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %13986 = torch.aten.mul.Tensor %13983, %13985 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %13987 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %13988 = torch.aten.view %13986, %13987 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %13989 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %13990 = torch.aten.transpose.int %13989, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %13991 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %13992 = torch.prims.convert_element_type %13991, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %13993 = torch.prims.convert_element_type %13988, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %13994 = torch.prims.convert_element_type %13990, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %13995 = torch.aten.mm %13993, %13994 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %13996 = torch.aten.mul.Scalar %13995, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13997 = torch.aten.mul.Scalar %13992, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %13998 = torch.aten.add.Tensor %13996, %13997, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %13999 = torch.prims.convert_element_type %13998, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14000 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14001 = torch.aten.view %13999, %14000 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14002 = torch.aten.add.Tensor %14001, %13954, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14003 = torch.prims.convert_element_type %14002, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14004 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_401, %result1_402 = torch.aten.var_mean.correction %14003, %14004, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14005 = torch.aten.add.Scalar %result0_401, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14006 = torch.aten.rsqrt %14005 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14007 = torch.aten.sub.Tensor %14002, %result1_402, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14008 = torch.aten.mul.Tensor %14007, %14006 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %14009 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14010 = torch.aten.mul.Tensor %14008, %14009 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %14011 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14012 = torch.aten.add.Tensor %14010, %14011, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14013 = torch.prims.convert_element_type %14012, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14014 = torch.prims.convert_element_type %result1_402, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14015 = torch.prims.convert_element_type %14006, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %14016 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14017 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14018 = torch.aten.view %14013, %14017 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14019 = torch_c.to_builtin_tensor %14018 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14020 = torch_c.to_builtin_tensor %14016 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14021 = tensor.empty() : tensor<2048x1280xf32>
    %14022 = linalg.fill ins(%cst : f32) outs(%14021 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14023 = tensor.empty() : tensor<2048x1280xf32>
    %14024 = linalg.fill ins(%cst : f32) outs(%14023 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14025:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14022, %14024, %14019, %14020, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14022, %14024)
    %14026 = arith.truncf %14025#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14027 = torch_c.from_builtin_tensor %14026 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14028 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14029 = torch.aten.view %14027, %14028 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %14030 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14031 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14032 = torch.aten.view %14013, %14031 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14033 = torch_c.to_builtin_tensor %14032 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14034 = torch_c.to_builtin_tensor %14030 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14035 = tensor.empty() : tensor<2048x1280xf32>
    %14036 = linalg.fill ins(%cst : f32) outs(%14035 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14037 = tensor.empty() : tensor<2048x1280xf32>
    %14038 = linalg.fill ins(%cst : f32) outs(%14037 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14039:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14036, %14038, %14033, %14034, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14036, %14038)
    %14040 = arith.truncf %14039#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14041 = torch_c.from_builtin_tensor %14040 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14042 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14043 = torch.aten.view %14041, %14042 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %14044 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14045 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14046 = torch.aten.view %14013, %14045 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14047 = torch_c.to_builtin_tensor %14046 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14048 = torch_c.to_builtin_tensor %14044 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14049 = tensor.empty() : tensor<2048x1280xf32>
    %14050 = linalg.fill ins(%cst : f32) outs(%14049 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14051 = tensor.empty() : tensor<2048x1280xf32>
    %14052 = linalg.fill ins(%cst : f32) outs(%14051 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14053:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14050, %14052, %14047, %14048, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14050, %14052)
    %14054 = arith.truncf %14053#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14055 = torch_c.from_builtin_tensor %14054 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14056 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14057 = torch.aten.view %14055, %14056 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14058 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14059 = torch.aten.view %14029, %14058 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14060 = torch.aten.transpose.int %14059, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14061 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14062 = torch.aten.view %14043, %14061 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14063 = torch.aten.transpose.int %14062, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14064 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14065 = torch.aten.view %14057, %14064 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14066 = torch.aten.transpose.int %14065, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14067:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14060, %14063, %14066, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14068 = torch.aten.transpose.int %14067#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14069 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14070 = torch.aten.view %14068, %14069 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14071 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14072 = torch.aten.view %14070, %14071 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14073 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14074 = torch.aten.transpose.int %14073, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %14075 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14076 = torch.prims.convert_element_type %14075, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14077 = torch.prims.convert_element_type %14072, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14078 = torch.prims.convert_element_type %14074, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14079 = torch.aten.mm %14077, %14078 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14080 = torch.aten.mul.Scalar %14079, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14081 = torch.aten.mul.Scalar %14076, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14082 = torch.aten.add.Tensor %14080, %14081, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14083 = torch.prims.convert_element_type %14082, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14084 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14085 = torch.aten.view %14083, %14084 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14086 = torch.aten.div.Scalar %14085, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14087 = torch.aten.add.Tensor %14086, %14002, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14088 = torch.prims.convert_element_type %14087, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14089 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_403, %result1_404 = torch.aten.var_mean.correction %14088, %14089, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14090 = torch.aten.add.Scalar %result0_403, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14091 = torch.aten.rsqrt %14090 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14092 = torch.aten.sub.Tensor %14087, %result1_404, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14093 = torch.aten.mul.Tensor %14092, %14091 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %14094 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14095 = torch.aten.mul.Tensor %14093, %14094 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %14096 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14097 = torch.aten.add.Tensor %14095, %14096, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14098 = torch.prims.convert_element_type %14097, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14099 = torch.prims.convert_element_type %result1_404, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14100 = torch.prims.convert_element_type %14091, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %14101 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14102 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14103 = torch.aten.view %14098, %14102 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14104 = torch_c.to_builtin_tensor %14103 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14105 = torch_c.to_builtin_tensor %14101 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14106 = tensor.empty() : tensor<2048x1280xf32>
    %14107 = linalg.fill ins(%cst : f32) outs(%14106 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14108 = tensor.empty() : tensor<2048x1280xf32>
    %14109 = linalg.fill ins(%cst : f32) outs(%14108 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14110:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14107, %14109, %14104, %14105, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14107, %14109)
    %14111 = arith.truncf %14110#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14112 = torch_c.from_builtin_tensor %14111 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14113 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14114 = torch.aten.view %14112, %14113 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %14115 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14116 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14117 = torch.aten.view %4, %14116 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14118 = torch_c.to_builtin_tensor %14117 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14119 = torch_c.to_builtin_tensor %14115 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14120 = tensor.empty() : tensor<128x1280xf32>
    %14121 = linalg.fill ins(%cst : f32) outs(%14120 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14122 = tensor.empty() : tensor<128x1280xf32>
    %14123 = linalg.fill ins(%cst : f32) outs(%14122 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14124:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14121, %14123, %14118, %14119, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14121, %14123)
    %14125 = arith.truncf %14124#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14126 = torch_c.from_builtin_tensor %14125 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14127 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14128 = torch.aten.view %14126, %14127 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %14129 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14130 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14131 = torch.aten.view %4, %14130 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14132 = torch_c.to_builtin_tensor %14131 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14133 = torch_c.to_builtin_tensor %14129 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14134 = tensor.empty() : tensor<128x1280xf32>
    %14135 = linalg.fill ins(%cst : f32) outs(%14134 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14136 = tensor.empty() : tensor<128x1280xf32>
    %14137 = linalg.fill ins(%cst : f32) outs(%14136 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14138:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14135, %14137, %14132, %14133, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14135, %14137)
    %14139 = arith.truncf %14138#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14140 = torch_c.from_builtin_tensor %14139 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14141 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14142 = torch.aten.view %14140, %14141 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %14143 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14144 = torch.aten.view %14114, %14143 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14145 = torch.aten.transpose.int %14144, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14146 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14147 = torch.aten.view %14128, %14146 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14148 = torch.aten.transpose.int %14147, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14149 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14150 = torch.aten.view %14142, %14149 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14151 = torch.aten.transpose.int %14150, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14152:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14145, %14148, %14151, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14153 = torch.aten.transpose.int %14152#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14154 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14155 = torch.aten.view %14153, %14154 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14156 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14157 = torch.aten.view %14155, %14156 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14158 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14159 = torch.aten.transpose.int %14158, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %14160 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14161 = torch.prims.convert_element_type %14160, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14162 = torch.prims.convert_element_type %14157, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14163 = torch.prims.convert_element_type %14159, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14164 = torch.aten.mm %14162, %14163 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14165 = torch.aten.mul.Scalar %14164, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14166 = torch.aten.mul.Scalar %14161, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14167 = torch.aten.add.Tensor %14165, %14166, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14168 = torch.prims.convert_element_type %14167, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14169 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14170 = torch.aten.view %14168, %14169 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14171 = torch.aten.div.Scalar %14170, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14172 = torch.aten.add.Tensor %14171, %14087, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14173 = torch.prims.convert_element_type %14172, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14174 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_405, %result1_406 = torch.aten.var_mean.correction %14173, %14174, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14175 = torch.aten.add.Scalar %result0_405, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14176 = torch.aten.rsqrt %14175 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14177 = torch.aten.sub.Tensor %14172, %result1_406, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14178 = torch.aten.mul.Tensor %14177, %14176 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %14179 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14180 = torch.aten.mul.Tensor %14178, %14179 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %14181 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14182 = torch.aten.add.Tensor %14180, %14181, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14183 = torch.prims.convert_element_type %14182, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14184 = torch.prims.convert_element_type %result1_406, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14185 = torch.prims.convert_element_type %14176, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14186 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14187 = torch.aten.view %14183, %14186 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14188 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14189 = torch.aten.transpose.int %14188, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %14190 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14191 = torch.prims.convert_element_type %14190, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14192 = torch.prims.convert_element_type %14187, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14193 = torch.prims.convert_element_type %14189, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14194 = torch.aten.mm %14192, %14193 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14195 = torch.aten.mul.Scalar %14194, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14196 = torch.aten.mul.Scalar %14191, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14197 = torch.aten.add.Tensor %14195, %14196, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14198 = torch.prims.convert_element_type %14197, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14199 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14200 = torch.aten.view %14198, %14199 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14201 = torch.aten.slice.Tensor %14200, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14202 = torch.aten.slice.Tensor %14200, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14203 = torch.aten.gelu %14202, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14204 = torch.aten.mul.Tensor %14201, %14203 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14205 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14206 = torch.aten.view %14204, %14205 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %14207 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14208 = torch.aten.transpose.int %14207, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %14209 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14210 = torch.prims.convert_element_type %14209, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14211 = torch.prims.convert_element_type %14206, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14212 = torch.prims.convert_element_type %14208, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14213 = torch.aten.mm %14211, %14212 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14214 = torch.aten.mul.Scalar %14213, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14215 = torch.aten.mul.Scalar %14210, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14216 = torch.aten.add.Tensor %14214, %14215, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14217 = torch.prims.convert_element_type %14216, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14218 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14219 = torch.aten.view %14217, %14218 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14220 = torch.aten.add.Tensor %14219, %14172, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14221 = torch.prims.convert_element_type %14220, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14222 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_407, %result1_408 = torch.aten.var_mean.correction %14221, %14222, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14223 = torch.aten.add.Scalar %result0_407, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14224 = torch.aten.rsqrt %14223 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14225 = torch.aten.sub.Tensor %14220, %result1_408, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14226 = torch.aten.mul.Tensor %14225, %14224 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %14227 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14228 = torch.aten.mul.Tensor %14226, %14227 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %14229 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14230 = torch.aten.add.Tensor %14228, %14229, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14231 = torch.prims.convert_element_type %14230, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14232 = torch.prims.convert_element_type %result1_408, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14233 = torch.prims.convert_element_type %14224, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %14234 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14235 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14236 = torch.aten.view %14231, %14235 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14237 = torch_c.to_builtin_tensor %14236 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14238 = torch_c.to_builtin_tensor %14234 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14239 = tensor.empty() : tensor<2048x1280xf32>
    %14240 = linalg.fill ins(%cst : f32) outs(%14239 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14241 = tensor.empty() : tensor<2048x1280xf32>
    %14242 = linalg.fill ins(%cst : f32) outs(%14241 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14243:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14240, %14242, %14237, %14238, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14240, %14242)
    %14244 = arith.truncf %14243#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14245 = torch_c.from_builtin_tensor %14244 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14246 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14247 = torch.aten.view %14245, %14246 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %14248 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14249 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14250 = torch.aten.view %14231, %14249 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14251 = torch_c.to_builtin_tensor %14250 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14252 = torch_c.to_builtin_tensor %14248 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14253 = tensor.empty() : tensor<2048x1280xf32>
    %14254 = linalg.fill ins(%cst : f32) outs(%14253 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14255 = tensor.empty() : tensor<2048x1280xf32>
    %14256 = linalg.fill ins(%cst : f32) outs(%14255 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14257:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14254, %14256, %14251, %14252, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14254, %14256)
    %14258 = arith.truncf %14257#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14259 = torch_c.from_builtin_tensor %14258 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14260 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14261 = torch.aten.view %14259, %14260 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %14262 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14263 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14264 = torch.aten.view %14231, %14263 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14265 = torch_c.to_builtin_tensor %14264 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14266 = torch_c.to_builtin_tensor %14262 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14267 = tensor.empty() : tensor<2048x1280xf32>
    %14268 = linalg.fill ins(%cst : f32) outs(%14267 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14269 = tensor.empty() : tensor<2048x1280xf32>
    %14270 = linalg.fill ins(%cst : f32) outs(%14269 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14271:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14268, %14270, %14265, %14266, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14268, %14270)
    %14272 = arith.truncf %14271#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14273 = torch_c.from_builtin_tensor %14272 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14274 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14275 = torch.aten.view %14273, %14274 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14276 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14277 = torch.aten.view %14247, %14276 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14278 = torch.aten.transpose.int %14277, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14279 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14280 = torch.aten.view %14261, %14279 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14281 = torch.aten.transpose.int %14280, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14282 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14283 = torch.aten.view %14275, %14282 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14284 = torch.aten.transpose.int %14283, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14285:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14278, %14281, %14284, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14286 = torch.aten.transpose.int %14285#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14287 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14288 = torch.aten.view %14286, %14287 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14289 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14290 = torch.aten.view %14288, %14289 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14291 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14292 = torch.aten.transpose.int %14291, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %14293 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14294 = torch.prims.convert_element_type %14293, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14295 = torch.prims.convert_element_type %14290, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14296 = torch.prims.convert_element_type %14292, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14297 = torch.aten.mm %14295, %14296 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14298 = torch.aten.mul.Scalar %14297, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14299 = torch.aten.mul.Scalar %14294, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14300 = torch.aten.add.Tensor %14298, %14299, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14301 = torch.prims.convert_element_type %14300, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14302 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14303 = torch.aten.view %14301, %14302 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14304 = torch.aten.div.Scalar %14303, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14305 = torch.aten.add.Tensor %14304, %14220, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14306 = torch.prims.convert_element_type %14305, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14307 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_409, %result1_410 = torch.aten.var_mean.correction %14306, %14307, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14308 = torch.aten.add.Scalar %result0_409, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14309 = torch.aten.rsqrt %14308 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14310 = torch.aten.sub.Tensor %14305, %result1_410, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14311 = torch.aten.mul.Tensor %14310, %14309 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %14312 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14313 = torch.aten.mul.Tensor %14311, %14312 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %14314 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14315 = torch.aten.add.Tensor %14313, %14314, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14316 = torch.prims.convert_element_type %14315, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14317 = torch.prims.convert_element_type %result1_410, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14318 = torch.prims.convert_element_type %14309, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %14319 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14320 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14321 = torch.aten.view %14316, %14320 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14322 = torch_c.to_builtin_tensor %14321 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14323 = torch_c.to_builtin_tensor %14319 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14324 = tensor.empty() : tensor<2048x1280xf32>
    %14325 = linalg.fill ins(%cst : f32) outs(%14324 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14326 = tensor.empty() : tensor<2048x1280xf32>
    %14327 = linalg.fill ins(%cst : f32) outs(%14326 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14328:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14325, %14327, %14322, %14323, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14325, %14327)
    %14329 = arith.truncf %14328#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14330 = torch_c.from_builtin_tensor %14329 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14331 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14332 = torch.aten.view %14330, %14331 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %14333 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14334 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14335 = torch.aten.view %4, %14334 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14336 = torch_c.to_builtin_tensor %14335 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14337 = torch_c.to_builtin_tensor %14333 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14338 = tensor.empty() : tensor<128x1280xf32>
    %14339 = linalg.fill ins(%cst : f32) outs(%14338 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14340 = tensor.empty() : tensor<128x1280xf32>
    %14341 = linalg.fill ins(%cst : f32) outs(%14340 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14342:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14339, %14341, %14336, %14337, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14339, %14341)
    %14343 = arith.truncf %14342#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14344 = torch_c.from_builtin_tensor %14343 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14345 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14346 = torch.aten.view %14344, %14345 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %14347 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14348 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14349 = torch.aten.view %4, %14348 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14350 = torch_c.to_builtin_tensor %14349 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14351 = torch_c.to_builtin_tensor %14347 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14352 = tensor.empty() : tensor<128x1280xf32>
    %14353 = linalg.fill ins(%cst : f32) outs(%14352 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14354 = tensor.empty() : tensor<128x1280xf32>
    %14355 = linalg.fill ins(%cst : f32) outs(%14354 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14356:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14353, %14355, %14350, %14351, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14353, %14355)
    %14357 = arith.truncf %14356#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14358 = torch_c.from_builtin_tensor %14357 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14359 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14360 = torch.aten.view %14358, %14359 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %14361 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14362 = torch.aten.view %14332, %14361 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14363 = torch.aten.transpose.int %14362, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14364 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14365 = torch.aten.view %14346, %14364 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14366 = torch.aten.transpose.int %14365, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14367 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14368 = torch.aten.view %14360, %14367 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14369 = torch.aten.transpose.int %14368, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14370:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14363, %14366, %14369, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14371 = torch.aten.transpose.int %14370#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14372 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14373 = torch.aten.view %14371, %14372 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14374 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14375 = torch.aten.view %14373, %14374 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14376 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14377 = torch.aten.transpose.int %14376, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %14378 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14379 = torch.prims.convert_element_type %14378, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14380 = torch.prims.convert_element_type %14375, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14381 = torch.prims.convert_element_type %14377, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14382 = torch.aten.mm %14380, %14381 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14383 = torch.aten.mul.Scalar %14382, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14384 = torch.aten.mul.Scalar %14379, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14385 = torch.aten.add.Tensor %14383, %14384, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14386 = torch.prims.convert_element_type %14385, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14387 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14388 = torch.aten.view %14386, %14387 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14389 = torch.aten.div.Scalar %14388, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14390 = torch.aten.add.Tensor %14389, %14305, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14391 = torch.prims.convert_element_type %14390, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14392 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_411, %result1_412 = torch.aten.var_mean.correction %14391, %14392, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14393 = torch.aten.add.Scalar %result0_411, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14394 = torch.aten.rsqrt %14393 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14395 = torch.aten.sub.Tensor %14390, %result1_412, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14396 = torch.aten.mul.Tensor %14395, %14394 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %14397 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14398 = torch.aten.mul.Tensor %14396, %14397 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %14399 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14400 = torch.aten.add.Tensor %14398, %14399, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14401 = torch.prims.convert_element_type %14400, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14402 = torch.prims.convert_element_type %result1_412, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14403 = torch.prims.convert_element_type %14394, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14404 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14405 = torch.aten.view %14401, %14404 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14406 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14407 = torch.aten.transpose.int %14406, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %14408 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14409 = torch.prims.convert_element_type %14408, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14410 = torch.prims.convert_element_type %14405, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14411 = torch.prims.convert_element_type %14407, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14412 = torch.aten.mm %14410, %14411 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14413 = torch.aten.mul.Scalar %14412, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14414 = torch.aten.mul.Scalar %14409, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14415 = torch.aten.add.Tensor %14413, %14414, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14416 = torch.prims.convert_element_type %14415, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14417 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14418 = torch.aten.view %14416, %14417 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14419 = torch.aten.slice.Tensor %14418, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14420 = torch.aten.slice.Tensor %14418, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14421 = torch.aten.gelu %14420, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14422 = torch.aten.mul.Tensor %14419, %14421 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14423 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14424 = torch.aten.view %14422, %14423 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %14425 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14426 = torch.aten.transpose.int %14425, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %14427 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14428 = torch.prims.convert_element_type %14427, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14429 = torch.prims.convert_element_type %14424, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14430 = torch.prims.convert_element_type %14426, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14431 = torch.aten.mm %14429, %14430 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14432 = torch.aten.mul.Scalar %14431, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14433 = torch.aten.mul.Scalar %14428, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14434 = torch.aten.add.Tensor %14432, %14433, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14435 = torch.prims.convert_element_type %14434, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14436 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14437 = torch.aten.view %14435, %14436 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14438 = torch.aten.add.Tensor %14437, %14390, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14439 = torch.prims.convert_element_type %14438, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14440 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_413, %result1_414 = torch.aten.var_mean.correction %14439, %14440, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14441 = torch.aten.add.Scalar %result0_413, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14442 = torch.aten.rsqrt %14441 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14443 = torch.aten.sub.Tensor %14438, %result1_414, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14444 = torch.aten.mul.Tensor %14443, %14442 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %14445 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14446 = torch.aten.mul.Tensor %14444, %14445 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %14447 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14448 = torch.aten.add.Tensor %14446, %14447, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14449 = torch.prims.convert_element_type %14448, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14450 = torch.prims.convert_element_type %result1_414, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14451 = torch.prims.convert_element_type %14442, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %14452 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14453 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14454 = torch.aten.view %14449, %14453 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14455 = torch_c.to_builtin_tensor %14454 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14456 = torch_c.to_builtin_tensor %14452 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14457 = tensor.empty() : tensor<2048x1280xf32>
    %14458 = linalg.fill ins(%cst : f32) outs(%14457 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14459 = tensor.empty() : tensor<2048x1280xf32>
    %14460 = linalg.fill ins(%cst : f32) outs(%14459 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14461:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14458, %14460, %14455, %14456, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14458, %14460)
    %14462 = arith.truncf %14461#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14463 = torch_c.from_builtin_tensor %14462 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14464 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14465 = torch.aten.view %14463, %14464 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %14466 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14467 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14468 = torch.aten.view %14449, %14467 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14469 = torch_c.to_builtin_tensor %14468 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14470 = torch_c.to_builtin_tensor %14466 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14471 = tensor.empty() : tensor<2048x1280xf32>
    %14472 = linalg.fill ins(%cst : f32) outs(%14471 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14473 = tensor.empty() : tensor<2048x1280xf32>
    %14474 = linalg.fill ins(%cst : f32) outs(%14473 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14475:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14472, %14474, %14469, %14470, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14472, %14474)
    %14476 = arith.truncf %14475#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14477 = torch_c.from_builtin_tensor %14476 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14478 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14479 = torch.aten.view %14477, %14478 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %14480 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14481 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14482 = torch.aten.view %14449, %14481 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14483 = torch_c.to_builtin_tensor %14482 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14484 = torch_c.to_builtin_tensor %14480 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14485 = tensor.empty() : tensor<2048x1280xf32>
    %14486 = linalg.fill ins(%cst : f32) outs(%14485 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14487 = tensor.empty() : tensor<2048x1280xf32>
    %14488 = linalg.fill ins(%cst : f32) outs(%14487 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14489:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14486, %14488, %14483, %14484, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14486, %14488)
    %14490 = arith.truncf %14489#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14491 = torch_c.from_builtin_tensor %14490 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14492 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14493 = torch.aten.view %14491, %14492 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14494 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14495 = torch.aten.view %14465, %14494 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14496 = torch.aten.transpose.int %14495, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14497 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14498 = torch.aten.view %14479, %14497 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14499 = torch.aten.transpose.int %14498, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14500 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14501 = torch.aten.view %14493, %14500 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14502 = torch.aten.transpose.int %14501, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14503:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14496, %14499, %14502, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14504 = torch.aten.transpose.int %14503#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14505 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14506 = torch.aten.view %14504, %14505 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14507 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14508 = torch.aten.view %14506, %14507 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14509 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14510 = torch.aten.transpose.int %14509, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %14511 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14512 = torch.prims.convert_element_type %14511, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14513 = torch.prims.convert_element_type %14508, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14514 = torch.prims.convert_element_type %14510, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14515 = torch.aten.mm %14513, %14514 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14516 = torch.aten.mul.Scalar %14515, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14517 = torch.aten.mul.Scalar %14512, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14518 = torch.aten.add.Tensor %14516, %14517, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14519 = torch.prims.convert_element_type %14518, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14520 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14521 = torch.aten.view %14519, %14520 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14522 = torch.aten.div.Scalar %14521, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14523 = torch.aten.add.Tensor %14522, %14438, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14524 = torch.prims.convert_element_type %14523, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14525 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_415, %result1_416 = torch.aten.var_mean.correction %14524, %14525, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14526 = torch.aten.add.Scalar %result0_415, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14527 = torch.aten.rsqrt %14526 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14528 = torch.aten.sub.Tensor %14523, %result1_416, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14529 = torch.aten.mul.Tensor %14528, %14527 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %14530 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14531 = torch.aten.mul.Tensor %14529, %14530 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %14532 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14533 = torch.aten.add.Tensor %14531, %14532, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14534 = torch.prims.convert_element_type %14533, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14535 = torch.prims.convert_element_type %result1_416, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14536 = torch.prims.convert_element_type %14527, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %14537 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14538 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14539 = torch.aten.view %14534, %14538 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14540 = torch_c.to_builtin_tensor %14539 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14541 = torch_c.to_builtin_tensor %14537 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14542 = tensor.empty() : tensor<2048x1280xf32>
    %14543 = linalg.fill ins(%cst : f32) outs(%14542 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14544 = tensor.empty() : tensor<2048x1280xf32>
    %14545 = linalg.fill ins(%cst : f32) outs(%14544 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14546:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14543, %14545, %14540, %14541, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14543, %14545)
    %14547 = arith.truncf %14546#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14548 = torch_c.from_builtin_tensor %14547 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14549 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14550 = torch.aten.view %14548, %14549 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %14551 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14552 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14553 = torch.aten.view %4, %14552 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14554 = torch_c.to_builtin_tensor %14553 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14555 = torch_c.to_builtin_tensor %14551 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14556 = tensor.empty() : tensor<128x1280xf32>
    %14557 = linalg.fill ins(%cst : f32) outs(%14556 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14558 = tensor.empty() : tensor<128x1280xf32>
    %14559 = linalg.fill ins(%cst : f32) outs(%14558 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14560:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14557, %14559, %14554, %14555, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14557, %14559)
    %14561 = arith.truncf %14560#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14562 = torch_c.from_builtin_tensor %14561 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14563 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14564 = torch.aten.view %14562, %14563 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %14565 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14566 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14567 = torch.aten.view %4, %14566 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14568 = torch_c.to_builtin_tensor %14567 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14569 = torch_c.to_builtin_tensor %14565 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14570 = tensor.empty() : tensor<128x1280xf32>
    %14571 = linalg.fill ins(%cst : f32) outs(%14570 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14572 = tensor.empty() : tensor<128x1280xf32>
    %14573 = linalg.fill ins(%cst : f32) outs(%14572 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14574:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14571, %14573, %14568, %14569, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14571, %14573)
    %14575 = arith.truncf %14574#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14576 = torch_c.from_builtin_tensor %14575 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14577 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14578 = torch.aten.view %14576, %14577 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %14579 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14580 = torch.aten.view %14550, %14579 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14581 = torch.aten.transpose.int %14580, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14582 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14583 = torch.aten.view %14564, %14582 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14584 = torch.aten.transpose.int %14583, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14585 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14586 = torch.aten.view %14578, %14585 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14587 = torch.aten.transpose.int %14586, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14588:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14581, %14584, %14587, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14589 = torch.aten.transpose.int %14588#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14590 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14591 = torch.aten.view %14589, %14590 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14592 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14593 = torch.aten.view %14591, %14592 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14594 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14595 = torch.aten.transpose.int %14594, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %14596 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14597 = torch.prims.convert_element_type %14596, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14598 = torch.prims.convert_element_type %14593, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14599 = torch.prims.convert_element_type %14595, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14600 = torch.aten.mm %14598, %14599 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14601 = torch.aten.mul.Scalar %14600, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14602 = torch.aten.mul.Scalar %14597, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14603 = torch.aten.add.Tensor %14601, %14602, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14604 = torch.prims.convert_element_type %14603, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14605 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14606 = torch.aten.view %14604, %14605 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14607 = torch.aten.div.Scalar %14606, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14608 = torch.aten.add.Tensor %14607, %14523, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14609 = torch.prims.convert_element_type %14608, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14610 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_417, %result1_418 = torch.aten.var_mean.correction %14609, %14610, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14611 = torch.aten.add.Scalar %result0_417, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14612 = torch.aten.rsqrt %14611 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14613 = torch.aten.sub.Tensor %14608, %result1_418, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14614 = torch.aten.mul.Tensor %14613, %14612 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %14615 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14616 = torch.aten.mul.Tensor %14614, %14615 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %14617 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14618 = torch.aten.add.Tensor %14616, %14617, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14619 = torch.prims.convert_element_type %14618, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14620 = torch.prims.convert_element_type %result1_418, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14621 = torch.prims.convert_element_type %14612, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14622 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14623 = torch.aten.view %14619, %14622 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14624 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14625 = torch.aten.transpose.int %14624, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %14626 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14627 = torch.prims.convert_element_type %14626, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14628 = torch.prims.convert_element_type %14623, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14629 = torch.prims.convert_element_type %14625, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14630 = torch.aten.mm %14628, %14629 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14631 = torch.aten.mul.Scalar %14630, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14632 = torch.aten.mul.Scalar %14627, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14633 = torch.aten.add.Tensor %14631, %14632, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14634 = torch.prims.convert_element_type %14633, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14635 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14636 = torch.aten.view %14634, %14635 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14637 = torch.aten.slice.Tensor %14636, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14638 = torch.aten.slice.Tensor %14636, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14639 = torch.aten.gelu %14638, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14640 = torch.aten.mul.Tensor %14637, %14639 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14641 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14642 = torch.aten.view %14640, %14641 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %14643 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14644 = torch.aten.transpose.int %14643, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %14645 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14646 = torch.prims.convert_element_type %14645, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14647 = torch.prims.convert_element_type %14642, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14648 = torch.prims.convert_element_type %14644, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14649 = torch.aten.mm %14647, %14648 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14650 = torch.aten.mul.Scalar %14649, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14651 = torch.aten.mul.Scalar %14646, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14652 = torch.aten.add.Tensor %14650, %14651, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14653 = torch.prims.convert_element_type %14652, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14654 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14655 = torch.aten.view %14653, %14654 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14656 = torch.aten.add.Tensor %14655, %14608, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14657 = torch.prims.convert_element_type %14656, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14658 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_419, %result1_420 = torch.aten.var_mean.correction %14657, %14658, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14659 = torch.aten.add.Scalar %result0_419, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14660 = torch.aten.rsqrt %14659 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14661 = torch.aten.sub.Tensor %14656, %result1_420, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14662 = torch.aten.mul.Tensor %14661, %14660 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %14663 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14664 = torch.aten.mul.Tensor %14662, %14663 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %14665 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14666 = torch.aten.add.Tensor %14664, %14665, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14667 = torch.prims.convert_element_type %14666, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14668 = torch.prims.convert_element_type %result1_420, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14669 = torch.prims.convert_element_type %14660, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %14670 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14671 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14672 = torch.aten.view %14667, %14671 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14673 = torch_c.to_builtin_tensor %14672 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14674 = torch_c.to_builtin_tensor %14670 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14675 = tensor.empty() : tensor<2048x1280xf32>
    %14676 = linalg.fill ins(%cst : f32) outs(%14675 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14677 = tensor.empty() : tensor<2048x1280xf32>
    %14678 = linalg.fill ins(%cst : f32) outs(%14677 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14679:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14676, %14678, %14673, %14674, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14676, %14678)
    %14680 = arith.truncf %14679#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14681 = torch_c.from_builtin_tensor %14680 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14682 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14683 = torch.aten.view %14681, %14682 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %14684 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14685 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14686 = torch.aten.view %14667, %14685 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14687 = torch_c.to_builtin_tensor %14686 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14688 = torch_c.to_builtin_tensor %14684 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14689 = tensor.empty() : tensor<2048x1280xf32>
    %14690 = linalg.fill ins(%cst : f32) outs(%14689 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14691 = tensor.empty() : tensor<2048x1280xf32>
    %14692 = linalg.fill ins(%cst : f32) outs(%14691 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14693:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14690, %14692, %14687, %14688, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14690, %14692)
    %14694 = arith.truncf %14693#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14695 = torch_c.from_builtin_tensor %14694 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14696 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14697 = torch.aten.view %14695, %14696 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %14698 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14699 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14700 = torch.aten.view %14667, %14699 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14701 = torch_c.to_builtin_tensor %14700 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14702 = torch_c.to_builtin_tensor %14698 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14703 = tensor.empty() : tensor<2048x1280xf32>
    %14704 = linalg.fill ins(%cst : f32) outs(%14703 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14705 = tensor.empty() : tensor<2048x1280xf32>
    %14706 = linalg.fill ins(%cst : f32) outs(%14705 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14707:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14704, %14706, %14701, %14702, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14704, %14706)
    %14708 = arith.truncf %14707#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14709 = torch_c.from_builtin_tensor %14708 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14710 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14711 = torch.aten.view %14709, %14710 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14712 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14713 = torch.aten.view %14683, %14712 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14714 = torch.aten.transpose.int %14713, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14715 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14716 = torch.aten.view %14697, %14715 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14717 = torch.aten.transpose.int %14716, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14718 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14719 = torch.aten.view %14711, %14718 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14720 = torch.aten.transpose.int %14719, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14721:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14714, %14717, %14720, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14722 = torch.aten.transpose.int %14721#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14723 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14724 = torch.aten.view %14722, %14723 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14725 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14726 = torch.aten.view %14724, %14725 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14727 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14728 = torch.aten.transpose.int %14727, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %14729 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14730 = torch.prims.convert_element_type %14729, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14731 = torch.prims.convert_element_type %14726, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14732 = torch.prims.convert_element_type %14728, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14733 = torch.aten.mm %14731, %14732 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14734 = torch.aten.mul.Scalar %14733, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14735 = torch.aten.mul.Scalar %14730, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14736 = torch.aten.add.Tensor %14734, %14735, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14737 = torch.prims.convert_element_type %14736, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14738 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14739 = torch.aten.view %14737, %14738 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14740 = torch.aten.div.Scalar %14739, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14741 = torch.aten.add.Tensor %14740, %14656, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14742 = torch.prims.convert_element_type %14741, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14743 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_421, %result1_422 = torch.aten.var_mean.correction %14742, %14743, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14744 = torch.aten.add.Scalar %result0_421, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14745 = torch.aten.rsqrt %14744 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14746 = torch.aten.sub.Tensor %14741, %result1_422, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14747 = torch.aten.mul.Tensor %14746, %14745 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %14748 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14749 = torch.aten.mul.Tensor %14747, %14748 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %14750 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14751 = torch.aten.add.Tensor %14749, %14750, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14752 = torch.prims.convert_element_type %14751, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14753 = torch.prims.convert_element_type %result1_422, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14754 = torch.prims.convert_element_type %14745, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %14755 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14756 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14757 = torch.aten.view %14752, %14756 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14758 = torch_c.to_builtin_tensor %14757 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14759 = torch_c.to_builtin_tensor %14755 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14760 = tensor.empty() : tensor<2048x1280xf32>
    %14761 = linalg.fill ins(%cst : f32) outs(%14760 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14762 = tensor.empty() : tensor<2048x1280xf32>
    %14763 = linalg.fill ins(%cst : f32) outs(%14762 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14764:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14761, %14763, %14758, %14759, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14761, %14763)
    %14765 = arith.truncf %14764#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14766 = torch_c.from_builtin_tensor %14765 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14767 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14768 = torch.aten.view %14766, %14767 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %14769 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14770 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14771 = torch.aten.view %4, %14770 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14772 = torch_c.to_builtin_tensor %14771 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14773 = torch_c.to_builtin_tensor %14769 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14774 = tensor.empty() : tensor<128x1280xf32>
    %14775 = linalg.fill ins(%cst : f32) outs(%14774 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14776 = tensor.empty() : tensor<128x1280xf32>
    %14777 = linalg.fill ins(%cst : f32) outs(%14776 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14778:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14775, %14777, %14772, %14773, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14775, %14777)
    %14779 = arith.truncf %14778#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14780 = torch_c.from_builtin_tensor %14779 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14781 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14782 = torch.aten.view %14780, %14781 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %14783 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14784 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14785 = torch.aten.view %4, %14784 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14786 = torch_c.to_builtin_tensor %14785 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14787 = torch_c.to_builtin_tensor %14783 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14788 = tensor.empty() : tensor<128x1280xf32>
    %14789 = linalg.fill ins(%cst : f32) outs(%14788 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14790 = tensor.empty() : tensor<128x1280xf32>
    %14791 = linalg.fill ins(%cst : f32) outs(%14790 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14792:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14789, %14791, %14786, %14787, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14789, %14791)
    %14793 = arith.truncf %14792#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14794 = torch_c.from_builtin_tensor %14793 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14795 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14796 = torch.aten.view %14794, %14795 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %14797 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14798 = torch.aten.view %14768, %14797 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14799 = torch.aten.transpose.int %14798, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14800 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14801 = torch.aten.view %14782, %14800 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14802 = torch.aten.transpose.int %14801, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14803 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14804 = torch.aten.view %14796, %14803 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %14805 = torch.aten.transpose.int %14804, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %14806:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14799, %14802, %14805, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14807 = torch.aten.transpose.int %14806#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14808 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14809 = torch.aten.view %14807, %14808 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14810 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14811 = torch.aten.view %14809, %14810 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %14812 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14813 = torch.aten.transpose.int %14812, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %14814 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14815 = torch.prims.convert_element_type %14814, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14816 = torch.prims.convert_element_type %14811, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14817 = torch.prims.convert_element_type %14813, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14818 = torch.aten.mm %14816, %14817 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14819 = torch.aten.mul.Scalar %14818, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14820 = torch.aten.mul.Scalar %14815, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14821 = torch.aten.add.Tensor %14819, %14820, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14822 = torch.prims.convert_element_type %14821, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14823 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14824 = torch.aten.view %14822, %14823 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14825 = torch.aten.div.Scalar %14824, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14826 = torch.aten.add.Tensor %14825, %14741, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14827 = torch.prims.convert_element_type %14826, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14828 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_423, %result1_424 = torch.aten.var_mean.correction %14827, %14828, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14829 = torch.aten.add.Scalar %result0_423, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14830 = torch.aten.rsqrt %14829 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14831 = torch.aten.sub.Tensor %14826, %result1_424, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14832 = torch.aten.mul.Tensor %14831, %14830 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %14833 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14834 = torch.aten.mul.Tensor %14832, %14833 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %14835 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14836 = torch.aten.add.Tensor %14834, %14835, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14837 = torch.prims.convert_element_type %14836, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14838 = torch.prims.convert_element_type %result1_424, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14839 = torch.prims.convert_element_type %14830, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14840 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14841 = torch.aten.view %14837, %14840 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %14842 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %14843 = torch.aten.transpose.int %14842, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %14844 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %14845 = torch.prims.convert_element_type %14844, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %14846 = torch.prims.convert_element_type %14841, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14847 = torch.prims.convert_element_type %14843, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %14848 = torch.aten.mm %14846, %14847 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %14849 = torch.aten.mul.Scalar %14848, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14850 = torch.aten.mul.Scalar %14845, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %14851 = torch.aten.add.Tensor %14849, %14850, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %14852 = torch.prims.convert_element_type %14851, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %14853 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14854 = torch.aten.view %14852, %14853 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %14855 = torch.aten.slice.Tensor %14854, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14856 = torch.aten.slice.Tensor %14854, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %14857 = torch.aten.gelu %14856, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %14858 = torch.aten.mul.Tensor %14855, %14857 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %14859 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %14860 = torch.aten.view %14858, %14859 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %14861 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %14862 = torch.aten.transpose.int %14861, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %14863 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14864 = torch.prims.convert_element_type %14863, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14865 = torch.prims.convert_element_type %14860, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %14866 = torch.prims.convert_element_type %14862, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %14867 = torch.aten.mm %14865, %14866 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14868 = torch.aten.mul.Scalar %14867, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14869 = torch.aten.mul.Scalar %14864, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14870 = torch.aten.add.Tensor %14868, %14869, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14871 = torch.prims.convert_element_type %14870, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14872 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14873 = torch.aten.view %14871, %14872 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14874 = torch.aten.add.Tensor %14873, %14826, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14875 = torch.prims.convert_element_type %14874, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14876 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_425, %result1_426 = torch.aten.var_mean.correction %14875, %14876, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14877 = torch.aten.add.Scalar %result0_425, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14878 = torch.aten.rsqrt %14877 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14879 = torch.aten.sub.Tensor %14874, %result1_426, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14880 = torch.aten.mul.Tensor %14879, %14878 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %14881 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14882 = torch.aten.mul.Tensor %14880, %14881 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %14883 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14884 = torch.aten.add.Tensor %14882, %14883, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14885 = torch.prims.convert_element_type %14884, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14886 = torch.prims.convert_element_type %result1_426, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14887 = torch.prims.convert_element_type %14878, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %14888 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14889 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14890 = torch.aten.view %14885, %14889 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14891 = torch_c.to_builtin_tensor %14890 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14892 = torch_c.to_builtin_tensor %14888 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14893 = tensor.empty() : tensor<2048x1280xf32>
    %14894 = linalg.fill ins(%cst : f32) outs(%14893 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14895 = tensor.empty() : tensor<2048x1280xf32>
    %14896 = linalg.fill ins(%cst : f32) outs(%14895 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14897:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14894, %14896, %14891, %14892, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14894, %14896)
    %14898 = arith.truncf %14897#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14899 = torch_c.from_builtin_tensor %14898 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14900 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14901 = torch.aten.view %14899, %14900 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %14902 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14903 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14904 = torch.aten.view %14885, %14903 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14905 = torch_c.to_builtin_tensor %14904 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14906 = torch_c.to_builtin_tensor %14902 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14907 = tensor.empty() : tensor<2048x1280xf32>
    %14908 = linalg.fill ins(%cst : f32) outs(%14907 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14909 = tensor.empty() : tensor<2048x1280xf32>
    %14910 = linalg.fill ins(%cst : f32) outs(%14909 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14911:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14908, %14910, %14905, %14906, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14908, %14910)
    %14912 = arith.truncf %14911#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14913 = torch_c.from_builtin_tensor %14912 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14914 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14915 = torch.aten.view %14913, %14914 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %14916 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14917 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14918 = torch.aten.view %14885, %14917 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14919 = torch_c.to_builtin_tensor %14918 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14920 = torch_c.to_builtin_tensor %14916 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14921 = tensor.empty() : tensor<2048x1280xf32>
    %14922 = linalg.fill ins(%cst : f32) outs(%14921 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14923 = tensor.empty() : tensor<2048x1280xf32>
    %14924 = linalg.fill ins(%cst : f32) outs(%14923 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14925:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14922, %14924, %14919, %14920, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14922, %14924)
    %14926 = arith.truncf %14925#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14927 = torch_c.from_builtin_tensor %14926 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14928 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14929 = torch.aten.view %14927, %14928 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14930 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14931 = torch.aten.view %14901, %14930 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14932 = torch.aten.transpose.int %14931, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14933 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14934 = torch.aten.view %14915, %14933 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14935 = torch.aten.transpose.int %14934, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14936 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14937 = torch.aten.view %14929, %14936 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %14938 = torch.aten.transpose.int %14937, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %14939:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%14932, %14935, %14938, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %14940 = torch.aten.transpose.int %14939#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %14941 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14942 = torch.aten.view %14940, %14941 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14943 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14944 = torch.aten.view %14942, %14943 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %14945 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14946 = torch.aten.transpose.int %14945, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %14947 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14948 = torch.prims.convert_element_type %14947, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %14949 = torch.prims.convert_element_type %14944, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14950 = torch.prims.convert_element_type %14946, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %14951 = torch.aten.mm %14949, %14950 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %14952 = torch.aten.mul.Scalar %14951, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14953 = torch.aten.mul.Scalar %14948, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %14954 = torch.aten.add.Tensor %14952, %14953, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %14955 = torch.prims.convert_element_type %14954, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %14956 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14957 = torch.aten.view %14955, %14956 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %14958 = torch.aten.div.Scalar %14957, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %14959 = torch.aten.add.Tensor %14958, %14874, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14960 = torch.prims.convert_element_type %14959, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14961 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_427, %result1_428 = torch.aten.var_mean.correction %14960, %14961, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %14962 = torch.aten.add.Scalar %result0_427, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %14963 = torch.aten.rsqrt %14962 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %14964 = torch.aten.sub.Tensor %14959, %result1_428, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14965 = torch.aten.mul.Tensor %14964, %14963 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %14966 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14967 = torch.aten.mul.Tensor %14965, %14966 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %14968 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %14969 = torch.aten.add.Tensor %14967, %14968, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %14970 = torch.prims.convert_element_type %14969, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %14971 = torch.prims.convert_element_type %result1_428, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %14972 = torch.prims.convert_element_type %14963, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %14973 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %14974 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %14975 = torch.aten.view %14970, %14974 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %14976 = torch_c.to_builtin_tensor %14975 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %14977 = torch_c.to_builtin_tensor %14973 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %14978 = tensor.empty() : tensor<2048x1280xf32>
    %14979 = linalg.fill ins(%cst : f32) outs(%14978 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14980 = tensor.empty() : tensor<2048x1280xf32>
    %14981 = linalg.fill ins(%cst : f32) outs(%14980 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %14982:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %14979, %14981, %14976, %14977, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14979, %14981)
    %14983 = arith.truncf %14982#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %14984 = torch_c.from_builtin_tensor %14983 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %14985 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %14986 = torch.aten.view %14984, %14985 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %14987 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %14988 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %14989 = torch.aten.view %4, %14988 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %14990 = torch_c.to_builtin_tensor %14989 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %14991 = torch_c.to_builtin_tensor %14987 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %14992 = tensor.empty() : tensor<128x1280xf32>
    %14993 = linalg.fill ins(%cst : f32) outs(%14992 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14994 = tensor.empty() : tensor<128x1280xf32>
    %14995 = linalg.fill ins(%cst : f32) outs(%14994 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %14996:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %14993, %14995, %14990, %14991, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%14993, %14995)
    %14997 = arith.truncf %14996#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %14998 = torch_c.from_builtin_tensor %14997 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %14999 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15000 = torch.aten.view %14998, %14999 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %15001 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %15002 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15003 = torch.aten.view %4, %15002 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15004 = torch_c.to_builtin_tensor %15003 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %15005 = torch_c.to_builtin_tensor %15001 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %15006 = tensor.empty() : tensor<128x1280xf32>
    %15007 = linalg.fill ins(%cst : f32) outs(%15006 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15008 = tensor.empty() : tensor<128x1280xf32>
    %15009 = linalg.fill ins(%cst : f32) outs(%15008 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15010:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %15007, %15009, %15004, %15005, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15007, %15009)
    %15011 = arith.truncf %15010#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %15012 = torch_c.from_builtin_tensor %15011 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %15013 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15014 = torch.aten.view %15012, %15013 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %15015 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15016 = torch.aten.view %14986, %15015 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15017 = torch.aten.transpose.int %15016, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15018 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15019 = torch.aten.view %15000, %15018 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %15020 = torch.aten.transpose.int %15019, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %15021 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15022 = torch.aten.view %15014, %15021 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %15023 = torch.aten.transpose.int %15022, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %15024:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15017, %15020, %15023, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %15025 = torch.aten.transpose.int %15024#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %15026 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15027 = torch.aten.view %15025, %15026 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15028 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15029 = torch.aten.view %15027, %15028 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %15030 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15031 = torch.aten.transpose.int %15030, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %15032 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15033 = torch.prims.convert_element_type %15032, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15034 = torch.prims.convert_element_type %15029, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15035 = torch.prims.convert_element_type %15031, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %15036 = torch.aten.mm %15034, %15035 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15037 = torch.aten.mul.Scalar %15036, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15038 = torch.aten.mul.Scalar %15033, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15039 = torch.aten.add.Tensor %15037, %15038, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15040 = torch.prims.convert_element_type %15039, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15041 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15042 = torch.aten.view %15040, %15041 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15043 = torch.aten.div.Scalar %15042, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %15044 = torch.aten.add.Tensor %15043, %14959, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15045 = torch.prims.convert_element_type %15044, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15046 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_429, %result1_430 = torch.aten.var_mean.correction %15045, %15046, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %15047 = torch.aten.add.Scalar %result0_429, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %15048 = torch.aten.rsqrt %15047 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %15049 = torch.aten.sub.Tensor %15044, %result1_430, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15050 = torch.aten.mul.Tensor %15049, %15048 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %15051 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15052 = torch.aten.mul.Tensor %15050, %15051 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %15053 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15054 = torch.aten.add.Tensor %15052, %15053, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15055 = torch.prims.convert_element_type %15054, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15056 = torch.prims.convert_element_type %result1_430, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15057 = torch.prims.convert_element_type %15048, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15058 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15059 = torch.aten.view %15055, %15058 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %15060 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %15061 = torch.aten.transpose.int %15060, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %15062 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %15063 = torch.prims.convert_element_type %15062, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %15064 = torch.prims.convert_element_type %15059, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15065 = torch.prims.convert_element_type %15061, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %15066 = torch.aten.mm %15064, %15065 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %15067 = torch.aten.mul.Scalar %15066, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %15068 = torch.aten.mul.Scalar %15063, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %15069 = torch.aten.add.Tensor %15067, %15068, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %15070 = torch.prims.convert_element_type %15069, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %15071 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15072 = torch.aten.view %15070, %15071 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %15073 = torch.aten.slice.Tensor %15072, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %15074 = torch.aten.slice.Tensor %15072, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %15075 = torch.aten.gelu %15074, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %15076 = torch.aten.mul.Tensor %15073, %15075 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %15077 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %15078 = torch.aten.view %15076, %15077 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %15079 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %15080 = torch.aten.transpose.int %15079, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %15081 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15082 = torch.prims.convert_element_type %15081, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15083 = torch.prims.convert_element_type %15078, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %15084 = torch.prims.convert_element_type %15080, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %15085 = torch.aten.mm %15083, %15084 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15086 = torch.aten.mul.Scalar %15085, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15087 = torch.aten.mul.Scalar %15082, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15088 = torch.aten.add.Tensor %15086, %15087, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15089 = torch.prims.convert_element_type %15088, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15090 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15091 = torch.aten.view %15089, %15090 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15092 = torch.aten.add.Tensor %15091, %15044, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15093 = torch.prims.convert_element_type %15092, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15094 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_431, %result1_432 = torch.aten.var_mean.correction %15093, %15094, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %15095 = torch.aten.add.Scalar %result0_431, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %15096 = torch.aten.rsqrt %15095 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %15097 = torch.aten.sub.Tensor %15092, %result1_432, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15098 = torch.aten.mul.Tensor %15097, %15096 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %15099 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15100 = torch.aten.mul.Tensor %15098, %15099 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %15101 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15102 = torch.aten.add.Tensor %15100, %15101, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15103 = torch.prims.convert_element_type %15102, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15104 = torch.prims.convert_element_type %result1_432, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15105 = torch.prims.convert_element_type %15096, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %15106 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15107 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15108 = torch.aten.view %15103, %15107 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15109 = torch_c.to_builtin_tensor %15108 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15110 = torch_c.to_builtin_tensor %15106 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15111 = tensor.empty() : tensor<2048x1280xf32>
    %15112 = linalg.fill ins(%cst : f32) outs(%15111 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15113 = tensor.empty() : tensor<2048x1280xf32>
    %15114 = linalg.fill ins(%cst : f32) outs(%15113 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15115:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15112, %15114, %15109, %15110, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15112, %15114)
    %15116 = arith.truncf %15115#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15117 = torch_c.from_builtin_tensor %15116 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15118 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15119 = torch.aten.view %15117, %15118 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %15120 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15121 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15122 = torch.aten.view %15103, %15121 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15123 = torch_c.to_builtin_tensor %15122 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15124 = torch_c.to_builtin_tensor %15120 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15125 = tensor.empty() : tensor<2048x1280xf32>
    %15126 = linalg.fill ins(%cst : f32) outs(%15125 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15127 = tensor.empty() : tensor<2048x1280xf32>
    %15128 = linalg.fill ins(%cst : f32) outs(%15127 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15129:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15126, %15128, %15123, %15124, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15126, %15128)
    %15130 = arith.truncf %15129#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15131 = torch_c.from_builtin_tensor %15130 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15132 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15133 = torch.aten.view %15131, %15132 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %15134 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15135 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15136 = torch.aten.view %15103, %15135 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15137 = torch_c.to_builtin_tensor %15136 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15138 = torch_c.to_builtin_tensor %15134 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15139 = tensor.empty() : tensor<2048x1280xf32>
    %15140 = linalg.fill ins(%cst : f32) outs(%15139 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15141 = tensor.empty() : tensor<2048x1280xf32>
    %15142 = linalg.fill ins(%cst : f32) outs(%15141 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15143:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15140, %15142, %15137, %15138, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15140, %15142)
    %15144 = arith.truncf %15143#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15145 = torch_c.from_builtin_tensor %15144 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15146 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15147 = torch.aten.view %15145, %15146 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15148 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15149 = torch.aten.view %15119, %15148 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15150 = torch.aten.transpose.int %15149, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15151 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15152 = torch.aten.view %15133, %15151 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15153 = torch.aten.transpose.int %15152, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15154 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15155 = torch.aten.view %15147, %15154 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15156 = torch.aten.transpose.int %15155, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15157:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15150, %15153, %15156, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %15158 = torch.aten.transpose.int %15157#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %15159 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15160 = torch.aten.view %15158, %15159 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15161 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15162 = torch.aten.view %15160, %15161 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %15163 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15164 = torch.aten.transpose.int %15163, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %15165 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15166 = torch.prims.convert_element_type %15165, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15167 = torch.prims.convert_element_type %15162, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15168 = torch.prims.convert_element_type %15164, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %15169 = torch.aten.mm %15167, %15168 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15170 = torch.aten.mul.Scalar %15169, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15171 = torch.aten.mul.Scalar %15166, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15172 = torch.aten.add.Tensor %15170, %15171, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15173 = torch.prims.convert_element_type %15172, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15174 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15175 = torch.aten.view %15173, %15174 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15176 = torch.aten.div.Scalar %15175, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %15177 = torch.aten.add.Tensor %15176, %15092, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15178 = torch.prims.convert_element_type %15177, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15179 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_433, %result1_434 = torch.aten.var_mean.correction %15178, %15179, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %15180 = torch.aten.add.Scalar %result0_433, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %15181 = torch.aten.rsqrt %15180 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %15182 = torch.aten.sub.Tensor %15177, %result1_434, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15183 = torch.aten.mul.Tensor %15182, %15181 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %15184 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15185 = torch.aten.mul.Tensor %15183, %15184 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %15186 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15187 = torch.aten.add.Tensor %15185, %15186, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15188 = torch.prims.convert_element_type %15187, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15189 = torch.prims.convert_element_type %result1_434, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15190 = torch.prims.convert_element_type %15181, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %15191 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15192 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15193 = torch.aten.view %15188, %15192 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15194 = torch_c.to_builtin_tensor %15193 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15195 = torch_c.to_builtin_tensor %15191 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15196 = tensor.empty() : tensor<2048x1280xf32>
    %15197 = linalg.fill ins(%cst : f32) outs(%15196 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15198 = tensor.empty() : tensor<2048x1280xf32>
    %15199 = linalg.fill ins(%cst : f32) outs(%15198 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15200:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15197, %15199, %15194, %15195, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15197, %15199)
    %15201 = arith.truncf %15200#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15202 = torch_c.from_builtin_tensor %15201 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15203 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15204 = torch.aten.view %15202, %15203 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %15205 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %15206 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15207 = torch.aten.view %4, %15206 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15208 = torch_c.to_builtin_tensor %15207 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %15209 = torch_c.to_builtin_tensor %15205 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %15210 = tensor.empty() : tensor<128x1280xf32>
    %15211 = linalg.fill ins(%cst : f32) outs(%15210 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15212 = tensor.empty() : tensor<128x1280xf32>
    %15213 = linalg.fill ins(%cst : f32) outs(%15212 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15214:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %15211, %15213, %15208, %15209, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15211, %15213)
    %15215 = arith.truncf %15214#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %15216 = torch_c.from_builtin_tensor %15215 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %15217 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15218 = torch.aten.view %15216, %15217 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %15219 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %15220 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15221 = torch.aten.view %4, %15220 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15222 = torch_c.to_builtin_tensor %15221 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %15223 = torch_c.to_builtin_tensor %15219 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %15224 = tensor.empty() : tensor<128x1280xf32>
    %15225 = linalg.fill ins(%cst : f32) outs(%15224 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15226 = tensor.empty() : tensor<128x1280xf32>
    %15227 = linalg.fill ins(%cst : f32) outs(%15226 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15228:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %15225, %15227, %15222, %15223, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15225, %15227)
    %15229 = arith.truncf %15228#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %15230 = torch_c.from_builtin_tensor %15229 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %15231 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15232 = torch.aten.view %15230, %15231 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %15233 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15234 = torch.aten.view %15204, %15233 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15235 = torch.aten.transpose.int %15234, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15236 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15237 = torch.aten.view %15218, %15236 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %15238 = torch.aten.transpose.int %15237, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %15239 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15240 = torch.aten.view %15232, %15239 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %15241 = torch.aten.transpose.int %15240, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %15242:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15235, %15238, %15241, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %15243 = torch.aten.transpose.int %15242#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %15244 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15245 = torch.aten.view %15243, %15244 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15246 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15247 = torch.aten.view %15245, %15246 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %15248 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15249 = torch.aten.transpose.int %15248, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %15250 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15251 = torch.prims.convert_element_type %15250, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15252 = torch.prims.convert_element_type %15247, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15253 = torch.prims.convert_element_type %15249, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %15254 = torch.aten.mm %15252, %15253 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15255 = torch.aten.mul.Scalar %15254, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15256 = torch.aten.mul.Scalar %15251, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15257 = torch.aten.add.Tensor %15255, %15256, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15258 = torch.prims.convert_element_type %15257, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15259 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15260 = torch.aten.view %15258, %15259 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15261 = torch.aten.div.Scalar %15260, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %15262 = torch.aten.add.Tensor %15261, %15177, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15263 = torch.prims.convert_element_type %15262, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15264 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_435, %result1_436 = torch.aten.var_mean.correction %15263, %15264, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %15265 = torch.aten.add.Scalar %result0_435, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %15266 = torch.aten.rsqrt %15265 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %15267 = torch.aten.sub.Tensor %15262, %result1_436, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15268 = torch.aten.mul.Tensor %15267, %15266 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %15269 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15270 = torch.aten.mul.Tensor %15268, %15269 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %15271 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15272 = torch.aten.add.Tensor %15270, %15271, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15273 = torch.prims.convert_element_type %15272, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15274 = torch.prims.convert_element_type %result1_436, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15275 = torch.prims.convert_element_type %15266, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15276 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15277 = torch.aten.view %15273, %15276 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %15278 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %15279 = torch.aten.transpose.int %15278, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %15280 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %15281 = torch.prims.convert_element_type %15280, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %15282 = torch.prims.convert_element_type %15277, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15283 = torch.prims.convert_element_type %15279, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %15284 = torch.aten.mm %15282, %15283 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %15285 = torch.aten.mul.Scalar %15284, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %15286 = torch.aten.mul.Scalar %15281, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %15287 = torch.aten.add.Tensor %15285, %15286, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %15288 = torch.prims.convert_element_type %15287, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %15289 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15290 = torch.aten.view %15288, %15289 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %15291 = torch.aten.slice.Tensor %15290, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %15292 = torch.aten.slice.Tensor %15290, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %15293 = torch.aten.gelu %15292, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %15294 = torch.aten.mul.Tensor %15291, %15293 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %15295 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %15296 = torch.aten.view %15294, %15295 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %15297 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %15298 = torch.aten.transpose.int %15297, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %15299 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15300 = torch.prims.convert_element_type %15299, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15301 = torch.prims.convert_element_type %15296, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %15302 = torch.prims.convert_element_type %15298, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %15303 = torch.aten.mm %15301, %15302 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15304 = torch.aten.mul.Scalar %15303, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15305 = torch.aten.mul.Scalar %15300, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15306 = torch.aten.add.Tensor %15304, %15305, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15307 = torch.prims.convert_element_type %15306, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15308 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15309 = torch.aten.view %15307, %15308 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15310 = torch.aten.add.Tensor %15309, %15262, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15311 = torch.prims.convert_element_type %15310, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15312 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_437, %result1_438 = torch.aten.var_mean.correction %15311, %15312, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %15313 = torch.aten.add.Scalar %result0_437, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %15314 = torch.aten.rsqrt %15313 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %15315 = torch.aten.sub.Tensor %15310, %result1_438, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15316 = torch.aten.mul.Tensor %15315, %15314 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %15317 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15318 = torch.aten.mul.Tensor %15316, %15317 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %15319 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15320 = torch.aten.add.Tensor %15318, %15319, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15321 = torch.prims.convert_element_type %15320, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15322 = torch.prims.convert_element_type %result1_438, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15323 = torch.prims.convert_element_type %15314, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %15324 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15325 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15326 = torch.aten.view %15321, %15325 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15327 = torch_c.to_builtin_tensor %15326 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15328 = torch_c.to_builtin_tensor %15324 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15329 = tensor.empty() : tensor<2048x1280xf32>
    %15330 = linalg.fill ins(%cst : f32) outs(%15329 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15331 = tensor.empty() : tensor<2048x1280xf32>
    %15332 = linalg.fill ins(%cst : f32) outs(%15331 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15333:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15330, %15332, %15327, %15328, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15330, %15332)
    %15334 = arith.truncf %15333#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15335 = torch_c.from_builtin_tensor %15334 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15336 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15337 = torch.aten.view %15335, %15336 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %15338 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15339 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15340 = torch.aten.view %15321, %15339 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15341 = torch_c.to_builtin_tensor %15340 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15342 = torch_c.to_builtin_tensor %15338 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15343 = tensor.empty() : tensor<2048x1280xf32>
    %15344 = linalg.fill ins(%cst : f32) outs(%15343 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15345 = tensor.empty() : tensor<2048x1280xf32>
    %15346 = linalg.fill ins(%cst : f32) outs(%15345 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15347:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15344, %15346, %15341, %15342, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15344, %15346)
    %15348 = arith.truncf %15347#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15349 = torch_c.from_builtin_tensor %15348 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15350 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15351 = torch.aten.view %15349, %15350 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %15352 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15353 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15354 = torch.aten.view %15321, %15353 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15355 = torch_c.to_builtin_tensor %15354 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15356 = torch_c.to_builtin_tensor %15352 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15357 = tensor.empty() : tensor<2048x1280xf32>
    %15358 = linalg.fill ins(%cst : f32) outs(%15357 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15359 = tensor.empty() : tensor<2048x1280xf32>
    %15360 = linalg.fill ins(%cst : f32) outs(%15359 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15361:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15358, %15360, %15355, %15356, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15358, %15360)
    %15362 = arith.truncf %15361#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15363 = torch_c.from_builtin_tensor %15362 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15364 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15365 = torch.aten.view %15363, %15364 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15366 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15367 = torch.aten.view %15337, %15366 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15368 = torch.aten.transpose.int %15367, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15369 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15370 = torch.aten.view %15351, %15369 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15371 = torch.aten.transpose.int %15370, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15372 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15373 = torch.aten.view %15365, %15372 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15374 = torch.aten.transpose.int %15373, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15375:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15368, %15371, %15374, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %15376 = torch.aten.transpose.int %15375#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %15377 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15378 = torch.aten.view %15376, %15377 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15379 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15380 = torch.aten.view %15378, %15379 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %15381 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15382 = torch.aten.transpose.int %15381, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %15383 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15384 = torch.prims.convert_element_type %15383, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15385 = torch.prims.convert_element_type %15380, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15386 = torch.prims.convert_element_type %15382, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %15387 = torch.aten.mm %15385, %15386 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15388 = torch.aten.mul.Scalar %15387, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15389 = torch.aten.mul.Scalar %15384, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15390 = torch.aten.add.Tensor %15388, %15389, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15391 = torch.prims.convert_element_type %15390, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15392 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15393 = torch.aten.view %15391, %15392 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15394 = torch.aten.div.Scalar %15393, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %15395 = torch.aten.add.Tensor %15394, %15310, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15396 = torch.prims.convert_element_type %15395, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15397 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_439, %result1_440 = torch.aten.var_mean.correction %15396, %15397, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %15398 = torch.aten.add.Scalar %result0_439, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %15399 = torch.aten.rsqrt %15398 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %15400 = torch.aten.sub.Tensor %15395, %result1_440, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15401 = torch.aten.mul.Tensor %15400, %15399 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %15402 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15403 = torch.aten.mul.Tensor %15401, %15402 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %15404 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15405 = torch.aten.add.Tensor %15403, %15404, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15406 = torch.prims.convert_element_type %15405, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15407 = torch.prims.convert_element_type %result1_440, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15408 = torch.prims.convert_element_type %15399, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %15409 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15410 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15411 = torch.aten.view %15406, %15410 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %15412 = torch_c.to_builtin_tensor %15411 : !torch.vtensor<[2048,1280],f16> -> tensor<2048x1280xf16>
    %15413 = torch_c.to_builtin_tensor %15409 : !torch.vtensor<[1280,1280],f16> -> tensor<1280x1280xf16>
    %15414 = tensor.empty() : tensor<2048x1280xf32>
    %15415 = linalg.fill ins(%cst : f32) outs(%15414 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15416 = tensor.empty() : tensor<2048x1280xf32>
    %15417 = linalg.fill ins(%cst : f32) outs(%15416 : tensor<2048x1280xf32>) -> tensor<2048x1280xf32>
    %15418:2 = flow.dispatch @hip_matmul_exe_2048x1280_1280x1280::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1[%c4096, %c16, %c1](%c1_i32, %c17301761_i32, %c2048_i32, %c1280_i32, %c1_i32, %c1280_i32, %15415, %15417, %15412, %15413, %c2048_i32, %c2621440_i32, %c2048_i32, %c2621440_i32, %c1280_i32, %c1638400_i32, %c1280_i32, %c2621440_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<2048x1280xf32>, tensor<2048x1280xf32>, tensor<2048x1280xf16>, tensor<1280x1280xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15415, %15417)
    %15419 = arith.truncf %15418#0 : tensor<2048x1280xf32> to tensor<2048x1280xf16>
    %15420 = torch_c.from_builtin_tensor %15419 : tensor<2048x1280xf16> -> !torch.vtensor<[2048,1280],f16>
    %15421 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15422 = torch.aten.view %15420, %15421 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %15423 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %15424 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15425 = torch.aten.view %4, %15424 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15426 = torch_c.to_builtin_tensor %15425 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %15427 = torch_c.to_builtin_tensor %15423 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %15428 = tensor.empty() : tensor<128x1280xf32>
    %15429 = linalg.fill ins(%cst : f32) outs(%15428 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15430 = tensor.empty() : tensor<128x1280xf32>
    %15431 = linalg.fill ins(%cst : f32) outs(%15430 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15432:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %15429, %15431, %15426, %15427, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15429, %15431)
    %15433 = arith.truncf %15432#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %15434 = torch_c.from_builtin_tensor %15433 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %15435 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15436 = torch.aten.view %15434, %15435 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %15437 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %15438 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15439 = torch.aten.view %4, %15438 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15440 = torch_c.to_builtin_tensor %15439 : !torch.vtensor<[128,2048],f16> -> tensor<128x2048xf16>
    %15441 = torch_c.to_builtin_tensor %15437 : !torch.vtensor<[1280,2048],f16> -> tensor<1280x2048xf16>
    %15442 = tensor.empty() : tensor<128x1280xf32>
    %15443 = linalg.fill ins(%cst : f32) outs(%15442 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15444 = tensor.empty() : tensor<128x1280xf32>
    %15445 = linalg.fill ins(%cst : f32) outs(%15444 : tensor<128x1280xf32>) -> tensor<128x1280xf32>
    %15446:2 = flow.dispatch @hip_matmul_exe_128x2048_1280x2048::@Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1[%c1024, %c40, %c1](%c1_i32, %c524545_i32, %c128_i32, %c1280_i32, %c1_i32, %c2048_i32, %15443, %15445, %15440, %15441, %c128_i32, %c163840_i32, %c128_i32, %c163840_i32, %c2048_i32, %c2621440_i32, %c2048_i32, %c262144_i32, %c1065353216_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32) : (i32, i32, i32, i32, i32, i32, tensor<128x1280xf32>, tensor<128x1280xf32>, tensor<128x2048xf16>, tensor<1280x2048xf16>, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) -> (%15443, %15445)
    %15447 = arith.truncf %15446#0 : tensor<128x1280xf32> to tensor<128x1280xf16>
    %15448 = torch_c.from_builtin_tensor %15447 : tensor<128x1280xf16> -> !torch.vtensor<[128,1280],f16>
    %15449 = torch.prim.ListConstruct %int2, %int64, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15450 = torch.aten.view %15448, %15449 : !torch.vtensor<[128,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,1280],f16>
    %15451 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15452 = torch.aten.view %15422, %15451 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,20,64],f16>
    %15453 = torch.aten.transpose.int %15452, %int1, %int2 : !torch.vtensor<[2,1024,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,1024,64],f16>
    %15454 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15455 = torch.aten.view %15436, %15454 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %15456 = torch.aten.transpose.int %15455, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %15457 = torch.prim.ListConstruct %int2, %int-1, %int20, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15458 = torch.aten.view %15450, %15457 : !torch.vtensor<[2,64,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,64,20,64],f16>
    %15459 = torch.aten.transpose.int %15458, %int1, %int2 : !torch.vtensor<[2,64,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,64,64],f16>
    %15460:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15453, %15456, %15459, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.vtensor<[2,20,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,1024,64],f16>, !torch.vtensor<[2,20,1024],f32>) 
    %15461 = torch.aten.transpose.int %15460#0, %int1, %int2 : !torch.vtensor<[2,20,1024,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,1024,20,64],f16>
    %15462 = torch.prim.ListConstruct %int2, %int-1, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15463 = torch.aten.view %15461, %15462 : !torch.vtensor<[2,1024,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15464 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15465 = torch.aten.view %15463, %15464 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %15466 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15467 = torch.aten.transpose.int %15466, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %15468 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15469 = torch.prims.convert_element_type %15468, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15470 = torch.prims.convert_element_type %15465, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15471 = torch.prims.convert_element_type %15467, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %15472 = torch.aten.mm %15470, %15471 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15473 = torch.aten.mul.Scalar %15472, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15474 = torch.aten.mul.Scalar %15469, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15475 = torch.aten.add.Tensor %15473, %15474, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15476 = torch.prims.convert_element_type %15475, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15477 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15478 = torch.aten.view %15476, %15477 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15479 = torch.aten.div.Scalar %15478, %float1.000000e00 : !torch.vtensor<[2,1024,1280],f16>, !torch.float -> !torch.vtensor<[2,1024,1280],f16>
    %15480 = torch.aten.add.Tensor %15479, %15395, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15481 = torch.prims.convert_element_type %15480, %int6 : !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15482 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_441, %result1_442 = torch.aten.var_mean.correction %15481, %15482, %int0, %true : !torch.vtensor<[2,1024,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,1024,1],f32>, !torch.vtensor<[2,1024,1],f32>
    %15483 = torch.aten.add.Scalar %result0_441, %float1.000000e-05, %int1 : !torch.vtensor<[2,1024,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,1024,1],f32>
    %15484 = torch.aten.rsqrt %15483 : !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1],f32>
    %15485 = torch.aten.sub.Tensor %15480, %result1_442, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15486 = torch.aten.mul.Tensor %15485, %15484 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[2,1024,1],f32> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %15487 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15488 = torch.aten.mul.Tensor %15486, %15487 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,1024,1280],f32>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %15489 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15490 = torch.aten.add.Tensor %15488, %15489, %int1 : !torch.vtensor<[2,1024,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f32>
    %15491 = torch.prims.convert_element_type %15490, %int5 : !torch.vtensor<[2,1024,1280],f32>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15492 = torch.prims.convert_element_type %result1_442, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15493 = torch.prims.convert_element_type %15484, %int5 : !torch.vtensor<[2,1024,1],f32>, !torch.int -> !torch.vtensor<[2,1024,1],f16>
    %15494 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15495 = torch.aten.view %15491, %15494 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %15496 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %15497 = torch.aten.transpose.int %15496, %int0, %int1 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %15498 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %15499 = torch.prims.convert_element_type %15498, %int6 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %15500 = torch.prims.convert_element_type %15495, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15501 = torch.prims.convert_element_type %15497, %int6 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %15502 = torch.aten.mm %15500, %15501 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[2048,10240],f32>
    %15503 = torch.aten.mul.Scalar %15502, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %15504 = torch.aten.mul.Scalar %15499, %int1 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %15505 = torch.aten.add.Tensor %15503, %15504, %int1 : !torch.vtensor<[2048,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f32>
    %15506 = torch.prims.convert_element_type %15505, %int5 : !torch.vtensor<[2048,10240],f32>, !torch.int -> !torch.vtensor<[2048,10240],f16>
    %15507 = torch.prim.ListConstruct %int2, %int1024, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15508 = torch.aten.view %15506, %15507 : !torch.vtensor<[2048,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,10240],f16>
    %15509 = torch.aten.slice.Tensor %15508, %int-1, %int0, %int5120, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %15510 = torch.aten.slice.Tensor %15508, %int-1, %int5120, %int10240, %int1 : !torch.vtensor<[2,1024,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1024,5120],f16>
    %15511 = torch.aten.gelu %15510, %str : !torch.vtensor<[2,1024,5120],f16>, !torch.str -> !torch.vtensor<[2,1024,5120],f16>
    %15512 = torch.aten.mul.Tensor %15509, %15511 : !torch.vtensor<[2,1024,5120],f16>, !torch.vtensor<[2,1024,5120],f16> -> !torch.vtensor<[2,1024,5120],f16>
    %15513 = torch.prim.ListConstruct %int2048, %int5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %15514 = torch.aten.view %15512, %15513 : !torch.vtensor<[2,1024,5120],f16>, !torch.list<int> -> !torch.vtensor<[2048,5120],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %15515 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %15516 = torch.aten.transpose.int %15515, %int0, %int1 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %15517 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15518 = torch.prims.convert_element_type %15517, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15519 = torch.prims.convert_element_type %15514, %int6 : !torch.vtensor<[2048,5120],f16>, !torch.int -> !torch.vtensor<[2048,5120],f32>
    %15520 = torch.prims.convert_element_type %15516, %int6 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %15521 = torch.aten.mm %15519, %15520 : !torch.vtensor<[2048,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15522 = torch.aten.mul.Scalar %15521, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15523 = torch.aten.mul.Scalar %15518, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15524 = torch.aten.add.Tensor %15522, %15523, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15525 = torch.prims.convert_element_type %15524, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15526 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15527 = torch.aten.view %15525, %15526 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15528 = torch.aten.add.Tensor %15527, %15480, %int1 : !torch.vtensor<[2,1024,1280],f16>, !torch.vtensor<[2,1024,1280],f16>, !torch.int -> !torch.vtensor<[2,1024,1280],f16>
    %15529 = torch.prim.ListConstruct %int2048, %int1280 : (!torch.int, !torch.int) -> !torch.list<int>
    %15530 = torch.aten.view %15528, %15529 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2048,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_out.weight = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_out.weight : tensor<1280x1280xf16>
    %15531 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %15532 = torch.aten.transpose.int %15531, %int0, %int1 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %_params.unet.up_blocks.0.attentions.2.proj_out.bias = util.global.load @_params.unet.up_blocks.0.attentions.2.proj_out.bias : tensor<1280xf16>
    %15533 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.attentions.2.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15534 = torch.prims.convert_element_type %15533, %int6 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %15535 = torch.prims.convert_element_type %15530, %int6 : !torch.vtensor<[2048,1280],f16>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15536 = torch.prims.convert_element_type %15532, %int6 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %15537 = torch.aten.mm %15535, %15536 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2048,1280],f32>
    %15538 = torch.aten.mul.Scalar %15537, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15539 = torch.aten.mul.Scalar %15534, %int1 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %15540 = torch.aten.add.Tensor %15538, %15539, %int1 : !torch.vtensor<[2048,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f32>
    %15541 = torch.prims.convert_element_type %15540, %int5 : !torch.vtensor<[2048,1280],f32>, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %15542 = torch.prim.ListConstruct %int2, %int1024, %int1280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15543 = torch.aten.view %15541, %15542 : !torch.vtensor<[2048,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1024,1280],f16>
    %15544 = torch.prim.ListConstruct %int2, %int32, %int32, %int1280 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15545 = torch.aten.view %15543, %15544 : !torch.vtensor<[2,1024,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,32,32,1280],f16>
    %15546 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15547 = torch.aten.permute %15545, %15546 : !torch.vtensor<[2,32,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,32,32],f16>
    %15548 = torch.aten.add.Tensor %15547, %13297, %int1 : !torch.vtensor<[2,1280,32,32],f16>, !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f16>
    %15549 = torch.prims.convert_element_type %15548, %int6 : !torch.vtensor<[2,1280,32,32],f16>, !torch.int -> !torch.vtensor<[2,1280,32,32],f32>
    %cpu_443 = torch.constant.device "cpu"
    %15550 = torch.aten.arange %int64, %int6, %none, %cpu_443, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[64],f32>
    %15551 = torch.aten.add.Scalar %15550, %float0.000000e00, %int1 : !torch.vtensor<[64],f32>, !torch.float, !torch.int -> !torch.vtensor<[64],f32>
    %15552 = torch.aten.mul.Scalar %15551, %float5.000000e-01 : !torch.vtensor<[64],f32>, !torch.float -> !torch.vtensor<[64],f32>
    %15553 = torch.prims.convert_element_type %15552, %int4 : !torch.vtensor<[64],f32>, !torch.int -> !torch.vtensor<[64],si64>
    %15554 = torch.aten.unsqueeze %15553, %int-1 : !torch.vtensor<[64],si64>, !torch.int -> !torch.vtensor<[64,1],si64>
    %cpu_444 = torch.constant.device "cpu"
    %15555 = torch.aten.arange %int64, %int6, %none, %cpu_444, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[64],f32>
    %15556 = torch.aten.add.Scalar %15555, %float0.000000e00, %int1 : !torch.vtensor<[64],f32>, !torch.float, !torch.int -> !torch.vtensor<[64],f32>
    %15557 = torch.aten.mul.Scalar %15556, %float5.000000e-01 : !torch.vtensor<[64],f32>, !torch.float -> !torch.vtensor<[64],f32>
    %15558 = torch.prims.convert_element_type %15557, %int4 : !torch.vtensor<[64],f32>, !torch.int -> !torch.vtensor<[64],si64>
    %15559 = torch.prim.ListConstruct %none, %none, %15554, %15558 : (!torch.none, !torch.none, !torch.vtensor<[64,1],si64>, !torch.vtensor<[64],si64>) -> !torch.list<optional<vtensor>>
    %15560 = torch.aten.index.Tensor %15549, %15559 : !torch.vtensor<[2,1280,32,32],f32>, !torch.list<optional<vtensor>> -> !torch.vtensor<[2,1280,64,64],f32>
    %15561 = torch.prims.convert_element_type %15560, %int5 : !torch.vtensor<[2,1280,64,64],f32>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %_params.unet.up_blocks.0.upsamplers.0.conv.weight = util.global.load @_params.unet.up_blocks.0.upsamplers.0.conv.weight : tensor<1280x1280x3x3xf16>
    %15562 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.upsamplers.0.conv.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %_params.unet.up_blocks.0.upsamplers.0.conv.bias = util.global.load @_params.unet.up_blocks.0.upsamplers.0.conv.bias : tensor<1280xf16>
    %15563 = torch_c.from_builtin_tensor %_params.unet.up_blocks.0.upsamplers.0.conv.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %15564 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15565 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15566 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15567 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15568 = torch.aten.convolution %15561, %15562, %15563, %15564, %15565, %15566, %false, %15567, %int1 : !torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %15569 = torch.prim.ListConstruct %15568, %1336 : (!torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>) -> !torch.list<vtensor>
    %15570 = torch.aten.cat %15569, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,1920,64,64],f16>
    %15571 = torch.prim.ListConstruct %int2, %int32, %int60, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15572 = torch.aten.view %15570, %15571 : !torch.vtensor<[2,1920,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,60,4096],f16>
    %15573 = torch.prims.convert_element_type %15572, %int6 : !torch.vtensor<[2,32,60,4096],f16>, !torch.int -> !torch.vtensor<[2,32,60,4096],f32>
    %15574 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_445, %result1_446 = torch.aten.var_mean.correction %15573, %15574, %int0, %true : !torch.vtensor<[2,32,60,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15575 = torch.aten.add.Scalar %result0_445, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15576 = torch.aten.rsqrt %15575 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15577 = torch.aten.sub.Tensor %15572, %result1_446, %int1 : !torch.vtensor<[2,32,60,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,60,4096],f32>
    %15578 = torch.aten.mul.Tensor %15577, %15576 : !torch.vtensor<[2,32,60,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,60,4096],f32>
    %15579 = torch.prim.ListConstruct %int2, %int1920, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15580 = torch.aten.view %15578, %15579 : !torch.vtensor<[2,32,60,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,1920,64,64],f32>
    %_params.unet.up_blocks.1.resnets.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.norm1.bias : tensor<1920xf16>
    %15581 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm1.bias : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %15582 = torch.aten.unsqueeze %15581, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %15583 = torch.aten.unsqueeze %15582, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %15584 = torch.aten.unsqueeze %15583, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %_params.unet.up_blocks.1.resnets.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.norm1.weight : tensor<1920xf16>
    %15585 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm1.weight : tensor<1920xf16> -> !torch.vtensor<[1920],f16>
    %15586 = torch.aten.unsqueeze %15585, %int0 : !torch.vtensor<[1920],f16>, !torch.int -> !torch.vtensor<[1,1920],f16>
    %15587 = torch.aten.unsqueeze %15586, %int2 : !torch.vtensor<[1,1920],f16>, !torch.int -> !torch.vtensor<[1,1920,1],f16>
    %15588 = torch.aten.unsqueeze %15587, %int3 : !torch.vtensor<[1,1920,1],f16>, !torch.int -> !torch.vtensor<[1,1920,1,1],f16>
    %15589 = torch.aten.mul.Tensor %15580, %15588 : !torch.vtensor<[2,1920,64,64],f32>, !torch.vtensor<[1,1920,1,1],f16> -> !torch.vtensor<[2,1920,64,64],f32>
    %15590 = torch.aten.add.Tensor %15589, %15584, %int1 : !torch.vtensor<[2,1920,64,64],f32>, !torch.vtensor<[1,1920,1,1],f16>, !torch.int -> !torch.vtensor<[2,1920,64,64],f32>
    %15591 = torch.prims.convert_element_type %15590, %int5 : !torch.vtensor<[2,1920,64,64],f32>, !torch.int -> !torch.vtensor<[2,1920,64,64],f16>
    %15592 = torch.prims.convert_element_type %result1_446, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15593 = torch.prims.convert_element_type %15576, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15594 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15595 = torch.prims.squeeze %15592, %15594 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15596 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15597 = torch.prims.squeeze %15595, %15596 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15598 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15599 = torch.prims.squeeze %15593, %15598 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15600 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15601 = torch.prims.squeeze %15599, %15600 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15602 = torch.aten.silu %15591 : !torch.vtensor<[2,1920,64,64],f16> -> !torch.vtensor<[2,1920,64,64],f16>
    %_params.unet.up_blocks.1.resnets.0.conv1.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.conv1.weight : tensor<640x1920x3x3xf16>
    %15603 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv1.weight : tensor<640x1920x3x3xf16> -> !torch.vtensor<[640,1920,3,3],f16>
    %_params.unet.up_blocks.1.resnets.0.conv1.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.conv1.bias : tensor<640xf16>
    %15604 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15605 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15606 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15607 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15608 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15609 = torch.aten.convolution %15602, %15603, %15604, %15605, %15606, %15607, %false, %15608, %int1 : !torch.vtensor<[2,1920,64,64],f16>, !torch.vtensor<[640,1920,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15610 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16>
    %15611 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %15612 = torch.aten.transpose.int %15611, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16>
    %15613 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15614 = torch.prims.convert_element_type %15613, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15615 = torch.prims.convert_element_type %15610, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %15616 = torch.prims.convert_element_type %15612, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %15617 = torch.aten.mm %15615, %15616 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %15618 = torch.aten.mul.Scalar %15617, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %15619 = torch.aten.mul.Scalar %15614, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15620 = torch.aten.add.Tensor %15618, %15619, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %15621 = torch.prims.convert_element_type %15620, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %15622 = torch.aten.unsqueeze %15621, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %15623 = torch.aten.unsqueeze %15622, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %15624 = torch.aten.add.Tensor %15609, %15623, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15625 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15626 = torch.aten.view %15624, %15625 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %15627 = torch.prims.convert_element_type %15626, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15628 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_447, %result1_448 = torch.aten.var_mean.correction %15627, %15628, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15629 = torch.aten.add.Scalar %result0_447, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15630 = torch.aten.rsqrt %15629 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15631 = torch.aten.sub.Tensor %15626, %result1_448, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15632 = torch.aten.mul.Tensor %15631, %15630 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %15633 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15634 = torch.aten.view %15632, %15633 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.resnets.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.norm2.bias : tensor<640xf16>
    %15635 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15636 = torch.aten.unsqueeze %15635, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15637 = torch.aten.unsqueeze %15636, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15638 = torch.aten.unsqueeze %15637, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.resnets.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.norm2.weight : tensor<640xf16>
    %15639 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15640 = torch.aten.unsqueeze %15639, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15641 = torch.aten.unsqueeze %15640, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15642 = torch.aten.unsqueeze %15641, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %15643 = torch.aten.mul.Tensor %15634, %15642 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %15644 = torch.aten.add.Tensor %15643, %15638, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %15645 = torch.prims.convert_element_type %15644, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15646 = torch.prims.convert_element_type %result1_448, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15647 = torch.prims.convert_element_type %15630, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15648 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15649 = torch.prims.squeeze %15646, %15648 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15650 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15651 = torch.prims.squeeze %15649, %15650 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15652 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15653 = torch.prims.squeeze %15647, %15652 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15654 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15655 = torch.prims.squeeze %15653, %15654 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15656 = torch.aten.silu %15645 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.0.conv2.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16>
    %15657 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.resnets.0.conv2.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.conv2.bias : tensor<640xf16>
    %15658 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15659 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15660 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15661 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15662 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15663 = torch.aten.convolution %15656, %15657, %15658, %15659, %15660, %15661, %false, %15662, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x1920x1x1xf16>
    %15664 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x1920x1x1xf16> -> !torch.vtensor<[640,1920,1,1],f16>
    %_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16>
    %15665 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15666 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15667 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15668 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %15669 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %15670 = torch.aten.convolution %15570, %15664, %15665, %15666, %15667, %15668, %false, %15669, %int1 : !torch.vtensor<[2,1920,64,64],f16>, !torch.vtensor<[640,1920,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15671 = torch.aten.add.Tensor %15670, %15663, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15672 = torch.aten.div.Scalar %15671, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %15673 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15674 = torch.aten.view %15672, %15673 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %15675 = torch.prims.convert_element_type %15674, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15676 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_449, %result1_450 = torch.aten.var_mean.correction %15675, %15676, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %15677 = torch.aten.add.Scalar %result0_449, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %15678 = torch.aten.rsqrt %15677 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %15679 = torch.aten.sub.Tensor %15674, %result1_450, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %15680 = torch.aten.mul.Tensor %15679, %15678 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %15681 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15682 = torch.aten.view %15680, %15681 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.attentions.0.norm.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.norm.bias : tensor<640xf16>
    %15683 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15684 = torch.aten.unsqueeze %15683, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15685 = torch.aten.unsqueeze %15684, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15686 = torch.aten.unsqueeze %15685, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.attentions.0.norm.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.norm.weight : tensor<640xf16>
    %15687 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15688 = torch.aten.unsqueeze %15687, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %15689 = torch.aten.unsqueeze %15688, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %15690 = torch.aten.unsqueeze %15689, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %15691 = torch.aten.mul.Tensor %15682, %15690 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %15692 = torch.aten.add.Tensor %15691, %15686, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %15693 = torch.prims.convert_element_type %15692, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %15694 = torch.prims.convert_element_type %result1_450, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15695 = torch.prims.convert_element_type %15678, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %15696 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15697 = torch.prims.squeeze %15694, %15696 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15698 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15699 = torch.prims.squeeze %15697, %15698 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15700 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %15701 = torch.prims.squeeze %15695, %15700 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %15702 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %15703 = torch.prims.squeeze %15701, %15702 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %15704 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15705 = torch.aten.permute %15693, %15704 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %15706 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15707 = torch.aten.view %15705, %15706 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_in.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16>
    %15708 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15709 = torch.aten.transpose.int %15708, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15710 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15711 = torch.aten._unsafe_view %15707, %15710 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15712 = torch.aten.mm %15711, %15709 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15713 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15714 = torch.aten.view %15712, %15713 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_in.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_in.bias : tensor<640xf16>
    %15715 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15716 = torch.aten.add.Tensor %15714, %15715, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15717 = torch.prims.convert_element_type %15716, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15718 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_451, %result1_452 = torch.aten.var_mean.correction %15717, %15718, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15719 = torch.aten.add.Scalar %result0_451, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15720 = torch.aten.rsqrt %15719 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15721 = torch.aten.sub.Tensor %15716, %result1_452, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15722 = torch.aten.mul.Tensor %15721, %15720 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %15723 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15724 = torch.aten.mul.Tensor %15722, %15723 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %15725 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15726 = torch.aten.add.Tensor %15724, %15725, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15727 = torch.prims.convert_element_type %15726, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15728 = torch.prims.convert_element_type %result1_452, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15729 = torch.prims.convert_element_type %15720, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %15730 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15731 = torch.aten.transpose.int %15730, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15732 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15733 = torch.aten.view %15727, %15732 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15734 = torch.aten.mm %15733, %15731 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15735 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15736 = torch.aten.view %15734, %15735 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %15737 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15738 = torch.aten.transpose.int %15737, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15739 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15740 = torch.aten.view %15727, %15739 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15741 = torch.aten.mm %15740, %15738 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15742 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15743 = torch.aten.view %15741, %15742 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %15744 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15745 = torch.aten.transpose.int %15744, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15746 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15747 = torch.aten.view %15727, %15746 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15748 = torch.aten.mm %15747, %15745 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15749 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15750 = torch.aten.view %15748, %15749 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15751 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15752 = torch.aten.view %15736, %15751 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15753 = torch.aten.transpose.int %15752, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15754 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15755 = torch.aten.view %15743, %15754 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15756 = torch.aten.transpose.int %15755, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15757 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15758 = torch.aten.view %15750, %15757 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15759 = torch.aten.transpose.int %15758, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15760:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15753, %15756, %15759, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15761 = torch.aten.transpose.int %15760#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15762 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15763 = torch.aten.view %15761, %15762 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15764 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15765 = torch.aten.view %15763, %15764 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %15766 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15767 = torch.aten.transpose.int %15766, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %15768 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15769 = torch.prims.convert_element_type %15768, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15770 = torch.prims.convert_element_type %15765, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15771 = torch.prims.convert_element_type %15767, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15772 = torch.aten.mm %15770, %15771 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15773 = torch.aten.mul.Scalar %15772, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15774 = torch.aten.mul.Scalar %15769, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15775 = torch.aten.add.Tensor %15773, %15774, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15776 = torch.prims.convert_element_type %15775, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15777 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15778 = torch.aten.view %15776, %15777 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15779 = torch.aten.div.Scalar %15778, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15780 = torch.aten.add.Tensor %15779, %15716, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15781 = torch.prims.convert_element_type %15780, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15782 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_453, %result1_454 = torch.aten.var_mean.correction %15781, %15782, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15783 = torch.aten.add.Scalar %result0_453, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15784 = torch.aten.rsqrt %15783 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15785 = torch.aten.sub.Tensor %15780, %result1_454, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15786 = torch.aten.mul.Tensor %15785, %15784 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %15787 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15788 = torch.aten.mul.Tensor %15786, %15787 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %15789 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15790 = torch.aten.add.Tensor %15788, %15789, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15791 = torch.prims.convert_element_type %15790, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15792 = torch.prims.convert_element_type %result1_454, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15793 = torch.prims.convert_element_type %15784, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %15794 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15795 = torch.aten.transpose.int %15794, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15796 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15797 = torch.aten.view %15791, %15796 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15798 = torch.aten.mm %15797, %15795 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15799 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15800 = torch.aten.view %15798, %15799 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %15801 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15802 = torch.aten.transpose.int %15801, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15803 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15804 = torch.aten.view %4, %15803 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15805 = torch.aten.mm %15804, %15802 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15806 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15807 = torch.aten.view %15805, %15806 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %15808 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15809 = torch.aten.transpose.int %15808, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15810 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15811 = torch.aten.view %4, %15810 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15812 = torch.aten.mm %15811, %15809 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15813 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15814 = torch.aten.view %15812, %15813 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %15815 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15816 = torch.aten.view %15800, %15815 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15817 = torch.aten.transpose.int %15816, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15818 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15819 = torch.aten.view %15807, %15818 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15820 = torch.aten.transpose.int %15819, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15821 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15822 = torch.aten.view %15814, %15821 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15823 = torch.aten.transpose.int %15822, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15824:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15817, %15820, %15823, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15825 = torch.aten.transpose.int %15824#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15826 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15827 = torch.aten.view %15825, %15826 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15828 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15829 = torch.aten.view %15827, %15828 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %15830 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15831 = torch.aten.transpose.int %15830, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %15832 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15833 = torch.prims.convert_element_type %15832, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15834 = torch.prims.convert_element_type %15829, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15835 = torch.prims.convert_element_type %15831, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15836 = torch.aten.mm %15834, %15835 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15837 = torch.aten.mul.Scalar %15836, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15838 = torch.aten.mul.Scalar %15833, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15839 = torch.aten.add.Tensor %15837, %15838, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15840 = torch.prims.convert_element_type %15839, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15841 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15842 = torch.aten.view %15840, %15841 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15843 = torch.aten.div.Scalar %15842, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15844 = torch.aten.add.Tensor %15843, %15780, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15845 = torch.prims.convert_element_type %15844, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15846 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_455, %result1_456 = torch.aten.var_mean.correction %15845, %15846, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15847 = torch.aten.add.Scalar %result0_455, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15848 = torch.aten.rsqrt %15847 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15849 = torch.aten.sub.Tensor %15844, %result1_456, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15850 = torch.aten.mul.Tensor %15849, %15848 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %15851 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15852 = torch.aten.mul.Tensor %15850, %15851 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %15853 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15854 = torch.aten.add.Tensor %15852, %15853, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15855 = torch.prims.convert_element_type %15854, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15856 = torch.prims.convert_element_type %result1_456, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15857 = torch.prims.convert_element_type %15848, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15858 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15859 = torch.aten.view %15855, %15858 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %15860 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %15861 = torch.aten.transpose.int %15860, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %15862 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %15863 = torch.prims.convert_element_type %15862, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %15864 = torch.prims.convert_element_type %15859, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15865 = torch.prims.convert_element_type %15861, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %15866 = torch.aten.mm %15864, %15865 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %15867 = torch.aten.mul.Scalar %15866, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15868 = torch.aten.mul.Scalar %15863, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %15869 = torch.aten.add.Tensor %15867, %15868, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %15870 = torch.prims.convert_element_type %15869, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %15871 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15872 = torch.aten.view %15870, %15871 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %15873 = torch.aten.slice.Tensor %15872, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15874 = torch.aten.slice.Tensor %15872, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %15875 = torch.aten.gelu %15874, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %15876 = torch.aten.mul.Tensor %15873, %15875 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %15877 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %15878 = torch.aten.view %15876, %15877 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %15879 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %15880 = torch.aten.transpose.int %15879, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %15881 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15882 = torch.prims.convert_element_type %15881, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15883 = torch.prims.convert_element_type %15878, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %15884 = torch.prims.convert_element_type %15880, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %15885 = torch.aten.mm %15883, %15884 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15886 = torch.aten.mul.Scalar %15885, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15887 = torch.aten.mul.Scalar %15882, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15888 = torch.aten.add.Tensor %15886, %15887, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15889 = torch.prims.convert_element_type %15888, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15890 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15891 = torch.aten.view %15889, %15890 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15892 = torch.aten.add.Tensor %15891, %15844, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15893 = torch.prims.convert_element_type %15892, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15894 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_457, %result1_458 = torch.aten.var_mean.correction %15893, %15894, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15895 = torch.aten.add.Scalar %result0_457, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15896 = torch.aten.rsqrt %15895 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15897 = torch.aten.sub.Tensor %15892, %result1_458, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15898 = torch.aten.mul.Tensor %15897, %15896 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %15899 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15900 = torch.aten.mul.Tensor %15898, %15899 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %15901 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15902 = torch.aten.add.Tensor %15900, %15901, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15903 = torch.prims.convert_element_type %15902, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15904 = torch.prims.convert_element_type %result1_458, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15905 = torch.prims.convert_element_type %15896, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %15906 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15907 = torch.aten.transpose.int %15906, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15908 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15909 = torch.aten.view %15903, %15908 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15910 = torch.aten.mm %15909, %15907 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15911 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15912 = torch.aten.view %15910, %15911 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %15913 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15914 = torch.aten.transpose.int %15913, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15915 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15916 = torch.aten.view %15903, %15915 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15917 = torch.aten.mm %15916, %15914 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15918 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15919 = torch.aten.view %15917, %15918 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %15920 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15921 = torch.aten.transpose.int %15920, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15922 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15923 = torch.aten.view %15903, %15922 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15924 = torch.aten.mm %15923, %15921 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15925 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15926 = torch.aten.view %15924, %15925 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15927 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15928 = torch.aten.view %15912, %15927 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15929 = torch.aten.transpose.int %15928, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15930 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15931 = torch.aten.view %15919, %15930 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15932 = torch.aten.transpose.int %15931, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15933 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15934 = torch.aten.view %15926, %15933 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15935 = torch.aten.transpose.int %15934, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15936:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15929, %15932, %15935, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %15937 = torch.aten.transpose.int %15936#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %15938 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15939 = torch.aten.view %15937, %15938 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15940 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15941 = torch.aten.view %15939, %15940 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %15942 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15943 = torch.aten.transpose.int %15942, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %15944 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15945 = torch.prims.convert_element_type %15944, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %15946 = torch.prims.convert_element_type %15941, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15947 = torch.prims.convert_element_type %15943, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %15948 = torch.aten.mm %15946, %15947 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %15949 = torch.aten.mul.Scalar %15948, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15950 = torch.aten.mul.Scalar %15945, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %15951 = torch.aten.add.Tensor %15949, %15950, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %15952 = torch.prims.convert_element_type %15951, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %15953 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15954 = torch.aten.view %15952, %15953 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %15955 = torch.aten.div.Scalar %15954, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %15956 = torch.aten.add.Tensor %15955, %15892, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15957 = torch.prims.convert_element_type %15956, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15958 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_459, %result1_460 = torch.aten.var_mean.correction %15957, %15958, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %15959 = torch.aten.add.Scalar %result0_459, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %15960 = torch.aten.rsqrt %15959 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %15961 = torch.aten.sub.Tensor %15956, %result1_460, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15962 = torch.aten.mul.Tensor %15961, %15960 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %15963 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15964 = torch.aten.mul.Tensor %15962, %15963 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %15965 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %15966 = torch.aten.add.Tensor %15964, %15965, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %15967 = torch.prims.convert_element_type %15966, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %15968 = torch.prims.convert_element_type %result1_460, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %15969 = torch.prims.convert_element_type %15960, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %15970 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %15971 = torch.aten.transpose.int %15970, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %15972 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %15973 = torch.aten.view %15967, %15972 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %15974 = torch.aten.mm %15973, %15971 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %15975 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15976 = torch.aten.view %15974, %15975 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %15977 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15978 = torch.aten.transpose.int %15977, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15979 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15980 = torch.aten.view %4, %15979 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15981 = torch.aten.mm %15980, %15978 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15982 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15983 = torch.aten.view %15981, %15982 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %15984 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %15985 = torch.aten.transpose.int %15984, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %15986 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %15987 = torch.aten.view %4, %15986 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %15988 = torch.aten.mm %15987, %15985 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %15989 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15990 = torch.aten.view %15988, %15989 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %15991 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15992 = torch.aten.view %15976, %15991 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %15993 = torch.aten.transpose.int %15992, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %15994 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15995 = torch.aten.view %15983, %15994 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15996 = torch.aten.transpose.int %15995, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %15997 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %15998 = torch.aten.view %15990, %15997 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %15999 = torch.aten.transpose.int %15998, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16000:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%15993, %15996, %15999, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16001 = torch.aten.transpose.int %16000#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16002 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16003 = torch.aten.view %16001, %16002 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16004 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16005 = torch.aten.view %16003, %16004 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %16006 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16007 = torch.aten.transpose.int %16006, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %16008 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16009 = torch.prims.convert_element_type %16008, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16010 = torch.prims.convert_element_type %16005, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16011 = torch.prims.convert_element_type %16007, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16012 = torch.aten.mm %16010, %16011 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16013 = torch.aten.mul.Scalar %16012, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16014 = torch.aten.mul.Scalar %16009, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16015 = torch.aten.add.Tensor %16013, %16014, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16016 = torch.prims.convert_element_type %16015, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16017 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16018 = torch.aten.view %16016, %16017 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16019 = torch.aten.div.Scalar %16018, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16020 = torch.aten.add.Tensor %16019, %15956, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16021 = torch.prims.convert_element_type %16020, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16022 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_461, %result1_462 = torch.aten.var_mean.correction %16021, %16022, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16023 = torch.aten.add.Scalar %result0_461, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16024 = torch.aten.rsqrt %16023 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16025 = torch.aten.sub.Tensor %16020, %result1_462, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16026 = torch.aten.mul.Tensor %16025, %16024 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %16027 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16028 = torch.aten.mul.Tensor %16026, %16027 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %16029 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16030 = torch.aten.add.Tensor %16028, %16029, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16031 = torch.prims.convert_element_type %16030, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16032 = torch.prims.convert_element_type %result1_462, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16033 = torch.prims.convert_element_type %16024, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16034 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16035 = torch.aten.view %16031, %16034 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %16036 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %16037 = torch.aten.transpose.int %16036, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %16038 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %16039 = torch.prims.convert_element_type %16038, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %16040 = torch.prims.convert_element_type %16035, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16041 = torch.prims.convert_element_type %16037, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %16042 = torch.aten.mm %16040, %16041 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %16043 = torch.aten.mul.Scalar %16042, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16044 = torch.aten.mul.Scalar %16039, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %16045 = torch.aten.add.Tensor %16043, %16044, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16046 = torch.prims.convert_element_type %16045, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %16047 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16048 = torch.aten.view %16046, %16047 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %16049 = torch.aten.slice.Tensor %16048, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16050 = torch.aten.slice.Tensor %16048, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16051 = torch.aten.gelu %16050, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %16052 = torch.aten.mul.Tensor %16049, %16051 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %16053 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %16054 = torch.aten.view %16052, %16053 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %16055 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %16056 = torch.aten.transpose.int %16055, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %16057 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16058 = torch.prims.convert_element_type %16057, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16059 = torch.prims.convert_element_type %16054, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %16060 = torch.prims.convert_element_type %16056, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %16061 = torch.aten.mm %16059, %16060 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16062 = torch.aten.mul.Scalar %16061, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16063 = torch.aten.mul.Scalar %16058, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16064 = torch.aten.add.Tensor %16062, %16063, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16065 = torch.prims.convert_element_type %16064, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16066 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16067 = torch.aten.view %16065, %16066 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16068 = torch.aten.add.Tensor %16067, %16020, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16069 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16070 = torch.aten.view %16068, %16069 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_out.weight = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16>
    %16071 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16072 = torch.aten.transpose.int %16071, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.0.proj_out.bias = util.global.load @_params.unet.up_blocks.1.attentions.0.proj_out.bias : tensor<640xf16>
    %16073 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.0.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16074 = torch.prims.convert_element_type %16073, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16075 = torch.prims.convert_element_type %16070, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16076 = torch.prims.convert_element_type %16072, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16077 = torch.aten.mm %16075, %16076 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16078 = torch.aten.mul.Scalar %16077, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16079 = torch.aten.mul.Scalar %16074, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16080 = torch.aten.add.Tensor %16078, %16079, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16081 = torch.prims.convert_element_type %16080, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16082 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16083 = torch.aten.view %16081, %16082 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16084 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16085 = torch.aten.view %16083, %16084 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %16086 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16087 = torch.aten.permute %16085, %16086 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %16088 = torch.aten.add.Tensor %16087, %15672, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16089 = torch.prim.ListConstruct %16088, %825 : (!torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>) -> !torch.list<vtensor>
    %16090 = torch.aten.cat %16089, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %16091 = torch.prim.ListConstruct %int2, %int32, %int40, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16092 = torch.aten.view %16090, %16091 : !torch.vtensor<[2,1280,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,4096],f16>
    %16093 = torch.prims.convert_element_type %16092, %int6 : !torch.vtensor<[2,32,40,4096],f16>, !torch.int -> !torch.vtensor<[2,32,40,4096],f32>
    %16094 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_463, %result1_464 = torch.aten.var_mean.correction %16093, %16094, %int0, %true : !torch.vtensor<[2,32,40,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16095 = torch.aten.add.Scalar %result0_463, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16096 = torch.aten.rsqrt %16095 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16097 = torch.aten.sub.Tensor %16092, %result1_464, %int1 : !torch.vtensor<[2,32,40,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,4096],f32>
    %16098 = torch.aten.mul.Tensor %16097, %16096 : !torch.vtensor<[2,32,40,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,4096],f32>
    %16099 = torch.prim.ListConstruct %int2, %int1280, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16100 = torch.aten.view %16098, %16099 : !torch.vtensor<[2,32,40,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,64,64],f32>
    %_params.unet.up_blocks.1.resnets.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.norm1.bias : tensor<1280xf16>
    %16101 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %16102 = torch.aten.unsqueeze %16101, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %16103 = torch.aten.unsqueeze %16102, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %16104 = torch.aten.unsqueeze %16103, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %_params.unet.up_blocks.1.resnets.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.norm1.weight : tensor<1280xf16>
    %16105 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %16106 = torch.aten.unsqueeze %16105, %int0 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %16107 = torch.aten.unsqueeze %16106, %int2 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %16108 = torch.aten.unsqueeze %16107, %int3 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %16109 = torch.aten.mul.Tensor %16100, %16108 : !torch.vtensor<[2,1280,64,64],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,64,64],f32>
    %16110 = torch.aten.add.Tensor %16109, %16104, %int1 : !torch.vtensor<[2,1280,64,64],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,64,64],f32>
    %16111 = torch.prims.convert_element_type %16110, %int5 : !torch.vtensor<[2,1280,64,64],f32>, !torch.int -> !torch.vtensor<[2,1280,64,64],f16>
    %16112 = torch.prims.convert_element_type %result1_464, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16113 = torch.prims.convert_element_type %16096, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16114 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16115 = torch.prims.squeeze %16112, %16114 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16116 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16117 = torch.prims.squeeze %16115, %16116 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16118 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16119 = torch.prims.squeeze %16113, %16118 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16120 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16121 = torch.prims.squeeze %16119, %16120 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16122 = torch.aten.silu %16111 : !torch.vtensor<[2,1280,64,64],f16> -> !torch.vtensor<[2,1280,64,64],f16>
    %_params.unet.up_blocks.1.resnets.1.conv1.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.conv1.weight : tensor<640x1280x3x3xf16>
    %16123 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv1.weight : tensor<640x1280x3x3xf16> -> !torch.vtensor<[640,1280,3,3],f16>
    %_params.unet.up_blocks.1.resnets.1.conv1.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.conv1.bias : tensor<640xf16>
    %16124 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16125 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16126 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16127 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16128 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16129 = torch.aten.convolution %16122, %16123, %16124, %16125, %16126, %16127, %false, %16128, %int1 : !torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[640,1280,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16130 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16>
    %16131 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %16132 = torch.aten.transpose.int %16131, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16>
    %16133 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16134 = torch.prims.convert_element_type %16133, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16135 = torch.prims.convert_element_type %16130, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %16136 = torch.prims.convert_element_type %16132, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %16137 = torch.aten.mm %16135, %16136 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %16138 = torch.aten.mul.Scalar %16137, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %16139 = torch.aten.mul.Scalar %16134, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16140 = torch.aten.add.Tensor %16138, %16139, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %16141 = torch.prims.convert_element_type %16140, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %16142 = torch.aten.unsqueeze %16141, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %16143 = torch.aten.unsqueeze %16142, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %16144 = torch.aten.add.Tensor %16129, %16143, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16145 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16146 = torch.aten.view %16144, %16145 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %16147 = torch.prims.convert_element_type %16146, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16148 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_465, %result1_466 = torch.aten.var_mean.correction %16147, %16148, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16149 = torch.aten.add.Scalar %result0_465, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16150 = torch.aten.rsqrt %16149 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16151 = torch.aten.sub.Tensor %16146, %result1_466, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16152 = torch.aten.mul.Tensor %16151, %16150 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %16153 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16154 = torch.aten.view %16152, %16153 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.resnets.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.norm2.bias : tensor<640xf16>
    %16155 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16156 = torch.aten.unsqueeze %16155, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16157 = torch.aten.unsqueeze %16156, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16158 = torch.aten.unsqueeze %16157, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.resnets.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.norm2.weight : tensor<640xf16>
    %16159 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16160 = torch.aten.unsqueeze %16159, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16161 = torch.aten.unsqueeze %16160, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16162 = torch.aten.unsqueeze %16161, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %16163 = torch.aten.mul.Tensor %16154, %16162 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %16164 = torch.aten.add.Tensor %16163, %16158, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %16165 = torch.prims.convert_element_type %16164, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16166 = torch.prims.convert_element_type %result1_466, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16167 = torch.prims.convert_element_type %16150, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16168 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16169 = torch.prims.squeeze %16166, %16168 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16170 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16171 = torch.prims.squeeze %16169, %16170 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16172 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16173 = torch.prims.squeeze %16167, %16172 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16174 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16175 = torch.prims.squeeze %16173, %16174 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16176 = torch.aten.silu %16165 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.1.conv2.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16>
    %16177 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.resnets.1.conv2.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.conv2.bias : tensor<640xf16>
    %16178 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16179 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16180 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16181 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16182 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16183 = torch.aten.convolution %16176, %16177, %16178, %16179, %16180, %16181, %false, %16182, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight : tensor<640x1280x1x1xf16>
    %16184 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv_shortcut.weight : tensor<640x1280x1x1xf16> -> !torch.vtensor<[640,1280,1,1],f16>
    %_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias : tensor<640xf16>
    %16185 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.1.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16186 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16187 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16188 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16189 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16190 = torch.aten.convolution %16090, %16184, %16185, %16186, %16187, %16188, %false, %16189, %int1 : !torch.vtensor<[2,1280,64,64],f16>, !torch.vtensor<[640,1280,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16191 = torch.aten.add.Tensor %16190, %16183, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16192 = torch.aten.div.Scalar %16191, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %16193 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16194 = torch.aten.view %16192, %16193 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %16195 = torch.prims.convert_element_type %16194, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16196 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_467, %result1_468 = torch.aten.var_mean.correction %16195, %16196, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16197 = torch.aten.add.Scalar %result0_467, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16198 = torch.aten.rsqrt %16197 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16199 = torch.aten.sub.Tensor %16194, %result1_468, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16200 = torch.aten.mul.Tensor %16199, %16198 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %16201 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16202 = torch.aten.view %16200, %16201 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.attentions.1.norm.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.norm.bias : tensor<640xf16>
    %16203 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16204 = torch.aten.unsqueeze %16203, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16205 = torch.aten.unsqueeze %16204, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16206 = torch.aten.unsqueeze %16205, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.attentions.1.norm.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.norm.weight : tensor<640xf16>
    %16207 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16208 = torch.aten.unsqueeze %16207, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16209 = torch.aten.unsqueeze %16208, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16210 = torch.aten.unsqueeze %16209, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %16211 = torch.aten.mul.Tensor %16202, %16210 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %16212 = torch.aten.add.Tensor %16211, %16206, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %16213 = torch.prims.convert_element_type %16212, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16214 = torch.prims.convert_element_type %result1_468, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16215 = torch.prims.convert_element_type %16198, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16216 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16217 = torch.prims.squeeze %16214, %16216 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16218 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16219 = torch.prims.squeeze %16217, %16218 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16220 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16221 = torch.prims.squeeze %16215, %16220 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16222 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16223 = torch.prims.squeeze %16221, %16222 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16224 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16225 = torch.aten.permute %16213, %16224 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %16226 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16227 = torch.aten.view %16225, %16226 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_in.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16>
    %16228 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16229 = torch.aten.transpose.int %16228, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16230 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16231 = torch.aten._unsafe_view %16227, %16230 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16232 = torch.aten.mm %16231, %16229 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16233 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16234 = torch.aten.view %16232, %16233 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_in.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_in.bias : tensor<640xf16>
    %16235 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16236 = torch.aten.add.Tensor %16234, %16235, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16237 = torch.prims.convert_element_type %16236, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16238 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_469, %result1_470 = torch.aten.var_mean.correction %16237, %16238, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16239 = torch.aten.add.Scalar %result0_469, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16240 = torch.aten.rsqrt %16239 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16241 = torch.aten.sub.Tensor %16236, %result1_470, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16242 = torch.aten.mul.Tensor %16241, %16240 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %16243 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16244 = torch.aten.mul.Tensor %16242, %16243 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %16245 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16246 = torch.aten.add.Tensor %16244, %16245, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16247 = torch.prims.convert_element_type %16246, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16248 = torch.prims.convert_element_type %result1_470, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16249 = torch.prims.convert_element_type %16240, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %16250 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16251 = torch.aten.transpose.int %16250, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16252 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16253 = torch.aten.view %16247, %16252 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16254 = torch.aten.mm %16253, %16251 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16255 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16256 = torch.aten.view %16254, %16255 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %16257 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16258 = torch.aten.transpose.int %16257, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16259 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16260 = torch.aten.view %16247, %16259 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16261 = torch.aten.mm %16260, %16258 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16262 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16263 = torch.aten.view %16261, %16262 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %16264 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16265 = torch.aten.transpose.int %16264, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16266 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16267 = torch.aten.view %16247, %16266 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16268 = torch.aten.mm %16267, %16265 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16269 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16270 = torch.aten.view %16268, %16269 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16271 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16272 = torch.aten.view %16256, %16271 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16273 = torch.aten.transpose.int %16272, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16274 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16275 = torch.aten.view %16263, %16274 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16276 = torch.aten.transpose.int %16275, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16277 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16278 = torch.aten.view %16270, %16277 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16279 = torch.aten.transpose.int %16278, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16280:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16273, %16276, %16279, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16281 = torch.aten.transpose.int %16280#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16282 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16283 = torch.aten.view %16281, %16282 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16284 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16285 = torch.aten.view %16283, %16284 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %16286 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16287 = torch.aten.transpose.int %16286, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %16288 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16289 = torch.prims.convert_element_type %16288, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16290 = torch.prims.convert_element_type %16285, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16291 = torch.prims.convert_element_type %16287, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16292 = torch.aten.mm %16290, %16291 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16293 = torch.aten.mul.Scalar %16292, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16294 = torch.aten.mul.Scalar %16289, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16295 = torch.aten.add.Tensor %16293, %16294, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16296 = torch.prims.convert_element_type %16295, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16297 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16298 = torch.aten.view %16296, %16297 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16299 = torch.aten.div.Scalar %16298, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16300 = torch.aten.add.Tensor %16299, %16236, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16301 = torch.prims.convert_element_type %16300, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16302 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_471, %result1_472 = torch.aten.var_mean.correction %16301, %16302, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16303 = torch.aten.add.Scalar %result0_471, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16304 = torch.aten.rsqrt %16303 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16305 = torch.aten.sub.Tensor %16300, %result1_472, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16306 = torch.aten.mul.Tensor %16305, %16304 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %16307 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16308 = torch.aten.mul.Tensor %16306, %16307 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %16309 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16310 = torch.aten.add.Tensor %16308, %16309, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16311 = torch.prims.convert_element_type %16310, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16312 = torch.prims.convert_element_type %result1_472, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16313 = torch.prims.convert_element_type %16304, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %16314 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16315 = torch.aten.transpose.int %16314, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16316 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16317 = torch.aten.view %16311, %16316 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16318 = torch.aten.mm %16317, %16315 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16319 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16320 = torch.aten.view %16318, %16319 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %16321 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16322 = torch.aten.transpose.int %16321, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16323 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16324 = torch.aten.view %4, %16323 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16325 = torch.aten.mm %16324, %16322 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16326 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16327 = torch.aten.view %16325, %16326 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %16328 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16329 = torch.aten.transpose.int %16328, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16330 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16331 = torch.aten.view %4, %16330 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16332 = torch.aten.mm %16331, %16329 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16333 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16334 = torch.aten.view %16332, %16333 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %16335 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16336 = torch.aten.view %16320, %16335 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16337 = torch.aten.transpose.int %16336, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16338 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16339 = torch.aten.view %16327, %16338 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16340 = torch.aten.transpose.int %16339, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16341 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16342 = torch.aten.view %16334, %16341 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16343 = torch.aten.transpose.int %16342, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16344:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16337, %16340, %16343, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16345 = torch.aten.transpose.int %16344#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16346 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16347 = torch.aten.view %16345, %16346 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16348 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16349 = torch.aten.view %16347, %16348 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %16350 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16351 = torch.aten.transpose.int %16350, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %16352 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16353 = torch.prims.convert_element_type %16352, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16354 = torch.prims.convert_element_type %16349, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16355 = torch.prims.convert_element_type %16351, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16356 = torch.aten.mm %16354, %16355 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16357 = torch.aten.mul.Scalar %16356, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16358 = torch.aten.mul.Scalar %16353, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16359 = torch.aten.add.Tensor %16357, %16358, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16360 = torch.prims.convert_element_type %16359, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16361 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16362 = torch.aten.view %16360, %16361 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16363 = torch.aten.div.Scalar %16362, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16364 = torch.aten.add.Tensor %16363, %16300, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16365 = torch.prims.convert_element_type %16364, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16366 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_473, %result1_474 = torch.aten.var_mean.correction %16365, %16366, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16367 = torch.aten.add.Scalar %result0_473, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16368 = torch.aten.rsqrt %16367 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16369 = torch.aten.sub.Tensor %16364, %result1_474, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16370 = torch.aten.mul.Tensor %16369, %16368 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %16371 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16372 = torch.aten.mul.Tensor %16370, %16371 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %16373 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16374 = torch.aten.add.Tensor %16372, %16373, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16375 = torch.prims.convert_element_type %16374, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16376 = torch.prims.convert_element_type %result1_474, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16377 = torch.prims.convert_element_type %16368, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16378 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16379 = torch.aten.view %16375, %16378 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %16380 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %16381 = torch.aten.transpose.int %16380, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %16382 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %16383 = torch.prims.convert_element_type %16382, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %16384 = torch.prims.convert_element_type %16379, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16385 = torch.prims.convert_element_type %16381, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %16386 = torch.aten.mm %16384, %16385 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %16387 = torch.aten.mul.Scalar %16386, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16388 = torch.aten.mul.Scalar %16383, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %16389 = torch.aten.add.Tensor %16387, %16388, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16390 = torch.prims.convert_element_type %16389, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %16391 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16392 = torch.aten.view %16390, %16391 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %16393 = torch.aten.slice.Tensor %16392, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16394 = torch.aten.slice.Tensor %16392, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16395 = torch.aten.gelu %16394, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %16396 = torch.aten.mul.Tensor %16393, %16395 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %16397 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %16398 = torch.aten.view %16396, %16397 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %16399 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %16400 = torch.aten.transpose.int %16399, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %16401 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16402 = torch.prims.convert_element_type %16401, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16403 = torch.prims.convert_element_type %16398, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %16404 = torch.prims.convert_element_type %16400, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %16405 = torch.aten.mm %16403, %16404 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16406 = torch.aten.mul.Scalar %16405, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16407 = torch.aten.mul.Scalar %16402, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16408 = torch.aten.add.Tensor %16406, %16407, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16409 = torch.prims.convert_element_type %16408, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16410 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16411 = torch.aten.view %16409, %16410 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16412 = torch.aten.add.Tensor %16411, %16364, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16413 = torch.prims.convert_element_type %16412, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16414 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_475, %result1_476 = torch.aten.var_mean.correction %16413, %16414, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16415 = torch.aten.add.Scalar %result0_475, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16416 = torch.aten.rsqrt %16415 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16417 = torch.aten.sub.Tensor %16412, %result1_476, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16418 = torch.aten.mul.Tensor %16417, %16416 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %16419 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16420 = torch.aten.mul.Tensor %16418, %16419 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %16421 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16422 = torch.aten.add.Tensor %16420, %16421, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16423 = torch.prims.convert_element_type %16422, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16424 = torch.prims.convert_element_type %result1_476, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16425 = torch.prims.convert_element_type %16416, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %16426 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16427 = torch.aten.transpose.int %16426, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16428 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16429 = torch.aten.view %16423, %16428 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16430 = torch.aten.mm %16429, %16427 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16431 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16432 = torch.aten.view %16430, %16431 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %16433 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16434 = torch.aten.transpose.int %16433, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16435 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16436 = torch.aten.view %16423, %16435 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16437 = torch.aten.mm %16436, %16434 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16438 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16439 = torch.aten.view %16437, %16438 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %16440 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16441 = torch.aten.transpose.int %16440, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16442 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16443 = torch.aten.view %16423, %16442 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16444 = torch.aten.mm %16443, %16441 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16445 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16446 = torch.aten.view %16444, %16445 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16447 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16448 = torch.aten.view %16432, %16447 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16449 = torch.aten.transpose.int %16448, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16450 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16451 = torch.aten.view %16439, %16450 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16452 = torch.aten.transpose.int %16451, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16453 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16454 = torch.aten.view %16446, %16453 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16455 = torch.aten.transpose.int %16454, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16456:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16449, %16452, %16455, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16457 = torch.aten.transpose.int %16456#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16458 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16459 = torch.aten.view %16457, %16458 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16460 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16461 = torch.aten.view %16459, %16460 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %16462 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16463 = torch.aten.transpose.int %16462, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %16464 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16465 = torch.prims.convert_element_type %16464, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16466 = torch.prims.convert_element_type %16461, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16467 = torch.prims.convert_element_type %16463, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16468 = torch.aten.mm %16466, %16467 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16469 = torch.aten.mul.Scalar %16468, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16470 = torch.aten.mul.Scalar %16465, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16471 = torch.aten.add.Tensor %16469, %16470, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16472 = torch.prims.convert_element_type %16471, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16473 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16474 = torch.aten.view %16472, %16473 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16475 = torch.aten.div.Scalar %16474, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16476 = torch.aten.add.Tensor %16475, %16412, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16477 = torch.prims.convert_element_type %16476, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16478 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_477, %result1_478 = torch.aten.var_mean.correction %16477, %16478, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16479 = torch.aten.add.Scalar %result0_477, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16480 = torch.aten.rsqrt %16479 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16481 = torch.aten.sub.Tensor %16476, %result1_478, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16482 = torch.aten.mul.Tensor %16481, %16480 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %16483 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16484 = torch.aten.mul.Tensor %16482, %16483 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %16485 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16486 = torch.aten.add.Tensor %16484, %16485, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16487 = torch.prims.convert_element_type %16486, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16488 = torch.prims.convert_element_type %result1_478, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16489 = torch.prims.convert_element_type %16480, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %16490 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16491 = torch.aten.transpose.int %16490, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16492 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16493 = torch.aten.view %16487, %16492 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16494 = torch.aten.mm %16493, %16491 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16495 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16496 = torch.aten.view %16494, %16495 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %16497 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16498 = torch.aten.transpose.int %16497, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16499 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16500 = torch.aten.view %4, %16499 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16501 = torch.aten.mm %16500, %16498 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16502 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16503 = torch.aten.view %16501, %16502 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %16504 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16505 = torch.aten.transpose.int %16504, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16506 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16507 = torch.aten.view %4, %16506 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16508 = torch.aten.mm %16507, %16505 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16509 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16510 = torch.aten.view %16508, %16509 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %16511 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16512 = torch.aten.view %16496, %16511 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16513 = torch.aten.transpose.int %16512, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16514 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16515 = torch.aten.view %16503, %16514 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16516 = torch.aten.transpose.int %16515, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16517 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16518 = torch.aten.view %16510, %16517 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16519 = torch.aten.transpose.int %16518, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16520:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16513, %16516, %16519, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16521 = torch.aten.transpose.int %16520#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16522 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16523 = torch.aten.view %16521, %16522 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16524 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16525 = torch.aten.view %16523, %16524 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %16526 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16527 = torch.aten.transpose.int %16526, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %16528 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16529 = torch.prims.convert_element_type %16528, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16530 = torch.prims.convert_element_type %16525, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16531 = torch.prims.convert_element_type %16527, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16532 = torch.aten.mm %16530, %16531 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16533 = torch.aten.mul.Scalar %16532, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16534 = torch.aten.mul.Scalar %16529, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16535 = torch.aten.add.Tensor %16533, %16534, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16536 = torch.prims.convert_element_type %16535, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16537 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16538 = torch.aten.view %16536, %16537 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16539 = torch.aten.div.Scalar %16538, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16540 = torch.aten.add.Tensor %16539, %16476, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16541 = torch.prims.convert_element_type %16540, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16542 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_479, %result1_480 = torch.aten.var_mean.correction %16541, %16542, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16543 = torch.aten.add.Scalar %result0_479, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16544 = torch.aten.rsqrt %16543 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16545 = torch.aten.sub.Tensor %16540, %result1_480, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16546 = torch.aten.mul.Tensor %16545, %16544 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %16547 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16548 = torch.aten.mul.Tensor %16546, %16547 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %16549 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16550 = torch.aten.add.Tensor %16548, %16549, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16551 = torch.prims.convert_element_type %16550, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16552 = torch.prims.convert_element_type %result1_480, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16553 = torch.prims.convert_element_type %16544, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16554 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16555 = torch.aten.view %16551, %16554 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %16556 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %16557 = torch.aten.transpose.int %16556, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %16558 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %16559 = torch.prims.convert_element_type %16558, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %16560 = torch.prims.convert_element_type %16555, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16561 = torch.prims.convert_element_type %16557, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %16562 = torch.aten.mm %16560, %16561 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %16563 = torch.aten.mul.Scalar %16562, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16564 = torch.aten.mul.Scalar %16559, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %16565 = torch.aten.add.Tensor %16563, %16564, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16566 = torch.prims.convert_element_type %16565, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %16567 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16568 = torch.aten.view %16566, %16567 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %16569 = torch.aten.slice.Tensor %16568, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16570 = torch.aten.slice.Tensor %16568, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16571 = torch.aten.gelu %16570, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %16572 = torch.aten.mul.Tensor %16569, %16571 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %16573 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %16574 = torch.aten.view %16572, %16573 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %16575 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %16576 = torch.aten.transpose.int %16575, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %16577 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16578 = torch.prims.convert_element_type %16577, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16579 = torch.prims.convert_element_type %16574, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %16580 = torch.prims.convert_element_type %16576, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %16581 = torch.aten.mm %16579, %16580 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16582 = torch.aten.mul.Scalar %16581, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16583 = torch.aten.mul.Scalar %16578, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16584 = torch.aten.add.Tensor %16582, %16583, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16585 = torch.prims.convert_element_type %16584, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16586 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16587 = torch.aten.view %16585, %16586 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16588 = torch.aten.add.Tensor %16587, %16540, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16589 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16590 = torch.aten.view %16588, %16589 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_out.weight = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16>
    %16591 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16592 = torch.aten.transpose.int %16591, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.1.proj_out.bias = util.global.load @_params.unet.up_blocks.1.attentions.1.proj_out.bias : tensor<640xf16>
    %16593 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.1.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16594 = torch.prims.convert_element_type %16593, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16595 = torch.prims.convert_element_type %16590, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16596 = torch.prims.convert_element_type %16592, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16597 = torch.aten.mm %16595, %16596 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16598 = torch.aten.mul.Scalar %16597, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16599 = torch.aten.mul.Scalar %16594, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16600 = torch.aten.add.Tensor %16598, %16599, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16601 = torch.prims.convert_element_type %16600, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16602 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16603 = torch.aten.view %16601, %16602 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16604 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16605 = torch.aten.view %16603, %16604 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %16606 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16607 = torch.aten.permute %16605, %16606 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %16608 = torch.aten.add.Tensor %16607, %16192, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16609 = torch.prim.ListConstruct %16608, %307 : (!torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,320,64,64],f16>) -> !torch.list<vtensor>
    %16610 = torch.aten.cat %16609, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,960,64,64],f16>
    %16611 = torch.prim.ListConstruct %int2, %int32, %int30, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16612 = torch.aten.view %16610, %16611 : !torch.vtensor<[2,960,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,30,4096],f16>
    %16613 = torch.prims.convert_element_type %16612, %int6 : !torch.vtensor<[2,32,30,4096],f16>, !torch.int -> !torch.vtensor<[2,32,30,4096],f32>
    %16614 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_481, %result1_482 = torch.aten.var_mean.correction %16613, %16614, %int0, %true : !torch.vtensor<[2,32,30,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16615 = torch.aten.add.Scalar %result0_481, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16616 = torch.aten.rsqrt %16615 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16617 = torch.aten.sub.Tensor %16612, %result1_482, %int1 : !torch.vtensor<[2,32,30,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,30,4096],f32>
    %16618 = torch.aten.mul.Tensor %16617, %16616 : !torch.vtensor<[2,32,30,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,30,4096],f32>
    %16619 = torch.prim.ListConstruct %int2, %int960, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16620 = torch.aten.view %16618, %16619 : !torch.vtensor<[2,32,30,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,960,64,64],f32>
    %_params.unet.up_blocks.1.resnets.2.norm1.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.norm1.bias : tensor<960xf16>
    %16621 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm1.bias : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %16622 = torch.aten.unsqueeze %16621, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %16623 = torch.aten.unsqueeze %16622, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %16624 = torch.aten.unsqueeze %16623, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %_params.unet.up_blocks.1.resnets.2.norm1.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.norm1.weight : tensor<960xf16>
    %16625 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm1.weight : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %16626 = torch.aten.unsqueeze %16625, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %16627 = torch.aten.unsqueeze %16626, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %16628 = torch.aten.unsqueeze %16627, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %16629 = torch.aten.mul.Tensor %16620, %16628 : !torch.vtensor<[2,960,64,64],f32>, !torch.vtensor<[1,960,1,1],f16> -> !torch.vtensor<[2,960,64,64],f32>
    %16630 = torch.aten.add.Tensor %16629, %16624, %int1 : !torch.vtensor<[2,960,64,64],f32>, !torch.vtensor<[1,960,1,1],f16>, !torch.int -> !torch.vtensor<[2,960,64,64],f32>
    %16631 = torch.prims.convert_element_type %16630, %int5 : !torch.vtensor<[2,960,64,64],f32>, !torch.int -> !torch.vtensor<[2,960,64,64],f16>
    %16632 = torch.prims.convert_element_type %result1_482, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16633 = torch.prims.convert_element_type %16616, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16634 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16635 = torch.prims.squeeze %16632, %16634 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16636 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16637 = torch.prims.squeeze %16635, %16636 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16638 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16639 = torch.prims.squeeze %16633, %16638 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16640 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16641 = torch.prims.squeeze %16639, %16640 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16642 = torch.aten.silu %16631 : !torch.vtensor<[2,960,64,64],f16> -> !torch.vtensor<[2,960,64,64],f16>
    %_params.unet.up_blocks.1.resnets.2.conv1.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.conv1.weight : tensor<640x960x3x3xf16>
    %16643 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv1.weight : tensor<640x960x3x3xf16> -> !torch.vtensor<[640,960,3,3],f16>
    %_params.unet.up_blocks.1.resnets.2.conv1.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.conv1.bias : tensor<640xf16>
    %16644 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16645 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16646 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16647 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16648 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16649 = torch.aten.convolution %16642, %16643, %16644, %16645, %16646, %16647, %false, %16648, %int1 : !torch.vtensor<[2,960,64,64],f16>, !torch.vtensor<[640,960,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16650 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight : tensor<640x1280xf16>
    %16651 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %16652 = torch.aten.transpose.int %16651, %int0, %int1 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias : tensor<640xf16>
    %16653 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16654 = torch.prims.convert_element_type %16653, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16655 = torch.prims.convert_element_type %16650, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %16656 = torch.prims.convert_element_type %16652, %int6 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %16657 = torch.aten.mm %16655, %16656 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %16658 = torch.aten.mul.Scalar %16657, %int1 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %16659 = torch.aten.mul.Scalar %16654, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16660 = torch.aten.add.Tensor %16658, %16659, %int1 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %16661 = torch.prims.convert_element_type %16660, %int5 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %16662 = torch.aten.unsqueeze %16661, %int2 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %16663 = torch.aten.unsqueeze %16662, %int3 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %16664 = torch.aten.add.Tensor %16649, %16663, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16665 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16666 = torch.aten.view %16664, %16665 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %16667 = torch.prims.convert_element_type %16666, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16668 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_483, %result1_484 = torch.aten.var_mean.correction %16667, %16668, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16669 = torch.aten.add.Scalar %result0_483, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16670 = torch.aten.rsqrt %16669 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16671 = torch.aten.sub.Tensor %16666, %result1_484, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16672 = torch.aten.mul.Tensor %16671, %16670 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %16673 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16674 = torch.aten.view %16672, %16673 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.resnets.2.norm2.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.norm2.bias : tensor<640xf16>
    %16675 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16676 = torch.aten.unsqueeze %16675, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16677 = torch.aten.unsqueeze %16676, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16678 = torch.aten.unsqueeze %16677, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.resnets.2.norm2.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.norm2.weight : tensor<640xf16>
    %16679 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16680 = torch.aten.unsqueeze %16679, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16681 = torch.aten.unsqueeze %16680, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16682 = torch.aten.unsqueeze %16681, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %16683 = torch.aten.mul.Tensor %16674, %16682 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %16684 = torch.aten.add.Tensor %16683, %16678, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %16685 = torch.prims.convert_element_type %16684, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16686 = torch.prims.convert_element_type %result1_484, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16687 = torch.prims.convert_element_type %16670, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16688 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16689 = torch.prims.squeeze %16686, %16688 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16690 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16691 = torch.prims.squeeze %16689, %16690 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16692 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16693 = torch.prims.squeeze %16687, %16692 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16694 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16695 = torch.prims.squeeze %16693, %16694 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16696 = torch.aten.silu %16685 : !torch.vtensor<[2,640,64,64],f16> -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.2.conv2.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.conv2.weight : tensor<640x640x3x3xf16>
    %16697 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.resnets.2.conv2.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.conv2.bias : tensor<640xf16>
    %16698 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16699 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16700 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16701 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16702 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16703 = torch.aten.convolution %16696, %16697, %16698, %16699, %16700, %16701, %false, %16702, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight : tensor<640x960x1x1xf16>
    %16704 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv_shortcut.weight : tensor<640x960x1x1xf16> -> !torch.vtensor<[640,960,1,1],f16>
    %_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias : tensor<640xf16>
    %16705 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.resnets.2.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16706 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16707 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16708 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %16709 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %16710 = torch.aten.convolution %16610, %16704, %16705, %16706, %16707, %16708, %false, %16709, %int1 : !torch.vtensor<[2,960,64,64],f16>, !torch.vtensor<[640,960,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16711 = torch.aten.add.Tensor %16710, %16703, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16712 = torch.aten.div.Scalar %16711, %float1.000000e00 : !torch.vtensor<[2,640,64,64],f16>, !torch.float -> !torch.vtensor<[2,640,64,64],f16>
    %16713 = torch.prim.ListConstruct %int2, %int32, %int20, %int4096 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16714 = torch.aten.view %16712, %16713 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,4096],f16>
    %16715 = torch.prims.convert_element_type %16714, %int6 : !torch.vtensor<[2,32,20,4096],f16>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16716 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_485, %result1_486 = torch.aten.var_mean.correction %16715, %16716, %int0, %true : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %16717 = torch.aten.add.Scalar %result0_485, %float9.999990e-07, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %16718 = torch.aten.rsqrt %16717 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %16719 = torch.aten.sub.Tensor %16714, %result1_486, %int1 : !torch.vtensor<[2,32,20,4096],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,4096],f32>
    %16720 = torch.aten.mul.Tensor %16719, %16718 : !torch.vtensor<[2,32,20,4096],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,4096],f32>
    %16721 = torch.prim.ListConstruct %int2, %int640, %int64, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16722 = torch.aten.view %16720, %16721 : !torch.vtensor<[2,32,20,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f32>
    %_params.unet.up_blocks.1.attentions.2.norm.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.norm.bias : tensor<640xf16>
    %16723 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16724 = torch.aten.unsqueeze %16723, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16725 = torch.aten.unsqueeze %16724, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16726 = torch.aten.unsqueeze %16725, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.1.attentions.2.norm.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.norm.weight : tensor<640xf16>
    %16727 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16728 = torch.aten.unsqueeze %16727, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %16729 = torch.aten.unsqueeze %16728, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %16730 = torch.aten.unsqueeze %16729, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %16731 = torch.aten.mul.Tensor %16722, %16730 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,64,64],f32>
    %16732 = torch.aten.add.Tensor %16731, %16726, %int1 : !torch.vtensor<[2,640,64,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %16733 = torch.prims.convert_element_type %16732, %int5 : !torch.vtensor<[2,640,64,64],f32>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %16734 = torch.prims.convert_element_type %result1_486, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16735 = torch.prims.convert_element_type %16718, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %16736 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16737 = torch.prims.squeeze %16734, %16736 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16738 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16739 = torch.prims.squeeze %16737, %16738 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16740 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %16741 = torch.prims.squeeze %16735, %16740 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %16742 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %16743 = torch.prims.squeeze %16741, %16742 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %16744 = torch.prim.ListConstruct %int0, %int2, %int3, %int1 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16745 = torch.aten.permute %16733, %16744 : !torch.vtensor<[2,640,64,64],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %16746 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16747 = torch.aten.view %16745, %16746 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_in.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_in.weight : tensor<640x640xf16>
    %16748 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16749 = torch.aten.transpose.int %16748, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16750 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16751 = torch.aten._unsafe_view %16747, %16750 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16752 = torch.aten.mm %16751, %16749 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16753 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16754 = torch.aten.view %16752, %16753 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_in.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_in.bias : tensor<640xf16>
    %16755 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16756 = torch.aten.add.Tensor %16754, %16755, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16757 = torch.prims.convert_element_type %16756, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16758 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_487, %result1_488 = torch.aten.var_mean.correction %16757, %16758, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16759 = torch.aten.add.Scalar %result0_487, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16760 = torch.aten.rsqrt %16759 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16761 = torch.aten.sub.Tensor %16756, %result1_488, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16762 = torch.aten.mul.Tensor %16761, %16760 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %16763 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16764 = torch.aten.mul.Tensor %16762, %16763 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %16765 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16766 = torch.aten.add.Tensor %16764, %16765, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16767 = torch.prims.convert_element_type %16766, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16768 = torch.prims.convert_element_type %result1_488, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16769 = torch.prims.convert_element_type %16760, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %16770 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16771 = torch.aten.transpose.int %16770, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16772 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16773 = torch.aten.view %16767, %16772 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16774 = torch.aten.mm %16773, %16771 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16775 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16776 = torch.aten.view %16774, %16775 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %16777 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16778 = torch.aten.transpose.int %16777, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16779 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16780 = torch.aten.view %16767, %16779 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16781 = torch.aten.mm %16780, %16778 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16782 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16783 = torch.aten.view %16781, %16782 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %16784 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16785 = torch.aten.transpose.int %16784, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16786 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16787 = torch.aten.view %16767, %16786 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16788 = torch.aten.mm %16787, %16785 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16789 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16790 = torch.aten.view %16788, %16789 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16791 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16792 = torch.aten.view %16776, %16791 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16793 = torch.aten.transpose.int %16792, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16794 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16795 = torch.aten.view %16783, %16794 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16796 = torch.aten.transpose.int %16795, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16797 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16798 = torch.aten.view %16790, %16797 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16799 = torch.aten.transpose.int %16798, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16800:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16793, %16796, %16799, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16801 = torch.aten.transpose.int %16800#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16802 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16803 = torch.aten.view %16801, %16802 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16804 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16805 = torch.aten.view %16803, %16804 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %16806 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16807 = torch.aten.transpose.int %16806, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %16808 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16809 = torch.prims.convert_element_type %16808, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16810 = torch.prims.convert_element_type %16805, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16811 = torch.prims.convert_element_type %16807, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16812 = torch.aten.mm %16810, %16811 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16813 = torch.aten.mul.Scalar %16812, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16814 = torch.aten.mul.Scalar %16809, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16815 = torch.aten.add.Tensor %16813, %16814, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16816 = torch.prims.convert_element_type %16815, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16817 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16818 = torch.aten.view %16816, %16817 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16819 = torch.aten.div.Scalar %16818, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16820 = torch.aten.add.Tensor %16819, %16756, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16821 = torch.prims.convert_element_type %16820, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16822 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_489, %result1_490 = torch.aten.var_mean.correction %16821, %16822, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16823 = torch.aten.add.Scalar %result0_489, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16824 = torch.aten.rsqrt %16823 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16825 = torch.aten.sub.Tensor %16820, %result1_490, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16826 = torch.aten.mul.Tensor %16825, %16824 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %16827 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16828 = torch.aten.mul.Tensor %16826, %16827 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %16829 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16830 = torch.aten.add.Tensor %16828, %16829, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16831 = torch.prims.convert_element_type %16830, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16832 = torch.prims.convert_element_type %result1_490, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16833 = torch.prims.convert_element_type %16824, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %16834 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16835 = torch.aten.transpose.int %16834, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16836 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16837 = torch.aten.view %16831, %16836 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16838 = torch.aten.mm %16837, %16835 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16839 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16840 = torch.aten.view %16838, %16839 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %16841 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16842 = torch.aten.transpose.int %16841, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16843 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16844 = torch.aten.view %4, %16843 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16845 = torch.aten.mm %16844, %16842 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16846 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16847 = torch.aten.view %16845, %16846 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %16848 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %16849 = torch.aten.transpose.int %16848, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %16850 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %16851 = torch.aten.view %4, %16850 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %16852 = torch.aten.mm %16851, %16849 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %16853 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16854 = torch.aten.view %16852, %16853 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %16855 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16856 = torch.aten.view %16840, %16855 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16857 = torch.aten.transpose.int %16856, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16858 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16859 = torch.aten.view %16847, %16858 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16860 = torch.aten.transpose.int %16859, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16861 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16862 = torch.aten.view %16854, %16861 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %16863 = torch.aten.transpose.int %16862, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %16864:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16857, %16860, %16863, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16865 = torch.aten.transpose.int %16864#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16866 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16867 = torch.aten.view %16865, %16866 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16868 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16869 = torch.aten.view %16867, %16868 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %16870 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16871 = torch.aten.transpose.int %16870, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %16872 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16873 = torch.prims.convert_element_type %16872, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16874 = torch.prims.convert_element_type %16869, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16875 = torch.prims.convert_element_type %16871, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16876 = torch.aten.mm %16874, %16875 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16877 = torch.aten.mul.Scalar %16876, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16878 = torch.aten.mul.Scalar %16873, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16879 = torch.aten.add.Tensor %16877, %16878, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16880 = torch.prims.convert_element_type %16879, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16881 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16882 = torch.aten.view %16880, %16881 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16883 = torch.aten.div.Scalar %16882, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16884 = torch.aten.add.Tensor %16883, %16820, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16885 = torch.prims.convert_element_type %16884, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16886 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_491, %result1_492 = torch.aten.var_mean.correction %16885, %16886, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16887 = torch.aten.add.Scalar %result0_491, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16888 = torch.aten.rsqrt %16887 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16889 = torch.aten.sub.Tensor %16884, %result1_492, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16890 = torch.aten.mul.Tensor %16889, %16888 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %16891 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16892 = torch.aten.mul.Tensor %16890, %16891 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %16893 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16894 = torch.aten.add.Tensor %16892, %16893, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16895 = torch.prims.convert_element_type %16894, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16896 = torch.prims.convert_element_type %result1_492, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16897 = torch.prims.convert_element_type %16888, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16898 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16899 = torch.aten.view %16895, %16898 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %16900 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %16901 = torch.aten.transpose.int %16900, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %16902 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %16903 = torch.prims.convert_element_type %16902, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %16904 = torch.prims.convert_element_type %16899, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16905 = torch.prims.convert_element_type %16901, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %16906 = torch.aten.mm %16904, %16905 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %16907 = torch.aten.mul.Scalar %16906, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16908 = torch.aten.mul.Scalar %16903, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %16909 = torch.aten.add.Tensor %16907, %16908, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %16910 = torch.prims.convert_element_type %16909, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %16911 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16912 = torch.aten.view %16910, %16911 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %16913 = torch.aten.slice.Tensor %16912, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16914 = torch.aten.slice.Tensor %16912, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %16915 = torch.aten.gelu %16914, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %16916 = torch.aten.mul.Tensor %16913, %16915 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %16917 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %16918 = torch.aten.view %16916, %16917 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %16919 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %16920 = torch.aten.transpose.int %16919, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %16921 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16922 = torch.prims.convert_element_type %16921, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16923 = torch.prims.convert_element_type %16918, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %16924 = torch.prims.convert_element_type %16920, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %16925 = torch.aten.mm %16923, %16924 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16926 = torch.aten.mul.Scalar %16925, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16927 = torch.aten.mul.Scalar %16922, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16928 = torch.aten.add.Tensor %16926, %16927, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16929 = torch.prims.convert_element_type %16928, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16930 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16931 = torch.aten.view %16929, %16930 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16932 = torch.aten.add.Tensor %16931, %16884, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16933 = torch.prims.convert_element_type %16932, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16934 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_493, %result1_494 = torch.aten.var_mean.correction %16933, %16934, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16935 = torch.aten.add.Scalar %result0_493, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %16936 = torch.aten.rsqrt %16935 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %16937 = torch.aten.sub.Tensor %16932, %result1_494, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16938 = torch.aten.mul.Tensor %16937, %16936 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %16939 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16940 = torch.aten.mul.Tensor %16938, %16939 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %16941 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16942 = torch.aten.add.Tensor %16940, %16941, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16943 = torch.prims.convert_element_type %16942, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16944 = torch.prims.convert_element_type %result1_494, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %16945 = torch.prims.convert_element_type %16936, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %16946 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16947 = torch.aten.transpose.int %16946, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16948 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16949 = torch.aten.view %16943, %16948 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16950 = torch.aten.mm %16949, %16947 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16951 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16952 = torch.aten.view %16950, %16951 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %16953 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16954 = torch.aten.transpose.int %16953, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16955 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16956 = torch.aten.view %16943, %16955 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16957 = torch.aten.mm %16956, %16954 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16958 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16959 = torch.aten.view %16957, %16958 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %16960 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16961 = torch.aten.transpose.int %16960, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %16962 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16963 = torch.aten.view %16943, %16962 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %16964 = torch.aten.mm %16963, %16961 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %16965 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16966 = torch.aten.view %16964, %16965 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16967 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16968 = torch.aten.view %16952, %16967 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16969 = torch.aten.transpose.int %16968, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16970 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16971 = torch.aten.view %16959, %16970 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16972 = torch.aten.transpose.int %16971, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16973 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16974 = torch.aten.view %16966, %16973 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %16975 = torch.aten.transpose.int %16974, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %16976:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%16969, %16972, %16975, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %16977 = torch.aten.transpose.int %16976#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %16978 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16979 = torch.aten.view %16977, %16978 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16980 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %16981 = torch.aten.view %16979, %16980 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %16982 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %16983 = torch.aten.transpose.int %16982, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %16984 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %16985 = torch.prims.convert_element_type %16984, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %16986 = torch.prims.convert_element_type %16981, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16987 = torch.prims.convert_element_type %16983, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %16988 = torch.aten.mm %16986, %16987 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %16989 = torch.aten.mul.Scalar %16988, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16990 = torch.aten.mul.Scalar %16985, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %16991 = torch.aten.add.Tensor %16989, %16990, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %16992 = torch.prims.convert_element_type %16991, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %16993 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %16994 = torch.aten.view %16992, %16993 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %16995 = torch.aten.div.Scalar %16994, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %16996 = torch.aten.add.Tensor %16995, %16932, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %16997 = torch.prims.convert_element_type %16996, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %16998 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_495, %result1_496 = torch.aten.var_mean.correction %16997, %16998, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %16999 = torch.aten.add.Scalar %result0_495, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %17000 = torch.aten.rsqrt %16999 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %17001 = torch.aten.sub.Tensor %16996, %result1_496, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %17002 = torch.aten.mul.Tensor %17001, %17000 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %17003 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17004 = torch.aten.mul.Tensor %17002, %17003 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %17005 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17006 = torch.aten.add.Tensor %17004, %17005, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %17007 = torch.prims.convert_element_type %17006, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %17008 = torch.prims.convert_element_type %result1_496, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %17009 = torch.prims.convert_element_type %17000, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %17010 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %17011 = torch.aten.transpose.int %17010, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %17012 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %17013 = torch.aten.view %17007, %17012 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %17014 = torch.aten.mm %17013, %17011 : !torch.vtensor<[8192,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[8192,640],f16>
    %17015 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17016 = torch.aten.view %17014, %17015 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %17017 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %17018 = torch.aten.transpose.int %17017, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %17019 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %17020 = torch.aten.view %4, %17019 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %17021 = torch.aten.mm %17020, %17018 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %17022 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17023 = torch.aten.view %17021, %17022 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %17024 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %17025 = torch.aten.transpose.int %17024, %int0, %int1 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %17026 = torch.prim.ListConstruct %int128, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %17027 = torch.aten.view %4, %17026 : !torch.vtensor<[2,64,2048],f16>, !torch.list<int> -> !torch.vtensor<[128,2048],f16>
    %17028 = torch.aten.mm %17027, %17025 : !torch.vtensor<[128,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[128,640],f16>
    %17029 = torch.prim.ListConstruct %int2, %int64, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17030 = torch.aten.view %17028, %17029 : !torch.vtensor<[128,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,640],f16>
    %17031 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17032 = torch.aten.view %17016, %17031 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,10,64],f16>
    %17033 = torch.aten.transpose.int %17032, %int1, %int2 : !torch.vtensor<[2,4096,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,4096,64],f16>
    %17034 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17035 = torch.aten.view %17023, %17034 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %17036 = torch.aten.transpose.int %17035, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %17037 = torch.prim.ListConstruct %int2, %int-1, %int10, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17038 = torch.aten.view %17030, %17037 : !torch.vtensor<[2,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,10,64],f16>
    %17039 = torch.aten.transpose.int %17038, %int1, %int2 : !torch.vtensor<[2,64,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,64,64],f16>
    %17040:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%17033, %17036, %17039, %float0.000000e00, %false, %none, %none) : (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.vtensor<[2,10,64,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,4096,64],f16>, !torch.vtensor<[2,10,4096],f32>) 
    %17041 = torch.aten.transpose.int %17040#0, %int1, %int2 : !torch.vtensor<[2,10,4096,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,4096,10,64],f16>
    %17042 = torch.prim.ListConstruct %int2, %int-1, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17043 = torch.aten.view %17041, %17042 : !torch.vtensor<[2,4096,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %17044 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %17045 = torch.aten.view %17043, %17044 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %17046 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %17047 = torch.aten.transpose.int %17046, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %17048 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17049 = torch.prims.convert_element_type %17048, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %17050 = torch.prims.convert_element_type %17045, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17051 = torch.prims.convert_element_type %17047, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %17052 = torch.aten.mm %17050, %17051 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %17053 = torch.aten.mul.Scalar %17052, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17054 = torch.aten.mul.Scalar %17049, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %17055 = torch.aten.add.Tensor %17053, %17054, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17056 = torch.prims.convert_element_type %17055, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %17057 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17058 = torch.aten.view %17056, %17057 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %17059 = torch.aten.div.Scalar %17058, %float1.000000e00 : !torch.vtensor<[2,4096,640],f16>, !torch.float -> !torch.vtensor<[2,4096,640],f16>
    %17060 = torch.aten.add.Tensor %17059, %16996, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %17061 = torch.prims.convert_element_type %17060, %int6 : !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %17062 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %result0_497, %result1_498 = torch.aten.var_mean.correction %17061, %17062, %int0, %true : !torch.vtensor<[2,4096,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,4096,1],f32>, !torch.vtensor<[2,4096,1],f32>
    %17063 = torch.aten.add.Scalar %result0_497, %float1.000000e-05, %int1 : !torch.vtensor<[2,4096,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,4096,1],f32>
    %17064 = torch.aten.rsqrt %17063 : !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,1],f32>
    %17065 = torch.aten.sub.Tensor %17060, %result1_498, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %17066 = torch.aten.mul.Tensor %17065, %17064 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[2,4096,1],f32> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %17067 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17068 = torch.aten.mul.Tensor %17066, %17067 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,4096,640],f32>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %17069 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17070 = torch.aten.add.Tensor %17068, %17069, %int1 : !torch.vtensor<[2,4096,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f32>
    %17071 = torch.prims.convert_element_type %17070, %int5 : !torch.vtensor<[2,4096,640],f32>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %17072 = torch.prims.convert_element_type %result1_498, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %17073 = torch.prims.convert_element_type %17064, %int5 : !torch.vtensor<[2,4096,1],f32>, !torch.int -> !torch.vtensor<[2,4096,1],f16>
    %17074 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %17075 = torch.aten.view %17071, %17074 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %17076 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %17077 = torch.aten.transpose.int %17076, %int0, %int1 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %17078 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %17079 = torch.prims.convert_element_type %17078, %int6 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %17080 = torch.prims.convert_element_type %17075, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17081 = torch.prims.convert_element_type %17077, %int6 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %17082 = torch.aten.mm %17080, %17081 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[8192,5120],f32>
    %17083 = torch.aten.mul.Scalar %17082, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %17084 = torch.aten.mul.Scalar %17079, %int1 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %17085 = torch.aten.add.Tensor %17083, %17084, %int1 : !torch.vtensor<[8192,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f32>
    %17086 = torch.prims.convert_element_type %17085, %int5 : !torch.vtensor<[8192,5120],f32>, !torch.int -> !torch.vtensor<[8192,5120],f16>
    %17087 = torch.prim.ListConstruct %int2, %int4096, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17088 = torch.aten.view %17086, %17087 : !torch.vtensor<[8192,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,5120],f16>
    %17089 = torch.aten.slice.Tensor %17088, %int-1, %int0, %int2560, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %17090 = torch.aten.slice.Tensor %17088, %int-1, %int2560, %int5120, %int1 : !torch.vtensor<[2,4096,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,4096,2560],f16>
    %17091 = torch.aten.gelu %17090, %str : !torch.vtensor<[2,4096,2560],f16>, !torch.str -> !torch.vtensor<[2,4096,2560],f16>
    %17092 = torch.aten.mul.Tensor %17089, %17091 : !torch.vtensor<[2,4096,2560],f16>, !torch.vtensor<[2,4096,2560],f16> -> !torch.vtensor<[2,4096,2560],f16>
    %17093 = torch.prim.ListConstruct %int8192, %int2560 : (!torch.int, !torch.int) -> !torch.list<int>
    %17094 = torch.aten.view %17092, %17093 : !torch.vtensor<[2,4096,2560],f16>, !torch.list<int> -> !torch.vtensor<[8192,2560],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %17095 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %17096 = torch.aten.transpose.int %17095, %int0, %int1 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %17097 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17098 = torch.prims.convert_element_type %17097, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %17099 = torch.prims.convert_element_type %17094, %int6 : !torch.vtensor<[8192,2560],f16>, !torch.int -> !torch.vtensor<[8192,2560],f32>
    %17100 = torch.prims.convert_element_type %17096, %int6 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %17101 = torch.aten.mm %17099, %17100 : !torch.vtensor<[8192,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[8192,640],f32>
    %17102 = torch.aten.mul.Scalar %17101, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17103 = torch.aten.mul.Scalar %17098, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %17104 = torch.aten.add.Tensor %17102, %17103, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17105 = torch.prims.convert_element_type %17104, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %17106 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17107 = torch.aten.view %17105, %17106 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %17108 = torch.aten.add.Tensor %17107, %17060, %int1 : !torch.vtensor<[2,4096,640],f16>, !torch.vtensor<[2,4096,640],f16>, !torch.int -> !torch.vtensor<[2,4096,640],f16>
    %17109 = torch.prim.ListConstruct %int8192, %int640 : (!torch.int, !torch.int) -> !torch.list<int>
    %17110 = torch.aten.view %17108, %17109 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[8192,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_out.weight = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_out.weight : tensor<640x640xf16>
    %17111 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %17112 = torch.aten.transpose.int %17111, %int0, %int1 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %_params.unet.up_blocks.1.attentions.2.proj_out.bias = util.global.load @_params.unet.up_blocks.1.attentions.2.proj_out.bias : tensor<640xf16>
    %17113 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.attentions.2.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17114 = torch.prims.convert_element_type %17113, %int6 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %17115 = torch.prims.convert_element_type %17110, %int6 : !torch.vtensor<[8192,640],f16>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17116 = torch.prims.convert_element_type %17112, %int6 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %17117 = torch.aten.mm %17115, %17116 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[8192,640],f32>
    %17118 = torch.aten.mul.Scalar %17117, %int1 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17119 = torch.aten.mul.Scalar %17114, %int1 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %17120 = torch.aten.add.Tensor %17118, %17119, %int1 : !torch.vtensor<[8192,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[8192,640],f32>
    %17121 = torch.prims.convert_element_type %17120, %int5 : !torch.vtensor<[8192,640],f32>, !torch.int -> !torch.vtensor<[8192,640],f16>
    %17122 = torch.prim.ListConstruct %int2, %int4096, %int640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17123 = torch.aten.view %17121, %17122 : !torch.vtensor<[8192,640],f16>, !torch.list<int> -> !torch.vtensor<[2,4096,640],f16>
    %17124 = torch.prim.ListConstruct %int2, %int64, %int64, %int640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17125 = torch.aten.view %17123, %17124 : !torch.vtensor<[2,4096,640],f16>, !torch.list<int> -> !torch.vtensor<[2,64,64,640],f16>
    %17126 = torch.prim.ListConstruct %int0, %int3, %int1, %int2 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17127 = torch.aten.permute %17125, %17126 : !torch.vtensor<[2,64,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,64,64],f16>
    %17128 = torch.aten.add.Tensor %17127, %16712, %int1 : !torch.vtensor<[2,640,64,64],f16>, !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f16>
    %17129 = torch.prims.convert_element_type %17128, %int6 : !torch.vtensor<[2,640,64,64],f16>, !torch.int -> !torch.vtensor<[2,640,64,64],f32>
    %cpu_499 = torch.constant.device "cpu"
    %17130 = torch.aten.arange %int128, %int6, %none, %cpu_499, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],f32>
    %17131 = torch.aten.add.Scalar %17130, %float0.000000e00, %int1 : !torch.vtensor<[128],f32>, !torch.float, !torch.int -> !torch.vtensor<[128],f32>
    %17132 = torch.aten.mul.Scalar %17131, %float5.000000e-01 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %17133 = torch.prims.convert_element_type %17132, %int4 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],si64>
    %17134 = torch.aten.unsqueeze %17133, %int-1 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128,1],si64>
    %cpu_500 = torch.constant.device "cpu"
    %17135 = torch.aten.arange %int128, %int6, %none, %cpu_500, %false : !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],f32>
    %17136 = torch.aten.add.Scalar %17135, %float0.000000e00, %int1 : !torch.vtensor<[128],f32>, !torch.float, !torch.int -> !torch.vtensor<[128],f32>
    %17137 = torch.aten.mul.Scalar %17136, %float5.000000e-01 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %17138 = torch.prims.convert_element_type %17137, %int4 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],si64>
    %17139 = torch.prim.ListConstruct %none, %none, %17134, %17138 : (!torch.none, !torch.none, !torch.vtensor<[128,1],si64>, !torch.vtensor<[128],si64>) -> !torch.list<optional<vtensor>>
    %17140 = torch.aten.index.Tensor %17129, %17139 : !torch.vtensor<[2,640,64,64],f32>, !torch.list<optional<vtensor>> -> !torch.vtensor<[2,640,128,128],f32>
    %17141 = torch.prims.convert_element_type %17140, %int5 : !torch.vtensor<[2,640,128,128],f32>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %_params.unet.up_blocks.1.upsamplers.0.conv.weight = util.global.load @_params.unet.up_blocks.1.upsamplers.0.conv.weight : tensor<640x640x3x3xf16>
    %17142 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.upsamplers.0.conv.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %_params.unet.up_blocks.1.upsamplers.0.conv.bias = util.global.load @_params.unet.up_blocks.1.upsamplers.0.conv.bias : tensor<640xf16>
    %17143 = torch_c.from_builtin_tensor %_params.unet.up_blocks.1.upsamplers.0.conv.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17144 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17145 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17146 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17147 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17148 = torch.aten.convolution %17141, %17142, %17143, %17144, %17145, %17146, %false, %17147, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %17149 = torch.prim.ListConstruct %17148, %300 : (!torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>) -> !torch.list<vtensor>
    %17150 = torch.aten.cat %17149, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,960,128,128],f16>
    %17151 = torch.prim.ListConstruct %int2, %int32, %int30, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17152 = torch.aten.view %17150, %17151 : !torch.vtensor<[2,960,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,30,16384],f16>
    %17153 = torch.prims.convert_element_type %17152, %int6 : !torch.vtensor<[2,32,30,16384],f16>, !torch.int -> !torch.vtensor<[2,32,30,16384],f32>
    %17154 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_501, %result1_502 = torch.aten.var_mean.correction %17153, %17154, %int0, %true : !torch.vtensor<[2,32,30,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %17155 = torch.aten.add.Scalar %result0_501, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %17156 = torch.aten.rsqrt %17155 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %17157 = torch.aten.sub.Tensor %17152, %result1_502, %int1 : !torch.vtensor<[2,32,30,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,30,16384],f32>
    %17158 = torch.aten.mul.Tensor %17157, %17156 : !torch.vtensor<[2,32,30,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,30,16384],f32>
    %17159 = torch.prim.ListConstruct %int2, %int960, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17160 = torch.aten.view %17158, %17159 : !torch.vtensor<[2,32,30,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,960,128,128],f32>
    %_params.unet.up_blocks.2.resnets.0.norm1.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.norm1.bias : tensor<960xf16>
    %17161 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm1.bias : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %17162 = torch.aten.unsqueeze %17161, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %17163 = torch.aten.unsqueeze %17162, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %17164 = torch.aten.unsqueeze %17163, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %_params.unet.up_blocks.2.resnets.0.norm1.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.norm1.weight : tensor<960xf16>
    %17165 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm1.weight : tensor<960xf16> -> !torch.vtensor<[960],f16>
    %17166 = torch.aten.unsqueeze %17165, %int0 : !torch.vtensor<[960],f16>, !torch.int -> !torch.vtensor<[1,960],f16>
    %17167 = torch.aten.unsqueeze %17166, %int2 : !torch.vtensor<[1,960],f16>, !torch.int -> !torch.vtensor<[1,960,1],f16>
    %17168 = torch.aten.unsqueeze %17167, %int3 : !torch.vtensor<[1,960,1],f16>, !torch.int -> !torch.vtensor<[1,960,1,1],f16>
    %17169 = torch.aten.mul.Tensor %17160, %17168 : !torch.vtensor<[2,960,128,128],f32>, !torch.vtensor<[1,960,1,1],f16> -> !torch.vtensor<[2,960,128,128],f32>
    %17170 = torch.aten.add.Tensor %17169, %17164, %int1 : !torch.vtensor<[2,960,128,128],f32>, !torch.vtensor<[1,960,1,1],f16>, !torch.int -> !torch.vtensor<[2,960,128,128],f32>
    %17171 = torch.prims.convert_element_type %17170, %int5 : !torch.vtensor<[2,960,128,128],f32>, !torch.int -> !torch.vtensor<[2,960,128,128],f16>
    %17172 = torch.prims.convert_element_type %result1_502, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17173 = torch.prims.convert_element_type %17156, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17174 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17175 = torch.prims.squeeze %17172, %17174 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17176 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17177 = torch.prims.squeeze %17175, %17176 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17178 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17179 = torch.prims.squeeze %17173, %17178 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17180 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17181 = torch.prims.squeeze %17179, %17180 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17182 = torch.aten.silu %17171 : !torch.vtensor<[2,960,128,128],f16> -> !torch.vtensor<[2,960,128,128],f16>
    %_params.unet.up_blocks.2.resnets.0.conv1.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.conv1.weight : tensor<320x960x3x3xf16>
    %17183 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv1.weight : tensor<320x960x3x3xf16> -> !torch.vtensor<[320,960,3,3],f16>
    %_params.unet.up_blocks.2.resnets.0.conv1.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.conv1.bias : tensor<320xf16>
    %17184 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17185 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17186 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17187 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17188 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17189 = torch.aten.convolution %17182, %17183, %17184, %17185, %17186, %17187, %false, %17188, %int1 : !torch.vtensor<[2,960,128,128],f16>, !torch.vtensor<[320,960,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17190 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight : tensor<320x1280xf16>
    %17191 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %17192 = torch.aten.transpose.int %17191, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias : tensor<320xf16>
    %17193 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17194 = torch.prims.convert_element_type %17193, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %17195 = torch.prims.convert_element_type %17190, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %17196 = torch.prims.convert_element_type %17192, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %17197 = torch.aten.mm %17195, %17196 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %17198 = torch.aten.mul.Scalar %17197, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %17199 = torch.aten.mul.Scalar %17194, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %17200 = torch.aten.add.Tensor %17198, %17199, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %17201 = torch.prims.convert_element_type %17200, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %17202 = torch.aten.unsqueeze %17201, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %17203 = torch.aten.unsqueeze %17202, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %17204 = torch.aten.add.Tensor %17189, %17203, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17205 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17206 = torch.aten.view %17204, %17205 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %17207 = torch.prims.convert_element_type %17206, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17208 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_503, %result1_504 = torch.aten.var_mean.correction %17207, %17208, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %17209 = torch.aten.add.Scalar %result0_503, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %17210 = torch.aten.rsqrt %17209 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %17211 = torch.aten.sub.Tensor %17206, %result1_504, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17212 = torch.aten.mul.Tensor %17211, %17210 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %17213 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17214 = torch.aten.view %17212, %17213 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.up_blocks.2.resnets.0.norm2.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.norm2.bias : tensor<320xf16>
    %17215 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17216 = torch.aten.unsqueeze %17215, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17217 = torch.aten.unsqueeze %17216, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17218 = torch.aten.unsqueeze %17217, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.up_blocks.2.resnets.0.norm2.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.norm2.weight : tensor<320xf16>
    %17219 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17220 = torch.aten.unsqueeze %17219, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17221 = torch.aten.unsqueeze %17220, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17222 = torch.aten.unsqueeze %17221, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %17223 = torch.aten.mul.Tensor %17214, %17222 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %17224 = torch.aten.add.Tensor %17223, %17218, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %17225 = torch.prims.convert_element_type %17224, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17226 = torch.prims.convert_element_type %result1_504, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17227 = torch.prims.convert_element_type %17210, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17228 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17229 = torch.prims.squeeze %17226, %17228 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17230 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17231 = torch.prims.squeeze %17229, %17230 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17232 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17233 = torch.prims.squeeze %17227, %17232 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17234 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17235 = torch.prims.squeeze %17233, %17234 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17236 = torch.aten.silu %17225 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.0.conv2.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.conv2.weight : tensor<320x320x3x3xf16>
    %17237 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.up_blocks.2.resnets.0.conv2.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.conv2.bias : tensor<320xf16>
    %17238 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17239 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17240 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17241 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17242 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17243 = torch.aten.convolution %17236, %17237, %17238, %17239, %17240, %17241, %false, %17242, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight : tensor<320x960x1x1xf16>
    %17244 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv_shortcut.weight : tensor<320x960x1x1xf16> -> !torch.vtensor<[320,960,1,1],f16>
    %_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias : tensor<320xf16>
    %17245 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.0.conv_shortcut.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17246 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17247 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17248 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17249 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17250 = torch.aten.convolution %17150, %17244, %17245, %17246, %17247, %17248, %false, %17249, %int1 : !torch.vtensor<[2,960,128,128],f16>, !torch.vtensor<[320,960,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17251 = torch.aten.add.Tensor %17250, %17243, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17252 = torch.aten.div.Scalar %17251, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %17253 = torch.prim.ListConstruct %17252, %205 : (!torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>) -> !torch.list<vtensor>
    %17254 = torch.aten.cat %17253, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %17255 = torch.prim.ListConstruct %int2, %int32, %int20, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17256 = torch.aten.view %17254, %17255 : !torch.vtensor<[2,640,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,16384],f16>
    %17257 = torch.prims.convert_element_type %17256, %int6 : !torch.vtensor<[2,32,20,16384],f16>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %17258 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_505, %result1_506 = torch.aten.var_mean.correction %17257, %17258, %int0, %true : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %17259 = torch.aten.add.Scalar %result0_505, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %17260 = torch.aten.rsqrt %17259 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %17261 = torch.aten.sub.Tensor %17256, %result1_506, %int1 : !torch.vtensor<[2,32,20,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %17262 = torch.aten.mul.Tensor %17261, %17260 : !torch.vtensor<[2,32,20,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,16384],f32>
    %17263 = torch.prim.ListConstruct %int2, %int640, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17264 = torch.aten.view %17262, %17263 : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,640,128,128],f32>
    %_params.unet.up_blocks.2.resnets.1.norm1.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.norm1.bias : tensor<640xf16>
    %17265 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17266 = torch.aten.unsqueeze %17265, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %17267 = torch.aten.unsqueeze %17266, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %17268 = torch.aten.unsqueeze %17267, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.1.norm1.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.norm1.weight : tensor<640xf16>
    %17269 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17270 = torch.aten.unsqueeze %17269, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %17271 = torch.aten.unsqueeze %17270, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %17272 = torch.aten.unsqueeze %17271, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %17273 = torch.aten.mul.Tensor %17264, %17272 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,128,128],f32>
    %17274 = torch.aten.add.Tensor %17273, %17268, %int1 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,128,128],f32>
    %17275 = torch.prims.convert_element_type %17274, %int5 : !torch.vtensor<[2,640,128,128],f32>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %17276 = torch.prims.convert_element_type %result1_506, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17277 = torch.prims.convert_element_type %17260, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17278 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17279 = torch.prims.squeeze %17276, %17278 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17280 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17281 = torch.prims.squeeze %17279, %17280 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17282 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17283 = torch.prims.squeeze %17277, %17282 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17284 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17285 = torch.prims.squeeze %17283, %17284 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17286 = torch.aten.silu %17275 : !torch.vtensor<[2,640,128,128],f16> -> !torch.vtensor<[2,640,128,128],f16>
    %_params.unet.up_blocks.2.resnets.1.conv1.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.conv1.weight : tensor<320x640x3x3xf16>
    %17287 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv1.weight : tensor<320x640x3x3xf16> -> !torch.vtensor<[320,640,3,3],f16>
    %_params.unet.up_blocks.2.resnets.1.conv1.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.conv1.bias : tensor<320xf16>
    %17288 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17289 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17290 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17291 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17292 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17293 = torch.aten.convolution %17286, %17287, %17288, %17289, %17290, %17291, %false, %17292, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17294 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight : tensor<320x1280xf16>
    %17295 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %17296 = torch.aten.transpose.int %17295, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias : tensor<320xf16>
    %17297 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17298 = torch.prims.convert_element_type %17297, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %17299 = torch.prims.convert_element_type %17294, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %17300 = torch.prims.convert_element_type %17296, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %17301 = torch.aten.mm %17299, %17300 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %17302 = torch.aten.mul.Scalar %17301, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %17303 = torch.aten.mul.Scalar %17298, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %17304 = torch.aten.add.Tensor %17302, %17303, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %17305 = torch.prims.convert_element_type %17304, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %17306 = torch.aten.unsqueeze %17305, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %17307 = torch.aten.unsqueeze %17306, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %17308 = torch.aten.add.Tensor %17293, %17307, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17309 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17310 = torch.aten.view %17308, %17309 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %17311 = torch.prims.convert_element_type %17310, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17312 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_507, %result1_508 = torch.aten.var_mean.correction %17311, %17312, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %17313 = torch.aten.add.Scalar %result0_507, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %17314 = torch.aten.rsqrt %17313 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %17315 = torch.aten.sub.Tensor %17310, %result1_508, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17316 = torch.aten.mul.Tensor %17315, %17314 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %17317 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17318 = torch.aten.view %17316, %17317 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.up_blocks.2.resnets.1.norm2.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.norm2.bias : tensor<320xf16>
    %17319 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17320 = torch.aten.unsqueeze %17319, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17321 = torch.aten.unsqueeze %17320, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17322 = torch.aten.unsqueeze %17321, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.up_blocks.2.resnets.1.norm2.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.norm2.weight : tensor<320xf16>
    %17323 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17324 = torch.aten.unsqueeze %17323, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17325 = torch.aten.unsqueeze %17324, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17326 = torch.aten.unsqueeze %17325, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %17327 = torch.aten.mul.Tensor %17318, %17326 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %17328 = torch.aten.add.Tensor %17327, %17322, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %17329 = torch.prims.convert_element_type %17328, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17330 = torch.prims.convert_element_type %result1_508, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17331 = torch.prims.convert_element_type %17314, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17332 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17333 = torch.prims.squeeze %17330, %17332 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17334 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17335 = torch.prims.squeeze %17333, %17334 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17336 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17337 = torch.prims.squeeze %17331, %17336 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17338 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17339 = torch.prims.squeeze %17337, %17338 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17340 = torch.aten.silu %17329 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.1.conv2.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.conv2.weight : tensor<320x320x3x3xf16>
    %17341 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.up_blocks.2.resnets.1.conv2.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.conv2.bias : tensor<320xf16>
    %17342 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17343 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17344 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17345 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17346 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17347 = torch.aten.convolution %17340, %17341, %17342, %17343, %17344, %17345, %false, %17346, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight : tensor<320x640x1x1xf16>
    %17348 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv_shortcut.weight : tensor<320x640x1x1xf16> -> !torch.vtensor<[320,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias : tensor<320xf16>
    %17349 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.1.conv_shortcut.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17350 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17351 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17352 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17353 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17354 = torch.aten.convolution %17254, %17348, %17349, %17350, %17351, %17352, %false, %17353, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17355 = torch.aten.add.Tensor %17354, %17347, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17356 = torch.aten.div.Scalar %17355, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %17357 = torch.prim.ListConstruct %17356, %110 : (!torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>) -> !torch.list<vtensor>
    %17358 = torch.aten.cat %17357, %int1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %17359 = torch.prim.ListConstruct %int2, %int32, %int20, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17360 = torch.aten.view %17358, %17359 : !torch.vtensor<[2,640,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,16384],f16>
    %17361 = torch.prims.convert_element_type %17360, %int6 : !torch.vtensor<[2,32,20,16384],f16>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %17362 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_509, %result1_510 = torch.aten.var_mean.correction %17361, %17362, %int0, %true : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %17363 = torch.aten.add.Scalar %result0_509, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %17364 = torch.aten.rsqrt %17363 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %17365 = torch.aten.sub.Tensor %17360, %result1_510, %int1 : !torch.vtensor<[2,32,20,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,16384],f32>
    %17366 = torch.aten.mul.Tensor %17365, %17364 : !torch.vtensor<[2,32,20,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,16384],f32>
    %17367 = torch.prim.ListConstruct %int2, %int640, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17368 = torch.aten.view %17366, %17367 : !torch.vtensor<[2,32,20,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,640,128,128],f32>
    %_params.unet.up_blocks.2.resnets.2.norm1.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.norm1.bias : tensor<640xf16>
    %17369 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17370 = torch.aten.unsqueeze %17369, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %17371 = torch.aten.unsqueeze %17370, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %17372 = torch.aten.unsqueeze %17371, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.2.norm1.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.norm1.weight : tensor<640xf16>
    %17373 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %17374 = torch.aten.unsqueeze %17373, %int0 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %17375 = torch.aten.unsqueeze %17374, %int2 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %17376 = torch.aten.unsqueeze %17375, %int3 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %17377 = torch.aten.mul.Tensor %17368, %17376 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,128,128],f32>
    %17378 = torch.aten.add.Tensor %17377, %17372, %int1 : !torch.vtensor<[2,640,128,128],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,128,128],f32>
    %17379 = torch.prims.convert_element_type %17378, %int5 : !torch.vtensor<[2,640,128,128],f32>, !torch.int -> !torch.vtensor<[2,640,128,128],f16>
    %17380 = torch.prims.convert_element_type %result1_510, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17381 = torch.prims.convert_element_type %17364, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17382 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17383 = torch.prims.squeeze %17380, %17382 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17384 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17385 = torch.prims.squeeze %17383, %17384 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17386 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17387 = torch.prims.squeeze %17381, %17386 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17388 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17389 = torch.prims.squeeze %17387, %17388 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17390 = torch.aten.silu %17379 : !torch.vtensor<[2,640,128,128],f16> -> !torch.vtensor<[2,640,128,128],f16>
    %_params.unet.up_blocks.2.resnets.2.conv1.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.conv1.weight : tensor<320x640x3x3xf16>
    %17391 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv1.weight : tensor<320x640x3x3xf16> -> !torch.vtensor<[320,640,3,3],f16>
    %_params.unet.up_blocks.2.resnets.2.conv1.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.conv1.bias : tensor<320xf16>
    %17392 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17393 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17394 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17395 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17396 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17397 = torch.aten.convolution %17390, %17391, %17392, %17393, %17394, %17395, %false, %17396, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17398 = torch.aten.silu %103 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight : tensor<320x1280xf16>
    %17399 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %17400 = torch.aten.transpose.int %17399, %int0, %int1 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias : tensor<320xf16>
    %17401 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17402 = torch.prims.convert_element_type %17401, %int6 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %17403 = torch.prims.convert_element_type %17398, %int6 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %17404 = torch.prims.convert_element_type %17400, %int6 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %17405 = torch.aten.mm %17403, %17404 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %17406 = torch.aten.mul.Scalar %17405, %int1 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %17407 = torch.aten.mul.Scalar %17402, %int1 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %17408 = torch.aten.add.Tensor %17406, %17407, %int1 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %17409 = torch.prims.convert_element_type %17408, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %17410 = torch.aten.unsqueeze %17409, %int2 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %17411 = torch.aten.unsqueeze %17410, %int3 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %17412 = torch.aten.add.Tensor %17397, %17411, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17413 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17414 = torch.aten.view %17412, %17413 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %17415 = torch.prims.convert_element_type %17414, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17416 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_511, %result1_512 = torch.aten.var_mean.correction %17415, %17416, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %17417 = torch.aten.add.Scalar %result0_511, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %17418 = torch.aten.rsqrt %17417 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %17419 = torch.aten.sub.Tensor %17414, %result1_512, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17420 = torch.aten.mul.Tensor %17419, %17418 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %17421 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17422 = torch.aten.view %17420, %17421 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.up_blocks.2.resnets.2.norm2.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.norm2.bias : tensor<320xf16>
    %17423 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17424 = torch.aten.unsqueeze %17423, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17425 = torch.aten.unsqueeze %17424, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17426 = torch.aten.unsqueeze %17425, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.up_blocks.2.resnets.2.norm2.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.norm2.weight : tensor<320xf16>
    %17427 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17428 = torch.aten.unsqueeze %17427, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17429 = torch.aten.unsqueeze %17428, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17430 = torch.aten.unsqueeze %17429, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %17431 = torch.aten.mul.Tensor %17422, %17430 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %17432 = torch.aten.add.Tensor %17431, %17426, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %17433 = torch.prims.convert_element_type %17432, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17434 = torch.prims.convert_element_type %result1_512, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17435 = torch.prims.convert_element_type %17418, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17436 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17437 = torch.prims.squeeze %17434, %17436 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17438 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17439 = torch.prims.squeeze %17437, %17438 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17440 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17441 = torch.prims.squeeze %17435, %17440 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17442 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17443 = torch.prims.squeeze %17441, %17442 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17444 = torch.aten.silu %17433 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.2.conv2.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.conv2.weight : tensor<320x320x3x3xf16>
    %17445 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %_params.unet.up_blocks.2.resnets.2.conv2.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.conv2.bias : tensor<320xf16>
    %17446 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17447 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17448 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17449 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17450 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17451 = torch.aten.convolution %17444, %17445, %17446, %17447, %17448, %17449, %false, %17450, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight = util.global.load @_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight : tensor<320x640x1x1xf16>
    %17452 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv_shortcut.weight : tensor<320x640x1x1xf16> -> !torch.vtensor<[320,640,1,1],f16>
    %_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias = util.global.load @_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias : tensor<320xf16>
    %17453 = torch_c.from_builtin_tensor %_params.unet.up_blocks.2.resnets.2.conv_shortcut.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17454 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17455 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17456 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17457 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17458 = torch.aten.convolution %17358, %17452, %17453, %17454, %17455, %17456, %false, %17457, %int1 : !torch.vtensor<[2,640,128,128],f16>, !torch.vtensor<[320,640,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17459 = torch.aten.add.Tensor %17458, %17451, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[2,320,128,128],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17460 = torch.aten.div.Scalar %17459, %float1.000000e00 : !torch.vtensor<[2,320,128,128],f16>, !torch.float -> !torch.vtensor<[2,320,128,128],f16>
    %17461 = torch.prim.ListConstruct %int2, %int32, %int10, %int16384 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17462 = torch.aten.view %17460, %17461 : !torch.vtensor<[2,320,128,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,16384],f16>
    %17463 = torch.prims.convert_element_type %17462, %int6 : !torch.vtensor<[2,32,10,16384],f16>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17464 = torch.prim.ListConstruct %int2, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %result0_513, %result1_514 = torch.aten.var_mean.correction %17463, %17464, %int0, %true : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %17465 = torch.aten.add.Scalar %result0_513, %float1.000000e-05, %int1 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %17466 = torch.aten.rsqrt %17465 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %17467 = torch.aten.sub.Tensor %17462, %result1_514, %int1 : !torch.vtensor<[2,32,10,16384],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,16384],f32>
    %17468 = torch.aten.mul.Tensor %17467, %17466 : !torch.vtensor<[2,32,10,16384],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,16384],f32>
    %17469 = torch.prim.ListConstruct %int2, %int320, %int128, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17470 = torch.aten.view %17468, %17469 : !torch.vtensor<[2,32,10,16384],f32>, !torch.list<int> -> !torch.vtensor<[2,320,128,128],f32>
    %_params.unet.conv_norm_out.bias = util.global.load @_params.unet.conv_norm_out.bias : tensor<320xf16>
    %17471 = torch_c.from_builtin_tensor %_params.unet.conv_norm_out.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17472 = torch.aten.unsqueeze %17471, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17473 = torch.aten.unsqueeze %17472, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17474 = torch.aten.unsqueeze %17473, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %_params.unet.conv_norm_out.weight = util.global.load @_params.unet.conv_norm_out.weight : tensor<320xf16>
    %17475 = torch_c.from_builtin_tensor %_params.unet.conv_norm_out.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %17476 = torch.aten.unsqueeze %17475, %int0 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %17477 = torch.aten.unsqueeze %17476, %int2 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %17478 = torch.aten.unsqueeze %17477, %int3 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %17479 = torch.aten.mul.Tensor %17470, %17478 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,128,128],f32>
    %17480 = torch.aten.add.Tensor %17479, %17474, %int1 : !torch.vtensor<[2,320,128,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,128,128],f32>
    %17481 = torch.prims.convert_element_type %17480, %int5 : !torch.vtensor<[2,320,128,128],f32>, !torch.int -> !torch.vtensor<[2,320,128,128],f16>
    %17482 = torch.prims.convert_element_type %result1_514, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17483 = torch.prims.convert_element_type %17466, %int5 : !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,1,1],f16>
    %17484 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17485 = torch.prims.squeeze %17482, %17484 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17486 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17487 = torch.prims.squeeze %17485, %17486 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17488 = torch.prim.ListConstruct %int3 : (!torch.int) -> !torch.list<int>
    %17489 = torch.prims.squeeze %17483, %17488 : !torch.vtensor<[2,32,1,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32,1],f16>
    %17490 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %17491 = torch.prims.squeeze %17489, %17490 : !torch.vtensor<[2,32,1],f16>, !torch.list<int> -> !torch.vtensor<[2,32],f16>
    %17492 = torch.aten.silu %17481 : !torch.vtensor<[2,320,128,128],f16> -> !torch.vtensor<[2,320,128,128],f16>
    %_params.unet.conv_out.weight = util.global.load @_params.unet.conv_out.weight : tensor<4x320x3x3xf16>
    %17493 = torch_c.from_builtin_tensor %_params.unet.conv_out.weight : tensor<4x320x3x3xf16> -> !torch.vtensor<[4,320,3,3],f16>
    %_params.unet.conv_out.bias = util.global.load @_params.unet.conv_out.bias : tensor<4xf16>
    %17494 = torch_c.from_builtin_tensor %_params.unet.conv_out.bias : tensor<4xf16> -> !torch.vtensor<[4],f16>
    %17495 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17496 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17497 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %17498 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %17499 = torch.aten.convolution %17492, %17493, %17494, %17495, %17496, %17497, %false, %17498, %int1 : !torch.vtensor<[2,320,128,128],f16>, !torch.vtensor<[4,320,3,3],f16>, !torch.vtensor<[4],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,4,128,128],f16>
    %17500 = torch.aten.slice.Tensor %17499, %int0, %int0, %int1, %int1 : !torch.vtensor<[2,4,128,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %17501 = torch.aten.slice.Tensor %17499, %int0, %int1, %int2, %int1 : !torch.vtensor<[2,4,128,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %17502 = torch.aten.sub.Tensor %17501, %17500, %int1 : !torch.vtensor<[1,4,128,128],f16>, !torch.vtensor<[1,4,128,128],f16>, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %17503 = torch.aten.mul.Tensor %7, %17502 : !torch.vtensor<[1],f16>, !torch.vtensor<[1,4,128,128],f16> -> !torch.vtensor<[1,4,128,128],f16>
    %17504 = torch.aten.add.Tensor %17500, %17503, %int1 : !torch.vtensor<[1,4,128,128],f16>, !torch.vtensor<[1,4,128,128],f16>, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %17505 = torch.aten.sub.Scalar %10, %int34, %int1 : !torch.vtensor<[1],si64>, !torch.int, !torch.int -> !torch.vtensor<[1],si64>
    %17506 = torch.aten.index_select %1, %int0, %10 : !torch.vtensor<[1000],f32>, !torch.int, !torch.vtensor<[1],si64> -> !torch.vtensor<[1],f32>
    %17507 = torch.aten.ge.Scalar %17505, %int0 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1],i1>
    %17508 = torch.aten.add.Scalar %17505, %int1000, %int1 : !torch.vtensor<[1],si64>, !torch.int, !torch.int -> !torch.vtensor<[1],si64>
    %17509 = torch.aten.where.self %17507, %17505, %17508 : !torch.vtensor<[1],i1>, !torch.vtensor<[1],si64>, !torch.vtensor<[1],si64> -> !torch.vtensor<[1],si64>
    %17510 = torch.aten.ge.Scalar %17505, %int0 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1],i1>
    %17511 = torch.aten.index_select %1, %int0, %17509 : !torch.vtensor<[1000],f32>, !torch.int, !torch.vtensor<[1],si64> -> !torch.vtensor<[1],f32>
    %17512 = torch.aten.where.self %17510, %17511, %2 : !torch.vtensor<[1],i1>, !torch.vtensor<[1],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1],f32>
    %17513 = torch.aten.rsub.Scalar %17506, %int1, %int1 : !torch.vtensor<[1],f32>, !torch.int, !torch.int -> !torch.vtensor<[1],f32>
    %17514 = torch.aten.rsub.Scalar %17512, %int1, %int1 : !torch.vtensor<[1],f32>, !torch.int, !torch.int -> !torch.vtensor<[1],f32>
    %17515 = torch.aten.div.Tensor %17512, %17506 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %17516 = torch.aten.pow.Tensor_Scalar %17515, %float5.000000e-01 : !torch.vtensor<[1],f32>, !torch.float -> !torch.vtensor<[1],f32>
    %17517 = torch.aten.pow.Tensor_Scalar %17514, %float5.000000e-01 : !torch.vtensor<[1],f32>, !torch.float -> !torch.vtensor<[1],f32>
    %17518 = torch.aten.mul.Tensor %17506, %17517 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %17519 = torch.aten.mul.Tensor %17506, %17513 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %17520 = torch.aten.mul.Tensor %17519, %17512 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1],f32>
    %17521 = torch.aten.pow.Tensor_Scalar %17520, %float5.000000e-01 : !torch.vtensor<[1],f32>, !torch.float -> !torch.vtensor<[1],f32>
    %17522 = torch.aten.add.Tensor %17518, %17521, %int1 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32>, !torch.int -> !torch.vtensor<[1],f32>
    %17523 = torch.aten.mul.Tensor %17516, %3 : !torch.vtensor<[1],f32>, !torch.vtensor<[1,4,128,128],f16> -> !torch.vtensor<[1,4,128,128],f32>
    %17524 = torch.aten.sub.Tensor %17512, %17506, %int1 : !torch.vtensor<[1],f32>, !torch.vtensor<[1],f32>, !torch.int -> !torch.vtensor<[1],f32>
    %17525 = torch.aten.mul.Tensor %17524, %17504 : !torch.vtensor<[1],f32>, !torch.vtensor<[1,4,128,128],f16> -> !torch.vtensor<[1,4,128,128],f32>
    %17526 = torch.aten.div.Tensor %17525, %17522 : !torch.vtensor<[1,4,128,128],f32>, !torch.vtensor<[1],f32> -> !torch.vtensor<[1,4,128,128],f32>
    %17527 = torch.aten.sub.Tensor %17523, %17526, %int1 : !torch.vtensor<[1,4,128,128],f32>, !torch.vtensor<[1,4,128,128],f32>, !torch.int -> !torch.vtensor<[1,4,128,128],f32>
    %17528 = torch.prims.convert_element_type %17527, %int5 : !torch.vtensor<[1,4,128,128],f32>, !torch.int -> !torch.vtensor<[1,4,128,128],f16>
    %17529 = torch_c.to_builtin_tensor %17528 : !torch.vtensor<[1,4,128,128],f16> -> tensor<1x4x128x128xf16>
    return %17529 : tensor<1x4x128x128xf16>
  }
  hal.executable.source private @hip_matmul_exe_2048x1280_1280x1280 attributes {objects = #hal.executable.objects<{#executable_target_rocm_hsaco_fb = [#hal.executable.object<{path = "../tensile/output_2048x1280x1280/Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128LGssD3D5uUpJpwMOz9bV1KUetBSFPuVbIJbtPQNALVk=.co"}>]}>} {
    hal.executable.export public @Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT128x80x64_MI16x16x1_SN_LDSB1_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB4_IU1_K1_LBSPPA256_LBSPPB128_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT2_5_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS16_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW2_TLDS1_USFGROn1_VWA2_VWB1_WSGRA0_WSGRB0_WG64_4_1 ordinal(0) layout(#pipeline_layout) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>, #hal.interface.binding<0, 3>], rocm.parameter_mapping = "c4:0:0,c4:4:4,c4:8:8,c4:12:12,c4:16:16,c4:20:20,b8:0:0:24,b8:0:1:32,b8:0:2:40,b8:0:3:48,c4:24:56,c4:28:60,c4:32:64,c4:36:68,c4:40:72,c4:44:76,c4:48:80,c4:52:84,c4:56:88,c4:60:92,c4:64:96,c4:68:100,c4:72:104,c4:76:108,c4:80:112,c4:84:116,c4:88:120,c4:92:124,c4:96:128", workgroup_size = [256 : index, 1 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device, %arg1: index):
      %c16 = arith.constant 16 : index
      %c1 = arith.constant 1 : index
      hal.return %c16, %c16, %c1 : index, index, index
    }
  }
  hal.executable.source private @hip_matmul_exe_128x2048_1280x2048 attributes {objects = #hal.executable.objects<{#executable_target_rocm_hsaco_fb = [#hal.executable.object<{path = "../tensile/output_128x1280x2048/Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32xWtnU2SRJ074qkQ6v4wfrhiWlW1wonyPBt_w30NplSC4=.co"}>]}>} {
    hal.executable.export public @Cijk_Alik_Bljk_HSS_BH_Bias_AS_SAV_UserArgs_MT32x32x128_MI16x16x1_SN_LDSB0_AFC1_AFEM1_AFEM1_ASEM1_CLR1_EPS0_GRVWA8_GRVWB8_IU1_K1_LBSPPA256_LBSPPB256_LBSPPM0_LPA16_LPB16_LPM0_LRVW8_LWPMn1_MIAV0_MIWT1_1_MO40_NTn1_NTA0_NTB0_NTD0_NEPBS0_NLCA1_NLCB1_ONLL1_PGR2_PLR1_PKA1_SIA3_SS1_SPO0_SRVW0_SSO0_SVW1_TLDS1_USFGROn1_VWA1_VWB1_WSGRA0_WSGRB0_WG32_8_1 ordinal(0) layout(#pipeline_layout) attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>, #hal.interface.binding<0, 2>, #hal.interface.binding<0, 3>], rocm.parameter_mapping = "c4:0:0,c4:4:4,c4:8:8,c4:12:12,c4:16:16,c4:20:20,b8:0:0:24,b8:0:1:32,b8:0:2:40,b8:0:3:48,c4:24:56,c4:28:60,c4:32:64,c4:36:68,c4:40:72,c4:44:76,c4:48:80,c4:52:84,c4:56:88,c4:60:92,c4:64:96,c4:68:100,c4:72:104,c4:76:108,c4:80:112,c4:84:116,c4:88:120,c4:92:124,c4:96:128", workgroup_size = [256 : index, 1 : index, 1 : index]} {
    ^bb0(%arg0: !hal.device, %arg1: index):
      %c4 = arith.constant 4 : index
      %c40 = arith.constant 40 : index
      %c1 = arith.constant 1 : index
      hal.return %c4, %c40, %c1 : index, index, index
    }
  }
}

{-#
  dialect_resources: {
    builtin: {
      torch_tensor_1_6_torch.int64: "0x08000000000400000000000000040000000000000000000000000000000000000000000000040000000000000004000000000000",
      torch_tensor_30_torch.int64: "0x08000000B903000000000000970300000000000097030000000000007503000000000000530300000000000031030000000000000F03000000000000ED02000000000000CB02000000000000A9020000000000008702000000000000650200000000000043020000000000002102000000000000FF01000000000000DD01000000000000BB0100000000000099010000000000007701000000000000550100000000000033010000000000001101000000000000EF00000000000000CD00000000000000AB0000000000000089000000000000006700000000000000450000000000000023000000000000000100000000000000",
      torch_tensor_1000_torch.float32: "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
    }
  }
#-}
